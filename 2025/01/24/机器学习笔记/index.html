<!DOCTYPE html><html lang="zn-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习笔记 | RXCCCCCC</title><meta name="author" content="RXCCCCCC"><meta name="copyright" content="RXCCCCCC"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习教程[TOC]  机器学习（Machine Learning）是人工智能（AI）的一个分支，它使计算机系统能够利用数据和算法自动学习和改进其性能。 机器学习是让**机器通过经验（数据）**来做决策和预测。 机器学习已经广泛应用于许多领域，包括推荐系统、图像识别、语音识别、金融分析等。 举个例子，通过机器学习，汽车可以学习如何识别交通标志、行人和障碍物，以实现自动驾驶。 机器学习与传统编程的">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记">
<meta property="og:url" content="http://rxcccccc.github.io/2025/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="RXCCCCCC">
<meta property="og:description" content="机器学习教程[TOC]  机器学习（Machine Learning）是人工智能（AI）的一个分支，它使计算机系统能够利用数据和算法自动学习和改进其性能。 机器学习是让**机器通过经验（数据）**来做决策和预测。 机器学习已经广泛应用于许多领域，包括推荐系统、图像识别、语音识别、金融分析等。 举个例子，通过机器学习，汽车可以学习如何识别交通标志、行人和障碍物，以实现自动驾驶。 机器学习与传统编程的">
<meta property="og:locale" content="zn_CN">
<meta property="og:image" content="http://rxcccccc.github.io/image/touxiang.jpg">
<meta property="article:published_time" content="2025-01-24T05:21:26.000Z">
<meta property="article:modified_time" content="2025-03-30T08:03:42.779Z">
<meta property="article:author" content="RXCCCCCC">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://rxcccccc.github.io/image/touxiang.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "机器学习笔记",
  "url": "http://rxcccccc.github.io/2025/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/",
  "image": "http://rxcccccc.github.io/image/touxiang.jpg",
  "datePublished": "2025-01-24T05:21:26.000Z",
  "dateModified": "2025-03-30T08:03:42.779Z",
  "author": [
    {
      "@type": "Person",
      "name": "RXCCCCCC",
      "url": "http://rxcccccc.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/image/touxiang.jpg"><link rel="canonical" href="http://rxcccccc.github.io/2025/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习笔记',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/image/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-gamepad"></i><span> 游戏</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/game/word-games"><i class="fa-fw fas fa-font"></i><span> wordle游戏</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 博客の时间线</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/image/preview.gif);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/image/touxiang.jpg" alt="Logo"><span class="site-name">RXCCCCCC</span></a><a class="nav-page-title" href="/"><span class="site-name">机器学习笔记</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-gamepad"></i><span> 游戏</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/game/word-games"><i class="fa-fw fas fa-font"></i><span> wordle游戏</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 博客の时间线</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">机器学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-01-24T05:21:26.000Z" title="Created 2025-01-24 13:21:26">2025-01-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-03-30T08:03:42.779Z" title="Updated 2025-03-30 16:03:42">2025-03-30</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="机器学习教程"><a href="#机器学习教程" class="headerlink" title="机器学习教程"></a>机器学习教程</h1><p>[TOC]</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Machine-learning-logo-1.webp" alt="img"></p>
<p>机器学习（Machine Learning）是人工智能（AI）的一个<strong>分支</strong>，它使计算机系统能够<strong>利用数据和算法自动学习和改进</strong>其性能。</p>
<p>机器学习是让**机器通过经验（数据）**来做决策和预测。</p>
<p>机器学习已经广泛应用于许多领域，包括推荐系统、图像识别、语音识别、金融分析等。</p>
<p>举个例子，通过机器学习，汽车可以学习如何识别交通标志、行人和障碍物，以实现自动驾驶。</p>
<h2 id="机器学习与传统编程的区别"><a href="#机器学习与传统编程的区别" class="headerlink" title="机器学习与传统编程的区别"></a>机器学习与传统编程的区别</h2><p>在传统的编程方法中，程序员会编写一系列规则或指令，告诉计算机如何执行任务。而在机器学习中，程序员并不是直接编写所有规则，而是<strong>训练计算机</strong>从数据中<strong>自动</strong>学习和推断模式。具体的差异可以总结如下：</p>
<ul>
<li><strong>传统编程：</strong> 程序员<strong>定义明确</strong>的规则和逻辑，计算机根据这些规则执行任务。</li>
<li><strong>机器学习：</strong> 计算机<strong>通过数据”学习”<strong>模式，<strong>生成模型</strong>并</strong>基于这些模式</strong>进行<strong>预测</strong>或<strong>决策</strong>。</li>
</ul>
<p>举个简单的例子，假设我们要训练一个模型来识别猫和狗的图片。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/68747470733a2f2f6d69726f25674a6b50587563386f672e676966.gif" alt="img"></p>
<p>在传统编程中，程序员需要手动定义哪些特征可以区分猫和狗（如耳朵形状、鼻子形状等），而在机器学习中，程序员只需要提供大量带标签的图片数据，计算机会<strong>自动学习</strong>如何区分猫和狗。</p>
<hr>
<h2 id="常见机器学习任务"><a href="#常见机器学习任务" class="headerlink" title="常见机器学习任务"></a>常见机器学习任务</h2><ul>
<li><strong>回归问题</strong>：<strong>预测连续</strong>值，例如房价预测。</li>
<li><strong>分类问题</strong>：将样本<strong>分为不同类别</strong>，例如垃圾邮件检测。</li>
<li><strong>聚类问题</strong>：将数据<strong>自动分组</strong>，例如客户细分。</li>
<li><strong>降维问题</strong>：将数据<strong>降到低维</strong>度，例如主成分分析（PCA）。</li>
</ul>
<hr>
<h2 id="机器学习常见算法"><a href="#机器学习常见算法" class="headerlink" title="机器学习常见算法"></a>机器学习常见算法</h2><p><strong>监督学习：</strong></p>
<ul>
<li><strong>线性</strong>回归（Linear Regression）</li>
<li><strong>逻辑</strong>回归（Logistic Regression）</li>
<li>支持向量机（SVM）</li>
<li>K-近邻算法（KNN）</li>
<li>决策树（Decision Tree）</li>
<li>随机森林（Random Forest）</li>
</ul>
<p><strong>无监督学习：</strong></p>
<ul>
<li>K-均值聚类（K-Means Clustering）</li>
<li>主成分分析（PCA）</li>
</ul>
<p><strong>深度学习：</strong></p>
<ul>
<li>神经网络（Neural Networks）</li>
<li>卷积神经网络（CNN）</li>
<li>循环神经网络（RNN）</li>
</ul>
<h1 id="机器学习简介"><a href="#机器学习简介" class="headerlink" title="机器学习简介"></a>机器学习简介</h1><h2 id="机器学习是如何工作的？"><a href="#机器学习是如何工作的？" class="headerlink" title="机器学习是如何工作的？"></a>机器学习是如何工作的？</h2><p>机器学习通过让计算机<strong>从大量数据</strong>中学习模式和规律来做出决策和预测。</p>
<ul>
<li>首先，<strong>收集并准备数据</strong>，然后选择一个<strong>合适的算法</strong>来训练模型。</li>
<li>然后，模型通过<strong>不断优化参数</strong>，<strong>最小化预测错误</strong>，直到能准确地对新数据进行预测。</li>
<li>最后，模型<strong>部署</strong>到实际应用中，<strong>实时</strong>做出预测或决策，并根据新的数据进行更新。</li>
</ul>
<p>机器学习是一个<strong>迭代</strong>过程，可能需要<strong>多次调整</strong>模型参数和特征选择，以提高模型的性能。</p>
<p>下面这张图展示了机器学习的基本流程：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/how-does-machine-learning-work.png" alt="img"></p>
<ol>
<li><strong>Labeled Data（标记数据）：</strong>：图中蓝色区域显示了标记数据，这些数据包括了不同的几何形状（如六边形、正方形、三角形）。</li>
<li><strong>Model Training（模型训练）：</strong>：在这个阶段，机器学习<strong>算法分析</strong>数据的特征，并学习如何根据这些特征来预测标签。</li>
<li><strong>Test Data（测试数据）：</strong>：图中深绿色区域显示了测试数据，包括一个正方形和一个三角形。</li>
<li><strong>Prediction（预测）：</strong>：模型使用从训练数据中学到的规则来预测测试数据的标签。在图中，模型预测了测试数据中的正方形和三角形。</li>
<li><strong>Evaluation（评估）：</strong>：预测结果与测试数据的真实标签进行<strong>比较</strong>，以评估模型的准确性。</li>
</ol>
<p>机器学习的工作流程可以大致分为以下几个步骤：</p>
<h3 id="1-数据收集"><a href="#1-数据收集" class="headerlink" title="1. 数据收集"></a>1. 数据收集</h3><ul>
<li><strong>收集数据</strong>：这是机器学习项目的第一步，涉及收集相关数据。数据可以来自数据库、文件、网络或实时数据流。</li>
<li><strong>数据类型</strong>：可以是结构化数据（如表格数据）或非结构化数据（如文本、图像、视频）。</li>
</ul>
<h3 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h3><ul>
<li><strong>清洗数据</strong>：处理缺失值、异常值、错误和重复数据。</li>
<li><strong>特征工程</strong>：<strong>选择</strong>有助于模型学习的最相关特征，可能包括创建新特征或转换现有特征。</li>
<li><strong>数据标准化&#x2F;归一化</strong>：调整数据的尺度，使其在<strong>同一范围</strong>内，有助于某些算法的性能。</li>
</ul>
<h3 id="3-选择模型"><a href="#3-选择模型" class="headerlink" title="3. 选择模型"></a>3. 选择模型</h3><ul>
<li><strong>确定问题类型</strong>：根据<strong>问题的性质</strong>（分类、回归、聚类等）<strong>选择合适</strong>的机器学习<strong>模型</strong>。</li>
<li><strong>选择算法</strong>：基于问题类型和数据特性，选择<strong>一个或多个算法</strong>进行实验。</li>
</ul>
<h3 id="4-训练模型"><a href="#4-训练模型" class="headerlink" title="4. 训练模型"></a>4. 训练模型</h3><ul>
<li><strong>划分数据集</strong>：将数据分为<strong>训练</strong>集、<strong>验证</strong>集和<strong>测试</strong>集。</li>
<li><strong>训练</strong>：使用训练集上的数据来训练模型，调整模型参数以<strong>最小化损失函数</strong>。</li>
<li><strong>验证</strong>：使用验证集来调整模型参数，<strong>防止过拟合</strong>。</li>
</ul>
<h3 id="5-评估模型"><a href="#5-评估模型" class="headerlink" title="5. 评估模型"></a>5. 评估模型</h3><ul>
<li><strong>性能指标</strong>：使用<strong>测试集来评估</strong>模型的性能，常用的指标包括<strong>准确</strong>率、<strong>召回</strong>率、<strong>F1分数</strong>等。</li>
<li><strong>交叉验证</strong>：一种评估模型泛化能力的技术，通过将数据<strong>分成多个子集</strong>进行训练和验证。</li>
</ul>
<h3 id="6-模型优化"><a href="#6-模型优化" class="headerlink" title="6. 模型优化"></a>6. 模型优化</h3><ul>
<li><strong>调整超参数</strong>：超参数是<strong>学习过程之前设置的</strong>参数，如学习率、树的深度等，可以通过网格搜索、随机搜索或贝叶斯优化等方法来调整。</li>
<li><strong>特征选择</strong>：可能需要<strong>重新</strong>评估和选择特征，以提高模型性能。</li>
</ul>
<h3 id="7-部署模型"><a href="#7-部署模型" class="headerlink" title="7. 部署模型"></a>7. 部署模型</h3><ul>
<li><strong>集成到应用</strong>：将训练好的模型集成到实际应用中，如网站、移动应用或软件中。</li>
<li><strong>监控和维护</strong>：持续监控模型的性能，并根据新数据更新模型。</li>
</ul>
<h3 id="8-反馈循环"><a href="#8-反馈循环" class="headerlink" title="8. 反馈循环"></a>8. 反馈循环</h3><ul>
<li><strong>持续学习</strong>：机器学习模型可以设计为随着时间的推移自动从新数据中学习，以适应变化。</li>
</ul>
<h3 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h3><ul>
<li><strong>损失函数</strong>：一个衡量模型<strong>预测与实际结果差异</strong>的函数，模型训练的<strong>目标是最小化</strong>这个函数。</li>
<li><strong>优化算法</strong>：如梯度下降，用于找到最小化损失函数的参数值。</li>
<li><strong>正则化</strong>：一种技术，通过<strong>添加惩罚项</strong>来防止模型过拟合。</li>
</ul>
<p>机器学习的工作流程是迭代的，可能需要多次调整和优化以达到最佳性能。此外，随着数据的积累和算法的发展，机器学习模型可以变得更加精确和高效。</p>
<h2 id="机器学习的类型"><a href="#机器学习的类型" class="headerlink" title="机器学习的类型"></a>机器学习的类型</h2><p>机器学习主要分为以下三种类型：</p>
<h3 id="1-监督学习（Supervised-Learning）"><a href="#1-监督学习（Supervised-Learning）" class="headerlink" title="1. 监督学习（Supervised Learning）"></a>1. <strong>监督学习（Supervised Learning）</strong></h3><ul>
<li><strong>定义：</strong> 监督学习是指使用<strong>带标签的数据</strong>进行训练，模型通过学习输入数据与标签之间的<strong>关系</strong>，来做出预测或分类。</li>
<li><strong>应用：</strong> <strong>分类</strong>（如垃圾邮件识别）、<strong>回归</strong>（如房价预测）。</li>
<li><strong>例子：</strong> 线性回归、决策树、支持向量机（SVM）。</li>
</ul>
<h3 id="2-无监督学习（Unsupervised-Learning）"><a href="#2-无监督学习（Unsupervised-Learning）" class="headerlink" title="2. 无监督学习（Unsupervised Learning）"></a>2. <strong>无监督学习（Unsupervised Learning）</strong></h3><ul>
<li><strong>定义：</strong> 无监督学习使用<strong>没有标签</strong>的数据，模型试图在数据中发现潜在的结构或模式。</li>
<li><strong>应用：</strong> <strong>聚类</strong>（如客户分群）、<strong>降维</strong>（如数据可视化）。</li>
<li><strong>例子：</strong> K-means 聚类、主成分分析（PCA）。</li>
</ul>
<h3 id="3-强化学习（Reinforcement-Learning）"><a href="#3-强化学习（Reinforcement-Learning）" class="headerlink" title="3. 强化学习（Reinforcement Learning）"></a>3. <strong>强化学习（Reinforcement Learning）</strong></h3><ul>
<li><strong>定义：</strong> 强化学习通过与环境互动，智能体<strong>在试错中学习</strong>最佳策略，以最大化长期回报。每次行动后，系统会<strong>收到奖励或惩罚</strong>，来指导行为的改进。</li>
<li><strong>应用：</strong> 游戏AI（如AlphaGo）、自动驾驶、机器人控制。</li>
<li><strong>例子：</strong> Q-learning、深度Q网络（DQN）。</li>
</ul>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/The-main-types-of-machine-learning-Main-approaches-include-classification-and-regression.png" alt="img"></p>
<p>这三种机器学习类型各有其应用场景和优势，监督学习适用于<strong>有明确标签的数据</strong>，无监督学习适用于<strong>探索数据内在结构</strong>，而强化学习适用于需要通过试错来学习最优策略的场景。</p>
<h2 id="机器学习的应用领域"><a href="#机器学习的应用领域" class="headerlink" title="机器学习的应用领域"></a>机器学习的应用领域</h2><ul>
<li><strong>推荐系统：</strong> 例如，抖音推荐你可能感兴趣的视频，淘宝推荐你可能会购买的商品，网易云音乐推荐你喜欢的音乐。</li>
<li><strong>自然语言处理（NLP）：</strong> 机器学习在语音识别、机器翻译、情感分析、聊天机器人等方面的应用。例如，Google 翻译、Siri 和智能客服等。</li>
<li><strong>计算机视觉：</strong> 机器学习在图像识别、物体检测、面部识别、自动驾驶等领域有广泛应用。例如，自动驾驶汽车通过摄像头和传感器识别周围的障碍物，识别行人和其他车辆。</li>
<li><strong>金融分析：</strong> 机器学习在股市预测、信用评分、欺诈检测等金融领域具有重要应用。例如，银行利用机器学习检测信用卡交易中的欺诈行为。</li>
<li><strong>医疗健康：</strong> 机器学习帮助医生诊断疾病、发现药物副作用、预测病情发展等。例如，IBM 的 Watson 系统帮助医生分析患者的病历数据，提供诊断和治疗建议。</li>
<li><strong>游戏和娱乐：</strong> 机器学习不仅用于游戏中的智能对手，还应用于游戏设计、动态难度调整等方面。例如，AlphaGo 使用深度学习技术战胜了围棋世界冠军。</li>
</ul>
<h2 id="机器学习的未来"><a href="#机器学习的未来" class="headerlink" title="机器学习的未来"></a>机器学习的未来</h2><p>随着数据量的爆炸式增长和计算能力的提升，机器学习的应用将继续扩展，带来更加智能和高效的系统。例如：</p>
<ul>
<li><strong>强化学习：</strong> 使计算机能够在没有明确指导的情况下通过试错来解决复杂问题。例如，AlphaGo 和 OpenAI 的 Dota 2 游戏 AI 都使用了强化学习。</li>
<li><strong>自监督学习：</strong> 目前的机器学习模型通常<strong>需要大量带标签</strong>的数据来进行训练，而自监督学习则能够在没有标签的数据下学习更有效的表示。</li>
<li><strong>深度学习：</strong> <strong>深度</strong>学习是机器学习中的一个分支，主要关注<strong>神经网络的应用</strong>，它已经在图像识别、自然语言处理等方面取得了突破性进展。未来，深度学习将继续推动人工智能的发展。</li>
</ul>
<h1 id="机器学习如何工作"><a href="#机器学习如何工作" class="headerlink" title="机器学习如何工作"></a>机器学习如何工作</h1><p>机器学习（Machine Learning, ML）的核心思想是让计算机能够通过<strong>数据学习</strong>，并从中推断出规律或模式，而不依赖于显式编写的规则或代码。</p>
<p>简单来说，机器学习的工作流程是让机器<strong>通过历史数据自动改进</strong>其决策和预测能力。</p>
<p>机器学习的工作流程可以简化为以下几个步骤：</p>
<ol>
<li><strong>收集数据</strong>：准备包含特征和标签的数据。</li>
<li><strong>选择模型</strong>：根据任务选择合适的机器学习算法。</li>
<li><strong>训练模型</strong>：让模型通过数据学习模式，最小化误差。</li>
<li><strong>评估与验证</strong>：通过测试集评估模型性能，并进行优化。</li>
<li><strong>部署模型</strong>：将训练好的模型应用到实际场景中进行预测。</li>
<li><strong>持续改进</strong>：随着新数据的产生，模型需要定期更新和优化。</li>
</ol>
<p>这个过程能够让计算机从经验中自动学习，并在各种任务中做出越来越准确的预测。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/machine-learning-how-machine-learning-work.png" alt="img"></p>
<p>我们可以从以下几个方面来理解机器学习是如何工作的：</p>
<h3 id="1-数据输入：数据是学习的基础"><a href="#1-数据输入：数据是学习的基础" class="headerlink" title="1. 数据输入：数据是学习的基础"></a>1. 数据输入：数据是学习的基础</h3><p>机器学习的<strong>第一步是数据收集</strong>。没有数据，机器学习模型无法进行训练。数据通常包括”输入<strong>特征</strong>“和”<strong>标签</strong>“：</p>
<ul>
<li><strong>输入特征（Features）：</strong> 这些是模型用来做预测或分类的信息。例如，在房价预测问题中，输入特征可以是房子的面积、地理位置、卧室数量等。</li>
<li><strong>标签（Labels）：</strong> 标签是我们<strong>想要预测或分类的结果</strong>，通常是一个数字或类别。例如，在房价预测问题中，标签是房子的价格。</li>
</ul>
<p>机器学习模型的目标是从数据中找出输入特征与标签之间的关系，基于这些关系做出预测。</p>
<h3 id="2-模型选择：选择合适的学习算法"><a href="#2-模型选择：选择合适的学习算法" class="headerlink" title="2. 模型选择：选择合适的学习算法"></a>2. 模型选择：选择合适的学习算法</h3><p>机器学习<strong>模型</strong>（<strong>也叫做算法</strong>）是帮助计算机学习数据并进行预测的工具。根据数据的性质和任务的不同，常见的机器学习模型包括：</p>
<ul>
<li><strong>监督学习模型：</strong> 给定带有标签的数据，模型通过学习输入和标签之间的关系来做预测。例如，<strong>线性回归</strong>、<strong>逻辑回归</strong>、<strong>支持向量机（SVM）</strong> 和 <strong>决策树</strong>。</li>
<li><strong>无监督学习模型：</strong> 没有标签的数据，模型通过探索数据中的结构或模式来进行学习。例如，<strong>K-means 聚类</strong>、<strong>主成分分析（PCA）</strong>。</li>
<li><strong>强化学习模型：</strong> 模型在与环境互动的过程中，通过奖励和惩罚来学习最佳行为。例如，<strong>Q-learning</strong>、<strong>深度强化学习</strong>（Deep Q-Networks, DQN）。</li>
</ul>
<h3 id="3-训练过程：让模型从数据中学习"><a href="#3-训练过程：让模型从数据中学习" class="headerlink" title="3. 训练过程：让模型从数据中学习"></a>3. 训练过程：让模型从数据中学习</h3><p>在训练阶段，模型通过历史数据”学习”输入和标签之间的关系，通常通过最小化一个损失函数（Loss Function）来优化模型的参数。训练过程可以概括为以下步骤：</p>
<ul>
<li><strong>初始状态：</strong> 模型从<strong>随机值开始</strong>。比如，神经网络的权重是随机初始化的。</li>
<li><strong>计算预测：</strong> 对于每个输入，模型会做出一个预测。这是通过将输入数据传递给模型，计算得到输出。</li>
<li><strong>计算误差（损失）：</strong> 误差是指模型预测的输出与实际标签之间的差异。例如，对于回归问题，误差可以通过均方误差（MSE）来衡量。</li>
<li><strong>优化模型：</strong> 通过反向传播（在神经网络中）或梯度下降等优化算法，<strong>不断调整模型的参数</strong>（如神经网络的权重），使得误差最小化。这个过程就是<strong>训练</strong>，直到模型能够在训练数据上做出比较准确的预测。</li>
</ul>
<h3 id="4-验证与评估：测试模型的性能"><a href="#4-验证与评估：测试模型的性能" class="headerlink" title="4. 验证与评估：测试模型的性能"></a>4. 验证与评估：测试模型的性能</h3><p>训练过程完成后，我们需要<strong>评估</strong>模型的性能。为了<strong>避免模型过度拟合</strong>训练数据，我们将数据分为<strong>训练集</strong>和<strong>测试集</strong>，其中：</p>
<ul>
<li><strong>训练集：</strong> 用于训练模型的部分数据。</li>
<li><strong>测试集：</strong> 用于评估模型性能的部分数据，通常不参与训练过程。</li>
</ul>
<p>常见的评估指标包括：</p>
<ul>
<li><strong>准确率（Accuracy）：</strong> 分类问题中正确分类的比例。</li>
<li><strong>均方误差（MSE）：</strong> <strong>回归</strong>问题中，预测值与真实值差的<strong>平方的平均值</strong>。</li>
<li><strong>精确率（Precision）与召回率（Recall）：</strong> 用于二分类问题，尤其是<strong>类别不平衡</strong>时。</li>
<li><strong>F1分数：</strong> <strong>精确率</strong>与<strong>召回率</strong>的<strong>调和平均数</strong>，综合考虑分类器的表现。</li>
</ul>
<h3 id="5-优化与调整：提高模型的精度"><a href="#5-优化与调整：提高模型的精度" class="headerlink" title="5. 优化与调整：提高模型的精度"></a>5. 优化与调整：提高模型的精度</h3><p>如果模型在测试集上的表现不理想，可能需要进一步优化。这通常包括：</p>
<ul>
<li><strong>调整超参数（Hyperparameters）：</strong> 比如<strong>学习率</strong>、<strong>正则化系数</strong>、<strong>树的深度</strong>等。这些超参数影响模型的学习能力。</li>
<li><strong>模型选择与融合：</strong> 尝试不同的模型或模型融合（比如集成学习方法，如随机森林、XGBoost 等）来提高精度。</li>
<li><strong>数据增强：</strong> <strong>扩展</strong>训练数据集，比如对图像进行旋转、翻转等操作，帮助模型提高泛化能力。</li>
</ul>
<h3 id="6-模型部署与预测：实际应用"><a href="#6-模型部署与预测：实际应用" class="headerlink" title="6. 模型部署与预测：实际应用"></a>6. 模型部署与预测：实际应用</h3><p>一旦模型在训练和测试数据上表现良好，就可以将模型部署到实际应用中：</p>
<ul>
<li><strong>模型部署：</strong> 将训练好的模型嵌入到应用程序、网站、服务器等系统中，供用户使用。</li>
<li><strong>实时预测：</strong> 在实际环境中，新的数据输入到模型中，模型根据之前学习到的模式进行实时预测或分类。</li>
</ul>
<h3 id="7-持续学习与模型更新："><a href="#7-持续学习与模型更新：" class="headerlink" title="7. 持续学习与模型更新："></a>7. 持续学习与模型更新：</h3><p>机器学习系统通常不是一次性完成的。在实际应用中，随着时间的推移，新的数据会不断产生，因此，模型需要定期更新和再训练，以保持其预测能力。这可以通过<strong>在线学习</strong>、<strong>迁移学习</strong>等方法来实现。</p>
<h1 id="机器学习基础概念"><a href="#机器学习基础概念" class="headerlink" title="机器学习基础概念"></a>机器学习基础概念</h1><p>在学习机器学习时，理解其核心基础概念至关重要。</p>
<p>这些基础概念帮助我们理解数据如何输入到模型中、模型如何学习、以及如何评估模型的表现。</p>
<p>接下来，我们将详细讲解几个机器学习中的基本概念：</p>
<ul>
<li><strong>训练集、测试集和验证集</strong>：帮助训练、评估和调优模型。</li>
<li><strong>特征与标签</strong>：特征是<strong>输入</strong>，标签是模型预测的<strong>目标</strong>。</li>
<li><strong>模型与算法</strong>：模型是<strong>通过算法训练得到</strong>的，算法帮助模型<strong>学习</strong>数据中的<strong>模式</strong>。</li>
<li><strong>监督学习、无监督学习和强化学习</strong>：三种常见的学习方式，分别用于不同的任务。</li>
<li><strong>过拟合与欠拟合</strong>：两种常见的问题，<strong>影响</strong>模型的<strong>泛化</strong>能力。</li>
<li><strong>训练误差与测试误差</strong>：反映模型是否能适应数据，并进行有效预测。</li>
<li><strong>评估指标</strong>：衡量模型好坏的标准，根据任务选择合适的指标。</li>
</ul>
<p>这些基础概念是理解和应用机器学习的基础，掌握它们是进一步学习的关键。</p>
<h3 id="训练集、测试集和验证集"><a href="#训练集、测试集和验证集" class="headerlink" title="训练集、测试集和验证集"></a>训练集、测试集和验证集</h3><ul>
<li><strong>训练集（Training Set）：</strong> 训练集是用于训练机器学习模型的数据集，它包含输入特征和对应的标签（在监督学习中）。模型通过学习训练集中的数据来调整参数，逐步提高预测的准确性。</li>
<li><strong>测试集（Test Set）：</strong> 测试集用于<strong>评估</strong>训练好的模型的<strong>性能</strong>。测试集中的数据不参与模型的训练，模型使用它来进行预测，并与真实标签进行比较，帮助我们了解模型在<strong>未见过</strong>的数据上的<strong>表现</strong>。</li>
<li><strong>验证集（Validation Set）：</strong> 验证集用于在训练过程中<strong>调整</strong>模型的<strong>超参数</strong>（如学习率、正则化参数等）。它通常被用于模型<strong>调优</strong>，帮助选择<strong>最佳的模型参数</strong>，<strong>避免过</strong>拟合。验证集的作用是对模型进行<strong>监控和调试</strong>。</li>
</ul>
<p><strong>总结：</strong></p>
<ul>
<li>训练集用于训练模型。</li>
<li>测试集用于<strong>评估</strong>模型的<strong>最终</strong>性能。</li>
<li>验证集用于模型<strong>调优</strong>。</li>
</ul>
<h3 id="特征（Features）和标签（Labels）"><a href="#特征（Features）和标签（Labels）" class="headerlink" title="特征（Features）和标签（Labels）"></a>特征（Features）和标签（Labels）</h3><ul>
<li><strong>特征（Features）：</strong> 特征是<strong>输入数据的不同属性</strong>，模型使用这些特征来做出预测或分类。例如，在房价预测中，特征可能包括房子的面积、地理位置、卧室数量等。</li>
<li><strong>标签（Labels）：</strong> 标签是机器学习任务中的<strong>目标变量</strong>，模型要预测的结果。对于监督学习任务，标签通常是已知的。例如，在房价预测中，标签就是房子的实际价格。</li>
</ul>
<p><strong>总结：</strong></p>
<ul>
<li>特征是模型输入的数据。</li>
<li>标签是模型需要预测的输出。</li>
</ul>
<h3 id="模型（Model）与算法（Algorithm）"><a href="#模型（Model）与算法（Algorithm）" class="headerlink" title="模型（Model）与算法（Algorithm）"></a>模型（Model）与算法（Algorithm）</h3><ul>
<li><strong>模型（Model）：</strong> 模型是通过学习数据中的模式而构建的<strong>数学结构</strong>。它接受输入特征，经过一系列计算和转化，输出一个预测结果。常见的模型有线性回归、决策树、神经网络等。</li>
<li><strong>算法（Algorithm）：</strong> 算法是实现机器学习的<strong>步骤或规则</strong>，它定义了模型如何从数据中学习。常见的算法有梯度下降法、随机森林、K近邻算法等。算法帮助模型调整其参数以最小化预测误差。</li>
</ul>
<p><strong>总结：</strong></p>
<ul>
<li>模型是<strong>学习到的结果</strong>，它可以用来进行预测。</li>
<li>算法是<strong>训练模型的过程</strong>，帮助模型从数据中学习。</li>
</ul>
<h3 id="监督学习、无监督学习和强化学习"><a href="#监督学习、无监督学习和强化学习" class="headerlink" title="监督学习、无监督学习和强化学习"></a>监督学习、无监督学习和强化学习</h3><ul>
<li><strong>监督学习（Supervised Learning）：</strong> 在监督学习中，训练数据包含<strong>已知的标签</strong>。模型通过学习输入特征与标签之间的关系来进行预测或分类。监督学习的目标是最小化预测错误，使模型能够在新数据上做出准确的预测。<ul>
<li><strong>例子：</strong> 线性回归、逻辑回归、支持向量机（SVM）、决策树。</li>
</ul>
</li>
<li><strong>无监督学习（Unsupervised Learning）：</strong> 无监督学习中，训练数据没有标签，模型通过分析输入数据中的结构或模式来进行学习。目标是发现数据的<strong>潜在规律</strong>，常见的任务包括聚类、降维等。<ul>
<li><strong>例子：</strong> K-means 聚类、主成分分析（PCA）。</li>
</ul>
</li>
<li><strong>强化学习（Reinforcement Learning）：</strong> 强化学习是让智能体（Agent）通过与环境（Environment）的互动，采取行动并<strong>根据奖励或惩罚来学习最优</strong>策略。智能体的目标是通过最大化长期奖励来优化行为。<ul>
<li><strong>例子：</strong> AlphaGo、自动驾驶、游戏AI。</li>
</ul>
</li>
</ul>
<p><strong>总结：</strong></p>
<ul>
<li>监督学习：有标签的训练数据，任务是预测或分类。</li>
<li>无监督学习：没有标签的训练数据，任务是发现数据中的模式或结构。</li>
<li>强化学习：通过与环境互动，智能体根据奖励和惩罚进行学习。</li>
</ul>
<h3 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h3><ul>
<li><strong>过拟合（Overfitting）：</strong> 过拟合是指模型<strong>在训练数据上</strong>表现非常好，但<strong>在测试数据上</strong>表现很差。这通常发生在模型<strong>复杂度过高</strong>、<strong>参数过多</strong>，导致模型”记住”了<strong>训练数据中的</strong>噪声或偶然性，而<strong>不具备泛化</strong>能力。过拟合的模型<strong>无法有效应对新</strong>数据。</li>
<li><strong>欠拟合（Underfitting）：</strong> 欠拟合是指模型在训练数据上和测试数据上<strong>都表现不佳</strong>，通常是因为模型<strong>过于简单</strong>，无法捕捉数据中的复杂模式。欠拟合的模型无法从数据中学习到有用的规律。</li>
</ul>
<p><strong>解决方法：</strong></p>
<ul>
<li>过拟合：可以通过<strong>简化</strong>模型、<strong>增加</strong>训练数据或使用<strong>正则化</strong>等方法来缓解。</li>
<li>欠拟合：可以通过增加模型复杂度或使用更复杂的算法来改进。</li>
</ul>
<h3 id="训练与测试误差"><a href="#训练与测试误差" class="headerlink" title="训练与测试误差"></a>训练与测试误差</h3><ul>
<li><strong>训练误差（Training Error）：</strong> 训练误差是模型在训练数据上的表现，反映了模型是否能够很好地适应训练数据。如果训练误差很大，可能说明模型不够复杂，<strong>欠</strong>拟合；如果训练误差很小，可能说明模型太复杂，容易<strong>过</strong>拟合。</li>
<li><strong>测试误差（Test Error）：</strong> 测试误差是模型在未见过的数据上的表现，反映了模型的泛化能力。测试误差应当与训练误差相匹配，若测试误差<strong>远高</strong>于训练误差，通常是<strong>过</strong>拟合。</li>
</ul>
<p><strong>总结：</strong></p>
<ul>
<li>训练误差和测试误差的差距可以帮助我们判断模型的适应性。</li>
<li>理想的情况是训练误差和测试误差<strong>都较小</strong>，并且<strong>相对接近</strong>。</li>
</ul>
<h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>根据任务的不同，机器学习模型的评估指标也不同。以下是常用的一些评估指标：</p>
<ul>
<li><strong>准确率（Accuracy）：</strong> <strong>分类</strong>任务中，<strong>正确</strong>分类的样本占总样本的比例。</li>
<li><strong>精确率（Precision）和召回率（Recall）：</strong> 主要用于处理<strong>不平衡</strong>数据集，精确率衡量的是被模型预<strong>测为正类</strong>的样本中，有多少是<strong>真正的正类</strong>；召回率衡量的是<strong>所有实际</strong>正类<strong>中</strong>，有多少被模型<strong>正确识别</strong>为正类。</li>
<li><strong>F1 分数：</strong> 精确率与召回率的<strong>调和</strong>平均数，用于综合考虑模型的表现。</li>
<li><strong>均方误差（MSE）：</strong> 回归任务中，预测值与真实值之间差异的平方的平均值。</li>
</ul>
<p><strong>总结：</strong><br>评估指标帮助我们衡量模型的表现，选择最合适的指标可以根据任务的需求来进行。</p>
<h1 id="深度学习-Deep-Learning-入门——基本概念"><a href="#深度学习-Deep-Learning-入门——基本概念" class="headerlink" title="深度学习(Deep Learning)入门——基本概念"></a>深度学习(Deep Learning)入门——基本概念</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a><strong>引言</strong></h2><p>本文是该系列文章中的第一篇，旨在介绍深度学习基础概念、<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95&zhida_source=entity">优化算法</a>、 调参基本思路、正则化方式等，后续文章将关注深度学习在<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&zhida_source=entity">自然语言处理</a>、语音识别、和计算机视觉领域的应用。</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a><strong>基本概念</strong></h2><p>深度学习是为了解决表示学习难题而被提出的。本节，我们介绍这些深度学习相关的基本概念。</p>
<p><strong>表示学习（representation learning）</strong> 机器学习旨在自动地学到从数据的<strong>表示</strong>（representation）到数据的<strong>标记</strong>（label）的<strong>映射</strong>。随着机器学习算法的日趋成熟，人们发现，在某些领域（如图像、语音、文本等），如何从数据中<strong>提取合适的表示</strong>成为整个任务的瓶颈所在，而数据表示的好坏直接影响后续学习任务（所谓garbage in，garbage out）。与其依赖人类专家<strong>设计手工特征</strong>（难设计还不见得好用），表示学习希望能从数据中<strong>自动地</strong>学到从数据的原始形式到数据的表示之间的映射。</p>
<p><strong>深度学习（deep learning，DL）</strong> 表示学习的理想很丰满，但实际中人们发现从数据的原始形式<strong>直接学得</strong>数据表示这件事<strong>很难</strong>。深度学习是<strong>目前最成功的表示学习方法</strong>，因此，目前国际表示学习大会（ICLR）的绝大部分论文都是关于深度学习的。深度学习是把表示学习的任务划分成几个小目标，先从数据的原始形式中先学习<strong>比较低级</strong>的表示，再<strong>从低级</strong>表示<strong>学得比较高级</strong>的表示。这样，每个小目标比较容易达到，综合起来我们就完成表示学习的任务。这类似于算法设计思想中的<strong>分治法</strong>（<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=divide-and-conquer&zhida_source=entity">divide-and-conquer</a>）。</p>
<p><strong>深度神经网络（deep neural networks，DNN）</strong> 深度学习<strong>目前几乎唯一行之有效</strong>的实现形式。简单的说，深度神经网络就是<strong>很深的</strong>神经网络。我们利用<strong>网络中逐层对特征进行加工</strong>的特性，<strong>逐渐从低级特征提取高级</strong>特征。除了深度神经网络之外，有学者在探索其他深度学习的实现形式，比如深度森林。</p>
<p>深度神经网络目前的成功取决于三大推动因素。1. <strong>大数据</strong>。当数据量小时，很难从数据中学得合适的表示，而传统算法+<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B&zhida_source=entity">特征工程</a>往往能取得很好的效果；2. <strong>计算能力</strong>。<strong>大的数据</strong>和<strong>大的网络</strong>需要有足够的快的计算能力才能使得模型的应用成为可能。3. <strong>算法创新</strong>。现在很多<strong>算法设计</strong>关注在如何使网络更好地训练、更快地运行、取得更好的性能。</p>
<p><strong>多层感知机（multi-layer perceptrons，MLP）</strong> <strong>多层</strong>由<strong>全连接层</strong>组成的深度神经网络。<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA&zhida_source=entity">多层感知机</a>的<strong>最后一层</strong>全连接层实质上是一个<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8&zhida_source=entity">线性分类器</a>，而其他部分则是为这个线性分类器<strong>学习一个合适的数据表示</strong>，使倒数第二层的特征<strong>线性可分</strong>。</p>
<p>**激活函数（activation function）**神经网络的必要组成部分。如果没有激活函数，多次线性运算的堆叠仍然是一个线性运算，即不管用再多层实质只起到了一层神经网络的作用。一个好的激活函数应满足以下性质。1. <strong>不会饱和</strong>。sigmoid和tanh激活函数在两侧尾端会有饱和现象，这会使导数在这些区域接近零，从而阻碍网络的训练。2. <strong>零均值</strong>。ReLU<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=5&q=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&zhida_source=entity">激活函数</a>的输出均值不为零，这会影响网络的训练。3. <strong>容易计算</strong>。</p>
<p><strong>迁移学习（transfer learning）</strong> 深度学习下的迁移学习旨在<strong>利用源任务数据辅助</strong>目标任务数据下的学习。迁移学习适用于源任务数据比目标任务数据多，并且源任务中学习得到的低层特征可以帮助目标任务的学习的情形。在<strong>计算机视觉</strong>领域，最常用的<strong>源任务数据是ImageNet</strong>。对ImageNet预训练模型的<strong>利用</strong>通常有两种方式。1. <strong>固定特征提取器</strong>。用ImageNett预训练模型提取目标任务数据的高层特征。2. <strong>微调（fine-tuning）</strong>。以ImageNet预训练模型作为目标任务模型的<strong>初始化初始化权值</strong>，之后<strong>在目标任务数据上进行微调</strong>。</p>
<p><strong>多任务学习（multi-task learning）</strong> 与其针对每个任务训练一个小网络，深度学习下的多任务学习旨在训<strong>练一个大网络以同时完成全部任务</strong>。这些任务中用于提取<strong>低层特征</strong>的层是<strong>共享</strong>的，之后产生分支，各任务拥有各自的若干层用于完成其任务。多任务学习适用于多个任务共享低层特征，并且各个任务的数据很相似的情况。</p>
<p><strong>端到端学习（end-to-end learning）</strong> 深度学习下的端到端学习旨在通过一个深度神经网络<strong>直接学习从数据的原始形式到数据的标记的映射</strong>。端到端学习并不应该作为我们的一个追求目标，是否要采用端到端学习的一个重要考虑因素是：有没有足够的数据对应端到端的过程，以及我们有没有一些领域知识能够用于整个系统中的一些模块。</p>
<h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a><strong>优化算法</strong></h2><p>在<strong>网络结构</strong>确定之后，我们需要对网络的<strong>权值</strong>（weights）进行优化。本节，我们介绍优化深度神经网络的基本思想。</p>
<p><strong>梯度下降（<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=gradient+descent&zhida_source=entity">gradient descent</a>，GD）</strong> 想象你去野足但却迷了路，在漆黑的深夜你一个人被困住山谷中，你<strong>知道谷底是出口</strong>但是天太黑了根本看不清楚路。于是你确定采取一个<strong>贪心</strong>(greedy)算法：先试探在当前位置往<strong>哪个方向走下降最快（即梯度方向）</strong>，再朝着这个方向走<strong>一小步</strong>，重复这个过程直到你到达谷底。这就是梯度下降的基本思想。</p>
<p><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95&zhida_source=entity">梯度下降算法</a>的性能大致取决于三个因素。1. <strong>初始位置</strong>。如果你初始位置就离谷底很近，自然很容易走到谷底。2. <strong>山谷地形</strong>。如果山谷是“九曲十八弯”，很有可能你在里面绕半天都绕不出来。3. <strong>步长</strong>。你每步迈多大，当你步子迈太小，很可能你走半天也没走多远，而当你步子迈太大，一不小心就容易撞到旁边的悬崖峭壁，或者错过了谷底。</p>
<p><strong>误差反向传播（error back-propagation，BP）</strong> 结合微积分中<strong>链式法则</strong>和算法设计中<strong>动态规划思想</strong>用于计算梯度。 直接用纸笔推导出中间某一层的梯度的数学表达式是很困难的，但<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99&zhida_source=entity">链式法则</a>告诉我们，一旦我们知道<strong>后一层的梯度</strong>，再结合后一层<strong>对当前层的导数</strong>，我们就可以得到<strong>当前层的梯度</strong>。<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92&zhida_source=entity">动态规划</a>是一个<strong>高效计算所有梯度</strong>的实现技巧，通过由<strong>高层往低层逐层计算梯度</strong>，避免了对高层梯度的重复计算。</p>
<p><strong>滑动平均（moving average）</strong> 要前进的方向<strong>不再由当前梯度方向完全</strong>决定，而是最近<strong>几次梯度方向的滑动平均</strong>。利用滑动平均思想的优化算法有带动量（momentum）的SGD、<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=Nesterov%E5%8A%A8%E9%87%8F&zhida_source=entity">Nesterov动量</a>、Adam（ADAptive Momentum estimation）等。</p>
<p><strong><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E8%87%AA%E9%80%82%E5%BA%94%E6%AD%A5%E9%95%BF&zhida_source=entity">自适应步长</a></strong> 自适应地确定权值<strong>每一维的步长</strong>。当某一维持续<strong>震荡</strong>时，我们希望这一维的步长小一些；当某一维<strong>一直沿着相同</strong>的方向前进时，我们希望这一维的步长大一些。利用自适应步长思想的优化算法有AdaGrad、RMSProp、Adam等。</p>
<p><strong>学习率衰减</strong> 当开始训练时，<strong>较大</strong>的学习率可以使你在参数空间有<strong>更大范围的探索</strong>；当优化接近<strong>收敛</strong>时，我们需要<strong>小一些的学习率</strong>使权值更接近<strong>局部最优</strong>点。</p>
<p><strong>深度神经网络优化的困难</strong> 有学者指出，在很高维的空间中，局部最优是比较少的，而大部分梯度为零的点是<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E9%9E%8D%E7%82%B9&zhida_source=entity">鞍点</a>。平原区域的鞍点会使梯度在<strong>很长一段时间内都接近零</strong>，这会使得拖慢优化过程。</p>
<h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a><strong>初始化</strong></h2><p><strong>权值初始化对网络优化至关重要</strong>。早年深度神经网络无法有效训练的一个重要原因就是早期人们对初始化不太重视。本节，我们介绍几个适用于深度神经网络的初始化方法。</p>
<p><strong>初始化的基本思想</strong> <strong>方差不变</strong>，即设法对权值进行初始化，使得各层神经元的<strong>方差保持不变</strong>。</p>
<p><strong>Xavier初始化</strong> 从高斯分布或<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83&zhida_source=entity">均匀分布</a>中对权值进行采样，使得<strong>权值的方差是1&#x2F;n</strong>，其中n是输入神经元的个数。该推导假设<strong>激活函数是线性</strong>的。</p>
<p><strong>He初始化&#x2F;MSRA初始化</strong> 从高斯分布或均匀分布中对权值进行采样，使得权值的<strong>方差是2&#x2F;n</strong>。该推导假设<strong>激活函数是ReLU</strong>。因为ReLU<strong>会将小于0的神经元置零</strong>，大致上会使一半的神经元置零，所以为了弥补丢失的这部分信息，<strong>方差要乘以2</strong>。</p>
<p><strong>批量规范化（batch-normalization，BN）</strong> 每层<strong>显式</strong>地对神经元的激活值做<strong>规范化</strong>，使其具有零均值和单位方差。批量规范化使<strong>激活值的分布固定</strong>下来，这样可以使各层<strong>更加独立地进行学习</strong>。批量规范化可以使得网络<strong>对初始化和学习率不太敏感</strong>。此外，批量规范化<strong>有些许正则化</strong>的作用，但不要用其作为正则化手段。</p>
<h2 id="偏差-方差（bias-variance）"><a href="#偏差-方差（bias-variance）" class="headerlink" title="偏差&#x2F;方差（bias&#x2F;variance）"></a><strong>偏差&#x2F;方差（bias&#x2F;variance）</strong></h2><p>优化完成后，你发现网络的表现不尽如人意，这时诊断网络处于高偏差&#x2F;<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E9%AB%98%E6%96%B9%E5%B7%AE&zhida_source=entity">高方差</a>状态是对你下一步<strong>调参方向的重要指导</strong>。与经典机器学习算法有所不同，因为深度神经网络通常要<strong>处理非常高维</strong>的特征，所以网络可能<strong>同时处于高偏差&#x2F;高方差</strong>的状态，即在<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4&zhida_source=entity">特征空间</a>的一些区域网络处于高偏差，而在另一些区域处于高方差。本节，我们对偏差&#x2F;方差作一简要介绍。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/v2-2d443d459279b52f130fa3096c7e2673_1440w.jpg" alt="img"></p>
<p><strong>偏差</strong> 偏差度量了网络的<strong>训练集误差</strong>和<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%AF%AF%E5%B7%AE&zhida_source=entity"><strong>贝叶斯误差</strong></a>（即能达到的<strong>最优误差</strong>）的差距。高偏差的网络有很高的训练集误差，说明网络对数据中隐含的一般规律还没有学好。当网络处于高偏差时，通常有以下几种解决方案。<strong>1. 训练更大的网络</strong>。网络<strong>越大</strong>，对数据<strong>潜在</strong>规律的<strong>拟合能力越强</strong>。<strong>2. 更多的训练轮数</strong>。通常训练<strong>时间越久</strong>，对训练集的<strong>拟合能力越强</strong>。<strong>3. 改变网络结构</strong>。不同的网络<strong>结构</strong>对训练集的<strong>拟合能力</strong>有所不同。</p>
<p><strong>方差</strong> 方差度量了网络的<strong>验证集误差</strong>和<strong>训练集误差</strong>的差距。<strong>高</strong>方差的网络<strong>学习能力太强</strong>，把训练集中<strong>自身独有</strong>的一些特点<strong>也当作一般规律</strong>学得，使网络<strong>不能很好的泛化</strong>（generalize）到<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E9%AA%8C%E8%AF%81%E9%9B%86&zhida_source=entity">验证集</a>。当网络处于高方差时，通常有以下几种解决方案。<strong>1. 更多的数据</strong>。这是对高方差问题<strong>最行之有效</strong>的解决方案。<strong>2. 正则化</strong>。<strong>3. 改变网络结构</strong>。不同的网络结构对方差也会有影响。</p>
<h2 id="正则化（regularization）"><a href="#正则化（regularization）" class="headerlink" title="正则化（regularization）"></a><strong>正则化（regularization）</strong></h2><p>正则化是<strong>解决高方差</strong>问题的重要方案之一。本节，我们将对常用<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95&zhida_source=entity">正则化方法</a>做一介绍。</p>
<p><strong><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=8&q=%E6%AD%A3%E5%88%99%E5%8C%96&zhida_source=entity">正则化</a>的基本思想</strong> 正则化的基本思想是使网络的<strong>有效</strong>大小<strong>变小</strong>。网络变小之后，网络的<strong>拟合能力随之降低</strong>，这会使网络不容易<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E8%BF%87%E6%8B%9F%E5%90%88&zhida_source=entity">过拟合</a>到训练集。</p>
<p><strong>L2正则化</strong> L2正则化倾向于使网络的<strong>权值接近0</strong>。这会使<strong>前</strong>一层神经元对<strong>后</strong>一层神经元的<strong>影响降低</strong>，使网络<strong>变得简单</strong>，降低网络的有效大小，降低网络的拟合能力。L2正则化实质上是<strong>对权值做线性衰减</strong>，所以<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=4&q=L2%E6%AD%A3%E5%88%99%E5%8C%96&zhida_source=entity">L2正则化</a>也被称为权值衰减（weight decay）。</p>
<p><strong>随机失活（dropout）</strong> 在训练时，随机失活随机选择一部分神经元，使其置零，不参与本次优化迭代。随机失活减少了每次参与优化迭代的神经元数目，使网络的有效大小变小。随机失活的作用有两点。<strong>1. 降低神经元之间耦合</strong>。因为神经元<strong>会被随机置零</strong>，所以每个神经元不能依赖于其他神经元，这会迫使每个神经元自身要能提取到合适的特征。<strong>2. 网络集成</strong>。随机失活可以看作在训练时<strong>每次迭代定义出一个新的网络</strong>，这些网络共享权值。在测试时的网络是这些网络的集成。</p>
<p><strong>数据扩充（<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=data+augmentation&zhida_source=entity">data augmentation</a>）</strong> 这实质是获得更多数据的方法。当收集数据很昂贵，或者我们拿到的是第二手数据，数据就这么多时，我们<strong>从现有数据中扩充</strong>生成更多数据，用<strong>生成的“伪造”<strong>数据当作更多的真实数据进行训练。以图像数据做分类任务为例，把图像水平翻转、移动一定位置、旋转一定角度、或做一点色彩变化等，这些操作通常都不会影响这幅图像对应的标记。并且你可以尝试</strong>这些操作的组合</strong>，理论上讲，你可以通过这些组合得到无穷多的训练样本。</p>
<p><strong>早停（early stopping）</strong> 随着训练的进行，当你发现验证集误差不再变化或者开始上升时，<strong>提前停止</strong>训练。</p>
<h2 id="调参技巧"><a href="#调参技巧" class="headerlink" title="调参技巧"></a><strong>调参技巧</strong></h2><p>深度神经网络涉及<strong>很多的超参数</strong>，如<strong>学习率大小</strong>、<strong>L2正则化系数</strong>、<strong>动量大小</strong>、<strong>批量大小</strong>、<strong>隐层神经元数目</strong>、<strong>层数</strong>、<strong>学习率衰减率</strong>等。本节，我们介绍调参的基本技巧。</p>
<p><strong>随机搜索</strong> 由于你<strong>事先并不知道</strong>哪些超参数对你的问题更重要，因此随机搜索通常是比<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2&zhida_source=entity">网格搜索</a>（grid search）更有效的<strong>调参策略</strong>。</p>
<p><strong>对数空间搜索</strong> 对于<strong>隐层神经元数目</strong>和<strong>层数</strong>，可以直接<strong>从均匀分布采样</strong>进行搜索。而对于<strong>学习率</strong>、L2<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E6%AD%A3%E5%88%99%E5%8C%96%E7%B3%BB%E6%95%B0&zhida_source=entity">正则化系数</a>、和<strong>动量</strong>，在<strong>对数空间搜索</strong>更加有效。例如：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">learning_rate = 10 ** random.uniform(-5, -1)  # From 1e-5 to 1e-1</span><br><span class="line">weight_decay = 10 ** random.uniform(-7, -1)   # From 1e-7 to 1e-1</span><br><span class="line">momentum = 1 - 10 ** random.uniform(-3, -1)   # From 0.9 to 0.999</span><br></pre></td></tr></table></figure>

<h2 id="实现技巧"><a href="#实现技巧" class="headerlink" title="实现技巧"></a><strong>实现技巧</strong></h2><p><strong><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E5%9B%BE%E5%BD%A2%E5%A4%84%E7%90%86%E5%8D%95%E5%85%83&zhida_source=entity">图形处理单元</a>（graphics processing units, GPU）</strong> 深度神经网络的<strong>高效实现工具</strong>。简单来说，CPU擅长<strong>串行、复杂</strong>的运算，而GPU擅长<strong>并行、简单</strong>的运算。深度神经网络中的<strong>矩阵运算都十分简单</strong>，但计算<strong>量巨大</strong>。因此，GPU无疑具有非常强大的优势。</p>
<p><strong><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E5%90%91%E9%87%8F%E5%8C%96&zhida_source=entity">向量化</a>（vectorization）</strong> 代码<strong>提速</strong>的<strong>基本技巧</strong>。能<strong>少写</strong>一个for循环就少写一个，能<strong>少做</strong>一次矩阵运算就少做一次。实质是尽量将<strong>多次标量运算转化为一次向量</strong>运算；将<strong>多次向量</strong>运算转化为<strong>一次矩阵</strong>运算。因为矩阵运算<strong>可以并行</strong>，这将会比多次单独运算快很多。</p>
<h1 id="计算机视觉基础"><a href="#计算机视觉基础" class="headerlink" title="计算机视觉基础"></a>计算机视觉基础</h1><h2 id="一、计算机视觉概述"><a href="#一、计算机视觉概述" class="headerlink" title="一、计算机视觉概述"></a>一、计算机视觉概述</h2><p>1、计算机视觉的背景知识<br>对计算机视觉的第一印象：<strong>用计算机代替人类的眼睛，模仿人类视觉去完成各项任务</strong>。</p>
<p>计算机视觉（Computer Vision） 是一门研究<strong>如何使机器“看</strong>”的科学，也可以看作是研究如何使人工系统<strong>从图像或多维</strong>数据中**“感知”**的科学。</p>
<p>终极目标：计算机视觉成为机器认知世界的基础，终极目的是使得计算机能够像人一样“看懂世界”。<br>2、计算机视觉与人类视觉的关系</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/b4c77b970b584ed38304b329b367e9c9.png" alt="在这里插入图片描述"></p>
<h2 id="二、计算机视觉的基本原理"><a href="#二、计算机视觉的基本原理" class="headerlink" title="二、计算机视觉的基本原理"></a>二、计算机视觉的基本原理</h2><p>1、计算机视觉的处理对象</p>
<h3 id="数字图像"><a href="#数字图像" class="headerlink" title="数字图像"></a>数字图像</h3><p>又称为数码图像或数位图像；<br>是用一个<strong>数字矩阵</strong>来表达客观物体的图像；<br>是由模拟图像数字化得到的；<br>是一个<strong>离散采样点</strong>的集合，每个点具<strong>有其各自的属性</strong>；<br>是<strong>以像素为基本元素</strong>的图像；<br>可以用数字计算机或数字电路存储和处理的图像。<br>数字图像处理包括的内容:</p>
<p>图像变换；<br>图像增强；<br>图像恢复；<br>图像压缩编码；<br>图像分割；<br>图像分析与描述；<br>图像的识别分类。<br>2、计算机视觉的工作原理<br>图像数字化的两个过程：</p>
<p><strong>采样</strong>是将空间上连续的图像<strong>变换成离散的点</strong>，采样<strong>频率越高</strong>，还原的图像<strong>越真实</strong>。<br><strong>量化</strong>是将采样出来的像素点<strong>转换成离散的数量值，<strong>一幅数字图像中不同灰度值的个数称为</strong>灰度等级</strong>，级数<strong>越大</strong>，图像<strong>越清晰</strong>。<br>计算机视觉的基础工作原理：<br><strong>①构造多层神经网络 –&gt; ②较低层识别初级的图像特征 –&gt; ③若干底层特征组成更上一层特征 –&gt; ④通过多个层级的组合 –&gt; ⑤最终在顶层做出分类</strong></p>
<p>3、计算机视觉的关键技术<br><strong>图像分类</strong>：给定一组各自被标记为单一类别的图像，对一组新的测试图像的类别进行预测，并测量预测的准确性结果。</p>
<p><strong>目标检测</strong>：给定一张图像，让计算机找出其中所有目标的位置，并给出每个目标的具体类别。</p>
<p><strong>语义分割</strong>：将整个图像分成像素组，然后对像素组进行标记和分类；语义分割是在语义上理解图中每个像素是什么，还须确定每个物体的边界。如一张“人驾驶摩托车行驶在林间小道上”的图片</p>
<p><strong>实例分割</strong>：在语义分割的基础上进行，将多个重叠物体和不同背景的复杂景象进行分类；同时确定对象的边界、差异和彼此之间的关系。</p>
<p><strong>视频分类</strong>：分类的对象是由多帧图像构成的、包含语音数据、运动信息等的视频对象需要理解每帧图像包含内容，还需要知道上下文关联信息。</p>
<p><strong>人体关键点检测</strong>：通过人体关键节点的组合和追踪来识别人的运动和行为对于描述人体姿态，预测人体行为至关重要。</p>
<p><strong>场景文字识别</strong>：在图像背景复杂、分辨率低下、字体多样、分布随意等情况下，将图像信息转化为文字序列的过程。</p>
<p><strong>目标跟踪任务</strong>：在特定场景跟踪某一个或多个特定感兴趣对象的过程。</p>
<p>三、图像分类基础</p>
<h4 id="1、图像分类的定义"><a href="#1、图像分类的定义" class="headerlink" title="1、图像分类的定义"></a>1、图像分类的定义</h4><p><code>图像分类的定义</code>：图像分类的核心是从给定的分类集合中给图像<strong>分配一个标签</strong>。 </p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/7873a95f36a0446aafe811ef71deaf8b.png" alt="在这里插入图片描述"></p>
<p>图像分类：<br>1、根据大类、小类加标签<br>2、可以多单个或多个标签<br>3、不同的标签粒度和个数会形成不同的分类任务</p>
<p>2、图像分类的类别<br>单标签与多标签分类的区别<br>单标签：数据样本<strong>属于一个大类</strong>的；数据进行分类后用可以用一个值代表；单标签内有二分类（两个选项）和多分类（多个选项）；例子：单标签三个样本的二分类整形（0&#x2F;1）输出为：[0,1,0]。</p>
<p>多标签数据样本可以<strong>划分到几个大的不冲突主题类别</strong>中；在大主题中分别可以进行二分类和多分类问题；例子：多标签(假设为两个标签)三个样本的二分类整形输出为：[[0,1], [0,0],[1,1]]。</p>
<p>跨物种语义级别的图像分类定义</p>
<p>在不同物种层次上识别不同类别的对象，如猫狗分类；<br>各个类别之间属于<strong>不同</strong>的物种或大类，往往具有 <strong>较大</strong>的类间方差，而<strong>类内</strong>具有 <strong>较小</strong>的类内方差；<br>多类别图像分类由传统的特征提取方法转到数据驱动的深度学习方向来，取得了较大进展。<br> 子类细粒度图像分类的定义</p>
<p>子类细粒度分类相较于跨物种图像分类难度更大；<br>是一个大类中的子类的分类，如不同鸟的分类等；<br>在区分出基本类别的基础上，进行更精细的子类划分；<br>由于图像之间具有更加相似的外观和特征，受采集过程中存在干扰影响，导致数据呈现类间差异性大，类内间差异小，分类难度也更高。<img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/9592d4d1d8f444ca8dcf3ac1bd58dcc5.png" alt="在这里插入图片描述"></p>
<p><strong>多标签图像分类的定义</strong></p>
<ul>
<li>给每个样本一系列的目标标签，表示的是样本各属性且不相互排斥的，预测出一个概念集合；</li>
<li>标签数量较大且复杂；</li>
<li>标签的标准<strong>很难统一</strong>，且往往类标之间相互依赖并不独立；</li>
<li>标注的标签并<strong>不能完美覆盖</strong>所有概念面；</li>
<li>标签往往较短语义少，理解困难。</li>
</ul>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/e3c61b5dac6c4e80a2e02d907618f293.png" alt="在这里插入图片描述"></p>
<p>图像分类会遇到的问题<br>1、一张图片包含的<strong>信息内容太多</strong>，不好分类<br>2、有些类别的<strong>图片太少</strong>，比如罕见害虫</p>
<p>3、图像分类遇到的挑战<br>虽然图像分类在大赛上的正确率已经接近极限，但在实际工程应用中，面临诸多挑战。如：类别不均衡；数据集小；巨大的类内差异；实际应用环境复杂</p>
<p>4、图像分类的常用数据集与网络<br>图像分类的常用数据集：CIFAR-10</p>
<p>介绍</p>
<p>CIFAR-10 ：一个用于识别普适物体的<strong>小型</strong>图像数据集；<br>包含6万张大小为32 x 32的彩色图像；<br>共有10个类，每类有6000张图；<br>共5万张图组成训练集合，训练集合中每一类均等且有5000张图；<br>共1万张图组成测试集合，测试集合中每一类均等且有1000张图；<br>10个类别：飞机（ airplane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）；<br><strong>类</strong>是<strong>完全互斥</strong>的：在⼀个类别中出现的图⽚不会出现在其它类中。使用的相关神经网络：LeNet-5、AlexNet<br>LeNet-5：是<strong>最早的卷积神经网络</strong>之一; 1998年第一次将LeNet-5应用到图像分类上，在手写数字识别任务中取得了巨大成功; LeNet-5通过连续使用卷积和池化层的组合提取图像特征,总共5层：3层卷积和2层全连接，池化层未计入层数; LeNet-5是卷积神经网络的开篇大作，完成了卷积神经网络从无到有的突破。<br>AlexNet：AlexNet将LeNet的思想发扬光大，把CNN的基本原理应用到了很深很宽的网络中。成功使用ReLU作为CNN的激活函数，并验证其效果优异；训练时使用数据增强和Dropout<strong>随机忽略</strong>一部分神经元，以避免模型过拟合，提升泛化能力；在CNN中使用重叠的最大池化，提升了特征的丰富性；提出了LRN层，增强了模型的泛化能力。</p>
<p>5、图像分类的典型应用<br>图像分类在图片搜索引擎中的应用</p>
<p>应用图像分类技术可以开发各种图片搜索引擎；<br>图片搜索引擎能通过用户上传图片，应用图像分类技术，识别出图片的内容并进行分类；<br>搜索互联网上与这张图片相同或相似信息的其他图片资源进行校对和匹配，识别图片的内容并提供相关信息。<br> 图像分类在垃圾分类中的应用-智能环卫</p>
<p>为了破解传统分类投放模式可能存在的乱扔垃圾等问题，可在传统垃圾分类投放站点部署摄像头进行智能化改造；<br>阿里云提出“智能环卫”产品，提供垃圾分类投放点AI智能检测分析功能；<br>有效针对垃圾桶内的未破袋垃圾包、残余垃圾袋等进行检测和识别，检测效率高，真实环境下检测准确率超过95%。<br>四、目标检测基础<br>目标检测：框出来，用坐标表示</p>
<p>1、目标检测的定义<br>目标检测的定义：目标检测就是识别图中<strong>有哪些物体</strong>，确定他们的类别并标出各自在图中的位置。目标检测模型读取该图片；寻找识别出图中的物体目标，对其进行定位，框起和标注。</p>
<p>图像分类与目标检测的区别<br>图像分类：<strong>整幅图像</strong>经过识别后被分类为单一的标签。<br>目标检测：除了识别出图像中的<strong>一个或多个</strong>目标，还需要找出目标在图像中的<strong>具体位置</strong>。</p>
<p>2、目标检测的评估指标<br>交并比：IoU</p>
<p>真实边界框：训练集中，<strong>人工标注的</strong>物体边界框；<br>预测边界框：模型预测到的物体边界框；<br>交并比：在分子项中，是真实边界框和预测边界框<strong>重叠的区域</strong>（Intersection）。分母是一个并集（Union），或者更简单地说，是由预测边界框和真实边界框所包括的区域。两者相除就得到了最终的得分<br>精确度（Precision）指目标检测模型判断该图片为正类，该图片<strong>确实是正类的概率</strong>；</p>
<p>和召回率（Recall）是指的是一个分类器能把<strong>所有的正类都找出来</strong>的能力；</p>
<p>平均精度值：mAP ：mAP，mean Average Precision, 即<strong>各类别平均</strong>精度均值；mAP是把每个类别的AP都单独拿出来，然后计算所有类别AP的平均值，代表着对检测到的目标平均精度的一个综合评价。每一个类别都可以根据Recall和Precision绘制一条曲线，那么<strong>AP就是该曲线下的面积</strong>，而mAP则是多个类别AP的平均值，这个值介于0到1之间。mAP是目标检测算法里最重要的一个评估指标。</p>
<p>3、目标检测遇到的挑战<br>目标<strong>数</strong>量问题：在图片输入模型前不清楚图片中有多少个目标，无法知道正确的输出数量。<br>目标<strong>大小</strong>问题：目标的大小不一致,甚至一些目标仅有十几个像素大小，占原始图像中非常小的比例。<br>如何建模：需要同时处理目标定位以及目标物体识别分类这两个问题。</p>
<p>4、目标检测的常用数据集与网络<br>目标检测的常用数据集：PASCAL VOC</p>
<p>PASCAL VOC ：一个常用于目标检测的小型图像数据集；<br>包含11530张彩色图像，标定了27450个目标识别区域；<br>从初始4个类发展成最终的20个类；<br>在整个数据集中，平均每张图片有2.4个目标；<br>20个类别：<br>动物：人、鸟、猫、狗、牛、马、羊；<br>运载工具：飞机、自行车、船、巴士、汽车、摩托车、火车；<br>物品：瓶子、椅子、餐桌、盆栽、沙发、电视机。</p>
<p>使用的相关神经网络：CenterNet</p>
<p>CenterNet结构优雅简单，直接检测目标的中心点和大小;<br>CenterNet把目标检测任务看作三个部分：寻找物体的中心点；计算物体中心点的偏移量；分析物体的大小;<br>CenterNet检测速度和精度相比于先前的框架都有明显且可观的提高，尤其是与著名的目标检测网络YOLOv3作比较，在相同速度的条件下，CenterNet的精度比YOLOv3提高了大约4个点。<br>5、目标检测的典型应用<br>智慧交通是目标检测的一个重要应用领域，主要包括如下场景：</p>
<p>检测各种交通异常事件，如车辆占用应急车道、车辆驾驶员的驾驶行为等；<br>第一时间将异常事件上报给交管部门，提高处理效率。<br>目标检测在智慧交通中的应用-智慧眼</p>
<p>通过目标检测算法，对道路视频图像进行分析；<br>根据分析车流量，调整红绿灯配时策略，提升交通通行能力。<br> 五、图像分割基础<br>“抠图软件”的操作流程？</p>
<p>1、选中图片中的目标主体<br>2、对主体的边界进行分割<br>3、主体与背景分离，突出显示主体<br>1、图像分割的定义<br>图像分割就是把图像分成若干个特定的、具有独特性质的区域并提出感兴趣目标的技术和过程；</p>
<p>图像分割包括：语义分割、实例分割和全景分割。</p>
<p>图像作为分割算法的输入，输出一组区域；</p>
<p>区域可以表示为一种掩码（灰度或颜色），其中每个部分被分配一个唯一的<strong>颜色或灰度值</strong>来代表它</p>
<p>2、图像分割的类别<br>语义分割的定义：</p>
<p>语义分割是在<strong>像素级别上</strong>的分类，属于<strong>同一类的像素</strong>都要被归为一类；<br>语义分割是从像素级别来理解图像的。<br>实例分割的定义：</p>
<p>实例分割比语义分割更进一步；<br>对于语义分割来说，只要将所有同类别（猫、狗）的像素都归为一类；<br>实例分割还要在具体类别（猫、狗）像素的基础上<strong>区分开不同的实例</strong>（短毛猫、虎斑猫、贵宾犬、柯基犬）。<br>全景分割的定义</p>
<p>全景分割是<strong>语义和实例分割的相结合</strong>；<br>每个像素都被分配一个类（比如：狗），如果一个类有多个实例，则可知道该像素属于该类的哪个实例（贵宾犬&#x2F;柯基犬）。<br>3、图像分割遇到的挑战<br>分割<strong>边缘不准</strong>：因为相邻临的像素对应感受野内的图像信息太过相似导致。</p>
<p>样本<strong>质量不一</strong>：样本中的目标物体具有多姿态、多视角问题，会出现物体之间的遮挡和重叠；<br>受场景光照影响，样本质量参差不齐。</p>
<p>标注成本高：对于数据样本的标注成本非常高，而且标注质量难以保证不含有噪声。</p>
<p>4、图像分割的常用数据集与网络<br>图像分割的常用数据集：COCO</p>
<p>COCO：一个常用于图像分割的大型图像数据集；<br>包含33万张彩色图像，标定了50万个目标实例；<br>具有80个目标类、91个物品类以及25万个人物关键点标注；<br>每张图片包含5个描述；<br>每一类的图像多，利于提升识别更多类别位于特定场景的能力；<br>类别包括： person(人) 、bicycle(自行车) 、car(汽车) 、motorbike(摩托车) 、aeroplane(飞机) 、bus(公共汽车) 、train(火车)、truck(卡车) 、boat(船) 、traffic light(信号灯) 、fire hydrant(消防栓) 、stop sign(停车标志) 、parking meter(停车计费器) 、bench(长凳) 、bird(鸟) 、cat(猫) 、dog(狗) 、horse(马) 、sheep(羊) 、cow(牛) 等等。<br>使用的相关神经网络：FCN</p>
<p><strong>FCN全卷积神经网络</strong>是图像分割的基础网络;<br>全卷积神经网络，顾名思义网络里的所有层<strong>都是卷积层</strong>；<br>卷积神经网络卷到最后特征图尺寸和分辨率越来越小，不适合做图像分割，为解决此问题FCN引入- 上采样的方法，卷积完之后再上采样到大尺寸图；<br>为避免层数不断叠加后原图的信息丢失得比较多，FCN引入一个跳层结构，把前面的层特征引过来- 进行叠加；<br>FCN实现了端到端的网络<br>端到端学习是一种解决问题的思路，与之对应的是多步骤解决问题，也就是将一个问题拆分为多个步骤分步解决，而端到端是由输入端的数据直接得到输出端的结果。</p>
<p>5、图像分割的典型应用<br>图像分割在抠图软件中的应用</p>
<p>应用图像分割技术可以开发各种抠图软件；<br>用户在软件平台上传图片，应用图像分割技术，分辨出图片具有独特特征的区域并进行边缘识别分割；<br>返回给用户经过图像分割处理的结果图片。<br>图像分割在智能证件照中的应用</p>
<p>基于智能视觉生产的人像分割能力，阿里云为用户提供证件照的智能制作与编辑能力；<br>自动从上传的生活照中分割出人像区域，精确到像素级别的分割保证证件照的专业性与准确性，将生活照完美转换成专业证件照。</p>
<h1 id="NLP-自然语言处理-—-NLP入门指南"><a href="#NLP-自然语言处理-—-NLP入门指南" class="headerlink" title="[NLP] 自然语言处理 — NLP入门指南"></a>[NLP] 自然语言处理 — NLP入门指南</h1><p><strong>NLP的全称是Natuarl Language Processing</strong>，中文意思是自然语言处理，是人工智能领域的一个重要方向</p>
<p>自然语言处理（NLP）的一个最伟大的方面是跨越多个领域的计算研究，从人工智能到计算语言学的多个计算研究领域都在研究计算机与人类语言之间的相互作用。它主要关注计算机如何准确并快速地处理大量的自然语言语料库。什么是自然语言语料库？它是用现实世界语言表达的语言学习，是从文本和语言与另一种语言的关系中理解一组抽象规则的综合方法。</p>
<p>人类语言是抽象的信息符号，其中蕴含着丰富的语义信息，人类可以很轻松地理解其中的含义。而<strong>计算机只能处理数值化的信息</strong>，无法直接理解人类语言，所以需要将人类语言进行<strong>数值化转换</strong>。不仅如此，人类间的沟通交流是有上下文信息的，这对于计算机也是巨大的挑战。</p>
<p>我们首先来看看NLP的任务类型，如下图所示：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/158d23180094de856f74843b80f131a0.png" alt="img"></p>
<p> 主要划分为了四大类：</p>
<p>类别到序列<br>序列到类别<br>同步的序列到序列<br>异步的序列到序列<br>其中“类别”可以理解为是<strong>标签或者分类</strong>，而“序列”可以理解为是<strong>一段文本或者一个数组</strong>。简单概况NLP的任务就是从<strong>一种数据类型转换成另一种数据类型</strong>的过程，这与绝大多数的机器学习模型相同或者类似，所以掌握了NLP的<strong>技术栈</strong>就等于掌握了机器学习的技术栈。</p>
<p>传统方式和深度学习方式 NLP 对比</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1393176ebd38c6650f9d65f2f7685e65.png" alt="img"></p>
<h2 id="NLP的预处理"><a href="#NLP的预处理" class="headerlink" title="NLP的预处理"></a>NLP的预处理</h2><p>为了能够完成上述的NLP任务，我们需要一些预处理，是NLP任务的基本流程。预处理包括：收集语料库、文本清洗、分词、去掉停用词（可选）、标准化和特征提取等。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/f5471c37eaf151e4bb48c07e1935d5d2.png" alt="img"></p>
<p>图中红色的部分就是NLP任务的预处理流程，有别于其它机器学习任务的流程</p>
<p><strong>英文 NLP 语料预处理的 6 个步骤</strong></p>
<ol>
<li><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/tokenization/">分词 – Tokenization</a></li>
<li><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/stemming-lemmatisation/">词干提取</a> – <a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/stemming-lemmatisation/">Stemming</a></li>
<li><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/stemming-lemmatisation/">词形还原</a> – Lemmatization</li>
<li><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/part-of-speech/">词性标注 – Parts of Speech</a></li>
<li><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/ner/">命名实体识别 – NER</a></li>
<li>分块 – Chunking</li>
</ol>
<p><strong>中文 NLP 语料预处理的 4 个步骤</strong></p>
<ol>
<li><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/tokenization/">中文分词 – Chinese Word Segmentation</a></li>
<li><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/part-of-speech/">词性标注 – Parts of Speech</a></li>
<li><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/ner/">命名实体识别 – NER</a></li>
<li>去除停用词</li>
</ol>
<h2 id="第1步：收集您的数据—语料库"><a href="#第1步：收集您的数据—语料库" class="headerlink" title="第1步：收集您的数据—语料库"></a>第1步：收集您的数据—语料库</h2><p>对于NLP任务来说，没有大量高质量的语料，就是巧妇难为无米之炊，是无法工作的。</p>
<p>而获取语料的途径有很多种，最常见的方式就是<strong>直接下载开源</strong>的语料库，如：维基百科的语料库。</p>
<p>但这样<strong>开源</strong>的语料库一般都<strong>无法满足业务的个性化</strong>需要，所以就需要<strong>自己动手开发爬虫</strong>去<strong>抓取特定</strong>的内容，这也是一种获取语料库的途径。当然，每家互联网公司根据自身的业务，也都会有大量的语料数据，如：用户评论、电子书、商品描述等等，都是很好的语料库。</p>
<p>示例数据源</p>
<p>每个机器学习问题都从数据开始，例如电子邮件，帖子或推文列表。常见的文字信息来源包括：</p>
<p>产品评论（在亚马逊，Yelp和各种应用商店）<br>用户生成的内容（推文，Facebook帖子，StackOverflow问题）<br>故障排除（客户请求，支持服务单，聊天记录）<br>现在，数据对于互联网公司来说就是<strong>石油</strong>，其中蕴含着巨大的商业价值。所以，小伙伴们在日常工作中一定要养成收集数据的习惯，遇到<strong>好的语料库一定要记得备份</strong>（当然是在合理合法的条件下），它将会对你解决问题提供巨大的帮助。</p>
<p>第2步：清理数据 — 文本清洗<br>我们遵循的首要规则是：“您的模型将永远与您的数据一样好。”</p>
<p>数据科学家的关键技能之一是了解<strong>下一步是应该对模型还是数据</strong>进行处理。一个好的经验法则是<strong>首先查看数据然后进行清理</strong>。一个<strong>干净的数据集</strong>将允许模型学习有意义的功能，而不是过度匹配无关的噪音。</p>
<p>我们通过不同的途径获取到了想要的语料库之后，接下来就需要对其进行清洗。因为很多的语料数据是无法直接使用的，其中包含了大量的无用符号、特殊的文本结构。</p>
<p>数据类型分为：</p>
<p><strong>结构化</strong>数据：关系型数据、json等<br><strong>半结构化</strong>数据：XML、HTML等<br><strong>非结构化****数据：Word、PDF、文本、日志等<br>需要将原始的语料数据</strong>转化成易于处理<strong>的格式，一般在处理HTML、XML时，会使用Python的</strong>lxml库**，功能非常丰富且易于使用。对一些日志或者纯文本的数据，我们可以使用<strong>正则表达式</strong>进行处理。</p>
<p>正则表达式是使用单个字符串来描述、匹配一系列符合某个句法规则的字符串。Python的示例代码如下：</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment"># 定义中文字符的正则表达式</span></span><br><span class="line">re_han_default = re.<span class="built_in">compile</span>(<span class="string">&quot;([\u4E00-\u9FD5]+)&quot;</span>, re.U)</span><br><span class="line">sentence = <span class="string">&quot;我/爱/自/然/语/言/处/理&quot;</span></span><br><span class="line"><span class="comment"># 根据正则表达式进行切分</span></span><br><span class="line">blocks= re_han_default.split(sentence)</span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> blocks:</span><br><span class="line">    <span class="comment"># 校验单个字符是否符合正则表达式</span></span><br><span class="line">    <span class="keyword">if</span> blk <span class="keyword">and</span> re_han_default.<span class="keyword">match</span>(blk):</span><br><span class="line">        <span class="built_in">print</span>(blk)</span><br></pre></td></tr></table></figure>

<p>除了上述的内容之外，我们还需要注意中文的编码问题，在windows平台下中文的默认编码是<strong>GBK（gb2312）</strong>，而在linux平台下中文的默认编码是<strong>UTF-8</strong>。在执行NLP任务之前，我们需要统一不同来源语料的编码，避免各种莫名其妙的问题。</p>
<p>如果大家事前无法判断语料的编码，那么我推荐大家可以使用Python的<strong>chardet库来检测编码</strong>，简单易用。既支持命令行：chardetect somefile，也支持代码开发。</p>
<p>以下是用于清理数据的清单:</p>
<p><strong>删除所有不相关</strong>的字符，例如任何非字母数字字符<br>令牌化通过将其<strong>分割</strong>成单个的单词文本<br>删除不相关的单词，例如“@”twitter提及或网址<br>将所有字符<strong>转换为小写</strong>，以便将诸如“hello”，“Hello”和“HELLO”之类的单词视为相同<br>考虑将拼写错误或交替拼写的单词<strong>组合成单个表示</strong>（例如“cool”&#x2F;“kewl”&#x2F;“cooool”）<br>考虑<strong>词开还原</strong>（将诸如“am”，“are”和“is”之类的词语简化为诸如“be”之类的常见形式）<br>按照这些步骤并检查其他错误后，我们可以开始使用干净的标记数据来训练模型！</p>
<p>第3步：分词<br>中英文分词的3个典型区别</p>
<p> 区别1：分词方式不同，中文更难</p>
<p>英文有<strong>天然的空格</strong>作为分隔符，但是<strong>中文没有</strong>。所以如何切分是一个难点，再加上中文里一词多意的情况非常多，导致很容易出现歧义。下文中难点部分会详细说明。</p>
<p>区别2：英文单词有多种形态</p>
<p>英文单词存在丰富的变形变换。为了应对这些复杂的变换，英文NLP相比中文存在一些独特的处理步骤，我们称为<strong>词形还原（Lemmatization）<strong>和</strong>词干提取(Stemming)</strong>。中文则不需要</p>
<p>词性还原：does，done，doing，did 需要通过词性还原恢复成 do。</p>
<p>词干提取：cities，children，teeth 这些词，需要转换为 city，child，tooth”这些基本形态</p>
<p>区别3：中文分词需要考虑<strong>粒度问题</strong></p>
<p>例如「中国科学技术大学」就有很<strong>多种分法</strong>：</p>
<p>中国科学技术大学<br>中国 \ 科学技术 \ 大学<br>中国 \ 科学 \ 技术 \ 大学<br>粒度越大，表达的<strong>意思就越准确</strong>，但是也会导致<strong>召回比较少</strong>。所以中文需要不同的场景和要求选择不同的粒度。这个在英文中是没有的。</p>
<p>中文分词是一个比较大的课题，相关的知识点和技术栈非常丰富，可以说搞懂了中文分词就等于搞懂了大半个NLP。</p>
<p>中文分词的3大难点<br> 难点 1：<strong>没有统一的标准</strong></p>
<p>目前中文分词没有统一的标准，也没有公认的规范。不同的公司和组织各有各的方法和规则。</p>
<p>难点 2：<strong>歧义词如何切分</strong></p>
<p>例如「兵乓球拍卖完了」就有2种分词方式表达了2种不同的含义：</p>
<p>乒乓球 \ 拍卖 \ 完了<br>乒乓 \ 球拍 \ 卖 \ 完了<br>难点 3：<strong>新词的识别</strong></p>
<p>信息爆炸的时代，三天两头就会冒出来一堆新词，如何快速的识别出这些新词是一大难点。比如当年「蓝瘦香菇」大火，就需要快速识别。</p>
<p>中文分词经历了20多年的发展，克服了重重困难，取得了巨大的进步，大体可以划分成两个阶段，如下图所示：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/d5e1e3b77629124b9a5aff5cc0c0955f.png" alt="img"></p>
<p><strong>词典</strong>匹配与规则</p>
<p>优点：<strong>速度</strong>快、成本低</p>
<p>缺点：适应性不强，不同领域效果差异大</p>
<p>基本思想是基于词典匹配，将待分词的中文文本根据一定规则切分和调整，然后跟词典中的词语进行匹配，匹配成功则按照词典的词分词，匹配失败通过调整或者重新选择，如此反复循环即可。代表方法有基于正向最大匹配和基于逆向最大匹配及双向匹配法。</p>
<p>基于统计与机器学习</p>
<p>优点：<strong>适应性</strong>较强</p>
<p>缺点：成本较高，速度较慢</p>
<p>这类目前常用的是算法是HMM、CRF等算法，比如stanford、Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。</p>
<p>常见的分词器都是使用<strong>机器学习算法和词典相结合</strong>，一方面能够提高分词准确率，另一方面能够改善领域适应性。</p>
<p>目前，主流的中文分词技术采用的都是基于词典最大概率路径+未登录词识别（HMM）的方案，其中典型的代表就是<strong>jieba分词</strong>，一个热门的多语言中文分词包。</p>
<p>中文分词工具</p>
<p>下面排名根据 GitHub 上的 star 数排名：</p>
<p>Hanlp<br>Stanford 分词<br>ansj 分词器<br>哈工大 LTP<br>KCWS分词器<br>jieba<br>IK<br>清华大学THULAC<br>ICTCLAS<br>英文分词工具</p>
<p>Keras<br>Spacy<br>Gensim<br>NLTK<br>第4步：标准化<br>标准化是为了给后续的处理提供一些<strong>必要的基础数据</strong>，包括：去掉停用词、词汇表、训练数据等等。</p>
<p>当我们完成了分词之后，可以去掉停用词，如：“其中”、“况且”、“什么”等等，但这一步不是必须的，要根据实际业务进行选择，像关键词挖掘就需要去掉停用词，而像训练词向量就不需要。</p>
<p>词汇表是为语料库建立一个<strong>所有不重复词的列表</strong>，每个词对应一个<strong>索引值</strong>，并索引值不可以改变。词汇表的最大作用就是可以<strong>将词转化成一个向量</strong>，即One-Hot编码。</p>
<p>假设我们有这样一个词汇表：</p>
<p>我<br>爱<br>自然<br>语言<br>处理<br>那么，我们就可以得到如下的One-Hot编码：</p>
<p>我：  [1, 0, 0, 0, 0]<br>爱：  [0, 1, 0, 0, 0]<br>自然：[0, 0, 1, 0, 0]<br>语言：[0, 0, 0, 1, 0]<br>处理：[0, 0, 0, 0, 1]<br>这样我们就可以简单的将词转化成了计算机可以直接处理的数值化数据了。虽然One-Hot编码可以较好的完成部分NLP任务，但它的问题还是不少的。</p>
<p>当词汇表的维度特别大的时候，就会导致经过One-Hot编码后的词向量非常稀疏，同时One-Hot编码也缺少词的语义信息。由于这些问题，才有了后面大名鼎鼎的Word2vec，以及Word2vec的升级版BERT。</p>
<p>除了词汇表之外，我们在训练模型时，还需要提供训练数据。模型的学习可以大体分为两类：</p>
<p><strong>监督</strong>学习，在已知答案的标注数据集上，模型给出的预测结果<strong>尽可能接近真实</strong>答案，适合<strong>预测</strong>任务<br><strong>非监督</strong>学习，学习没有标注的数据，是要<strong>揭示</strong>关于数据隐藏结构的一些规律，适合<strong>描述</strong>任务<br>根据不同的学习任务，我们需要提供不同的标准化数据。一般情况下，标注数据的获取成本非常昂贵，非监督学习虽然不需要花费这样的成本，但在实际问题的解决上，主流的方式还<strong>选择监督学习，因为效果更好</strong>。</p>
<p>带标注的训练数据大概如下所示（情感分析的训练数据）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">距离 川沙 公路 较近 公交 指示 蔡陆线 麻烦 建议 路线 房间 较为简单	__label__1</span><br><span class="line">商务 大床 房 房间 很大 床有 2M 宽 整体 感觉 经济 实惠 不错 !	__label__1</span><br><span class="line">半夜 没 暖气 住 ! 	__label__0</span><br></pre></td></tr></table></figure>

<p>其中每一行就是一条训练样本，<code>__label__0</code>和<code>__label__1</code>是分类信息，其余的部分就是分词后的文本数据。</p>
<p>第5步：特征提取<br>为了能够更好的训练模型，我们需要将文本的原始特征<strong>转化成具体</strong>特征，转化的方式主要有两种：统计和Embedding。</p>
<p>原始特征：需要人类或者机器进行转化，如：文本、图像。</p>
<p>具体特征：已经被人类进行整理和分析，可以直接使用，如：物体的重要、大小。</p>
<p>NLP表示方式<br>目前常用的文本表示方式分为：</p>
<p><strong>离散</strong>式表示（Discrete Representation）；<br><strong>分布</strong>式表示（Distributed Representation）；</p>
<h2 id="离散式表示（Discrete-Representation）"><a href="#离散式表示（Discrete-Representation）" class="headerlink" title="离散式表示（Discrete Representation）"></a>离散式表示（Discrete Representation）</h2><p>One-Hot<br>One-Hot 编码又称为“独热编码”或“哑编码”，是最传统、最基础的词（或字）特征表示方法。这种编码将词（或字）<strong>表示成一个向量</strong>，该向量的<strong>维度是词典（或字典）的长度</strong>（该词典是通过语料库生成的），该向量中，<strong>当前词的位置的值为1</strong>，其余的位置为0。</p>
<p>文本使用one-hot 编码步骤：</p>
<p>根据<strong>语料库创建 词典</strong>（vocabulary），并创建词和索引的 <strong>映射</strong>（stoi，itos)；<br>将句子转换为用<strong>索引表示</strong>；<br>创建OneHot 编码器；<br>使用OneHot 编码器对句子<strong>进行编码</strong>；<br>One-Hot 编码的特点如下：</p>
<p>词向量长度是词典长度；<br>在向量中，该单词的索引位置的值为  1 ，其余的值都是  0<br>使用One-Hot 进行编码的文本，得到的矩阵是<strong>稀疏矩阵</strong><br>缺点：</p>
<p>不同词的向量表示互相正交，<strong>无法衡量</strong>不同词之间的<strong>关系</strong>；<br>该编码只能反映某个词<strong>是否在句中出现</strong>，无法衡量不同词的<strong>重要程度</strong>；<br>使用One-Hot 对文本进行编码后得到的是<strong>高维稀疏</strong>矩阵，会<strong>浪费计算和存储资源</strong>；<br>词袋模型（Bag Of Word，BOW）<br>例句：</p>
<p>Jane wants to go to Shenzhen.<br>Bob wants to go to Shanghai.<br>在词袋模型中不考虑语序和词法的信息，每个单词都是<strong>相互独立</strong>的，将词语放入一个“袋子”里，统计每个单词出现的频率。</p>
<p>词袋模型编码特点：</p>
<p>词袋模型是<strong>对文本</strong>（而不是字或词）进行编码；<br>编码后的向量长度是<strong>词典的长度</strong>；<br>该编码<strong>忽略词出现的次序</strong>；<br>在向量中，该单词的<strong>索引位置的值</strong>为单词在文本中出现的<strong>次数</strong>；如果索引位置的单词没有在文本中出现，则该值为  0 ；<br>缺点</p>
<p>该编码<strong>忽略词的位置信息</strong>，位置信息在文本中是一个很重要信息，词的位置不一样语义会有很大的差别（如 “猫爱吃老鼠” 和 “老鼠爱吃猫” 的编码一样）；<br>该编码方式虽然统计了词在文本中出现的次数，但仅仅通过“出现次数”这个属性<strong>无法区分常用词</strong>（如：“我”、“是”、“的”等）和<strong>关键词</strong>（如：“自然语言处理”、“NLP ”等）在文本中的<strong>重要程度</strong>；<br>TF-IDF（词频-逆文档频率）<br>为了解决词袋模型无法区分常用词（如：“是”、“的”等）和专有名词（如：“自然语言处理”、“NLP ”等）对文本的重要性的问题，TF-IDF 算法应运而生。</p>
<p>TF-IDF 全称是：term frequency–inverse document frequency 又称 词频-逆文本频率。其中：</p>
<p>统计的方式主要是计算词的词频（TF）和逆向文件频率（IDF）：</p>
<p>TF （Term Frequency ）：某个词<strong>在当前文本中</strong>出现的频率，频率高的词语或者是重要的词（如：“自然语言处理”）或者是常用词（如：“我”、“是”、“的”等）；<br>IDF （Inverse Document frequency ）：逆文本频率。文本频率是指：含有某个词的文本<strong>在整个语料库中</strong>所占的比例。逆文本频率是<strong>文本频率的倒数</strong>；<br>那么，每个词都会得到一个TF-IDF值，用来<strong>衡量它的重要程度</strong>，计算公式如下：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4b52b3a5ad29985b686e084588e842fb.png" alt="img"></p>
<p>优点</p>
<p>实现简单，算法容易理解且解释性较强；<br>从IDF 的计算方法可以看出常用词（如：“我”、“是”、“的”等）在<strong>语料库中</strong>的很多文章都会出现，故<strong>IDF的值会很小</strong>；而关键词（如：“自然语言处理”、“NLP ”等）只会在<strong>某领域</strong>的文章出现，IDF 的值会<strong>比较大</strong>；故：TF-IDF 在保留文章的重要词的同时可以过滤掉一些常见的、无关紧要的词；<br>缺点</p>
<p><strong>不能反映词的位置信息</strong>，在对关键词进行提取时，词的位置信息（如：标题、句首、句尾的词应该赋予更高的权重）；<br>IDF 是一种试图抑制噪声的加权，本身倾向于文本中频率比较小的词，这使得IDF 的精度不高；<br>TF-IDF <strong>严重依赖于语料库</strong>（尤其在训练同类语料库时，往往会掩盖一些同类型的关键词；如：在进行TF-IDF 训练时，语料库中的 娱乐 新闻较多，则与 娱乐 相关的关键词的权重就会偏低 ），因此需要选取质量高的语料库进行训练；</p>
<h2 id="分布式表示（Distributed-Representation"><a href="#分布式表示（Distributed-Representation" class="headerlink" title="分布式表示（Distributed Representation"></a>分布式表示（Distributed Representation</h2><p>理论基础：</p>
<p>1954年，Harris提出分布式假说（distributional hypothesis）奠定了这种方法的理论基础：A word’s meaning is given by the words that frequently appear close-by（上下文相似的词，其语义也相似）；<br>1957年，Firth对分布式假说做出进一步的阐述和明确：A word is characterized by the company it keeps（词的语义<strong>由其上下文决定</strong>）；<br>n-gram<br>n-gram 是一种 语言模型(Language Model, LM)。语言模型是一种基于概率的判别式模型，该模型的输入是一句话（单词的序列），输出的是这句话的概率，也就是这些单词的<strong>联合概率</strong>（joint probability）。（备注：语言模型就是判断一句话是不是正常人说的。）</p>
<p>共现矩阵（Co-Occurrence Matrix）<br>首先指定窗口大小，然后统计窗口（和对称窗口）内词语<strong>共同出现</strong>的<strong>次数</strong>作为词的向量（vector）。</p>
<p>语料库：</p>
<p>I like deep learning.<br>I like NLP.<br>I enjoy flying.<br>备注： 指定窗口大小为1（即：左右的 window_length&#x3D;1，相当于 bi-gram）统计数据如下：（I, like），（Iike, deep），（deep, learning），（learning, .），（I, like），（like, NLP），（NLP, .），（I, enjoy），（enjoy, flying）， （flying, .）。则语料库的共现矩阵如下表所示：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/8b07f13487955c6279acbecd802fbcf5.png" alt="img"></p>
<p>从以上的共现矩阵可以看出，单词  like 和  enjoy 都在单词  I 附件出现且统计数目大概相等，则它们在 <strong>语义 和 语法 上的含义大概相同</strong>。</p>
<p>优点</p>
<p>考虑了句子中<strong>词的顺序</strong>；<br>缺点</p>
<p>词表的长度很大，导致词的向量长度也很大；<br>共现矩阵也是稀疏矩阵（可以使用 SVD、PCA 等算法进行降维，但是计算量很大）；<br>Word2Vec<br>word2vec 模型是Google团队在2013年发布的 word representation 方法。该方法一出让 预训练词向量 的使用在NLP 领域遍地开花。</p>
<p>word2vec模型</p>
<p>word2vec有两种模型：CBOW 和 SKIP-GRAM；</p>
<ul>
<li>CBOW：利用上下文的词预测中心词；</li>
</ul>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6a9e694f893c53f1d031e02b03a96dcb.png" alt="img"></p>
<ul>
<li>SKIP-GRAM：利用中心词预测上下文的词；</li>
</ul>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/a9321a80eaf4aaf89f60ee6c94b3598e.png" alt="img"></p>
<p><strong>优点</strong></p>
<ol>
<li>考虑到词语的<strong>上下文</strong>，学习到了语义和语法的信息；</li>
<li>得到的词向量<strong>维度小</strong>，节省存储和计算资源；</li>
<li><strong>通用性</strong>强，可以应用到各种<em>NLP</em> 任务中；</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>词和向量是一对一的关系，无法解决多义词的问题；</li>
<li>word2vec是一种<strong>静态</strong>的模型，虽然通用性强，但无法真的特定的任务做动态优化；</li>
</ol>
<p>GloVe<br>GloVe 是斯坦福大学Jeffrey、Richard 等提供的一种词向量表示算法，GloVe 的全称是Global Vectors for Word Representation，是一个基于全局词频统计（count-based &amp; overall staticstics）的词表征（word representation）算法。该算法综合了global matrix factorization（全局矩阵分解） 和 local context window（局部上下文窗口） 两种方法的优点。</p>
<p>效果</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/89c700f1bbd4be29476d9139b05a9679.png" alt="img"></p>
<p>优点</p>
<p>考虑到词语的<strong>上下文</strong>、和全局语料库的信息，学习到了语义和语法的信息；<br>得到的词向量<strong>维度小</strong>，节省存储和计算资源；<br>通用性强，可以应用到各种NLP 任务中；<br>缺点</p>
<p>词和向量是一对一的关系，无法解决多义词的问题；<br>glove也是一种静态的模型，虽然通用性强，但无法真的特定的任务做动态优化；<br>ELMO<br>word2vec 和 glove 算法得到的词向量都是静态词向量（静态词向量会把多义词的语义进行融合，训练结束之后不会根据上下文进行改变），静态词向量无法解决多义词的问题（如：“我今天买了7斤苹果” 和 “我今天买了苹果7” 中的 苹果 就是一个多义词）。而ELMO模型进行训练的词向量可以<strong>解决多义词</strong>的问题。</p>
<p>ELMO 的全称是“ Embedding from Language Models ”，这个名字不能很好的反映出该模型的特点，提出ELMO 的论文题目可以更准确的表达出该算法的特点“ Deep contextualized word representation ”。</p>
<p>该算法的精髓是：<strong>用语言模型训练神经网络</strong>，在使用word embedding 时，单词已经具备上下文信息，这个时候神经网络可以根据上下文信息对word embedding 进行调整，这样经过调整之后的word embedding 更能表达在这个上下文中的具体含义，这就解决了静态词向量无法表示多义词的问题。</p>
<p>网络模型</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/b20fb10062f8b5e18f8e7ea973e17b6d.png" alt="img"></p>
<p>过程</p>
<p>上图中的结构使用<strong>字符级卷积神经网络（convolutional neural network, CNN）<strong>来将文本中的词转换成</strong>原始词向量（raw word vector）</strong> ；<br>将原始词向量输入双向语言模型中第一层 ；<br><strong>前向迭代</strong>中包含了该词以及该词之前的一些词汇或语境的信息（即<strong>上文</strong>）；<br><strong>后向迭代</strong>中包含了该词以及该词之后的一些词汇或语境的信息（即<strong>下文</strong>） ；<br>这两种迭代的信息组成了<strong>中间词向量（intermediate word vector）</strong>；<br>中间词向量被输入到模型的<strong>下一层</strong> ；<br>最终向量就是原始词向量和两个中间词向量的<strong>加权和</strong>；</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/0b2bd5a91dfe856b390172a2589311ac.png" alt="img"></p>
<p>如上图所示：</p>
<ul>
<li>使用glove训练的词向量中，与 play 相近的词大多与<strong>体育相关</strong>，这是因为语料中与play相关的语料多时体育领域的有关；</li>
<li>在使用elmo训练的词向量中，当 play 取 <strong>演出</strong> 的意思时，与其相近的也是 演出 相近的句子</li>
</ul>
<h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-e1576624094357.webp" alt="img"></p>
<p>PyTorch 是一个开源的机器学习库，主要用于进行<strong>计算机视觉（CV）</strong>、<strong>自然语言处理（NLP）</strong>、语音识别等领域的研究和开发。</p>
<p>PyTorch 以其灵活性和易用性而闻名，特别适合于<strong>深度学习</strong>研究和开发。</p>
<h2 id="谁适合阅读本教程？"><a href="#谁适合阅读本教程？" class="headerlink" title="谁适合阅读本教程？"></a>谁适合阅读本教程？</h2><p>只要您具备编程的基础知识，您就可以阅读本教程，学习 PyTorch 适合对深度学习和机器学习感兴趣的人，包括数据科学家、工程师、研究人员和学生。</p>
<h2 id="阅读本教程前，您需要了解的知识："><a href="#阅读本教程前，您需要了解的知识：" class="headerlink" title="阅读本教程前，您需要了解的知识："></a>阅读本教程前，您需要了解的知识：</h2><p>在您开始阅读本教程之前，您必须具备的基础知识包括 Python 编程、基础数学（线性代数、概率论、微积分）、机器学习的基本概念、神经网络知识，以及一定的英语阅读能力来查阅文档和资料。</p>
<ul>
<li><strong>编程基础</strong>：熟悉至少一种编程语言，尤其是 <a target="_blank" rel="noopener" href="https://www.runoob.com/python3/python3-tutorial.html">Python</a>，因为 PyTorch 主要是用 Python 编写的。</li>
<li><strong>数学基础</strong>：了解线性代数、概率论和统计学、微积分等基础数学知识，这些是理解和实现机器学习算法的基石。</li>
<li><strong>机器学习基础</strong>：了解机器学习的基本概念，如监督学习、无监督学习、强化学习、模型评估指标（准确率、召回率、F1分数等）。</li>
<li><strong>深度学习基础</strong>：熟悉神经网络的基本概念，包括前馈神经网络、卷积神经网络（CNN）、循环神经网络（RNN）、长短期记忆网络（LSTM）等。</li>
<li><strong>计算机视觉和自然语言处理基础</strong>：如果你打算在这些领域应用 PyTorch，了解相关的背景知识会很有帮助。</li>
<li><strong>Linux&#x2F;Unix 基础</strong>：虽然不是必需的，但了解 Linux&#x2F;Unix 操作系统的基础知识可以帮助你更有效地使用命令行工具和脚本，特别是在数据预处理和模型训练中。</li>
<li><strong>英语阅读能力</strong>：由于许多文档、教程和社区讨论都是用英语进行的，具备一定的英语阅读能力将有助于你更好地学习和解决问题。</li>
</ul>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>下面的是 PyTorch 中一些<strong>基本的张量操作</strong>：如何创建随机张量、进行逐元素运算、访问特定元素以及计算总和和最大值。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 设置数据类型和设备</span></span><br><span class="line">dtype = torch.<span class="built_in">float</span>  <span class="comment"># 张量数据类型为浮点型</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)  <span class="comment"># 本次计算在 CPU 上进行</span></span><br><span class="line"><span class="comment"># 创建并打印两个随机张量 a 和 b</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">3</span>, device=device, dtype=dtype)  <span class="comment"># 创建一个 2x3 的随机张量</span></span><br><span class="line">b = torch.randn(<span class="number">2</span>, <span class="number">3</span>, device=device, dtype=dtype)  <span class="comment"># 创建另一个 2x3 的随机张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 a:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 b:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># 逐元素相乘并输出结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;a 和 b 的逐元素乘积:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a * b)</span><br><span class="line"><span class="comment"># 输出张量 a 所有元素的总和</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 a 所有元素的总和:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># 输出张量 a 中第 2 行第 3 列的元素（注意索引从 0 开始）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 a 第 2 行第 3 列的元素:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 输出张量 a 中的最大值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 a 中的最大值:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">max</span>())</span><br></pre></td></tr></table></figure>

<p><strong>创建张量：</strong></p>
<ul>
<li><code>torch.randn(2, 3)</code> 创建一个 2 行 3 列的张量，<strong>填充随机数</strong>（遵循<strong>正态分布</strong>）。</li>
<li><code>device=device</code> 和 <code>dtype=dtype</code> 分别指定了<strong>计算设备</strong>（CPU 或 GPU）和<strong>数据类型</strong>（浮点型）。</li>
</ul>
<p><strong>张量操作：</strong></p>
<ul>
<li><code>a * b</code>：<strong>逐</strong>元素相乘。</li>
<li><code>a.sum()</code>：计算张量 <code>a</code> 所有元素的和。</li>
<li><code>a[1, 2]</code>：访问张量 <code>a</code> 第 2 行第 3 列的元素（注意索引从 0 开始）。</li>
<li><code>a.max()</code>：获取张量 <code>a</code> 中的最大值。</li>
</ul>
<p>输出：（<strong>每次</strong>运行时值会有所<strong>不同</strong>）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">张量 a:</span><br><span class="line">tensor([[-0.1460, -0.3490,  0.3705],</span><br><span class="line">        [-1.1141,  0.7661,  1.0823]])</span><br><span class="line">张量 b:</span><br><span class="line">tensor([[ 0.6901, -0.9663,  0.3634],</span><br><span class="line">        [-0.6538, -0.3728, -1.1323]])</span><br><span class="line">a 和 b 的逐元素乘积:</span><br><span class="line">tensor([[-0.1007,  0.3372,  0.1346],</span><br><span class="line">        [ 0.7284, -0.2856, -1.2256]])</span><br><span class="line">张量 a 所有元素的总和:</span><br><span class="line">tensor(0.6097)</span><br><span class="line">张量 a 第 2 行第 3 列的元素:</span><br><span class="line">tensor(1.0823)</span><br><span class="line">张量 a 中的最大值:</span><br><span class="line">tensor(1.0823)</span><br></pre></td></tr></table></figure>

<h1 id="PyTorch-简介"><a href="#PyTorch-简介" class="headerlink" title="PyTorch 简介"></a>PyTorch 简介</h1><p>PyTorch 是一个开源的 Python 机器学习库，<strong>基于 Torch 库</strong>，**底层由C++**实现，应用于人工智能领域，如计算机视觉和自然语言处理。</p>
<p>PyTorch 最初由 Meta Platforms 的人工智能研究团队开发，现在属 于Linux 基金会的一部分。</p>
<p>许多深度学习软件都是基于 PyTorch 构建的，包括特斯拉自动驾驶、Uber 的 Pyro、<strong>Hugging Face 的 Transformers</strong>、 PyTorch Lightning 和 Catalyst。</p>
<p><strong>PyTorch 主要有两大特征：</strong></p>
<ul>
<li>类似于 NumPy 的<strong>张量计算</strong>，能在 GPU 或 MPS 等硬件加速器上加速。</li>
<li>基于带自动微分系统的<strong>深度神经网络</strong>。</li>
</ul>
<p>PyTorch 包括 torch.autograd、torch.nn、torch.optim 等子模块。</p>
<p>PyTorch 包含多种损失函数，包括 MSE（均方误差 &#x3D; L2 范数）、交叉熵损失和负熵似然损失（对分类器有用）等。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1567769062953.png" alt="img"></p>
<h2 id="PyTorch-特性"><a href="#PyTorch-特性" class="headerlink" title="PyTorch 特性"></a>PyTorch 特性</h2><ul>
<li><strong>动态计算图（Dynamic Computation Graphs）</strong>： PyTorch 的计算图是动态的，这意味着它们<strong>在运行时构建</strong>，并且<strong>可以随时改变</strong>。这为实验和调试提供了极大的灵活性，因为开发者可以逐行执行代码，查看中间结果。</li>
<li><strong>自动微分（Automatic Differentiation）</strong>： PyTorch 的自动微分系统允许开发者轻松地<strong>计算梯度</strong>，这对于训练深度学习模型至关重要。它通过反向传播算法自动计算出<strong>损失函数对模型参数的梯度</strong>。</li>
<li><strong>张量计算（Tensor Computation）</strong>： PyTorch 提供了类似于 NumPy 的张量操作，这些操作可以在 CPU 和 GPU 上执行，从而<strong>加速</strong>计算过程。张量是 PyTorch 中的<strong>基本数据结构</strong>，用于存储和操作数据。</li>
<li><strong>丰富的 API</strong>： PyTorch 提供了大量的<strong>预定义层</strong>、<strong>损失函数</strong>和<strong>优化算法</strong>，这些都是构建深度学习模型的常用组件。</li>
<li><strong>多语言支持</strong>： PyTorch 虽然以 Python 为主要接口，但也提供了 C++ 接口，<strong>允许更底层</strong>的集成和控制。</li>
</ul>
<h3 id="动态计算图（Dynamic-Computation-Graph）"><a href="#动态计算图（Dynamic-Computation-Graph）" class="headerlink" title="动态计算图（Dynamic Computation Graph）"></a>动态计算图（Dynamic Computation Graph）</h3><p>PyTorch 最显著的特点之一是其<strong>动态计算图</strong>的机制。</p>
<p>与 TensorFlow 的静态计算图（graph）不同，PyTorch 在执行时构建计算图，这意味着在每次计算时，图都会根据输入数据的形状自动变化。</p>
<p><strong>动态计算图的优点：</strong></p>
<ul>
<li>更加<strong>灵活</strong>，特别适用于需要条件判断或递归的场景。</li>
<li>方便调试和修改，能够直接查看中间结果。</li>
<li>更接近 Python 编程的风格，易于上手。</li>
</ul>
<h3 id="张量（Tensor）与自动求导（Autograd）"><a href="#张量（Tensor）与自动求导（Autograd）" class="headerlink" title="张量（Tensor）与自动求导（Autograd）"></a>张量（Tensor）与自动求导（Autograd）</h3><p>PyTorch 中的核心数据结构是 <strong>张量（Tensor）</strong>，它是一个<strong>多维矩阵</strong>，可以在 CPU 或 GPU 上高效地进行计算。张量的操作**支持自动求导（Autograd）**机制，使得在反向传播过程中自动计算梯度，这对于深度学习中的梯度下降优化算法至关重要。</p>
<p><strong>张量（Tensor）：</strong></p>
<ul>
<li>支持在 <strong>CPU 和 GPU 之间</strong>进行切换。</li>
<li>提供了类似 NumPy 的接口，支持元素级运算。</li>
<li>支持自动求导，可以方便地进行梯度计算。</li>
</ul>
<p><strong>自动求导（Autograd）：</strong></p>
<ul>
<li>PyTorch 内置的自动求导引擎，能够<strong>自动追踪</strong>所有张量的操作，并在反向传播时计算梯度。</li>
<li>通过 <code>requires_grad</code> 属性，<strong>可以指定</strong>张量需要计算梯度。</li>
<li>支持高效的反向传播，适用于神经网络的训练。</li>
</ul>
<h3 id="模型定义与训练"><a href="#模型定义与训练" class="headerlink" title="模型定义与训练"></a>模型定义与训练</h3><p>PyTorch 提供了 <code>torch.nn</code> 模块，允许用户通过继承 <code>nn.Module</code> 类来定义神经网络模型。使用 <code>forward</code> 函数指定前向传播，自动反向传播（通过 <code>autograd</code>）和梯度计算也由 PyTorch 内部处理。</p>
<p><strong>神经网络模块（torch.nn）：</strong></p>
<ul>
<li>提供了<strong>常用的层</strong>（如线性层、卷积层、池化层等）。</li>
<li>支持定义复杂的神经网络架构（包括多输入、多输出的网络）。</li>
<li>兼容与优化器（如 <code>torch.optim</code>）一起使用。</li>
</ul>
<h3 id="GPU-加速"><a href="#GPU-加速" class="headerlink" title="GPU 加速"></a>GPU 加速</h3><p>PyTorch 完全支持在 GPU 上运行，以加速深度学习模型的训练。通过<strong>简单的 <code>.to(device)</code> 方法</strong>，用户可以将模型和张量转移到 GPU 上进行计算。PyTorch <strong>支持多 GPU</strong> 训练，能够利用 <strong>NVIDIA CUDA</strong> 技术显著提高计算效率。</p>
<p><strong>GPU 支持：</strong></p>
<ul>
<li>自动选择 GPU 或 CPU。</li>
<li>支持通过 CUDA 加速运算。</li>
<li>支持多 GPU 并行计算（<code>DataParallel</code> 或 <code>torch.distributed</code>）。</li>
</ul>
<h3 id="生态系统与社区支持"><a href="#生态系统与社区支持" class="headerlink" title="生态系统与社区支持"></a>生态系统与社区支持</h3><p>PyTorch 作为一个开源项目，拥有一个庞大的社区和生态系统。它不仅在学术界得到了广泛的应用，也在工业界，特别是在计算机视觉、自然语言处理等领域中得到了广泛部署。PyTorch 还提供了许多与深度学习相关的工具和库，如：</p>
<ul>
<li><strong>torchvision</strong>：用于<strong>计算机视觉任务</strong>的<strong>数据集</strong>和<strong>模型</strong>。</li>
<li><strong>torchtext</strong>：用于<strong>自然语言处理任务</strong>的<strong>数据集</strong>和<strong>预处理工具</strong>。</li>
<li><strong>torchaudio</strong>：用于音频处理的工具包。</li>
<li><strong>PyTorch Lightning</strong>：一种<strong>简化</strong> PyTorch 代码的<strong>高层库</strong>，专注于研究和实验的快速迭代。</li>
</ul>
<h3 id="与其他框架的对比"><a href="#与其他框架的对比" class="headerlink" title="与其他框架的对比"></a>与其他框架的对比</h3><p>PyTorch 由于其灵活性、易用性和社区支持，已经成为很多深度学习研究者和开发者的首选框架。</p>
<p><strong>TensorFlow vs PyTorch：</strong></p>
<ul>
<li>PyTorch 的动态计算图使得它更加灵活，适合<strong>快速实验和研究</strong>；而 TensorFlow 的静态计算图在生产环境中更具优化空间。</li>
<li>PyTorch 在调试时更加方便，TensorFlow 则在部署上更加成熟，支持更广泛的硬件和平台。</li>
<li>近年来，TensorFlow 也引入了动态图（如 TensorFlow 2.x），使得两者在功能上趋于接近。</li>
<li>其他深度学习框架，如 Keras、Caffe 等也有一定应用，但 PyTorch 由于其灵活性、易用性和社区支持，已经成为很多深度学习研究者和开发者的首选框架。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">特性</th>
<th align="left"><strong>TensorFlow</strong></th>
<th align="left"><strong>PyTorch</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>开发公司</strong></td>
<td align="left">Google</td>
<td align="left">Facebook (FAIR)</td>
</tr>
<tr>
<td align="left"><strong>计算图类型</strong></td>
<td align="left">静态计算图（定义后再执行）</td>
<td align="left">动态计算图（定义即执行）</td>
</tr>
<tr>
<td align="left"><strong>灵活性</strong></td>
<td align="left">低（计算图在编译时构建，不易修改）</td>
<td align="left">高（计算图在执行时动态创建，易于修改和调试）</td>
</tr>
<tr>
<td align="left"><strong>调试</strong></td>
<td align="left">较难（需要使用 <code>tf.debugging</code> 或外部工具调试）</td>
<td align="left">容易（可以<strong>直接在 Python 中进行调试</strong>）</td>
</tr>
<tr>
<td align="left"><strong>易用性</strong></td>
<td align="left">低（较复杂，API 较多，学习曲线较陡峭）</td>
<td align="left">高（API 简洁，语法更加接近 Python，容易上手）</td>
</tr>
<tr>
<td align="left"><strong>部署</strong></td>
<td align="left">强（支持广泛的硬件，如 TensorFlow Lite、TensorFlow.js）</td>
<td align="left">较弱（部署工具和平台相对较少，虽然有 TensorFlow 支持）</td>
</tr>
<tr>
<td align="left"><strong>社区支持</strong></td>
<td align="left">很强（成熟且庞大的社区，广泛的教程和文档）</td>
<td align="left">很强（社区活跃，特别是在学术界，快速发展的生态）</td>
</tr>
<tr>
<td align="left"><strong>模型训练</strong></td>
<td align="left">支持分布式训练，支持多种设备（如 CPU、GPU、TPU）</td>
<td align="left">支持分布式训练，支持多 GPU、CPU 和 TPU</td>
</tr>
<tr>
<td align="left"><strong>API 层级</strong></td>
<td align="left">高级API：Keras；低级API：TensorFlow Core</td>
<td align="left">高级API：TorchVision、TorchText 等；低级API：Torch</td>
</tr>
<tr>
<td align="left"><strong>性能</strong></td>
<td align="left">高（优化方面成熟，适合生产环境）</td>
<td align="left">高（适合<strong>研究</strong>和<strong>原型</strong>开发，生产性能也在提升）</td>
</tr>
<tr>
<td align="left"><strong>自动求导</strong></td>
<td align="left">通过 <code>tf.GradientTape</code> 实现动态求导（较复杂）</td>
<td align="left">通过 <code>autograd</code> 动态求导（更简洁和直观）</td>
</tr>
<tr>
<td align="left"><strong>调优与可扩展性</strong></td>
<td align="left">强（支持在多平台上运行，如 TensorFlow Serving 等）</td>
<td align="left">较弱（虽然在学术和实验环境中表现优越，但生产环境支持相对较少）</td>
</tr>
<tr>
<td align="left"><strong>框架灵活性</strong></td>
<td align="left">较低（TensorFlow 2.x 引入了动态图特性，但仍不完全灵活）</td>
<td align="left">高（动态图带来更高的灵活性）</td>
</tr>
<tr>
<td align="left"><strong>支持多种语言</strong></td>
<td align="left">支持多种语言（Python, C++, Java, JavaScript, etc.）</td>
<td align="left">主要支持 Python（但也有 C++ API）</td>
</tr>
<tr>
<td align="left"><strong>兼容性与迁移</strong></td>
<td align="left">TensorFlow 2.x 与旧版本兼容性较好</td>
<td align="left">与 TensorFlow 兼容性差，迁移较难</td>
</tr>
</tbody></table>
<p>PyTorch 是一个强大且灵活的深度学习框架，适合学术研究和工业应用。它的动态计算图、自动求导机制、GPU 加速等特点，使得其成为深度学习研究和实验中不可或缺的工具。</p>
<h1 id="PyTorch-基础"><a href="#PyTorch-基础" class="headerlink" title="PyTorch 基础"></a>PyTorch 基础</h1><p>PyTorch 是一个开源的深度学习框架，以其灵活性和动态计算图而广受欢迎。</p>
<p>PyTorch 主要有以下几个基础概念：张量（Tensor）、自动求导（Autograd）、神经网络模块（nn.Module）、优化器（optim）等。</p>
<ul>
<li><strong>张量（Tensor）</strong>：PyTorch 的核心数据结构，支持多维数组，并可以在 CPU 或 GPU 上进行加速计算。</li>
<li><strong>自动求导（Autograd）</strong>：PyTorch 提供了自动求导功能，可以轻松计算模型的梯度，便于进行反向传播和优化。</li>
<li><strong>神经网络（nn.Module）</strong>：PyTorch 提供了简单且强大的 API 来构建神经网络模型，可以方便地进行前向传播和模型定义。</li>
<li><strong>优化器（Optimizers）</strong>：使用优化器（如 Adam、SGD 等）来更新模型的参数，使得损失最小化。</li>
<li><strong>设备（Device）</strong>：可以将模型和张量移动到 GPU 上以加速计算。</li>
</ul>
<h2 id="张量（Tensor）"><a href="#张量（Tensor）" class="headerlink" title="张量（Tensor）"></a>张量（Tensor）</h2><p>张量（Tensor）是 PyTorch 中的核心数据结构，用于存储和操作多维数组。</p>
<p>张量可以视为一个多维数组，支持加速计算的操作。</p>
<p>在 PyTorch 中，张量的概念类似于 NumPy 中的数组，但是 PyTorch 的张量可以运行在不同的设备上，比如 CPU 和 GPU，这使得它们非常适合于进行大规模并行计算，特别是在深度学习领域。</p>
<ul>
<li><strong>维度（Dimensionality）</strong>：张量的维度指的是<strong>数据的多维数组结构</strong>。例如，一个标量（0维张量）是一个单独的数字，一个向量（1维张量）是一个一维数组，一个矩阵（2维张量）是一个二维数组，以此类推。</li>
<li><strong>形状（Shape）</strong>：张量的形状是指每个维度上的大小。例如，一个形状为<code>(3, 4)</code>的张量意味着它有3行4列。</li>
<li><strong>数据类型（Dtype）</strong>：张量中的数据类型定义了存储每个元素所需的内存大小和解释方式。PyTorch支持多种数据类型，包括整数型（如<code>torch.int8</code>、<code>torch.int32</code>）、浮点型（如<code>torch.float32</code>、<code>torch.float64</code>）和布尔型（<code>torch.bool</code>）。</li>
</ul>
<p><strong>张量创建：</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个 2x3 的全 0 张量</span></span><br><span class="line">a = torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 创建一个 2x3 的全 1 张量</span></span><br><span class="line">b = torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># 创建一个 2x3 的随机数张量</span></span><br><span class="line">c = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="comment"># 从 NumPy 数组创建张量</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">numpy_array = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tensor_from_numpy = torch.from_numpy(numpy_array)</span><br><span class="line"><span class="built_in">print</span>(tensor_from_numpy)</span><br><span class="line"><span class="comment"># 在指定设备（CPU/GPU）上创建张量</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">d = torch.randn(<span class="number">2</span>, <span class="number">3</span>, device=device)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br></pre></td></tr></table></figure>

<p><strong>常用张量操作：</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 张量相加</span></span><br><span class="line">e = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">f = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(e + f)</span><br><span class="line"><span class="comment"># 逐元素乘法(不同于矩阵乘法)</span></span><br><span class="line"><span class="built_in">print</span>(e * f)</span><br><span class="line"><span class="comment"># 张量的转置</span></span><br><span class="line">g = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(g.t())  <span class="comment"># 或者 g.transpose(0, 1)</span></span><br><span class="line"><span class="comment"># 张量的形状</span></span><br><span class="line"><span class="built_in">print</span>(g.shape)  <span class="comment"># 返回形状</span></span><br></pre></td></tr></table></figure>

<h3 id="张量与设备"><a href="#张量与设备" class="headerlink" title="张量与设备"></a>张量与设备</h3><p>PyTorch 张量可以存在于不同的设备上，包括CPU和GPU，你可以将张量<strong>移动到 GPU 上</strong>以加速计算：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if torch.cuda.is_available():</span><br><span class="line">    tensor_gpu = tensor_from_list.to(&#x27;cuda&#x27;)  # 将张量移动到GPU</span><br></pre></td></tr></table></figure>

<h3 id="梯度和自动微分"><a href="#梯度和自动微分" class="headerlink" title="梯度和自动微分"></a>梯度和自动微分</h3><p>PyTorch的张量<strong>支持自动微分</strong>，这是深度学习中的关键特性。当你创建一个<strong>需要梯度</strong>的张量时，PyTorch可以<strong>自动计算其梯度</strong>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个需要梯度的张量</span></span><br><span class="line">tensor_requires_grad = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 进行一些操作</span></span><br><span class="line">tensor_result = tensor_requires_grad * <span class="number">2</span></span><br><span class="line"><span class="comment"># 计算梯度</span></span><br><span class="line">tensor_result.backward()</span><br><span class="line"><span class="built_in">print</span>(tensor_requires_grad.grad)  <span class="comment"># 输出梯度</span></span><br></pre></td></tr></table></figure>

<h3 id="内存和性能"><a href="#内存和性能" class="headerlink" title="内存和性能"></a>内存和性能</h3><p>PyTorch 张量还提供了一些内存管理功能，比如.clone()、.detach() 和 .to() 方法，它们可以帮助你优化内存使用和提高性能。</p>
<hr>
<h2 id="自动求导（Autograd）"><a href="#自动求导（Autograd）" class="headerlink" title="自动求导（Autograd）"></a>自动求导（Autograd）</h2><p>自动求导（Automatic Differentiation，简称Autograd）是深度学习框架中的一个核心特性，它允许计算机自动计算数学函数的导数。</p>
<p>在深度学习中，自动求导主要用于两个方面：<strong>一是在训练神经网络时计算梯度</strong>，<strong>二是进行反向传播算法的实现</strong>。</p>
<p>自动求导基于链式法则（Chain Rule），这是一个用于计算复杂函数导数的数学法则。链式法则表明，复合函数的导数是其各个组成部分导数的乘积。在深度学习中，模型<strong>通常是由许多层组成的复杂函数</strong>，自动求导能够高效地计算这些层的梯度。</p>
<p><strong>动态图与静态图：</strong></p>
<ul>
<li><strong>动态图（Dynamic Graph）</strong>：在动态图中，计算图在运行时动态构建。每次执行操作时，计算图都会更新，这使得调试和修改模型变得更加容易。PyTorch使用的是<strong>动态图</strong>。</li>
<li><strong>静态图（Static Graph）</strong>：在静态图中，计算图在开始执行之前构建完成，并且不会改变。TensorFlow最初使用的是静态图，但后来也支持动态图。</li>
</ul>
<p>PyTorch 提供了自动求导功能，通过 autograd 模块来自动计算梯度。</p>
<p>torch.Tensor 对象有一个 <strong>requires_grad</strong> 属性，用于指示<strong>是否需要计算该张量的梯度</strong>。</p>
<p>当你创建一个 requires_grad&#x3D;True 的张量时，PyTorch 会自动跟踪所有对它的操作，以便在之后计算梯度。</p>
<p>创建需要梯度的张量:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个需要计算梯度的张量</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># 执行某些操作</span></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure>

<h3 id="反向传播（Backpropagation）"><a href="#反向传播（Backpropagation）" class="headerlink" title="反向传播（Backpropagation）"></a>反向传播（Backpropagation）</h3><p>一旦定义了计算图，可以通过 <strong>.backward()</strong> 方法来计算梯度。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">out.backward()</span><br><span class="line"><span class="comment"># 查看 x 的梯度</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>

<p>在神经网络训练中，<strong>自动求导</strong>主要用于实现<strong>反向传播</strong>算法。</p>
<p>反向传播是一种通过<strong>计算损失函数关于网络参数的梯度</strong>来训练神经网络的方法。在<strong>每次迭代</strong>中，网络的<strong>前向</strong>传播会<strong>计算输出和损失</strong>，然后反向传播会<strong>计算损失关于每个参数的梯度</strong>，并<strong>使用这些梯度来更新</strong>参数。</p>
<h3 id="停止梯度计算"><a href="#停止梯度计算" class="headerlink" title="停止梯度计算"></a>停止梯度计算</h3><p>如果你不希望某些张量的梯度被计算（例如，当你<strong>不需要反向传播</strong>时），可以使用 <strong>torch.no_grad()</strong> 或设置 <strong>requires_grad&#x3D;False</strong>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 torch.no_grad() 禁用梯度计算</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y = x * <span class="number">2</span></span><br></pre></td></tr></table></figure>

<h2 id="神经网络（nn-Module）"><a href="#神经网络（nn-Module）" class="headerlink" title="神经网络（nn.Module）"></a>神经网络（nn.Module）</h2><p>神经网络是一种<strong>模仿人脑神经元连接的计算模型</strong>，由多层节点（神经元）组成，用于学习数据之间的复杂模式和关系。</p>
<p>神经网络通过调整<strong>神经元之间的连接权重</strong>来优化预测结果，这一过程涉及前向传播、损失计算、反向传播和参数更新。</p>
<p>神经网络的类型包括<strong>前馈神经网络</strong>、<strong>卷积神经网络（CNN）</strong>、<strong>循环神经网络（RNN）<strong>和</strong>长短期记忆网络（LSTM）</strong>，它们在图像识别、语音处理、自然语言处理等多个领域都有广泛应用。</p>
<p>PyTorch 提供了一个非常方便的接口来<strong>构建神经网络模</strong>型，即 <strong>torch.nn.Module</strong>。</p>
<p>我们可以继承 nn.Module 类并定义自己的网络层。</p>
<p>创建一个简单的神经网络：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 定义一个简单的全连接神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 输入层到隐藏层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 隐藏层到输出层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># ReLU 激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建网络实例</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"><span class="comment"># 打印模型结构</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<p><strong>训练过程：</strong></p>
<ol>
<li><strong>前向传播（Forward Propagation）</strong>： 在前向传播阶段，<strong>输入</strong>数据通过网络层传递，每层<strong>应用权重和激活函数</strong>，<strong>直到产生输出</strong>。</li>
<li><strong>计算损失（Calculate Loss）</strong>： 根据网络的<strong>输出</strong>和<strong>真实</strong>标签，计算<strong>损失函数的值</strong>。</li>
<li><strong>反向传播（Backpropagation）</strong>： 反向传播利用<strong>自动求导</strong>技术计算损失函数关于每个参数的<strong>梯度</strong>。</li>
<li><strong>参数更新（Parameter Update）</strong>： 使用<strong>优化器</strong>根据梯度更新网络的权重和偏置。</li>
<li><strong>迭代（Iteration）</strong>： 重复上述过程，直到模型在训练数据上的性能达到满意的水平。</li>
</ol>
<h3 id="前向传播与损失计算"><a href="#前向传播与损失计算" class="headerlink" title="前向传播与损失计算"></a>前向传播与损失计算</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机输入</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = model(x)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="comment"># 定义损失函数（例如均方误差 MSE）</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"><span class="comment"># 假设目标值为 1</span></span><br><span class="line">target = torch.randn(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure>

<h3 id="优化器（Optimizers）"><a href="#优化器（Optimizers）" class="headerlink" title="优化器（Optimizers）"></a>优化器（Optimizers）</h3><p>优化器在训练过程中更新神经网络的参数，以减少损失函数的值。</p>
<p>PyTorch 提供了多种优化器，例如 <strong>SGD、Adam</strong> 等。</p>
<p>使用优化器进行参数更新：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义优化器（使用 Adam 优化器）</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 训练步骤</span></span><br><span class="line">optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">optimizer.step()  <span class="comment"># 更新参数</span></span><br></pre></td></tr></table></figure>

<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>训练模型是机器学习和深度学习中的核心过程，旨在通过大量数据学习模型参数，以便模型能够对新的、未见过的数据做出准确的预测。</p>
<p>训练模型通常包括以下几个步骤：</p>
<ol>
<li><strong>数据准备</strong>：<ul>
<li>收集和处理数据，包括清洗、标准化和归一化。</li>
<li>将数据分为训练集、验证集和测试集。</li>
</ul>
</li>
<li><strong>定义模型</strong>：<ul>
<li>选择<strong>模型架构</strong>，例如决策树、神经网络等。</li>
<li><strong>初始化模型参数</strong>（权重和偏置）。</li>
</ul>
</li>
<li><strong>选择损失函数</strong>：<ul>
<li><strong>根据任务类型</strong>（如分类、回归）选择合适的损失函数。</li>
</ul>
</li>
<li><strong>选择优化器</strong>：<ul>
<li><strong>选择一个优化算法</strong>，如SGD、Adam等，来更新模型参数。</li>
</ul>
</li>
<li><strong>前向传播</strong>：<ul>
<li>在每次迭代中，将输入数据<strong>通过模型传递</strong>，计算预测输出。</li>
</ul>
</li>
<li><strong>计算损失</strong>：<ul>
<li>使用<strong>损失函数评估</strong>预测输出与真实标签之间的差异。</li>
</ul>
</li>
<li><strong>反向传播</strong>：<ul>
<li>利用自动求导计算损失相对于模型参数的梯度。</li>
</ul>
</li>
<li><strong>参数更新</strong>：<ul>
<li>根据计算出的梯度和优化器的策略更新模型参数。</li>
</ul>
</li>
<li><strong>迭代优化</strong>：<ul>
<li><strong>重复步骤5-8</strong>，直到模型在验证集上的性能不再提升或达到预定的迭代次数。</li>
</ul>
</li>
<li><strong>评估和测试</strong>：<ul>
<li>使用测试集评估模型的最终性能，确保模型没有过拟合。</li>
</ul>
</li>
<li><strong>模型调优</strong>：<ul>
<li>根据模型在测试集上的表现进行调参，如改变学习率、增加正则化等。</li>
</ul>
</li>
<li><strong>部署模型</strong>：<ul>
<li>将训练好的模型部署到生产环境中，用于实际的预测任务。</li>
</ul>
</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 1. 定义一个简单的神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 输入层到隐藏层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 隐藏层到输出层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># ReLU 激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 2. 创建模型实例</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"><span class="comment"># 3. 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.MSELoss()  <span class="comment"># 均方误差损失函数</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)  <span class="comment"># Adam 优化器</span></span><br><span class="line"><span class="comment"># 4. 假设我们有训练数据 X 和 Y</span></span><br><span class="line">X = torch.randn(<span class="number">10</span>, <span class="number">2</span>)  <span class="comment"># 10 个样本，2 个特征</span></span><br><span class="line">Y = torch.randn(<span class="number">10</span>, <span class="number">1</span>)  <span class="comment"># 10 个目标值</span></span><br><span class="line"><span class="comment"># 5. 训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 训练 100 轮</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清空之前的梯度</span></span><br><span class="line">    output = model(X)  <span class="comment"># 前向传播</span></span><br><span class="line">    loss = criterion(output, Y)  <span class="comment"># 计算损失</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="comment"># 每 10 轮输出一次损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/100], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>在每 10 轮，程序会输出当前的损失值，帮助我们跟踪模型的<strong>训练进度</strong>。随着训练的进行，损失值应该会逐渐降低，表示模型在不断学习并优化其参数。</p>
<p>训练模型是一个迭代的过程，需要不断地调整和优化，直到达到满意的性能。这个过程涉及到大量的实验和调优，目的是使模型在新的、未见过的数据上也能有良好的泛化能力。</p>
<hr>
<h2 id="设备（Device）"><a href="#设备（Device）" class="headerlink" title="设备（Device）"></a>设备（Device）</h2><p>PyTorch 允许你将模型和数据移动到 GPU 上进行加速。</p>
<p>使用 <strong>torch.device</strong> 来指定计算设备。</p>
<p>将模型和数据移至 GPU:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># 将模型移动到设备</span></span><br><span class="line">model.to(device)</span><br><span class="line"><span class="comment"># 将数据移动到设备</span></span><br><span class="line">X = X.to(device)</span><br><span class="line">Y = Y.to(device)</span><br></pre></td></tr></table></figure>

<h1 id="PyTorch-张量（Tensor）"><a href="#PyTorch-张量（Tensor）" class="headerlink" title="PyTorch 张量（Tensor）"></a>PyTorch 张量（Tensor）</h1><p>张量是一个多维数组，可以是标量、向量、矩阵或更高维度的数据结构。</p>
<p>在 PyTorch 中，张量（Tensor）是<strong>数据的核心表示形式</strong>，类似于 NumPy 的多维数组，但具有更强大的功能，例如<strong>支持 GPU 加速</strong>和自动梯度计算。</p>
<p>张量支持多种数据类型（整型、浮点型、布尔型等）。</p>
<p>张量可以存储在 CPU 或 GPU 中，GPU 张量可显著加速计算。</p>
<p>下图展示了不同维度的张量（Tensor）在 PyTorch 中的表示方法：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1__D5ZvufDS38WkhK9rK32hQ.jpg" alt="img"></p>
<p><strong>说明：</strong></p>
<ul>
<li><strong>1D Tensor &#x2F; Vector（一维张量&#x2F;向量）:</strong> 最基本的张量形式，可以看作是一个数组，图中的例子是一个包含 10 个元素的向量。</li>
<li><strong>2D Tensor &#x2F; Matrix（二维张量&#x2F;矩阵）:</strong> 二维数组，通常用于表示矩阵，图中的例子是一个 4x5 的矩阵，包含了 20 个元素。</li>
<li><strong>3D Tensor &#x2F; Cube（三维张量&#x2F;立方体）:</strong> 三维数组，可以看作是由多个矩阵堆叠而成的立方体，图中的例子展示了一个 3x4x5 的立方体，其中每个 5x5 的矩阵代表立方体的一个”层”。</li>
<li><strong>4D Tensor &#x2F; Vector of Cubes（四维张量&#x2F;立方体向量）:</strong> 四维数组，可以看作是由多个立方体组成的向量，图中的例子没有具体数值，但可以理解为一个<strong>包含多个 3D 张量的集合</strong>。</li>
<li><strong>5D Tensor &#x2F; Matrix of Cubes（五维张量&#x2F;立方体矩阵）:</strong> 五维数组，可以看作是由多个4D张量组成的矩阵，图中的例子同样没有具体数值，但可以理解为一个包含多个 4D 张量的集合。</li>
</ul>
<h2 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h2><p>张量创建的方式有：</p>
<table>
<thead>
<tr>
<th align="left"><strong>方法</strong></th>
<th align="left"><strong>说明</strong></th>
<th align="left"><strong>示例代码</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.tensor(data)</code></td>
<td align="left"><strong>从 Python 列表或 NumPy 数组</strong>创建张量。</td>
<td align="left"><code>x = torch.tensor([[1, 2], [3, 4]])</code></td>
</tr>
<tr>
<td align="left"><code>torch.zeros(size)</code></td>
<td align="left">创建一个<strong>全为零</strong>的张量。</td>
<td align="left"><code>x = torch.zeros((2, 3))</code></td>
</tr>
<tr>
<td align="left"><code>torch.ones(size)</code></td>
<td align="left">创建一个<strong>全为 1</strong> 的张量。</td>
<td align="left"><code>x = torch.ones((2, 3))</code></td>
</tr>
<tr>
<td align="left"><code>torch.empty(size)</code></td>
<td align="left">创建一个<strong>未初始化</strong>的张量。</td>
<td align="left"><code>x = torch.empty((2, 3))</code></td>
</tr>
<tr>
<td align="left"><code>torch.rand(size)</code></td>
<td align="left">创建一个<strong>服从均匀分布的随机张量</strong>，值在 <code>[0, 1)</code>。</td>
<td align="left"><code>x = torch.rand((2, 3))</code></td>
</tr>
<tr>
<td align="left"><code>torch.randn(size)</code></td>
<td align="left">创建一个<strong>服从正态分布的随机张量</strong>，均值为 0，标准差为 1。</td>
<td align="left"><code>x = torch.randn((2, 3))</code></td>
</tr>
<tr>
<td align="left"><code>torch.arange(start, end, step)</code></td>
<td align="left">创建一个<strong>一维序列</strong>张量，类似于 Python 的 <code>range</code>。</td>
<td align="left"><code>x = torch.arange(0, 10, 2)</code></td>
</tr>
<tr>
<td align="left"><code>torch.linspace(start, end, steps)</code></td>
<td align="left">创建一个在指定范围内<strong>等间隔</strong>的序列张量。</td>
<td align="left"><code>x = torch.linspace(0, 1, 5)</code></td>
</tr>
<tr>
<td align="left"><code>torch.eye(size)</code></td>
<td align="left">创建一个<strong>单位矩阵</strong>（对角线为 1，其他为 0）。</td>
<td align="left"><code>x = torch.eye(3)</code></td>
</tr>
<tr>
<td align="left"><code>torch.from_numpy(ndarray)</code></td>
<td align="left"><strong>将 NumPy 数组转换</strong>为张量。</td>
<td align="left"><code>x = torch.from_numpy(np.array([1, 2, 3]))</code></td>
</tr>
</tbody></table>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tensor = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>

<p>如果你有一个 NumPy 数组，可以使用 torch.from_numpy() 将其转换为张量：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np_array = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor = torch.from_numpy(np_array)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>

<p>输出如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3])</span><br></pre></td></tr></table></figure>

<p>创建 2D 张量（矩阵）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tensor_2d = torch.tensor([</span><br><span class="line">    [-<span class="number">9</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">0</span>, <span class="number">12</span>, <span class="number">8</span>, <span class="number">6</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">23</span>, -<span class="number">6</span>, <span class="number">45</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">22</span>, <span class="number">3</span>, -<span class="number">1</span>, <span class="number">72</span>, <span class="number">6</span>]</span><br><span class="line">])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;2D Tensor (Matrix):\n&quot;</span>, tensor_2d)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor_2d.shape)  <span class="comment"># 形状</span></span><br></pre></td></tr></table></figure>

<p>其他维度的创建：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 3D 张量（立方体）</span></span><br><span class="line">tensor_3d = torch.stack([tensor_2d, tensor_2d + <span class="number">10</span>, tensor_2d - <span class="number">5</span>])  <span class="comment"># 堆叠 3 个 2D 张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;3D Tensor (Cube):\n&quot;</span>, tensor_3d)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor_3d.shape)  <span class="comment"># 形状</span></span><br><span class="line"><span class="comment"># 创建 4D 张量（向量的立方体）</span></span><br><span class="line">tensor_4d = torch.stack([tensor_3d, tensor_3d + <span class="number">100</span>])  <span class="comment"># 堆叠 2 个 3D 张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;4D Tensor (Vector of Cubes):\n&quot;</span>, tensor_4d)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor_4d.shape)  <span class="comment"># 形状</span></span><br><span class="line"><span class="comment"># 创建 5D 张量（矩阵的立方体）</span></span><br><span class="line">tensor_5d = torch.stack([tensor_4d, tensor_4d + <span class="number">1000</span>])  <span class="comment"># 堆叠 2 个 4D 张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;5D Tensor (Matrix of Cubes):\n&quot;</span>, tensor_5d)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor_5d.shape)  <span class="comment"># 形状</span></span><br></pre></td></tr></table></figure>

<h2 id="张量的属性"><a href="#张量的属性" class="headerlink" title="张量的属性"></a>张量的属性</h2><p>张量的属性如下表：</p>
<table>
<thead>
<tr>
<th align="left"><strong>属性</strong></th>
<th align="left"><strong>说明</strong></th>
<th align="left"><strong>示例</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>.shape</code></td>
<td align="left">获取张量的形状</td>
<td align="left"><code>tensor.shape</code></td>
</tr>
<tr>
<td align="left"><code>.size()</code></td>
<td align="left">获取张量的形状</td>
<td align="left"><code>tensor.size()</code></td>
</tr>
<tr>
<td align="left"><code>.dtype</code></td>
<td align="left">获取张量的<strong>数据类型</strong></td>
<td align="left"><code>tensor.dtype</code></td>
</tr>
<tr>
<td align="left"><code>.device</code></td>
<td align="left">查看张量<strong>所在的设备</strong> (CPU&#x2F;GPU)</td>
<td align="left"><code>tensor.device</code></td>
</tr>
<tr>
<td align="left"><code>.dim()</code></td>
<td align="left">获取张量的<strong>维度数</strong></td>
<td align="left"><code>tensor.dim()</code></td>
</tr>
<tr>
<td align="left"><code>.requires_grad</code></td>
<td align="left">是否启用<strong>梯度</strong>计算</td>
<td align="left"><code>tensor.requires_grad</code></td>
</tr>
<tr>
<td align="left"><code>.numel()</code></td>
<td align="left">获取张量中的<strong>元素总数</strong></td>
<td align="left"><code>tensor.numel()</code></td>
</tr>
<tr>
<td align="left"><code>.is_cuda</code></td>
<td align="left">检查张量<strong>是否在 GPU 上</strong></td>
<td align="left"><code>tensor.is_cuda</code></td>
</tr>
<tr>
<td align="left"><code>.T</code></td>
<td align="left">获取张量的<strong>转置</strong>（适用于 <strong>2D 张量</strong>）</td>
<td align="left"><code>tensor.T</code></td>
</tr>
<tr>
<td align="left"><code>.item()</code></td>
<td align="left">获取<strong>单元素张量的值</strong></td>
<td align="left"><code>tensor.item()</code></td>
</tr>
<tr>
<td align="left"><code>.is_contiguous()</code></td>
<td align="left">检查张量<strong>是否连续</strong>存储</td>
<td align="left"><code>tensor.is_contiguous()</code></td>
</tr>
</tbody></table>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个 2D 张量</span></span><br><span class="line">tensor = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 张量的属性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor:\n&quot;</span>, tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor.shape)  <span class="comment"># 获取形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Size:&quot;</span>, tensor.size())  <span class="comment"># 获取形状（另一种方法）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Data Type:&quot;</span>, tensor.dtype)  <span class="comment"># 数据类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Device:&quot;</span>, tensor.device)  <span class="comment"># 设备</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Dimensions:&quot;</span>, tensor.dim())  <span class="comment"># 维度数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Total Elements:&quot;</span>, tensor.numel())  <span class="comment"># 元素总数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Requires Grad:&quot;</span>, tensor.requires_grad)  <span class="comment"># 是否启用梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Is CUDA:&quot;</span>, tensor.is_cuda)  <span class="comment"># 是否在 GPU 上</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Is Contiguous:&quot;</span>, tensor.is_contiguous())  <span class="comment"># 是否连续存储</span></span><br><span class="line"><span class="comment"># 获取单元素值</span></span><br><span class="line">single_value = torch.tensor(<span class="number">42</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Single Element Value:&quot;</span>, single_value.item())</span><br><span class="line"><span class="comment"># 转置张量</span></span><br><span class="line">tensor_T = tensor.T</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Transposed Tensor:\n&quot;</span>, tensor_T)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">         [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">Shape: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Data <span class="type">Type</span>: torch.float32</span><br><span class="line">Device: cpu</span><br><span class="line">Dimensions: <span class="number">2</span></span><br><span class="line">Total Elements: <span class="number">6</span></span><br><span class="line">Requires Grad: <span class="literal">False</span></span><br><span class="line">Is CUDA: <span class="literal">False</span></span><br><span class="line">Is Contiguous: <span class="literal">True</span></span><br><span class="line">Single Element Value: <span class="number">42</span></span><br><span class="line">Transposed Tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">4.</span>],</span><br><span class="line">         [<span class="number">2.</span>, <span class="number">5.</span>],</span><br><span class="line">         [<span class="number">3.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure>

<h2 id="张量的操作"><a href="#张量的操作" class="headerlink" title="张量的操作"></a>张量的操作</h2><p>张量操作方法说明如下。</p>
<h4 id="基础操作："><a href="#基础操作：" class="headerlink" title="基础操作："></a>基础操作：</h4><table>
<thead>
<tr>
<th align="left"><strong>操作</strong></th>
<th align="left"><strong>说明</strong></th>
<th align="left"><strong>示例代码</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code></td>
<td align="left">元素级加法、减法、乘法、除法。</td>
<td align="left"><code>z = x + y</code></td>
</tr>
<tr>
<td align="left"><code>torch.matmul(x, y)</code></td>
<td align="left">矩阵乘法。</td>
<td align="left"><code>z = torch.matmul(x, y)</code></td>
</tr>
<tr>
<td align="left"><code>torch.dot(x, y)</code></td>
<td align="left">向量点积（仅适用于 1D 张量）。</td>
<td align="left"><code>z = torch.dot(x, y)</code></td>
</tr>
<tr>
<td align="left"><code>torch.sum(x)</code></td>
<td align="left">求和。</td>
<td align="left"><code>z = torch.sum(x)</code></td>
</tr>
<tr>
<td align="left"><code>torch.mean(x)</code></td>
<td align="left">求均值。</td>
<td align="left"><code>z = torch.mean(x)</code></td>
</tr>
<tr>
<td align="left"><code>torch.max(x)</code></td>
<td align="left">求最大值。</td>
<td align="left"><code>z = torch.max(x)</code></td>
</tr>
<tr>
<td align="left"><code>torch.min(x)</code></td>
<td align="left">求最小值。</td>
<td align="left"><code>z = torch.min(x)</code></td>
</tr>
<tr>
<td align="left"><code>torch.argmax(x, dim)</code></td>
<td align="left">返回<strong>最大值的索引</strong>（指定维度）。</td>
<td align="left"><code>z = torch.argmax(x, dim=1)</code></td>
</tr>
<tr>
<td align="left"><code>torch.softmax(x, dim)</code></td>
<td align="left">计算 softmax（指定维度）。</td>
<td align="left"><code>z = torch.softmax(x, dim=1)</code></td>
</tr>
</tbody></table>
<h4 id="形状操作"><a href="#形状操作" class="headerlink" title="形状操作"></a><strong>形状操作</strong></h4><table>
<thead>
<tr>
<th align="left"><strong>操作</strong></th>
<th align="left"><strong>说明</strong></th>
<th align="left"><strong>示例代码</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>x.view(shape)</code></td>
<td align="left">改变张量的<strong>形状</strong>（<strong>不改变数据</strong>）。</td>
<td align="left"><code>z = x.view(3, 4)</code></td>
</tr>
<tr>
<td align="left"><code>x.reshape(shape)</code></td>
<td align="left">类似于 <code>view</code>，但更灵活。</td>
<td align="left"><code>z = x.reshape(3, 4)</code></td>
</tr>
<tr>
<td align="left"><code>x.t()</code></td>
<td align="left"><strong>转置</strong>矩阵。</td>
<td align="left"><code>z = x.t()</code></td>
</tr>
<tr>
<td align="left"><code>x.unsqueeze(dim)</code></td>
<td align="left">在<strong>指定维度</strong>添加一个维度。</td>
<td align="left"><code>z = x.unsqueeze(0)</code></td>
</tr>
<tr>
<td align="left"><code>x.squeeze(dim)</code></td>
<td align="left">去掉指定维度为 1 的维度。</td>
<td align="left"><code>z = x.squeeze(0)</code></td>
</tr>
<tr>
<td align="left"><code>torch.cat((x, y), dim)</code></td>
<td align="left">按指定维度连接多个张量。</td>
<td align="left"><code>z = torch.cat((x, y), dim=1)</code></td>
</tr>
</tbody></table>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个 2D 张量</span></span><br><span class="line">tensor = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始张量:\n&quot;</span>, tensor)</span><br><span class="line"><span class="comment"># 1. **索引和切片操作**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【索引和切片】&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;获取第一行:&quot;</span>, tensor[<span class="number">0</span>])  <span class="comment"># 获取第一行</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;获取第一行第一列的元素:&quot;</span>, tensor[<span class="number">0</span>, <span class="number">0</span>])  <span class="comment"># 获取特定元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;获取第二列的所有元素:&quot;</span>, tensor[:, <span class="number">1</span>])  <span class="comment"># 获取第二列所有元素</span></span><br><span class="line"><span class="comment"># 2. **形状变换操作**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【形状变换】&quot;</span>)</span><br><span class="line">reshaped = tensor.view(<span class="number">3</span>, <span class="number">2</span>)  <span class="comment"># 改变张量形状为 3x2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;改变形状后的张量:\n&quot;</span>, reshaped)</span><br><span class="line">flattened = tensor.flatten()  <span class="comment"># 将张量展平成一维</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;展平后的张量:\n&quot;</span>, flattened)</span><br><span class="line"><span class="comment"># 3. **数学运算操作**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【数学运算】&quot;</span>)</span><br><span class="line">tensor_add = tensor + <span class="number">10</span>  <span class="comment"># 张量加法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量加 10:\n&quot;</span>, tensor_add)</span><br><span class="line">tensor_mul = tensor * <span class="number">2</span>  <span class="comment"># 张量乘法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量乘 2:\n&quot;</span>, tensor_mul)</span><br><span class="line">tensor_sum = tensor.<span class="built_in">sum</span>()  <span class="comment"># 计算所有元素的和</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量元素的和:&quot;</span>, tensor_sum.item())</span><br><span class="line"><span class="comment"># 4. **与其他张量的操作**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【与其他张量操作】&quot;</span>)</span><br><span class="line">tensor2 = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;另一个张量:\n&quot;</span>, tensor2)</span><br><span class="line">tensor_dot = torch.matmul(tensor, tensor2.T)  <span class="comment"># 张量矩阵乘法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵乘法结果:\n&quot;</span>, tensor_dot)</span><br><span class="line"><span class="comment"># 5. **条件判断和筛选**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【条件判断和筛选】&quot;</span>)</span><br><span class="line">mask = tensor &gt; <span class="number">3</span>  <span class="comment"># 创建一个布尔掩码</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;大于 3 的元素的布尔掩码:\n&quot;</span>, mask)</span><br><span class="line">filtered_tensor = tensor[tensor &gt; <span class="number">3</span>]  <span class="comment"># 筛选出符合条件的元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;大于 3 的元素:\n&quot;</span>, filtered_tensor)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">原始张量:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">         [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">【索引和切片】</span><br><span class="line">获取第一行: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">获取第一行第一列的元素: tensor(<span class="number">1.</span>)</span><br><span class="line">获取第二列的所有元素: tensor([<span class="number">2.</span>, <span class="number">5.</span>])</span><br><span class="line">【形状变换】</span><br><span class="line">改变形状后的张量:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">         [<span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">         [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">展平后的张量:</span><br><span class="line"> tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line">【数学运算】</span><br><span class="line">张量加 <span class="number">10</span>:</span><br><span class="line"> tensor([[<span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>],</span><br><span class="line">         [<span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>]])</span><br><span class="line">张量乘 <span class="number">2</span>:</span><br><span class="line"> tensor([[ <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>],</span><br><span class="line">         [ <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">12.</span>]])</span><br><span class="line">张量元素的和: <span class="number">21.0</span></span><br><span class="line">【与其他张量操作】</span><br><span class="line">另一个张量:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">矩阵乘法结果:</span><br><span class="line"> tensor([[ <span class="number">6.</span>,  <span class="number">6.</span>],</span><br><span class="line">         [<span class="number">15.</span>, <span class="number">15.</span>]])</span><br><span class="line">【条件判断和筛选】</span><br><span class="line">大于 <span class="number">3</span> 的元素的布尔掩码:</span><br><span class="line"> tensor([[<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">         [ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>]])</span><br><span class="line">大于 <span class="number">3</span> 的元素:</span><br><span class="line"> tensor([<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br></pre></td></tr></table></figure>

<h2 id="张量的-GPU-加速"><a href="#张量的-GPU-加速" class="headerlink" title="张量的 GPU 加速"></a>张量的 GPU 加速</h2><p>将张量转移到 GPU：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line">x = torch.tensor([1.0, 2.0, 3.0], device=device)</span><br></pre></td></tr></table></figure>

<p>检查 GPU 是否可用：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available()  # 返回 True 或 False</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="张量与-NumPy-的互操作"><a href="#张量与-NumPy-的互操作" class="headerlink" title="张量与 NumPy 的互操作"></a>张量与 NumPy 的互操作</h2><p>张量与 NumPy 的互操作如下表所示：</p>
<table>
<thead>
<tr>
<th align="left"><strong>操作</strong></th>
<th align="left"><strong>说明</strong></th>
<th align="left"><strong>示例代码</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.from_numpy(ndarray)</code></td>
<td align="left">将 NumPy 数组转换为张量。</td>
<td align="left"><code>x = torch.from_numpy(np_array)</code></td>
</tr>
<tr>
<td align="left"><code>x.numpy()</code></td>
<td align="left">将张量<strong>转换为 NumPy 数组</strong>（仅限 <strong>CPU 张量</strong>）。</td>
<td align="left"><code>np_array = x.numpy()</code></td>
</tr>
</tbody></table>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 1. NumPy 数组转换为 PyTorch 张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1. NumPy 转为 PyTorch 张量&quot;</span>)</span><br><span class="line">numpy_array = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy 数组:\n&quot;</span>, numpy_array)</span><br><span class="line"><span class="comment"># 使用 torch.from_numpy() 将 NumPy 数组转换为张量</span></span><br><span class="line">tensor_from_numpy = torch.from_numpy(numpy_array)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;转换后的 PyTorch 张量:\n&quot;</span>, tensor_from_numpy)</span><br><span class="line"><span class="comment"># 修改 NumPy 数组，观察张量的变化（共享内存）</span></span><br><span class="line">numpy_array[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;修改后的 NumPy 数组:\n&quot;</span>, numpy_array)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;PyTorch 张量也会同步变化:\n&quot;</span>, tensor_from_numpy)</span><br><span class="line"><span class="comment"># 2. PyTorch 张量转换为 NumPy 数组</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n2. PyTorch 张量转为 NumPy 数组&quot;</span>)</span><br><span class="line">tensor = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;PyTorch 张量:\n&quot;</span>, tensor)</span><br><span class="line"><span class="comment"># 使用 tensor.numpy() 将张量转换为 NumPy 数组</span></span><br><span class="line">numpy_from_tensor = tensor.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;转换后的 NumPy 数组:\n&quot;</span>, numpy_from_tensor)</span><br><span class="line"><span class="comment"># 修改张量，观察 NumPy 数组的变化（共享内存）</span></span><br><span class="line">tensor[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">77</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;修改后的 PyTorch 张量:\n&quot;</span>, tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy 数组也会同步变化:\n&quot;</span>, numpy_from_tensor)</span><br><span class="line"><span class="comment"># 3. 注意：不共享内存的情况（需要复制数据）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n3. 使用 clone() 保证独立数据&quot;</span>)</span><br><span class="line">tensor_independent = torch.tensor([[<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>], [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]], dtype=torch.float32)</span><br><span class="line">numpy_independent = tensor_independent.clone().numpy()  <span class="comment"># 使用 clone 复制数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始张量:\n&quot;</span>, tensor_independent)</span><br><span class="line">tensor_independent[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># 修改张量数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;修改后的张量:\n&quot;</span>, tensor_independent)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy 数组（不会同步变化）:\n&quot;</span>, numpy_independent)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> NumPy 转为 PyTorch 张量</span><br><span class="line">NumPy 数组:</span><br><span class="line"> [[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line">转换后的 PyTorch 张量:</span><br><span class="line"> tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">         [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">修改后的 NumPy 数组:</span><br><span class="line"> [[<span class="number">100</span>   <span class="number">2</span>   <span class="number">3</span>]</span><br><span class="line"> [  <span class="number">4</span>   <span class="number">5</span>   <span class="number">6</span>]]</span><br><span class="line">PyTorch 张量也会同步变化:</span><br><span class="line"> tensor([[<span class="number">100</span>,   <span class="number">2</span>,   <span class="number">3</span>],</span><br><span class="line">         [  <span class="number">4</span>,   <span class="number">5</span>,   <span class="number">6</span>]])</span><br><span class="line"><span class="number">2.</span> PyTorch 张量转为 NumPy 数组</span><br><span class="line">PyTorch 张量:</span><br><span class="line"> tensor([[ <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">         [<span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>]])</span><br><span class="line">转换后的 NumPy 数组:</span><br><span class="line"> [[ <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span>]</span><br><span class="line"> [<span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span>]]</span><br><span class="line">修改后的 PyTorch 张量:</span><br><span class="line"> tensor([[<span class="number">77.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">         [<span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>]])</span><br><span class="line">NumPy 数组也会同步变化:</span><br><span class="line"> [[<span class="number">77.</span>  <span class="number">8.</span>  <span class="number">9.</span>]</span><br><span class="line"> [<span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span>]]</span><br><span class="line"><span class="number">3.</span> 使用 clone() 保证独立数据</span><br><span class="line">原始张量:</span><br><span class="line"> tensor([[<span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>]])</span><br><span class="line">修改后的张量:</span><br><span class="line"> tensor([[ <span class="number">0.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>]])</span><br><span class="line">NumPy 数组（不会同步变化）:</span><br><span class="line"> [[<span class="number">13.</span> <span class="number">14.</span> <span class="number">15.</span>]</span><br><span class="line"> [<span class="number">16.</span> <span class="number">17.</span> <span class="number">18.</span>]]</span><br></pre></td></tr></table></figure>

<h1 id="PyTorch-神经网络基础"><a href="#PyTorch-神经网络基础" class="headerlink" title="PyTorch 神经网络基础"></a>PyTorch 神经网络基础</h1><p>神经网络是一种模仿人脑处理信息方式的计算模型，它由许多相互连接的节点（神经元）组成，这些节点按层次排列。</p>
<p>神经网络的强大之处在于其能够<strong>自动</strong>从大量数据中学习复杂的模式和特征，<strong>无需人工设计特征提取器</strong>。</p>
<p>随着深度学习的发展，神经网络已经成为解决许多复杂问题的关键技术。</p>
<h3 id="神经元（Neuron）"><a href="#神经元（Neuron）" class="headerlink" title="神经元（Neuron）"></a>神经元（Neuron）</h3><p>神经元是神经网络的基本单元，它接收输入信号，通过<strong>加权求和后与偏置（bias）相加</strong>，然后<strong>通过激活函数处理以产生输出</strong>。</p>
<p>神经元的<strong>权重</strong>和<strong>偏置</strong>是网络学习过程中<strong>需要调整的参数</strong>。</p>
<p><strong>输入和输出:</strong></p>
<ul>
<li><strong>输入（Input）</strong>：输入是网络的起始点，可以是特征数据，如图像的像素值或文本的词向量。</li>
<li><strong>输出（Output）</strong>：输出是网络的终点，表示模型的预测结果，如分类任务中的类别标签。</li>
</ul>
<p>神经元接收<strong>多个输入</strong>（例如x1, x2, …, xn），如果输入的<strong>加权和大于激活阈值</strong>（activation potential），则<strong>产生二进制输出</strong>。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_upfpVueoUuKPkyX3PR3KBg.png" alt="img">神经元的输出可以看作是<strong>输入的加权和加上偏置（bias)</strong>，神经元的数学表示：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/f0b929045ae6eef23514bd7024be62f0.png" alt="img"></p>
<p>这里，<strong>wj</strong> 是权重，<strong>xj</strong> 是输入，而 <strong>Bias</strong> 是偏置项。</p>
<h3 id="层（Layer）"><a href="#层（Layer）" class="headerlink" title="层（Layer）"></a>层（Layer）</h3><p>输入层和输出层之间的层被称为隐藏层，层与层之间的连接密度和类型构成了网络的配置。</p>
<p>神经网络由多个层组成，包括：</p>
<ul>
<li><strong>输入层（Input Layer）</strong>：接收原始输入数据。</li>
<li><strong>隐藏层（Hidden Layer）</strong>：对输入数据进行处理，可以有多个隐藏层。</li>
<li><strong>输出层（Output Layer）</strong>：产生最终的输出结果。</li>
</ul>
<p>典型的神经网络架构:</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_3fA77_mLNiJTSgZFhYnU0Q3K5DV4.webp" alt="img"></p>
<h3 id="前馈神经网络（Feedforward-Neural-Network，FNN）"><a href="#前馈神经网络（Feedforward-Neural-Network，FNN）" class="headerlink" title="前馈神经网络（Feedforward Neural Network，FNN）"></a>前馈神经网络（Feedforward Neural Network，FNN）</h3><p>前馈神经网络（Feedforward Neural Network，FNN）是神经网络家族中的基本单元。</p>
<p>前馈神经网络特点是数据从输入层开始，<strong>经过一个或多个隐藏</strong>层，最后到达输出层，全过程没有循环或反馈。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/neural-net.png" alt="img"></p>
<p><strong>前馈神经网络的基本结构：</strong></p>
<ul>
<li><strong>输入层：</strong> 数据进入网络的入口点。输入层的每个节点代表一个输入特征。</li>
<li><strong>隐藏层：<strong>一个或多个层，用于捕获数据的</strong>非线性</strong>特征。每个隐藏层由多个神经元组成，每个神经元通过激活函数增加非线性能力。</li>
<li>**输出层：**输出网络的预测结果。<strong>节点数和问题类型相关</strong>，例如分类问题的输出节点数等于类别数。</li>
<li>**连接权重与偏置：**每个神经元的输入通过权重进行加权求和，并加上偏置值，然后通过激活函数传递。</li>
</ul>
<h3 id="循环神经网络（Recurrent-Neural-Network-RNN）"><a href="#循环神经网络（Recurrent-Neural-Network-RNN）" class="headerlink" title="循环神经网络（Recurrent Neural Network, RNN）"></a>循环神经网络（Recurrent Neural Network, RNN）</h3><p>循环神经网络（Recurrent Neural Network, RNN）络是一类<strong>专门处理序列数据</strong>的神经网络，能够捕获输入数据中<strong>时间或顺序信息的依赖</strong>关系。</p>
<p>RNN 的特别之处在于它具有”<strong>记忆</strong>能力”，可以在网络的隐藏状态中保存之前时间步的信息。</p>
<p>循环神经网络用于<strong>处理随时间变化的数据模式</strong>。</p>
<p>在 RNN 中，<strong>相同的层被用来接收</strong>输入参数，并在<strong>指定的神经网络中显示输出</strong>参数。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/0_xs3Dya3qQBx6IU7C.png" alt="img"></p>
<p>PyTorch 提供了强大的工具来构建和训练神经网络。</p>
<p>神经网络在 PyTorch 中是通过 <strong>torch.nn</strong> 模块来实现的。</p>
<p><strong>torch.nn</strong> 模块提供了各种网络层（如全连接层、卷积层等）、损失函数和优化器，让神经网络的构建和训练变得更加方便。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_3DUs-90altOgaBcVJ9LTGg.png" alt="img"></p>
<p>在 PyTorch 中，构建神经网络通常需要继承 nn.Module 类。</p>
<p>nn.Module 是所有神经网络模块的基类，你需要定义以下两个部分：</p>
<ul>
<li><strong><code>__init__()</code></strong>：定义网络层。</li>
<li><strong><code>forward()</code></strong>：定义数据的前向传播过程。</li>
</ul>
<p>简单的全连接神经网络（Fully Connected Network）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># 定义一个简单的神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个输入层到隐藏层的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 输入 2 个特征，输出 2 个特征</span></span><br><span class="line">        <span class="comment"># 定义一个隐藏层到输出层的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 输入 2 个特征，输出 1 个预测值    </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 前向传播过程</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># 使用 ReLU 激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)  <span class="comment"># 输出层</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"><span class="comment"># 打印模型</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SimpleNN(</span><br><span class="line">  (fc1): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc2): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>PyTorch 提供了许多常见的神经网络层，以下是几个常见的：</p>
<ul>
<li><strong><code>nn.Linear(in_features, out_features)</code></strong>：<strong>全连接层</strong>，输入 <code>in_features</code> 个特征，输出 <code>out_features</code> 个特征。</li>
<li><strong><code>nn.Conv2d(in_channels, out_channels, kernel_size)</code></strong>：<strong>2D 卷积层</strong>，用于图像处理。</li>
<li><strong><code>nn.MaxPool2d(kernel_size)</code></strong>：<strong>2D 最大池化</strong>层，用于降维。</li>
<li><strong><code>nn.ReLU()</code></strong>：ReLU <strong>激活函数</strong>，常用于隐藏层。</li>
<li><strong><code>nn.Softmax(dim)</code></strong>：Softmax 激活函数，<strong>通常用于输出层</strong>，适用于<strong>多类分类</strong>问题。</li>
</ul>
<h3 id="激活函数（Activation-Function）"><a href="#激活函数（Activation-Function）" class="headerlink" title="激活函数（Activation Function）"></a>激活函数（Activation Function）</h3><p>激活函数决定了神经元是否应该被激活。它们是<strong>非线性函数</strong>，使得神经网络能够学习和执行更复杂的任务。常见的激活函数包括：</p>
<ul>
<li><strong>Sigmoid</strong>：用于二分类问题，输出值在 0 和 1 之间。</li>
<li><strong>Tanh</strong>：输出值在 -1 和 1 之间，常用于输出层之前。</li>
<li><strong>ReLU</strong>（Rectified Linear Unit）：目前最流行的激活函数之一，定义为 <code>f(x) = max(0, x)</code>，有助于<strong>解决梯度消失问题</strong>。</li>
<li><strong>Softmax</strong>：常用于多分类问题的输出层，将输出转换为<strong>概率分布</strong>。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># ReLU 激活</span></span><br><span class="line">output = F.relu(input_tensor)</span><br><span class="line"><span class="comment"># Sigmoid 激活</span></span><br><span class="line">output = torch.sigmoid(input_tensor)</span><br><span class="line"><span class="comment"># Tanh 激活</span></span><br><span class="line">output = torch.tanh(input_tensor)</span><br></pre></td></tr></table></figure>

<h3 id="损失函数（Loss-Function）"><a href="#损失函数（Loss-Function）" class="headerlink" title="损失函数（Loss Function）"></a>损失函数（Loss Function）</h3><p>损失函数用于衡量模型的预测值与真实值之间的差异。</p>
<p>常见的损失函数包括：</p>
<ul>
<li><strong>均方误差（MSELoss）</strong>：<strong>回归</strong>问题常用，计算输出与目标值的<strong>平方差</strong>。</li>
<li><strong>交叉熵损失（CrossEntropyLoss）</strong>：<strong>分类</strong>问题常用，计算输出和真实标签之间的<strong>交叉熵</strong>。</li>
<li><strong>BCEWithLogitsLoss</strong>：<strong>二分类</strong>问题，结合了 Sigmoid 激活和二元交叉熵损失。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 均方误差损失</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"><span class="comment"># 交叉熵损失</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 二分类交叉熵损失</span></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br></pre></td></tr></table></figure>

<h3 id="优化器（Optimizer）"><a href="#优化器（Optimizer）" class="headerlink" title="优化器（Optimizer）"></a>优化器（Optimizer）</h3><p>优化器负责在训练过程中更新网络的权重和偏置。</p>
<p>常见的优化器包括：</p>
<ul>
<li><strong>SGD</strong>（随机梯度下降）</li>
<li><strong>Adam</strong>（自适应矩估计）</li>
<li><strong>RMSprop</strong>（均方根传播）</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 使用 SGD 优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 使用 Adam 优化器</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>

<h3 id="训练过程（Training-Process）"><a href="#训练过程（Training-Process）" class="headerlink" title="训练过程（Training Process）"></a>训练过程（Training Process）</h3><p>训练神经网络涉及以下步骤：</p>
<ol>
<li><strong>准备数据</strong>：通过 <code>DataLoader</code> 加载数据。</li>
<li><strong>定义损失函数和优化器</strong>。</li>
<li><strong>前向传播</strong>：计算模型的输出。</li>
<li><strong>计算损失</strong>：与目标进行比较，得到损失值。</li>
<li><strong>反向传播</strong>：通过 <code>loss.backward()</code> 计算梯度。</li>
<li><strong>更新参数</strong>：通过 <code>optimizer.step()</code> 更新模型的参数。</li>
<li><strong>重复上述步骤</strong>，直到达到预定的训练轮数。</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设已经定义好了模型、损失函数和优化器</span></span><br><span class="line"><span class="comment"># 训练数据示例</span></span><br><span class="line">X = torch.randn(<span class="number">10</span>, <span class="number">2</span>)  <span class="comment"># 10 个样本，每个样本有 2 个特征</span></span><br><span class="line">Y = torch.randn(<span class="number">10</span>, <span class="number">1</span>)  <span class="comment"># 10 个目标标签</span></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 训练 100 轮</span></span><br><span class="line">    model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清除梯度</span></span><br><span class="line">    output = model(X)  <span class="comment"># 前向传播</span></span><br><span class="line">    loss = criterion(output, Y)  <span class="comment"># 计算损失</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新权重</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:  <span class="comment"># 每 10 轮输出一次损失</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/100], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="测试与评估"><a href="#测试与评估" class="headerlink" title="测试与评估"></a>测试与评估</h3><p>训练完成后，需要对模型进行测试和评估。</p>
<p>常见的步骤包括：</p>
<ul>
<li><strong>计算测试集的损失</strong>：测试模型在未见过的数据上的表现。</li>
<li><strong>计算准确率（Accuracy）</strong>：对于分类问题，计算正确预测的比例。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设你有测试集 X_test 和 Y_test</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 在评估过程中禁用梯度计算</span></span><br><span class="line">    output = model(X_test)</span><br><span class="line">    loss = criterion(output, Y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Test Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="神经网络类型"><a href="#神经网络类型" class="headerlink" title="神经网络类型"></a>神经网络类型</h3><ol>
<li><strong>前馈神经网络（Feedforward Neural Networks）</strong>：数据<strong>单向流动</strong>，从输入层到输出层，无反馈连接。</li>
<li><strong>卷积神经网络（Convolutional Neural Networks, CNNs）</strong>：适用于<strong>图像处理</strong>，使用卷积层<strong>提取空间特征</strong>。</li>
<li><strong>循环神经网络（Recurrent Neural Networks, RNNs）</strong>：适用于<strong>序列数据</strong>，如<strong>时间序列分析</strong>和<strong>自然语言处理</strong>，允许信息反馈循环。</li>
<li><strong>长短期记忆网络（Long Short-Term Memory, LSTM）</strong>：一种特殊的RNN，能够<strong>学习长期依赖关系</strong>。</li>
</ol>
<h1 id="PyTorch-第一个神经网络"><a href="#PyTorch-第一个神经网络" class="headerlink" title="PyTorch 第一个神经网络"></a>PyTorch 第一个神经网络</h1><p>本章节我们将介绍如何用 PyTorch 实现一个<strong>简单的前馈神经网络</strong>，完成一个<strong>二分类</strong>任务。</p>
<p>以下实例展示了如何使用 PyTorch 实现一个简单的神经网络进行二分类任务训练。</p>
<p>网络结构包括输入层、隐藏层和输出层，使用了 ReLU 激活函数和 Sigmoid 激活函数。</p>
<p>采用了均方误差损失函数和随机梯度下降优化器。</p>
<p>训练过程是通过前向传播、计算损失、反向传播和参数更新来逐步调整模型参数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入PyTorch库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># 定义输入层大小、隐藏层大小、输出层大小和批量大小</span></span><br><span class="line">n_in, n_h, n_out, batch_size = <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># 创建虚拟输入数据和目标数据</span></span><br><span class="line">x = torch.randn(batch_size, n_in)  <span class="comment"># 随机生成输入数据</span></span><br><span class="line">y = torch.tensor([[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], </span><br><span class="line">                 [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>]])  <span class="comment"># 目标输出数据</span></span><br><span class="line"><span class="comment"># 创建顺序模型，包含线性层、ReLU激活函数和Sigmoid激活函数</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">   nn.Linear(n_in, n_h),  <span class="comment"># 输入层到隐藏层的线性变换</span></span><br><span class="line">   nn.ReLU(),            <span class="comment"># 隐藏层的ReLU激活函数</span></span><br><span class="line">   nn.Linear(n_h, n_out),  <span class="comment"># 隐藏层到输出层的线性变换</span></span><br><span class="line">   nn.Sigmoid()           <span class="comment"># 输出层的Sigmoid激活函数</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 定义均方误差损失函数和随机梯度下降优化器</span></span><br><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 学习率为0.01</span></span><br><span class="line"><span class="comment"># 执行梯度下降算法进行模型训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):  <span class="comment"># 迭代50次</span></span><br><span class="line">   y_pred = model(x)  <span class="comment"># 前向传播，计算预测值</span></span><br><span class="line">   loss = criterion(y_pred, y)  <span class="comment"># 计算损失</span></span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span>, epoch, <span class="string">&#x27;loss: &#x27;</span>, loss.item())  <span class="comment"># 打印损失值</span></span><br><span class="line">   optimizer.zero_grad()  <span class="comment"># 清零梯度</span></span><br><span class="line">   loss.backward()  <span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">   optimizer.step()  <span class="comment"># 更新模型参数</span></span><br></pre></td></tr></table></figure>

<p><strong>定义网络参数：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n_in, n_h, n_out, batch_size = 10, 5, 1, 10</span><br></pre></td></tr></table></figure>

<ul>
<li><code>n_in</code>：输入层大小为 10，即<strong>每个数据点有 10 个特征</strong>。</li>
<li><code>n_h</code>：隐藏层大小为 5，即<strong>隐藏层包含 5 个神经元</strong>。</li>
<li><code>n_out</code>：输出层大小为 1，即<strong>输出一个标量</strong>，表示<strong>二分类结果</strong>（0 或 1）。</li>
<li><code>batch_size</code>：每个批次包含 <strong>10 个样本</strong>。</li>
</ul>
<p>生成输入数据和目标数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(batch_size, n_in)  # 随机生成输入数据</span><br><span class="line">y = torch.tensor([[1.0], [0.0], [0.0], </span><br><span class="line">                 [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])  # 目标输出数据</span><br></pre></td></tr></table></figure>

<ul>
<li><code>x</code>：随机生成一个<strong>形状为 <code>(10, 10)</code> 的输入数据矩阵</strong>，表示 10 个样本，每个样本有 10 个特征。</li>
<li><code>y</code>：目标输出数据（标签），表示每个输入样本的类别标签（0 或 1），是一个 10×1 的张量。</li>
</ul>
<p><strong>定义神经网络模型：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">   nn.Linear(n_in, n_h),  # 输入层到隐藏层的线性变换</span><br><span class="line">   nn.ReLU(),            # 隐藏层的ReLU激活函数</span><br><span class="line">   nn.Linear(n_h, n_out),  # 隐藏层到输出层的线性变换</span><br><span class="line">   nn.Sigmoid()           # 输出层的Sigmoid激活函数</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><code>nn.Sequential</code> 用于<strong>按顺序定义</strong>网络层。</p>
<ul>
<li><code>nn.Linear(n_in, n_h)</code>：定义<strong>输入层到隐藏层的线性变换</strong>，<strong>输入特征</strong>是 10 个，<strong>隐藏层</strong>有 5 个神经元。</li>
<li><code>nn.ReLU()</code>：在隐藏层后<strong>添加 ReLU 激活函数</strong>，<strong>增加非线性</strong>。</li>
<li><code>nn.Linear(n_h, n_out)</code>：定义隐藏层到输出层的线性变换，<strong>输出为 1 个神经元</strong>。</li>
<li><code>nn.Sigmoid()</code>：输出层使用 <strong>Sigmoid 激活函数</strong>，将<strong>结果映射到 0 到 1 之间</strong>，用于二分类任务。</li>
</ul>
<p><strong>定义损失函数和优化器：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.MSELoss()  # 使用均方误差损失函数</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # 使用随机梯度下降优化器，学习率为 0.01</span><br></pre></td></tr></table></figure>

<p><strong>训练循环：</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):  <span class="comment"># 训练50轮</span></span><br><span class="line">   y_pred = model(x)  <span class="comment"># 前向传播，计算预测值</span></span><br><span class="line">   loss = criterion(y_pred, y)  <span class="comment"># 计算损失</span></span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span>, epoch, <span class="string">&#x27;loss: &#x27;</span>, loss.item())  <span class="comment"># 打印损失值</span></span><br><span class="line">   optimizer.zero_grad()  <span class="comment"># 清零梯度</span></span><br><span class="line">   loss.backward()  <span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">   optimizer.step()  <span class="comment"># 更新模型参数</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>for epoch in range(50)</code>：进行 <strong>50 次训练迭代</strong>。</li>
<li><code>y_pred = model(x)</code>：进行<strong>前向</strong>传播，使用<strong>当前模型参数计算输入数据 <code>x</code> 的预测值</strong>。</li>
<li><code>loss = criterion(y_pred, y)</code>：计算<strong>预测值和目标值 <code>y</code></strong> 之间的<strong>损失</strong>。</li>
<li><code>optimizer.zero_grad()</code>：<strong>清除上一轮</strong>训练时的<strong>梯度值</strong>。</li>
<li><code>loss.backward()</code>：<strong>反向</strong>传播，计算<strong>损失函数相对于模型参数的梯度</strong>。</li>
<li><code>optimizer.step()</code><strong>：根据计算出的梯度更新</strong>模型参数。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 定义输入层大小、隐藏层大小、输出层大小和批量大小</span></span><br><span class="line">n_in, n_h, n_out, batch_size = <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># 创建虚拟输入数据和目标数据</span></span><br><span class="line">x = torch.randn(batch_size, n_in)  <span class="comment"># 随机生成输入数据</span></span><br><span class="line">y = torch.tensor([[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], </span><br><span class="line">                  [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>]])  <span class="comment"># 目标输出数据</span></span><br><span class="line"><span class="comment"># 创建顺序模型，包含线性层、ReLU激活函数和Sigmoid激活函数</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Linear(n_in, n_h),  <span class="comment"># 输入层到隐藏层的线性变换</span></span><br><span class="line">    nn.ReLU(),            <span class="comment"># 隐藏层的ReLU激活函数</span></span><br><span class="line">    nn.Linear(n_h, n_out),  <span class="comment"># 隐藏层到输出层的线性变换</span></span><br><span class="line">    nn.Sigmoid()           <span class="comment"># 输出层的Sigmoid激活函数</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 定义均方误差损失函数和随机梯度下降优化器</span></span><br><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 学习率为0.01</span></span><br><span class="line"><span class="comment"># 用于存储每轮的损失值</span></span><br><span class="line">losses = []</span><br><span class="line"><span class="comment"># 执行梯度下降算法进行模型训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):  <span class="comment"># 迭代50次</span></span><br><span class="line">    y_pred = model(x)  <span class="comment"># 前向传播，计算预测值</span></span><br><span class="line">    loss = criterion(y_pred, y)  <span class="comment"># 计算损失</span></span><br><span class="line">    losses.append(loss.item())  <span class="comment"># 记录损失值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/50], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)  <span class="comment"># 打印损失值</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清零梯度</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新模型参数</span></span><br><span class="line"><span class="comment"># 可视化损失变化曲线</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">51</span>), losses, label=<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss Over Epochs&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 可视化预测结果与实际目标值对比</span></span><br><span class="line">y_pred_final = model(x).detach().numpy()  <span class="comment"># 最终预测值</span></span><br><span class="line">y_actual = y.numpy()  <span class="comment"># 实际值</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, batch_size + <span class="number">1</span>), y_actual, <span class="string">&#x27;o-&#x27;</span>, label=<span class="string">&#x27;Actual&#x27;</span>, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, batch_size + <span class="number">1</span>), y_pred_final, <span class="string">&#x27;x--&#x27;</span>, label=<span class="string">&#x27;Predicted&#x27;</span>, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Sample Index&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Value&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Actual vs Predicted Values&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/first_n_runoob_1.png" alt="img"></p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/a7d9d307c8172f34fc906368ab8cf0fe.png" alt="a7d9d307c8172f34fc906368ab8cf0fe"></p>
<h2 id="另外一个实例"><a href="#另外一个实例" class="headerlink" title="另外一个实例"></a>另外一个实例</h2><p>我们假设有一个二维数据集，目标是根据点的位置将它们<strong>分类到两个类别中</strong>（例如，红色和蓝色点）。</p>
<p>以下实例展示了如何使用神经网络完成<strong>简单的二分类</strong>任务，为更复杂的任务奠定了基础，通过 PyTorch 的<strong>模块化接口</strong>，神经网络的构建、训练和可视化都非常直观。</p>
<h3 id="1、数据准备"><a href="#1、数据准备" class="headerlink" title="1、数据准备"></a>1、数据准备</h3><p>首先，我们生成一些简单的二维数据：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 生成一些随机数据</span></span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">data = torch.randn(n_samples, <span class="number">2</span>)  <span class="comment"># 生成 100 个二维数据点</span></span><br><span class="line">labels = (data[:, <span class="number">0</span>]**<span class="number">2</span> + data[:, <span class="number">1</span>]**<span class="number">2</span> &lt; <span class="number">1</span>).<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)  <span class="comment"># 点在圆内为1，圆外为0</span></span><br><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line">plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], c=labels.squeeze(), cmap=<span class="string">&#x27;coolwarm&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Generated Data&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 2&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><strong>数据说明：</strong></p>
<ul>
<li><code>data</code> 是输入的二维点，每个点有两个特征。</li>
<li><code>labels</code> 是目标分类，点在圆形区域内为 1，否则为 0。</li>
</ul>
<p>显示如下：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/fnn_runoob_1.png" alt="img"></p>
<h3 id="2、定义神经网络"><a href="#2、定义神经网络" class="headerlink" title="2、定义神经网络"></a>2、定义神经网络</h3><p>用 PyTorch 创建一个简单的前馈神经网络。</p>
<p>前馈神经网络使用了一层隐藏层，通过简单的线性变换和激活函数捕获数据的非线性模式。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义神经网络的层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">4</span>)  <span class="comment"># 输入层有 2 个特征，隐藏层有 4 个神经元</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">4</span>, <span class="number">1</span>)  <span class="comment"># 隐藏层输出到 1 个神经元（用于二分类）</span></span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = nn.Sigmoid()  <span class="comment"># 二分类激活函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># 使用 ReLU 激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.sigmoid(<span class="variable language_">self</span>.fc2(x))  <span class="comment"># 输出层使用 Sigmoid 激活函数</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = SimpleNN()</span><br></pre></td></tr></table></figure>

<h3 id="3、定义损失函数和优化器"><a href="#3、定义损失函数和优化器" class="headerlink" title="3、定义损失函数和优化器"></a>3、定义损失函数和优化器</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义二分类的损失函数和优化器</span></span><br><span class="line">criterion = nn.BCELoss()  <span class="comment"># 二元交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)  <span class="comment"># 使用随机梯度下降优化器</span></span><br></pre></td></tr></table></figure>

<h3 id="4、训练模型"><a href="#4、训练模型" class="headerlink" title="4、训练模型"></a>4、训练模型</h3><p>用数据训练模型，让它<strong>学会分类</strong>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    outputs = model(data)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment"># 每 10 轮打印一次损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="5、测试模型并可视化结果"><a href="#5、测试模型并可视化结果" class="headerlink" title="5、测试模型并可视化结果"></a>5、测试模型并可视化结果</h3><p>我们测试模型，并在<strong>图像上绘制决策边界</strong>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化决策边界</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_decision_boundary</span>(<span class="params">model, data</span>):</span><br><span class="line">    x_min, x_max = data[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span>, data[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = data[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span>, data[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    xx, yy = torch.meshgrid(torch.arange(x_min, x_max, <span class="number">0.1</span>), torch.arange(y_min, y_max, <span class="number">0.1</span>), indexing=<span class="string">&#x27;ij&#x27;</span>)</span><br><span class="line">    grid = torch.cat([xx.reshape(-<span class="number">1</span>, <span class="number">1</span>), yy.reshape(-<span class="number">1</span>, <span class="number">1</span>)], dim=<span class="number">1</span>)</span><br><span class="line">    predictions = model(grid).detach().numpy().reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, predictions, levels=[<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1</span>], cmap=<span class="string">&#x27;coolwarm&#x27;</span>, alpha=<span class="number">0.7</span>)</span><br><span class="line">    plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], c=labels.squeeze(), cmap=<span class="string">&#x27;coolwarm&#x27;</span>, edgecolors=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Decision Boundary&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(model, data)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epoch [10/100], Loss: 0.5247</span><br><span class="line">Epoch [20/100], Loss: 0.3142</span><br><span class="line">...</span><br><span class="line">Epoch [100/100], Loss: 0.0957</span><br></pre></td></tr></table></figure>

<p>图中显示了原始数据点（红色和蓝色），以及<strong>模型学习到的分类边界</strong>。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/fnn_runoob_3.png" alt="img"></p>
<h1 id="PyTorch-数据处理与加载"><a href="#PyTorch-数据处理与加载" class="headerlink" title="PyTorch 数据处理与加载"></a>PyTorch 数据处理与加载</h1><p>在 PyTorch 中，处理和加载数据是深度学习训练过程中的关键步骤。</p>
<p>为了高效地处理数据，PyTorch 提供了强大的工具，包括 <strong>torch.utils.data.Dataset</strong> 和 <strong>torch.utils.data.DataLoader</strong>，帮助我们<strong>管理数据集</strong>、<strong>批量加载</strong>和<strong>数据增强</strong>等任务。</p>
<p>PyTorch 数据处理与加载的介绍：</p>
<ul>
<li><strong>自定义 Dataset</strong>：通过<strong>继承 <code>torch.utils.data.Dataset</code> 来加载自己的</strong>数据集。</li>
<li><strong>DataLoader</strong>：<code>DataLoader</code> <strong>按批次加载</strong>数据，支持多线程加载并进行数据<strong>打乱</strong>。</li>
<li><strong>数据预处理与增强</strong>：使用 <code>torchvision.transforms</code> 进行常见的图像<strong>预处理和增强</strong>操作，提高模型的<strong>泛化能力</strong>。</li>
<li><strong>加载标准数据集</strong>：<code>torchvision.datasets</code> 提供了许多<strong>常见的数据集</strong>，简化了数据加载过程。</li>
<li><strong>多个数据源</strong>：通过组合多个 <code>Dataset</code> 实例来处理来自不同来源的数据。</li>
</ul>
<h2 id="自定义-Dataset"><a href="#自定义-Dataset" class="headerlink" title="自定义 Dataset"></a>自定义 Dataset</h2><p><strong>torch.utils.data.Dataset</strong> 是一个抽象类，允许你从自己的数据源中创建数据集。</p>
<p>我们需要继承该类并实现以下两个方法：</p>
<ul>
<li><code>__len__(self)</code>：返回数据集中的样本数量。</li>
<li><code>__getitem__(self, idx)</code>：通过索引返回一个样本。</li>
</ul>
<p>假设我们有一个简单的 CSV 文件或一些列表数据，我们可以通过继承 Dataset 类来创建自己的数据集。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="comment"># 自定义数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X_data, Y_data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化数据集，X_data 和 Y_data 是两个列表或数组</span></span><br><span class="line"><span class="string">        X_data: 输入特征</span></span><br><span class="line"><span class="string">        Y_data: 目标标签</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.X_data = X_data</span><br><span class="line">        <span class="variable language_">self</span>.Y_data = Y_data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回数据集的大小&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.X_data)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回指定索引的数据&quot;&quot;&quot;</span></span><br><span class="line">        x = torch.tensor(<span class="variable language_">self</span>.X_data[idx], dtype=torch.float32)  <span class="comment"># 转换为 Tensor</span></span><br><span class="line">        y = torch.tensor(<span class="variable language_">self</span>.Y_data[idx], dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">X_data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]  <span class="comment"># 输入特征</span></span><br><span class="line">Y_data = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># 目标标签</span></span><br><span class="line"><span class="comment"># 创建数据集实例</span></span><br><span class="line">dataset = MyDataset(X_data, Y_data)</span><br></pre></td></tr></table></figure>

<h2 id="使用-DataLoader-加载数据"><a href="#使用-DataLoader-加载数据" class="headerlink" title="使用 DataLoader 加载数据"></a>使用 DataLoader 加载数据</h2><p>DataLoader 是 PyTorch 提供的一个重要工具，用于从 Dataset 中**按批次（batch）**加载数据。</p>
<p>DataLoader 允许我们批量读取数据并进行多线程加载，从而提高训练效率。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment"># 创建 DataLoader 实例，batch_size 设置每次加载的样本数量</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 打印加载的数据</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Batch <span class="subst">&#123;batch_idx + <span class="number">1</span>&#125;</span>:&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Inputs: <span class="subst">&#123;inputs&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Labels: <span class="subst">&#123;labels&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>batch_size</code></strong>: 每次加载的<strong>样本数量</strong>。</li>
<li><strong><code>shuffle</code></strong>: <strong>是否</strong>对数据进行<strong>洗牌</strong>，通常训练时需要将数据打乱。</li>
<li><strong><code>drop_last</code></strong>: 如果数据集中的样本数<strong>不能被 <code>batch_size</code> 整除</strong>，设置为 <code>True</code> 时，<strong>丢弃最后一个</strong>不完整的 batch。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Batch <span class="number">1</span>:</span><br><span class="line">Inputs: tensor([[<span class="number">3.</span>, <span class="number">4.</span>], [<span class="number">1.</span>, <span class="number">2.</span>]])</span><br><span class="line">Labels: tensor([<span class="number">0.</span>, <span class="number">1.</span>])</span><br><span class="line">Batch <span class="number">2</span>:</span><br><span class="line">Inputs: tensor([[<span class="number">7.</span>, <span class="number">8.</span>], [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">Labels: tensor([<span class="number">0.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>

<p>每次循环中，DataLoader 会返回一个批次的数据，包括<strong>输入特征（inputs）<strong>和</strong>目标标签（labels）</strong>。</p>
<h2 id="预处理与数据增强"><a href="#预处理与数据增强" class="headerlink" title="预处理与数据增强"></a>预处理与数据增强</h2><p>数据预处理和增强对于提高模型的性能至关重要。</p>
<p>PyTorch 提供了 torchvision.transforms 模块来进行常见的图像预处理和增强操作，如旋转、裁剪、归一化等。</p>
<p>常见的图像预处理操作:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="comment"># 定义数据预处理的流水线</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),  <span class="comment"># 将图像调整为 128x128</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为张量</span></span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># 加载图像</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;image.jpg&#x27;</span>)</span><br><span class="line"><span class="comment"># 应用预处理</span></span><br><span class="line">image_tensor = transform(image)</span><br><span class="line"><span class="built_in">print</span>(image_tensor.shape)  <span class="comment"># 输出张量的形状</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>transforms.Compose()</code></strong>：将多个变换操作组合在一起。</li>
<li><strong><code>transforms.Resize()</code></strong>：调整图像大小。</li>
<li><strong><code>transforms.ToTensor()</code></strong>：将图像转换为 PyTorch 张量，值会被归一化到 <code>[0, 1]</code> 范围。</li>
<li><strong><code>transforms.Normalize()</code></strong>：标准化图像数据，通常使用预训练模型时需要进行标准化处理。</li>
</ul>
<h3 id="图像数据增强"><a href="#图像数据增强" class="headerlink" title="图像数据增强"></a>图像数据增强</h3><p>数据增强技术通过对训练数据进行随机变换，增加数据的多样性，帮助模型更好地泛化。例如，随机翻转、旋转、裁剪等。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(),  <span class="comment"># 随机水平翻转</span></span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),  <span class="comment"># 随机旋转 30 度</span></span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">128</span>),  <span class="comment"># 随机裁剪并调整为 128x128</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>这些数据增强方法可以通过 transforms.Compose() 组合使用，保证每个图像在训练时具有不同的变换。</p>
<h2 id="加载图像数据集"><a href="#加载图像数据集" class="headerlink" title="加载图像数据集"></a>加载图像数据集</h2><p>对于图像数据集，torchvision.datasets 提供了许多<strong>常见数据集</strong>（如 CIFAR-10、ImageNet、MNIST 等）以及用于加载图像数据的工具。</p>
<p>加载 MNIST 数据集:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="comment"># 定义预处理操作</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 对灰度图像进行标准化</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># 下载并加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"><span class="comment"># 创建 DataLoader</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 迭代训练数据</span></span><br><span class="line"><span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="built_in">print</span>(inputs.shape)  <span class="comment"># 每个批次的输入数据形状</span></span><br><span class="line">    <span class="built_in">print</span>(labels.shape)  <span class="comment"># 每个批次的标签形状</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>datasets.MNIST()</code> 会自动<strong>下载</strong> MNIST 数据集<strong>并加载</strong>。</li>
<li><code>transform</code> 参数允许我们对数据进行<strong>预处理</strong>。</li>
<li><code>train=True</code> 和 <code>train=False</code> 分别表示<strong>训练</strong>集和<strong>测试</strong>集。</li>
</ul>
<h2 id="用多个数据源（Multi-source-Dataset）"><a href="#用多个数据源（Multi-source-Dataset）" class="headerlink" title="用多个数据源（Multi-source Dataset）"></a>用多个数据源（Multi-source Dataset）</h2><p>如果你的数据集由多个文件、多个来源（例如多个图像文件夹）组成，可以通过<strong>继承 Dataset 类自定义加载多个</strong>数据源。</p>
<p>PyTorch 提供了 <strong>ConcatDataset 和 ChainDataset 等类</strong>来连接多个数据集。</p>
<p>例如，假设我们有多个图像文件夹的数据，可以将它们合并为一个数据集：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> ConcatDataset</span><br><span class="line"><span class="comment"># 假设 dataset1 和 dataset2 是两个 Dataset 对象</span></span><br><span class="line">combined_dataset = ConcatDataset([dataset1, dataset2])</span><br><span class="line">combined_loader = DataLoader(combined_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h1 id="PyTorch-线性回归"><a href="#PyTorch-线性回归" class="headerlink" title="PyTorch 线性回归"></a>PyTorch 线性回归</h1><p>线性回归是最基本的机器学习算法之一，用于<strong>预测一个连续值</strong>。</p>
<p>线性回归是一种<strong>简单且常见的回归分析方法</strong>，目的是通过<strong>拟合一个线性函数来预测输出</strong>。</p>
<p>对于一个简单的线性回归问题，模型可以表示为：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/lg-1.png" alt="img"></p>
<ul>
<li>y 是预测值（目标值）。</li>
<li>x1，x2，xn 是输入特征。</li>
<li>w1，w2，wn是待学习的权重（模型参数）。</li>
<li>b 是偏置项。</li>
</ul>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Linear_regression.svg.png" alt="img"></p>
<p>在 PyTorch 中，线性回归模型可以通过<strong>继承 nn.Module 类</strong>来实现。我们将通过一个简单的示例来详细说明如何使用 PyTorch 实现线性回归模型。</p>
<h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>我们首先准备一些假数据，用于训练我们的线性回归模型。这里，我们可以生成一个简单的线性关系的数据集，其中每个样本有两个特征 x1，x2。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 随机种子，确保每次运行结果一致</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line">X = torch.randn(<span class="number">100</span>, <span class="number">2</span>)  <span class="comment"># 100 个样本，每个样本 2 个特征</span></span><br><span class="line">true_w = torch.tensor([<span class="number">2.0</span>, <span class="number">3.0</span>])  <span class="comment"># 假设真实权重</span></span><br><span class="line">true_b = <span class="number">4.0</span>  <span class="comment"># 偏置项</span></span><br><span class="line">Y = X @ true_w + true_b + torch.randn(<span class="number">100</span>) * <span class="number">0.1</span>  <span class="comment"># 加入一些噪声</span></span><br><span class="line"><span class="comment"># 打印部分数据</span></span><br><span class="line"><span class="built_in">print</span>(X[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(Y[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.9269,  1.4873],</span><br><span class="line">        [ 0.9007, -2.1055],</span><br><span class="line">        [ 0.6784, -1.2345],</span><br><span class="line">        [-0.0431, -1.6047],</span><br><span class="line">        [-0.7521,  1.6487]])</span><br><span class="line">tensor([12.4460, -0.4663,  1.7666, -0.9357,  7.4781])</span><br></pre></td></tr></table></figure>

<p>这段代码创建了一个带有噪声的线性数据集，输入 X 为 100x2 的矩阵，每个样本有<strong>两个特征</strong>，输出 Y 由真实的权重和偏置生成，并<strong>加上了一些随机噪声</strong>。</p>
<h3 id="定义线性回归模型"><a href="#定义线性回归模型" class="headerlink" title="定义线性回归模型"></a>定义线性回归模型</h3><p>我们可以通过继承 <code>nn.Module</code> 来定义一个简单的线性回归模型。在 PyTorch 中，线性回归的核心是 <code>nn.Linear()</code> 层，它会自动处理权重和偏置的初始化。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearRegressionModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个线性层，输入为2个特征，输出为1个预测值</span></span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 输入维度2，输出维度1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x)  <span class="comment"># 前向传播，返回预测结果</span></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = LinearRegressionModel()</span><br></pre></td></tr></table></figure>

<p>这里的 <code>nn.Linear(2, 1)</code> 表示一个线性层，它有 2 个输入特征和 1 个输出。<code>forward</code> 方法定义了如何通过这个层进行前向传播。</p>
<h2 id="定义损失函数与优化器"><a href="#定义损失函数与优化器" class="headerlink" title="定义损失函数与优化器"></a>定义损失函数与优化器</h2><p>线性回归的常见损失函数是 <strong>均方误差损失（MSELoss）</strong>，用于衡量预测值与真实值之间的差异。PyTorch 中提供了现成的 MSELoss 函数。</p>
<p>我们将使用 <strong>SGD（随机梯度下降）</strong> 或 <strong>Adam</strong> 优化器来最小化损失函数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数（均方误差）</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"><span class="comment"># 优化器（使用 SGD 或 Adam）</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 学习率设置为0.01</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>MSELoss</code></strong>：计算预测值与真实值的均方误差。</li>
<li><strong><code>SGD</code></strong>：使用随机梯度下降法更新参数。</li>
</ul>
<h3 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h3><p>在训练过程中，我们将执行以下步骤：</p>
<ol>
<li>使用输入数据 X 进行前向传播，得到预测值。</li>
<li>计算损失（预测值与实际值之间的差异）。</li>
<li>使用反向传播计算梯度。</li>
<li>更新模型参数（权重和偏置）。</li>
</ol>
<p>我们将<strong>训练模型 1000 轮</strong>，并在每 100 轮打印一次损失。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">num_epochs = <span class="number">1000</span>  <span class="comment"># 训练 1000 轮</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    predictions = model(X)  <span class="comment"># 模型输出预测值</span></span><br><span class="line">    loss = criterion(predictions.squeeze(), Y)  <span class="comment"># 计算损失（注意预测值需要压缩为1D）</span></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清空之前的梯度</span></span><br><span class="line">    loss.backward()  <span class="comment"># 计算梯度</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新模型参数</span></span><br><span class="line">    <span class="comment"># 打印损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/1000], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>predictions.squeeze()</code></strong>：我们在这里将模型的输出从 2D 张量压缩为 1D，因为目标值 <code>Y</code> 是一个一维数组。</li>
<li><strong><code>optimizer.zero_grad()</code></strong>：每次反向传播前需要清空之前的梯度。</li>
<li><strong><code>loss.backward()</code></strong>：计算梯度。</li>
<li><strong><code>optimizer.step()</code></strong>：更新权重和偏置。</li>
</ul>
<h3 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h3><p>训练完成后，我们可以通过查看<strong>模型的权重和偏置来评估模型的效果</strong>。我们还可以在新的数据上进行预测并<strong>与实际值进行比较</strong>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看训练后的权重和偏置</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Predicted weight: <span class="subst">&#123;model.linear.weight.data.numpy()&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Predicted bias: <span class="subst">&#123;model.linear.bias.data.numpy()&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 在新数据上做预测</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 评估时不需要计算梯度</span></span><br><span class="line">    predictions = model(X)</span><br><span class="line"><span class="comment"># 可视化预测与实际值</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], Y, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;True values&#x27;</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], predictions, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Predictions&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>model.linear.weight.data</code></strong> 和 <strong><code>model.linear.bias.data</code></strong>：这些属性存储了模型的权重和偏置。</li>
<li><strong><code>torch.no_grad()</code></strong>：在评估模式下，不需要计算梯度，节省内存。</li>
</ul>
<h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p>在训练过程中，随着损失逐渐减小，我们希望最终的模型能够拟合我们生成的数据。通过查看训练后的权重和偏置，我们可以比较其与真实值（<code>true_w</code> 和 <code>true_b</code>）的差异。理论上，模型的输出权重应该接近 <code>true_w</code> 和 <code>true_b</code>。</p>
<p>在可视化的散点图中，蓝色点表示真实值，红色点表示模型的预测值。我们希望看到红色点与蓝色点尽可能接近，表明模型成功学习了数据的线性关系。</p>
<h1 id="PyTorch-卷积神经网络"><a href="#PyTorch-卷积神经网络" class="headerlink" title="PyTorch 卷积神经网络"></a>PyTorch 卷积神经网络</h1><p>PyTorch <strong>卷积神经网络 (Convolutional Neural Networks, CNN)</strong> 是一类<strong>专门用于处理具有网格状拓扑结构数据（如图像）<strong>的</strong>深度学习</strong>模型。</p>
<p>CNN 是<strong>计算机视觉任务（如图像分类、目标检测和分割）的核心技术。</strong></p>
<p>下面这张图展示了一个典型的卷积神经网络（CNN）的结构和工作流程，用于图像识别任务。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1686919918947.jpeg" alt="img"></p>
<p>在图中，CNN 的输出层给出了三个类别的概率：Donald（0.2）、Goofy（0.1）和Tweety（0.7），这表明网络认为输入图像最有可能是 Tweety。</p>
<p>以下是各个部分的简要说明：</p>
<ul>
<li><strong>输入图像（Input Image）</strong>：网络接收的原始图像数据。</li>
<li><strong>卷积（Convolution）</strong>：使用<strong>卷积核（Kernel）在输入图像上滑动，提取特征</strong>，<strong>生成特征图（Feature Maps）</strong>。</li>
<li><strong>池化（Pooling）</strong>：通常在卷积层之后，通过<strong>最大池化或平均池化减少特征图的尺寸</strong>，同时<strong>保留重要特征，生成池化特征图（Pooled Feature Maps）</strong>。</li>
<li><strong>特征提取（Feature Extraction）</strong>：通过<strong>多个</strong>卷积和池化层的组合，<strong>逐步提取</strong>图像的<strong>高级</strong>特征。</li>
<li><strong>展平层（Flatten Layer）</strong>：将多维的特征图<strong>转换为一维向量</strong>，以便输入到全连接层。</li>
<li><strong>全连接层（Fully Connected Layer）</strong>：类似于传统的神经网络层，用于将提取的特征映射到输出类别。</li>
<li><strong>分类（Classification）</strong>：网络的输出层，根据全连接层的输出进行分类。</li>
<li><strong>概率分布（Probabilistic Distribution）</strong>：输出层给出每个类别的概率，表示<strong>输入图像属于各个类别的可能性。</strong></li>
</ul>
<h3 id="卷积神经网络的基本结构"><a href="#卷积神经网络的基本结构" class="headerlink" title="卷积神经网络的基本结构"></a>卷积神经网络的基本结构</h3><p><strong>1、输入层（Input Layer）</strong></p>
<p>接收原始图像数据，图像通常被表示为一个<strong>三维数组</strong>，其中两个维度代表图像的宽度和高度，第三个维度代表<strong>颜色通道</strong>（例如，RGB图像有三个通道）。</p>
<p><strong>2、卷积层（Convolutional Layer）</strong></p>
<p>用卷积核<strong>提取局部特征</strong>，如边缘、纹理等。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-cnn-1.png" alt="img"></p>
<ul>
<li>x：输入图像。</li>
<li>k：卷积核（权重矩阵）。</li>
<li>b：偏置。</li>
</ul>
<p>**应用一组可学习的滤波器（或卷积核）**在输入图像上进行卷积操作，以提取局部特征。</p>
<p>每个滤波器在输入图像上滑动，生成一个特征图（Feature Map），表示滤波器在不同位置的激活。</p>
<p>卷积层可以有多个滤波器，每个滤波器生成一个特征图，所有特征图组成一个特征图集合。</p>
<h3 id="3、激活函数（Activation-Function）"><a href="#3、激活函数（Activation-Function）" class="headerlink" title="3、激活函数（Activation Function）"></a>3、激活函数（Activation Function）</h3><p>通常在<strong>卷积层之后应用非线性激活函数，如 ReLU</strong>（Rectified Linear Unit），以引入非线性特性，使网络能够<strong>学习更复杂</strong>的模式。</p>
<p>ReLU 函数定义为 ：<strong>f(x)&#x3D;max(0,x)</strong>，即如果输入<strong>小于 0 则输出 0，否则输出输入值</strong>。</p>
<p><strong>4、池化层（Pooling Layer）</strong></p>
<ul>
<li>用于<strong>降低特征图的空间维度</strong>，减少计算量和参数数量，同时保留最重要的特征信息。</li>
<li>最常见的池化操作是最大池化（Max Pooling）和平均池化（Average Pooling）。</li>
<li>最大池化选择区域内的最大值，而平均池化计算区域内的平均值。</li>
</ul>
<p><strong>5、归一化层（Normalization Layer，可选）</strong></p>
<ul>
<li>例如，局部响应归一化（Local Response Normalization, LRN）或批归一化（Batch Normalization）。</li>
<li>这些层有助于<strong>加速训练过程，提高模型的稳定性</strong>。</li>
</ul>
<p><strong>6、全连接层（Fully Connected Layer）</strong></p>
<ul>
<li>在 <strong>CNN 的末端</strong>，将前面层提取的特征图展平（Flatten）成一维向量，然后输入到全连接层。</li>
<li>全连接层的每个神经元都与前一层的所有神经元相连，<strong>用于综合特征并进行最终的分类或回归</strong>。</li>
</ul>
<p><strong>7、输出层（Output Layer）</strong></p>
<p>根据任务的不同，输出层可以有不同的形式。</p>
<p>对于分类任务，通常使用 <strong>Softmax 函数将输出转换为概率分布</strong>，表示输入属于各个类别的概率。</p>
<p><strong>8、损失函数（Loss Function）</strong></p>
<p>用于衡量模型预测<strong>与真实标签之间的差异</strong>。</p>
<p>常见的损失函数包括<strong>交叉熵损失（Cross-Entropy Loss）<strong>用于</strong>多分类</strong>任务，<strong>均方误差（Mean Squared Error, MSE）<strong>用于</strong>回归</strong>任务。</p>
<p><strong>9、优化器（Optimizer）</strong></p>
<p>用于<strong>根据损失函数的梯度更新</strong>网络的权重。常见的优化器包括随机梯度下降（SGD）、Adam、RMSprop等。</p>
<p><strong>10、正则化（Regularization，可选）</strong></p>
<p>包括 Dropout、L1&#x2F;L2 正则化等技术，用于<strong>防止模型过拟合</strong>。</p>
<p><strong>这些层可以堆叠形成更深的网络结构，以提高模型的学习能力。</strong></p>
<p>CNN 的深度和复杂性可以根据任务的需求进行调整。</p>
<h2 id="PyTorch-实现一个-CNN-实例"><a href="#PyTorch-实现一个-CNN-实例" class="headerlink" title="PyTorch 实现一个 CNN 实例"></a>PyTorch 实现一个 CNN 实例</h2><p>以下示例展示如何用 PyTorch 构建一个简单的 CNN 模型，用于 MNIST 数据集的数字分类。</p>
<p>主要步骤：</p>
<ul>
<li><strong>数据加载与预处理</strong>：使用 <code>torchvision</code> 加载和预处理 MNIST 数据。</li>
<li><strong>模型构建</strong>：定义卷积层、池化层和全连接层。</li>
<li><strong>训练</strong>：通过损失函数和优化器进行模型训练。</li>
<li><strong>评估</strong>：测试集上计算模型的准确率。</li>
<li><strong>可视化</strong>：展示部分测试样本及其预测结果。</li>
</ul>
<h3 id="1、导入必要库"><a href="#1、导入必要库" class="headerlink" title="1、导入必要库"></a>1、导入必要库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.optim as optim</span><br></pre></td></tr></table></figure>

<h3 id="2、数据加载"><a href="#2、数据加载" class="headerlink" title="2、数据加载"></a>2、数据加载</h3><p>使用 torchvision 提供的 MNIST 数据集，<strong>加载和预处理</strong>数据。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h3 id="3、定义-CNN-模型"><a href="#3、定义-CNN-模型" class="headerlink" title="3、定义 CNN 模型"></a>3、定义 CNN 模型</h3><p>使用 <strong>nn.Module</strong> 构建一个 CNN。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层：输入1通道，输出32通道，卷积核大小3x3</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义卷积层：输入32通道，输出64通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)  <span class="comment"># 输入大小 = 特征图大小 * 通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)  <span class="comment"># 10 个类别</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))  <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))  <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>) <span class="comment"># 展平操作</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))    <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)            <span class="comment"># 全连接层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleCNN()</span><br></pre></td></tr></table></figure>

<h3 id="4、定义损失函数与优化器"><a href="#4、定义损失函数与优化器" class="headerlink" title="4、定义损失函数与优化器"></a>4、定义损失函数与优化器</h3><p>使用交叉熵损失和随机梯度下降优化器。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()  # 多分类交叉熵损失</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # 学习率和动量</span><br></pre></td></tr></table></figure>

<h3 id="5、训练模型"><a href="#5、训练模型" class="headerlink" title="5、训练模型"></a>5、训练模型</h3><p>训练模型 5 个 epoch，每个 epoch 后输出训练损失。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">model.train()  <span class="comment"># 设为训练模式</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="6、测试模型"><a href="#6、测试模型" class="headerlink" title="6、测试模型"></a>6、测试模型</h3><p>在测试集上评估模型的准确率。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 评估时不需要计算梯度</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)  <span class="comment"># 预测类别</span></span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 数据加载与预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 定义 CNN 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># 输入1通道，输出32通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># 输入32通道，输出64通道</span></span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)  <span class="comment"># 展平后输入到全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)  <span class="comment"># 10 个类别</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))  <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))  <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>) <span class="comment"># 展平</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))    <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)            <span class="comment"># 最后一层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleCNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 定义损失函数与优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 多分类交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 模型训练</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(images)  <span class="comment"># 前向传播</span></span><br><span class="line">        loss = criterion(outputs, labels)  <span class="comment"># 计算损失</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">        loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 模型测试</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 关闭梯度计算</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="运行结果说明"><a href="#运行结果说明" class="headerlink" title="运行结果说明"></a>运行结果说明</h3><p><strong>1. 输出的训练损失</strong></p>
<p>代码中每个 epoch 会输出一次平均损失，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Epoch [1/5], Loss: 0.2325</span><br><span class="line">Epoch [2/5], Loss: 0.0526</span><br><span class="line">Epoch [3/5], Loss: 0.0366</span><br><span class="line">Epoch [4/5], Loss: 0.0273</span><br><span class="line">Epoch [5/5], Loss: 0.0221</span><br></pre></td></tr></table></figure>

<p>**解释：**损失逐渐下降表明模型在逐步收敛。</p>
<p><strong>2. 测试集的准确率</strong></p>
<p>代码在测试集上输出<strong>最终的分类准确率</strong>，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Test Accuracy: 98.96%</span><br></pre></td></tr></table></figure>

<p>**解释：**模型对 MNIST 测试集的分类准确率为 98.96%，对于简单的 CNN 模型来说是一个不错的结果。</p>
<h3 id="7、可视化结果"><a href="#7、可视化结果" class="headerlink" title="7、可视化结果"></a>7、可视化结果</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 数据加载与预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 定义 CNN 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># 输入1通道，输出32通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># 输入32通道，输出64通道</span></span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)  <span class="comment"># 展平后输入到全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)  <span class="comment"># 10 个类别</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))  <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))  <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>) <span class="comment"># 展平</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))    <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)            <span class="comment"># 最后一层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleCNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 定义损失函数与优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 多分类交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 模型训练</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(images)  <span class="comment"># 前向传播</span></span><br><span class="line">        loss = criterion(outputs, labels)  <span class="comment"># 计算损失</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">        loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 模型测试</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 关闭梯度计算</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 可视化测试结果</span></span><br><span class="line">dataiter = <span class="built_in">iter</span>(test_loader)</span><br><span class="line">images, labels = <span class="built_in">next</span>(dataiter)</span><br><span class="line">outputs = model(images)</span><br><span class="line">_, predictions = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">6</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    axes[i].imshow(images[i][<span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    axes[i].set_title(<span class="string">f&quot;Label: <span class="subst">&#123;labels[i]&#125;</span>\nPred: <span class="subst">&#123;predictions[i]&#125;</span>&quot;</span>)</span><br><span class="line">    axes[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h1 id="PyTorch-循环神经网络（RNN）"><a href="#PyTorch-循环神经网络（RNN）" class="headerlink" title="PyTorch 循环神经网络（RNN）"></a>PyTorch 循环神经网络（RNN）</h1><p><strong>循环神经网络（Recurrent Neural Networks, RNN）是一类神经网络架构</strong>，专门用于<strong>处理序列数据</strong>，能够<strong>捕捉时间序列或有序数据的动态信息</strong>，能够处理序列数据，如文本、时间序列或音频。</p>
<p>RNN 在自然语言处理（NLP）、语音识别、时间序列预测等任务中有着广泛的应用。</p>
<p>RNN 的关键特性是其能够保持隐状态（hidden state），使得网络能够记住先前时间步的信息，这对于处理序列数据至关重要。</p>
<h3 id="RNN-的基本结构"><a href="#RNN-的基本结构" class="headerlink" title="RNN 的基本结构"></a>RNN 的基本结构</h3><p>在传统的前馈神经网络（Feedforward Neural Network）中，数据是<strong>从输入层流向输出层</strong>的，而在 RNN 中，数据不仅沿着网络层级流动，还会在每个时间步骤上传播到当前的隐层状态，从而将之前的信息传递到下一个时间步骤。</p>
<p><strong>隐状态（Hidden State）：</strong> RNN <strong>通过隐状态来记住序列中的信息</strong>。</p>
<p>隐状态是通过上一时间步的隐状态和当前输入共同计算得到的。</p>
<p>公式：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-rnn-1.png" alt="img"></p>
<ul>
<li>ht：当前时刻的隐状态。</li>
<li>ht-1：<strong>前一时刻</strong>的隐状态。</li>
<li>Xt：<strong>当前</strong>时刻的<strong>输入</strong>。</li>
<li>Whh、Wxh：<strong>权重矩阵</strong>。</li>
<li>b：偏置项。</li>
<li>f：<strong>激活函数（如 Tanh 或 ReLU）</strong>。</li>
</ul>
<p><strong>输出（Output）：</strong> RNN 的输出<strong>不仅依赖当前的输入，还依赖于隐状态的历史信息</strong>。</p>
<p>公式：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-rnn-2.png" alt="img"></p>
<ul>
<li>yt：当前时刻的隐状态。</li>
<li>Why：当前时刻的隐状态。</li>
</ul>
<h3 id="RNN-如何处理序列数据"><a href="#RNN-如何处理序列数据" class="headerlink" title="RNN 如何处理序列数据"></a>RNN 如何处理序列数据</h3><p>循环神经网络（RNN）在处理序列数据时的展开（unfold）视图如下：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_dznTsiaHCvRc70fxWWEcgw.png" alt="img"></p>
<p>RNN 是一种处理序列数据的神经网络，它通过<strong>循环连接来处理序列中的每个元素</strong>，并在每个时间步传递信息，以下是图中各部分的说明：</p>
<ul>
<li><strong>输入序列（Xt, Xt-1, Xt+1, …）</strong>：图中的粉色圆圈代表输入序列中的各个元素，如Xt表示当前时间步的输入，Xt-1表示前一个时间步的输入，以此类推。</li>
<li><strong>隐藏状态（ht, ht-1, ht+1, …）</strong>：绿色矩形代表<strong>RNN的隐藏状态</strong>，它在每个时间步存储有关序列的信息。ht是当前时间步的隐藏状态，ht-1是前一个时间步的隐藏状态。</li>
<li><strong>权重矩阵（U, W, V）</strong>：<ul>
<li><code>U</code>：<strong>输入到隐藏状态的权重矩阵</strong>，用于将输入<code>Xt</code><strong>转换为隐藏状态的一部分</strong>。</li>
<li><code>W</code>：<strong>隐藏状态到隐藏状态</strong>的权重矩阵，用于将<strong>前一时间步的隐藏状态<code>ht-1</code>转换为当前</strong>时间步隐藏状态的一部分。</li>
<li><code>V</code>：<strong>隐藏状态到输出的权重矩阵</strong>，用于将隐藏状态<code>ht</code>转换为输出<code>Yt</code>。</li>
</ul>
</li>
<li><strong>输出序列（Yt, Yt-1, Yt+1, …）</strong>：蓝色圆圈代表RNN在每个时间步的输出，如Yt是当前时间步的输出。</li>
<li><strong>循环连接</strong>：RNN的特点是<strong>隐藏状态的循环连接</strong>，这允许网络在<strong>处理当前时间步的输入时考虑到之前时间步的信息</strong>。</li>
<li><strong>展开（Unfold）</strong>：图中展示了RNN在序列上的展开过程，这有助于理解RNN如何<strong>在时间上处理序列数据</strong>。在实际的RNN实现中，这些步骤是并行处理的，但在概念上，我们可以将其展开来理解信息是如何流动的。</li>
<li><strong>信息流动</strong>：信息从输入序列通过权重矩阵U传递到隐藏状态，然后通过权重矩阵W在时间步之间传递，最后通过权重矩阵V从隐藏状态传递到输出序列。</li>
</ul>
<hr>
<h2 id="PyTorch-中的-RNN-基础"><a href="#PyTorch-中的-RNN-基础" class="headerlink" title="PyTorch 中的 RNN 基础"></a>PyTorch 中的 RNN 基础</h2><p>在 PyTorch 中，RNN 可以用于构建复杂的序列模型。</p>
<p>PyTorch 提供了几种 RNN 模块，包括：</p>
<ul>
<li><code>torch.nn.RNN</code>：<strong>基本的RNN单元。</strong></li>
<li><code>torch.nn.LSTM</code>：**长短期记忆单元，**能够学习长期依赖关系。</li>
<li><code>torch.nn.GRU</code>：门控循环单元，是<strong>LSTM的简化版本</strong>，但通常更容易训练。</li>
</ul>
<p>使用 RNN 类时，您需要指定输入的维度、隐藏层的维度以及其他一些超参数。</p>
<h3 id="PyTorch-实现一个简单的-RNN-实例"><a href="#PyTorch-实现一个简单的-RNN-实例" class="headerlink" title="PyTorch 实现一个简单的 RNN 实例"></a>PyTorch 实现一个简单的 RNN 实例</h3><p>以下是一个简单的 PyTorch 实现例子，使用 RNN 模型来处理序列数据并进行分类。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集：字符序列预测（Hello -&gt; Elloh）</span></span><br><span class="line">char_set = <span class="built_in">list</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">char_to_idx = &#123;c: i <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(char_set)&#125;</span><br><span class="line">idx_to_char = &#123;i: c <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(char_set)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备</span></span><br><span class="line">input_str = <span class="string">&quot;hello&quot;</span></span><br><span class="line">target_str = <span class="string">&quot;elloh&quot;</span></span><br><span class="line">input_data = [char_to_idx[c] <span class="keyword">for</span> c <span class="keyword">in</span> input_str]</span><br><span class="line">target_data = [char_to_idx[c] <span class="keyword">for</span> c <span class="keyword">in</span> target_str]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为独热编码</span></span><br><span class="line">input_one_hot = np.eye(<span class="built_in">len</span>(char_set))[input_data]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为 PyTorch Tensor</span></span><br><span class="line">inputs = torch.tensor(input_one_hot, dtype=torch.float32)</span><br><span class="line">targets = torch.tensor(target_data, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型超参数</span></span><br><span class="line">input_size = <span class="built_in">len</span>(char_set)</span><br><span class="line">hidden_size = <span class="number">8</span></span><br><span class="line">output_size = <span class="built_in">len</span>(char_set)</span><br><span class="line">num_epochs = <span class="number">200</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 RNN 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.RNN(input_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, hidden</span>):</span><br><span class="line">        out, hidden = <span class="variable language_">self</span>.rnn(x, hidden)</span><br><span class="line">        out = <span class="variable language_">self</span>.fc(out)  <span class="comment"># 应用全连接层</span></span><br><span class="line">        <span class="keyword">return</span> out, hidden</span><br><span class="line"></span><br><span class="line">model = RNNModel(input_size, hidden_size, output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练 RNN</span></span><br><span class="line">losses = []</span><br><span class="line">hidden = <span class="literal">None</span>  <span class="comment"># 初始隐藏状态为 None</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    outputs, hidden = model(inputs.unsqueeze(<span class="number">0</span>), hidden)</span><br><span class="line">    hidden = hidden.detach()  <span class="comment"># 防止梯度爆炸</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = criterion(outputs.view(-<span class="number">1</span>, output_size), targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试 RNN</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    test_hidden = <span class="literal">None</span></span><br><span class="line">    test_output, _ = model(inputs.unsqueeze(<span class="number">0</span>), test_hidden)</span><br><span class="line">    predicted = torch.argmax(test_output, dim=<span class="number">2</span>).squeeze().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Input sequence: &quot;</span>, <span class="string">&#x27;&#x27;</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> input_data]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Predicted sequence: &quot;</span>, <span class="string">&#x27;&#x27;</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> predicted]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化损失</span></span><br><span class="line">plt.plot(losses, label=<span class="string">&quot;Training Loss&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Epoch&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;RNN Training Loss Over Epochs&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><strong>代码解析：</strong></p>
<ol>
<li><strong>数据准备</strong>：<ul>
<li>使用字符序列 <code>hello</code>，并将其转化为独热编码。</li>
<li>目标序列为 <code>elloh</code>，即向右旋转一个字符。</li>
</ul>
</li>
<li><strong>模型构建</strong>：<ul>
<li>使用 <code>torch.nn.RNN</code> 创建循环神经网络。</li>
<li>加入全连接层 <code>torch.nn.Linear</code> 用于映射隐藏状态到输出。</li>
</ul>
</li>
<li><strong>训练部分</strong>：<ul>
<li>每一轮都计算损失并反向传播。</li>
<li>隐藏状态通过 <code>hidden.detach()</code> <strong>防止梯度爆炸</strong>。</li>
</ul>
</li>
<li><strong>测试部分</strong>：<ul>
<li>模型输出字符的预测结果。</li>
</ul>
</li>
<li><strong>可视化</strong>：<ul>
<li>用 Matplotlib 绘制训练损失的变化趋势。</li>
</ul>
</li>
</ol>
<p>假设你的模型训练良好，输出可能如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Epoch [20/200], Loss: 0.0013</span><br><span class="line">Epoch [40/200], Loss: 0.0003</span><br><span class="line">Epoch [60/200], Loss: 0.0002</span><br><span class="line">Epoch [80/200], Loss: 0.0001</span><br><span class="line">Epoch [100/200], Loss: 0.0001</span><br><span class="line">Epoch [120/200], Loss: 0.0001</span><br><span class="line">Epoch [140/200], Loss: 0.0001</span><br><span class="line">Epoch [160/200], Loss: 0.0001</span><br><span class="line">Epoch [180/200], Loss: 0.0001</span><br><span class="line">Epoch [200/200], Loss: 0.0001</span><br><span class="line">Input sequence:  hello</span><br></pre></td></tr></table></figure>

<p>从结果来看，图像显示损失逐渐减少，表明模型训练有效。</p>
<h1 id="PyTorch-数据集"><a href="#PyTorch-数据集" class="headerlink" title="PyTorch 数据集"></a>PyTorch 数据集</h1><p>在深度学习任务中，数据加载和处理是至关重要的一环。</p>
<p>PyTorch 提供了<strong>强大的数据加载和处理工具</strong>，主要包括：</p>
<ul>
<li><strong><code>torch.utils.data.Dataset</code></strong>：数据集的<strong>抽象类</strong>，需要<strong>自定义并实现 <code>__len__</code>（数据集大小）和 <code>__getitem__</code>（按索引获取样本）</strong>。</li>
<li><strong><code>torch.utils.data.TensorDataset</code></strong>：<strong>基于张量</strong>的数据集，适合<strong>处理数据-标签对</strong>，直接支持批处理和迭代。</li>
<li><strong><code>torch.utils.data.DataLoader</code></strong>：<strong>封装 Dataset 的迭代器</strong>，提供<strong>批处理、数据打乱、多线程加载</strong>等功能，便于数据输入模型训练。</li>
<li><strong><code>torchvision.datasets.ImageFolder</code></strong>：<strong>从文件夹加载图像数据</strong>，每个子文件夹代表一个类别，适用于图像分类任务。</li>
</ul>
<h3 id="PyTorch-内置数据集"><a href="#PyTorch-内置数据集" class="headerlink" title="PyTorch 内置数据集"></a>PyTorch 内置数据集</h3><p>PyTorch 通过 torchvision.datasets 模块提供了许多常用的数据集，例如：</p>
<ul>
<li><strong>MNIST</strong>：<strong>手写数字图像数据集</strong>，用于图像分类任务。</li>
<li><strong>CIFAR</strong>：包含 10 个类别、60000 张 32x32 的<strong>彩色图像数据集</strong>，用于图像分类任务。</li>
<li><strong>COCO</strong>：<strong>通用物体检测、分割、关键点检测数据集</strong>，包含超过 330k 个图像和 2.5M 个目标实例的大规模数据集。</li>
<li><strong>ImageNet</strong>：包含超过 1400 万张图像，用于<strong>图像分类和物体检测等任务</strong>。</li>
<li><strong>STL-10</strong>：包含 100k 张 96x96 的彩色图像数据集，<strong>用于图像分类任务</strong>。</li>
<li><strong>Cityscapes</strong>：包含 5000 张精细注释的城市街道场景图像，<strong>用于语义分割任务</strong>。</li>
<li><strong>SQUAD</strong>：<strong>用于机器阅读理解任务的数据集</strong>。</li>
</ul>
<p>以上数据集可以通过 <strong>torchvision.datasets 模块中的函数</strong>进行加载，也可以通过自定义的方式加载其他数据集。</p>
<h3 id="torchvision-和-torchtext"><a href="#torchvision-和-torchtext" class="headerlink" title="torchvision 和 torchtext"></a>torchvision 和 torchtext</h3><ul>
<li><strong>torchvision</strong>： 一个<strong>图形库，提供了图片数据处理相关的 API 和数据集接口</strong>，包括数据集加载函数和常用的图像变换。</li>
<li><strong>torchtext</strong>： <strong>自然语言处理工具包，提供了文本数据处理和建模的工具</strong>，包括数据预处理和数据加载的方式。</li>
</ul>
<hr>
<h2 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h2><p>Dataset 是 PyTorch 中用于数据集抽象的类。</p>
<p>自定义数据集需要继承 torch.utils.data.Dataset 并重写以下两个方法：</p>
<ul>
<li><code>__len__</code>：返回数据集的大小。</li>
<li><code>__getitem__</code>：按索引获取一个数据样本及其标签。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, labels</span>):</span><br><span class="line">        <span class="comment"># 数据初始化</span></span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 按索引返回数据和标签</span></span><br><span class="line">        sample = <span class="variable language_">self</span>.data[idx]</span><br><span class="line">        label = <span class="variable language_">self</span>.labels[idx]</span><br><span class="line">        <span class="keyword">return</span> sample, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">data = torch.randn(<span class="number">100</span>, <span class="number">5</span>)  <span class="comment"># 100 个样本，每个样本有 5 个特征</span></span><br><span class="line">labels = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">100</span>,))  <span class="comment"># 100 个标签，取值为 0 或 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化数据集</span></span><br><span class="line">dataset = MyDataset(data, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集大小:&quot;</span>, <span class="built_in">len</span>(dataset))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第 0 个样本:&quot;</span>, dataset[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据集大小: <span class="number">100</span></span><br><span class="line">第 <span class="number">0</span> 个样本: (tensor([-<span class="number">0.2006</span>,  <span class="number">0.7304</span>, -<span class="number">1.3911</span>, -<span class="number">0.4408</span>,  <span class="number">1.1447</span>]), tensor(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<h2 id="torch-utils-data-DataLoader"><a href="#torch-utils-data-DataLoader" class="headerlink" title="torch.utils.data.DataLoader"></a>torch.utils.data.DataLoader</h2><p>DataLoader 是 PyTorch 提供的<strong>数据加载器</strong>，用于批量加载数据集。</p>
<p>提供了以下功能：</p>
<ul>
<li><strong>批量加载</strong>：通过设置 <code>batch_size</code>。</li>
<li><strong>数据打乱</strong>：通过设置 <code>shuffle=True</code>。</li>
<li><strong>多线程加速</strong>：通过设置 <code>num_workers</code>。</li>
<li><strong>迭代访问</strong>：方便地按批次访问数据。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, labels</span>):</span><br><span class="line">        <span class="comment"># 数据初始化</span></span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 按索引返回数据和标签</span></span><br><span class="line">        sample = <span class="variable language_">self</span>.data[idx]</span><br><span class="line">        label = <span class="variable language_">self</span>.labels[idx]</span><br><span class="line">        <span class="keyword">return</span> sample, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">data = torch.randn(<span class="number">100</span>, <span class="number">5</span>)  <span class="comment"># 100 个样本，每个样本有 5 个特征</span></span><br><span class="line">labels = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">100</span>,))  <span class="comment"># 100 个标签，取值为 0 或 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化数据集</span></span><br><span class="line">dataset = MyDataset(data, labels)</span><br><span class="line"><span class="comment"># 实例化 DataLoader</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历 DataLoader</span></span><br><span class="line"><span class="keyword">for</span> batch_idx, (batch_data, batch_labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;批次 <span class="subst">&#123;batch_idx + <span class="number">1</span>&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;数据:&quot;</span>, batch_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;标签:&quot;</span>, batch_labels)</span><br><span class="line">    <span class="keyword">if</span> batch_idx == <span class="number">2</span>:  <span class="comment"># 仅显示前 3 个批次</span></span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">批次 <span class="number">1</span></span><br><span class="line">数据: tensor([[ <span class="number">0.4689</span>,  <span class="number">0.6666</span>, -<span class="number">1.0234</span>,  <span class="number">0.8948</span>,  <span class="number">0.4503</span>],</span><br><span class="line">        [ <span class="number">0.0273</span>, -<span class="number">0.4684</span>, -<span class="number">0.7762</span>,  <span class="number">0.7963</span>,  <span class="number">0.2168</span>],</span><br><span class="line">        [ <span class="number">1.0677</span>, -<span class="number">0.3502</span>, -<span class="number">0.9594</span>, -<span class="number">1.1318</span>, -<span class="number">0.2196</span>],</span><br><span class="line">        [-<span class="number">1.4989</span>,  <span class="number">0.0267</span>,  <span class="number">1.0405</span>, -<span class="number">0.7284</span>,  <span class="number">0.2335</span>],</span><br><span class="line">        [-<span class="number">0.5887</span>, -<span class="number">0.4934</span>,  <span class="number">1.6283</span>,  <span class="number">1.4638</span>,  <span class="number">0.0157</span>],</span><br><span class="line">        [-<span class="number">1.1047</span>, -<span class="number">0.6550</span>, -<span class="number">0.0381</span>,  <span class="number">0.3617</span>, -<span class="number">1.2792</span>],</span><br><span class="line">        [ <span class="number">0.3592</span>, -<span class="number">0.8264</span>,  <span class="number">0.0231</span>, -<span class="number">1.5508</span>,  <span class="number">0.6833</span>],</span><br><span class="line">        [-<span class="number">0.6835</span>,  <span class="number">0.6979</span>,  <span class="number">0.9048</span>, -<span class="number">0.4756</span>,  <span class="number">0.3003</span>],</span><br><span class="line">        [ <span class="number">1.1562</span>, -<span class="number">0.4516</span>, -<span class="number">1.2415</span>,  <span class="number">0.2859</span>,  <span class="number">0.5837</span>],</span><br><span class="line">        [ <span class="number">0.7937</span>,  <span class="number">1.5316</span>, -<span class="number">0.6139</span>,  <span class="number">0.7999</span>,  <span class="number">0.5506</span>]])</span><br><span class="line">标签: tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">批次 <span class="number">2</span></span><br><span class="line">数据: tensor([[-<span class="number">0.0388</span>, -<span class="number">0.3658</span>,  <span class="number">0.8993</span>, -<span class="number">1.5027</span>,  <span class="number">1.0738</span>],</span><br><span class="line">        [-<span class="number">0.6182</span>,  <span class="number">1.0684</span>, -<span class="number">2.3049</span>,  <span class="number">0.8338</span>,  <span class="number">0.1363</span>],</span><br><span class="line">        [-<span class="number">0.5289</span>,  <span class="number">0.1661</span>, -<span class="number">0.0349</span>,  <span class="number">0.2112</span>,  <span class="number">1.4745</span>],</span><br><span class="line">        [-<span class="number">0.3304</span>, -<span class="number">1.2114</span>, -<span class="number">0.2982</span>, -<span class="number">0.3006</span>,  <span class="number">0.5252</span>],</span><br><span class="line">        [-<span class="number">1.4394</span>, -<span class="number">0.3732</span>,  <span class="number">1.0281</span>,  <span class="number">0.5754</span>,  <span class="number">1.0081</span>],</span><br><span class="line">        [ <span class="number">0.8714</span>, -<span class="number">0.1945</span>, -<span class="number">0.2451</span>, -<span class="number">0.2879</span>, -<span class="number">2.0520</span>],</span><br><span class="line">        [ <span class="number">0.0235</span>,  <span class="number">0.4360</span>,  <span class="number">0.1233</span>,  <span class="number">0.0504</span>,  <span class="number">0.5908</span>],</span><br><span class="line">        [ <span class="number">0.5927</span>,  <span class="number">0.1785</span>, -<span class="number">0.9052</span>, -<span class="number">0.9012</span>,  <span class="number">0.8914</span>],</span><br><span class="line">        [ <span class="number">0.4693</span>,  <span class="number">0.5533</span>, -<span class="number">0.1903</span>,  <span class="number">0.0267</span>,  <span class="number">0.4077</span>],</span><br><span class="line">        [-<span class="number">1.1683</span>,  <span class="number">1.6699</span>, -<span class="number">0.4846</span>, -<span class="number">0.7404</span>,  <span class="number">0.3370</span>]])</span><br><span class="line">标签: tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">批次 <span class="number">3</span></span><br><span class="line">数据: tensor([[ <span class="number">0.2103</span>, -<span class="number">0.7839</span>,  <span class="number">1.4899</span>,  <span class="number">2.2749</span>, -<span class="number">0.7548</span>],</span><br><span class="line">        [-<span class="number">1.2836</span>,  <span class="number">1.0025</span>, -<span class="number">1.1162</span>, -<span class="number">0.4261</span>,  <span class="number">1.0690</span>],</span><br><span class="line">        [-<span class="number">0.7969</span>,  <span class="number">1.0418</span>, -<span class="number">0.7405</span>,  <span class="number">0.8766</span>,  <span class="number">0.2347</span>],</span><br><span class="line">        [-<span class="number">1.1071</span>,  <span class="number">1.8560</span>, -<span class="number">1.2979</span>, -<span class="number">0.8364</span>, -<span class="number">0.2925</span>],</span><br><span class="line">        [-<span class="number">1.0488</span>,  <span class="number">0.4802</span>, -<span class="number">0.6453</span>,  <span class="number">0.2009</span>,  <span class="number">0.5693</span>],</span><br><span class="line">        [ <span class="number">0.8883</span>,  <span class="number">0.4619</span>, -<span class="number">0.2087</span>,  <span class="number">0.2189</span>, -<span class="number">0.3708</span>],</span><br><span class="line">        [-<span class="number">1.4578</span>,  <span class="number">0.3629</span>,  <span class="number">1.8282</span>,  <span class="number">0.5353</span>, -<span class="number">1.1783</span>],</span><br><span class="line">        [-<span class="number">1.2813</span>,  <span class="number">0.5129</span>, -<span class="number">0.4598</span>, -<span class="number">0.2131</span>, -<span class="number">1.2804</span>],</span><br><span class="line">        [ <span class="number">1.7831</span>,  <span class="number">1.1730</span>, -<span class="number">0.2305</span>, -<span class="number">0.6550</span>,  <span class="number">0.1197</span>],</span><br><span class="line">        [-<span class="number">0.9384</span>, -<span class="number">0.0483</span>,  <span class="number">1.9626</span>,  <span class="number">0.3342</span>,  <span class="number">0.1700</span>]])</span><br><span class="line">标签: tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h2 id="使用内置数据集"><a href="#使用内置数据集" class="headerlink" title="使用内置数据集"></a>使用内置数据集</h2><p>PyTorch 提供了多个常用数据集，存放在 <strong>torchvision 中</strong>，特别适合图像任务。</p>
<p>加载 MNIST 数据集:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转换为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练数据集</span></span><br><span class="line">train_dataset = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 DataLoader 加载数据</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看一个批次的数据</span></span><br><span class="line">data_iter = <span class="built_in">iter</span>(train_loader)</span><br><span class="line">images, labels = <span class="built_in">next</span>(data_iter)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;批次图像大小: <span class="subst">&#123;images.shape&#125;</span>&quot;</span>)  <span class="comment"># 输出形状为 [batch_size, 1, 28, 28]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;批次标签: <span class="subst">&#123;labels&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">批次图像大小: torch.Size([32, 1, 28, 28])</span><br><span class="line">批次标签: tensor([0, 4, 9, 8, 1, 3, 8, 1, 7, 2, 1, 1, 1, 2, 6, 3, 9, 7, 6, 9, 4, 9, 7, 1,</span><br><span class="line">        3, 7, 3, 0, 7, 7, 6, 7])</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Dataset-与-DataLoader-的自定义应用"><a href="#Dataset-与-DataLoader-的自定义应用" class="headerlink" title="Dataset 与 DataLoader 的自定义应用"></a>Dataset 与 DataLoader 的自定义应用</h2><p>以下是一个将 <strong>CSV 文件 作为数据源</strong>，并通过<strong>自定义 Dataset 和 DataLoader 读取数据</strong>。</p>
<p><strong>CSV 文件内容如下（下载<a target="_blank" rel="noopener" href="https://static.jyshare.com/download/runoob_pytorch_data.csv">runoob_pytorch_data.csv</a>）：</strong></p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-dataset-21.png" alt="img"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义 CSV 数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CSVDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path</span>):</span><br><span class="line">        <span class="comment"># 读取 CSV 文件</span></span><br><span class="line">        <span class="variable language_">self</span>.data = pd.read_csv(file_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 使用 .iloc 明确基于位置索引</span></span><br><span class="line">        row = <span class="variable language_">self</span>.data.iloc[idx]</span><br><span class="line">        <span class="comment"># 将特征和标签分开</span></span><br><span class="line">        features = torch.tensor(row.iloc[:-<span class="number">1</span>].to_numpy(), dtype=torch.float32)  <span class="comment"># 特征</span></span><br><span class="line">        label = torch.tensor(row.iloc[-<span class="number">1</span>], dtype=torch.float32)  <span class="comment"># 标签</span></span><br><span class="line">        <span class="keyword">return</span> features, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化数据集和 DataLoader</span></span><br><span class="line">dataset = CSVDataset(<span class="string">&quot;runoob_pytorch_data.csv&quot;</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历 DataLoader</span></span><br><span class="line"><span class="keyword">for</span> features, label <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;特征:&quot;</span>, features)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;标签:&quot;</span>, label)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">特征: tensor([[ 1.2000,  2.1000, -3.0000],</span><br><span class="line">        [ 1.0000,  1.1000, -2.0000],</span><br><span class="line">        [ 0.5000, -1.2000,  3.3000],</span><br><span class="line">        [-0.3000,  0.8000,  1.2000]])</span><br><span class="line">标签: tensor([1., 0., 1., 0.])</span><br><span class="line">tianqixin@Mac-mini runoob-test % python3 test.py</span><br><span class="line">特征: tensor([[ 1.5000,  2.2000, -1.1000],</span><br><span class="line">        [ 2.1000, -3.3000,  0.0000],</span><br><span class="line">        [-2.3000,  0.4000,  0.7000],</span><br><span class="line">        [-0.3000,  0.8000,  1.2000]])</span><br><span class="line">标签: tensor([0., 1., 0., 0.])</span><br></pre></td></tr></table></figure>

<h1 id="PyTorch-数据转换"><a href="#PyTorch-数据转换" class="headerlink" title="PyTorch 数据转换"></a>PyTorch 数据转换</h1><p>在 PyTorch 中，数据转换（Data Transformation） 是一种在加载数据时<strong>对数据进行处理的机制</strong>，将原始数据<strong>转换成适合模型训练的格式</strong>，主要通过 <strong>torchvision.transforms</strong> 提供的工具完成。</p>
<p>数据转换不仅可以实现基本的<strong>数据预处理（如归一化、大小调整等）</strong>，还能帮助进行<strong>数据增强（如随机裁剪、翻转等）</strong>，提高模型的<strong>泛化能力</strong>。</p>
<h3 id="为什么需要数据转换？"><a href="#为什么需要数据转换？" class="headerlink" title="为什么需要数据转换？"></a>为什么需要数据转换？</h3><p><strong>数据预处理</strong>：</p>
<ul>
<li><strong>调整数据格式、大小和范围</strong>，使其<strong>适合模型输入</strong>。</li>
<li>例如，图像需要调整为固定大小、张量格式并归一化到 [0,1]。</li>
</ul>
<p><strong>数据增强</strong>：</p>
<ul>
<li>在训练时对数据进行<strong>变换</strong>，以<strong>增加多样性</strong>。</li>
<li>例如，通过随机旋转、翻转和裁剪增加数据样本的变种，<strong>避免过拟合</strong>。</li>
</ul>
<p><strong>灵活性</strong>：</p>
<ul>
<li>通过定义一系列转换操作，可以动态地对数据进行处理，简化数据加载的复杂度。</li>
</ul>
<p>在 PyTorch 中，torchvision.transforms 模块提供了多种用于图像处理的变换操作。</p>
<h3 id="基础变换操作"><a href="#基础变换操作" class="headerlink" title="基础变换操作"></a>基础变换操作</h3><table>
<thead>
<tr>
<th align="left">变换函数名称</th>
<th align="left">描述</th>
<th align="left">实例</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>transforms.ToTensor()</strong></td>
<td align="left">将<strong>PIL图像或NumPy数组转换为PyTorch张量</strong>，并自动<strong>将像素值归一化到 [0, 1]</strong>。</td>
<td align="left"><code>transform = transforms.ToTensor()</code></td>
</tr>
<tr>
<td align="left"><strong>transforms.Normalize(mean, std)</strong></td>
<td align="left">对图像进行<strong>标准化</strong>，使数据<strong>符合零均值和单位方差</strong>。</td>
<td align="left"><code>transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</code></td>
</tr>
<tr>
<td align="left"><strong>transforms.Resize(size)</strong></td>
<td align="left">调整图像尺寸，确保输入到网络的图像<strong>大小一致</strong>。</td>
<td align="left"><code>transform = transforms.Resize((256, 256))</code></td>
</tr>
<tr>
<td align="left"><strong>transforms.CenterCrop(size)</strong></td>
<td align="left"><strong>从图像中心裁剪指定大小的区域。</strong></td>
<td align="left"><code>transform = transforms.CenterCrop(224)</code></td>
</tr>
</tbody></table>
<p><strong>1、ToTensor</strong></p>
<p>将 PIL 图像或 NumPy 数组转换为 PyTorch 张量。</p>
<p><strong>同时将像素值从 [0, 255] 归一化为 [0, 1]。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from torchvision import transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.ToTensor()</span><br></pre></td></tr></table></figure>

<p><strong>2、Normalize</strong></p>
<p>对数据进行<strong>标准化</strong>，使其符合特定的均值和标准差。</p>
<p><strong>通常用于图像数据</strong>，将其像素值归一化为零均值和单位方差。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Normalize(mean=[0.5], std=[0.5])  # 归一化到 [-1, 1]</span><br></pre></td></tr></table></figure>

<p><strong>3、Resize</strong></p>
<p>调整图像的大小。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Resize((128, 128))  # 将图像调整为 128x128</span><br></pre></td></tr></table></figure>

<p><strong>4、CenterCrop</strong></p>
<p>从图像中心裁剪指定大小的区域。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.CenterCrop(128)  # 裁剪 128x128 的区域</span><br></pre></td></tr></table></figure>

<h3 id="数据增强操作"><a href="#数据增强操作" class="headerlink" title="数据增强操作"></a>数据增强操作</h3><table>
<thead>
<tr>
<th align="left">变换函数名称</th>
<th align="left">描述</th>
<th align="left">实例</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>transforms.RandomHorizontalFlip(p)</strong></td>
<td align="left"><strong>随机水平翻转</strong>图像。</td>
<td align="left"><code>transform = transforms.RandomHorizontalFlip(p=0.5)</code></td>
</tr>
<tr>
<td align="left"><strong>transforms.RandomRotation(degrees)</strong></td>
<td align="left"><strong>随机旋转</strong>图像。</td>
<td align="left"><code>transform = transforms.RandomRotation(degrees=45)</code></td>
</tr>
<tr>
<td align="left"><strong>transforms.ColorJitter(brightness, contrast, saturation, hue)</strong></td>
<td align="left">调整图像的<strong>亮度、对比度、饱和度和色调</strong>。</td>
<td align="left"><code>transform = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)</code></td>
</tr>
<tr>
<td align="left"><strong>transforms.RandomCrop(size)</strong></td>
<td align="left"><strong>随机裁剪指定大小</strong>的区域。</td>
<td align="left"><code>transform = transforms.RandomCrop(224)</code></td>
</tr>
<tr>
<td align="left"><strong>transforms.RandomResizedCrop(size)</strong></td>
<td align="left"><strong>随机裁剪图像并调整到指定大小。</strong></td>
<td align="left"><code>transform = transforms.RandomResizedCrop(224)</code></td>
</tr>
</tbody></table>
<p><strong>1、RandomCrop</strong></p>
<p>从图像中随机裁剪指定大小。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.RandomCrop(128)</span><br></pre></td></tr></table></figure>

<p><strong>2、RandomHorizontalFlip</strong></p>
<p>以一定概率水平翻转图像。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.RandomHorizontalFlip(p=0.5)  # 50% 概率翻转</span><br></pre></td></tr></table></figure>

<p><strong>3、RandomRotation</strong></p>
<p>随机旋转一定角度。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.RandomRotation(degrees=30)  # 随机旋转 -30 到 +30 度</span><br></pre></td></tr></table></figure>

<p><strong>4、ColorJitter</strong></p>
<p>随机改变图像的亮度、对比度、饱和度或色调。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.ColorJitter(brightness=0.5, contrast=0.5)</span><br></pre></td></tr></table></figure>

<h3 id="组合变换"><a href="#组合变换" class="headerlink" title="组合变换"></a>组合变换</h3><table>
<thead>
<tr>
<th align="left">变换函数名称</th>
<th align="left">描述</th>
<th align="left">实例</th>
</tr>
</thead>
<tbody><tr>
<td align="left">transforms.Compose()</td>
<td align="left">将多个变换组合在一起，按照顺序依次应用。</td>
<td align="left"><code>transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), transforms.Resize((256, 256))])</code></td>
</tr>
</tbody></table>
<h3 id="自定义转换"><a href="#自定义转换" class="headerlink" title="自定义转换"></a>自定义转换</h3><p>如果 transforms 提供的功能无法满足需求，可以通过自定义类或函数实现。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomTransform</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 这里可以自定义任何变换逻辑</span></span><br><span class="line">        <span class="keyword">return</span> x * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">transform = CustomTransform()</span><br></pre></td></tr></table></figure>

<h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h2><h3 id="对图像数据集应用转换"><a href="#对图像数据集应用转换" class="headerlink" title="对图像数据集应用转换"></a>对图像数据集应用转换</h3><p>加载 MNIST 数据集，并应用转换。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment"># 定义转换</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.5</span>], std=[<span class="number">0.5</span>])</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 使用 DataLoader</span></span><br><span class="line">train_loader = DataLoader(dataset=train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 查看转换后的数据</span></span><br><span class="line"><span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;图像张量大小:&quot;</span>, images.size())  <span class="comment"># [batch_size, 1, 128, 128]</span></span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">图像张量大小: torch.Size([32, 1, 128, 128])</span><br></pre></td></tr></table></figure>

<h3 id="可视化转换效果"><a href="#可视化转换效果" class="headerlink" title="可视化转换效果"></a>可视化转换效果</h3><p>以下代码展示了原始数据和经过转换后的数据对比。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="comment"># 原始和增强后的图像可视化</span></span><br><span class="line">transform_augment = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform_augment)</span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">dataset</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        image, label = dataset[i]</span><br><span class="line">        axs[i].imshow(image.squeeze(<span class="number">0</span>), cmap=<span class="string">&#x27;gray&#x27;</span>)  <span class="comment"># 将 (1, H, W) 转为 (H, W)</span></span><br><span class="line">        axs[i].set_title(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br><span class="line">        axs[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">show_images(dataset)</span><br></pre></td></tr></table></figure>

<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-transforms-1.png" alt="img"></p>
<h1 id="Pytorch-torch-参考手册"><a href="#Pytorch-torch-参考手册" class="headerlink" title="Pytorch torch 参考手册"></a>Pytorch torch 参考手册</h1><p>PyTorch 软件包包含了用于多维张量的数据结构，并定义了在这些张量上执行的数学运算。此外，它还提供了许多实用工具，用于高效地序列化张量和任意类型的数据，以及其他有用的工具。</p>
<p>它还有一个 CUDA 版本，可以让你在计算能力 &gt;&#x3D; 3.0 的 NVIDIA GPU 上运行张量计算。</p>
<h2 id="PyTorch-torch-API-手册"><a href="#PyTorch-torch-API-手册" class="headerlink" title="PyTorch torch API 手册"></a>PyTorch torch API 手册</h2><table>
<thead>
<tr>
<th align="left">类别</th>
<th align="left">API</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Tensors</td>
<td align="left"><code>is_tensor(obj)</code></td>
<td align="left">检查 <code>obj</code> 是否为 PyTorch 张量。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>is_storage(obj)</code></td>
<td align="left">检查 <code>obj</code> 是否为 PyTorch 存储对象。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>is_complex(input)</code></td>
<td align="left">检查 <code>input</code> 数据类型是否为复数数据类型。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>is_conj(input)</code></td>
<td align="left">检查 <code>input</code> 是否为共轭张量。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>is_floating_point(input)</code></td>
<td align="left">检查 <code>input</code> 数据类型是否为浮点数据类型。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>is_nonzero(input)</code></td>
<td align="left">检查 <code>input</code> 是否为非零单一元素张量。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>set_default_dtype(d)</code></td>
<td align="left">设置默认浮点数据类型为 <code>d</code>。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>get_default_dtype()</code></td>
<td align="left">获取当前默认浮点 <code>torch.dtype</code>。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>set_default_device(device)</code></td>
<td align="left">设置默认 <code>torch.Tensor</code> 分配的设备为 <code>device</code>。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>get_default_device()</code></td>
<td align="left">获取默认 <code>torch.Tensor</code> 分配的设备。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>numel(input)</code></td>
<td align="left">返回 <code>input</code> 张量中的元素总数。</td>
</tr>
<tr>
<td align="left">Creation Ops</td>
<td align="left"><code>tensor(data)</code></td>
<td align="left">通过复制 <code>data</code> 构造无自动梯度历史的张量。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>sparse_coo_tensor(indices, values)</code></td>
<td align="left">在指定的 <code>indices</code> 处构造稀疏张量，具有指定的值。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>as_tensor(data)</code></td>
<td align="left">将 <code>data</code> 转换为张量，共享数据并尽可能保留自动梯度历史。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>zeros(size)</code></td>
<td align="left">返回一个用标量值 0 填充的张量，形状由 <code>size</code> 定义。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>ones(size)</code></td>
<td align="left">返回一个用标量值 1 填充的张量，形状由 <code>size</code> 定义。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>arange(start, end, step)</code></td>
<td align="left">返回一个 1-D 张量，包含从 <code>start</code> 到 <code>end</code> 的值，步长为 <code>step</code>。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>rand(size)</code></td>
<td align="left">返回一个从 [0, 1) 区间均匀分布的随机数填充的张量。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>randn(size)</code></td>
<td align="left">返回一个从标准正态分布填充的张量。</td>
</tr>
<tr>
<td align="left">Math operations</td>
<td align="left"><code>add(input, other, alpha)</code></td>
<td align="left">将 <code>other</code>（由 <code>alpha</code> 缩放）加到 <code>input</code> 上。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>mul(input, other)</code></td>
<td align="left">将 <code>input</code> 与 <code>other</code> 相乘。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>matmul(input, other)</code></td>
<td align="left">执行 <code>input</code> 和 <code>other</code> 的矩阵乘法。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>mean(input, dim)</code></td>
<td align="left">计算 <code>input</code> 在维度 <code>dim</code> 上的均值。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>sum(input, dim)</code></td>
<td align="left">计算 <code>input</code> 在维度 <code>dim</code> 上的和。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>max(input, dim)</code></td>
<td align="left">返回 <code>input</code> 在维度 <code>dim</code> 上的最大值。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>min(input, dim)</code></td>
<td align="left">返回 <code>input</code> 在维度 <code>dim</code> 上的最小值。</td>
</tr>
</tbody></table>
<h3 id="Tensor-创建"><a href="#Tensor-创建" class="headerlink" title="Tensor 创建"></a><strong>Tensor 创建</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.tensor(data, dtype, device, requires_grad)</code></td>
<td align="left">从数据创建张量。</td>
</tr>
<tr>
<td align="left"><code>torch.as_tensor(data, dtype, device)</code></td>
<td align="left">将数据转换为张量（共享内存）。</td>
</tr>
<tr>
<td align="left"><code>torch.from_numpy(ndarray)</code></td>
<td align="left">从 NumPy 数组创建张量（共享内存）。</td>
</tr>
<tr>
<td align="left"><code>torch.zeros(*size, dtype, device, requires_grad)</code></td>
<td align="left">创建全零张量。</td>
</tr>
<tr>
<td align="left"><code>torch.ones(*size, dtype, device, requires_grad)</code></td>
<td align="left">创建全一张量。</td>
</tr>
<tr>
<td align="left"><code>torch.empty(*size, dtype, device, requires_grad)</code></td>
<td align="left">创建未初始化的张量。</td>
</tr>
<tr>
<td align="left"><code>torch.arange(start, end, step, dtype, device, requires_grad)</code></td>
<td align="left">创建等差序列张量。</td>
</tr>
<tr>
<td align="left"><code>torch.linspace(start, end, steps, dtype, device, requires_grad)</code></td>
<td align="left">创建等间隔序列张量。</td>
</tr>
<tr>
<td align="left"><code>torch.logspace(start, end, steps, base, dtype, device, requires_grad)</code></td>
<td align="left">创建对数间隔序列张量。</td>
</tr>
<tr>
<td align="left"><code>torch.eye(n, m, dtype, device, requires_grad)</code></td>
<td align="left">创建单位矩阵。</td>
</tr>
<tr>
<td align="left"><code>torch.full(size, fill_value, dtype, device, requires_grad)</code></td>
<td align="left">创建填充指定值的张量。</td>
</tr>
<tr>
<td align="left"><code>torch.rand(*size, dtype, device, requires_grad)</code></td>
<td align="left">创建均匀分布随机张量（范围 [0, 1)）。</td>
</tr>
<tr>
<td align="left"><code>torch.randn(*size, dtype, device, requires_grad)</code></td>
<td align="left">创建标准正态分布随机张量。</td>
</tr>
<tr>
<td align="left"><code>torch.randint(low, high, size, dtype, device, requires_grad)</code></td>
<td align="left">创建整数随机张量。</td>
</tr>
<tr>
<td align="left"><code>torch.randperm(n, dtype, device, requires_grad)</code></td>
<td align="left">创建 0 到 n-1 的随机排列。</td>
</tr>
</tbody></table>
<hr>
<h3 id="Tensor-操作"><a href="#Tensor-操作" class="headerlink" title="Tensor 操作"></a><strong>Tensor 操作</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.cat(tensors, dim)</code></td>
<td align="left">沿指定维度连接张量。</td>
</tr>
<tr>
<td align="left"><code>torch.stack(tensors, dim)</code></td>
<td align="left">沿新维度堆叠张量。</td>
</tr>
<tr>
<td align="left"><code>torch.split(tensor, split_size, dim)</code></td>
<td align="left">将张量沿指定维度分割。</td>
</tr>
<tr>
<td align="left"><code>torch.chunk(tensor, chunks, dim)</code></td>
<td align="left">将张量沿指定维度分块。</td>
</tr>
<tr>
<td align="left"><code>torch.reshape(input, shape)</code></td>
<td align="left">改变张量的形状。</td>
</tr>
<tr>
<td align="left"><code>torch.transpose(input, dim0, dim1)</code></td>
<td align="left">交换张量的两个维度。</td>
</tr>
<tr>
<td align="left"><code>torch.squeeze(input, dim)</code></td>
<td align="left">移除大小为 1 的维度。</td>
</tr>
<tr>
<td align="left"><code>torch.unsqueeze(input, dim)</code></td>
<td align="left">在指定位置插入大小为 1 的维度。</td>
</tr>
<tr>
<td align="left"><code>torch.expand(input, size)</code></td>
<td align="left">扩展张量的尺寸。</td>
</tr>
<tr>
<td align="left"><code>torch.narrow(input, dim, start, length)</code></td>
<td align="left">返回张量的切片。</td>
</tr>
<tr>
<td align="left"><code>torch.permute(input, dims)</code></td>
<td align="left">重新排列张量的维度。</td>
</tr>
<tr>
<td align="left"><code>torch.masked_select(input, mask)</code></td>
<td align="left">根据布尔掩码选择元素。</td>
</tr>
<tr>
<td align="left"><code>torch.index_select(input, dim, index)</code></td>
<td align="left">沿指定维度选择索引对应的元素。</td>
</tr>
<tr>
<td align="left"><code>torch.gather(input, dim, index)</code></td>
<td align="left">沿指定维度收集指定索引的元素。</td>
</tr>
<tr>
<td align="left"><code>torch.scatter(input, dim, index, src)</code></td>
<td align="left">将 <code>src</code> 的值散布到 <code>input</code> 的指定位置。</td>
</tr>
<tr>
<td align="left"><code>torch.nonzero(input)</code></td>
<td align="left">返回非零元素的索引。</td>
</tr>
</tbody></table>
<hr>
<h3 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a><strong>数学运算</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.add(input, other)</code></td>
<td align="left">逐元素加法。</td>
</tr>
<tr>
<td align="left"><code>torch.sub(input, other)</code></td>
<td align="left">逐元素减法。</td>
</tr>
<tr>
<td align="left"><code>torch.mul(input, other)</code></td>
<td align="left">逐元素乘法。</td>
</tr>
<tr>
<td align="left"><code>torch.div(input, other)</code></td>
<td align="left">逐元素除法。</td>
</tr>
<tr>
<td align="left"><code>torch.matmul(input, other)</code></td>
<td align="left">矩阵乘法。</td>
</tr>
<tr>
<td align="left"><code>torch.pow(input, exponent)</code></td>
<td align="left">逐元素幂运算。</td>
</tr>
<tr>
<td align="left"><code>torch.sqrt(input)</code></td>
<td align="left">逐元素平方根。</td>
</tr>
<tr>
<td align="left"><code>torch.exp(input)</code></td>
<td align="left">逐元素指数函数。</td>
</tr>
<tr>
<td align="left"><code>torch.log(input)</code></td>
<td align="left">逐元素自然对数。</td>
</tr>
<tr>
<td align="left"><code>torch.sum(input, dim)</code></td>
<td align="left">沿指定维度求和。</td>
</tr>
<tr>
<td align="left"><code>torch.mean(input, dim)</code></td>
<td align="left">沿指定维度求均值。</td>
</tr>
<tr>
<td align="left"><code>torch.max(input, dim)</code></td>
<td align="left">沿指定维度求最大值。</td>
</tr>
<tr>
<td align="left"><code>torch.min(input, dim)</code></td>
<td align="left">沿指定维度求最小值。</td>
</tr>
<tr>
<td align="left"><code>torch.abs(input)</code></td>
<td align="left">逐元素绝对值。</td>
</tr>
<tr>
<td align="left"><code>torch.clamp(input, min, max)</code></td>
<td align="left">将张量值限制在指定范围内。</td>
</tr>
<tr>
<td align="left"><code>torch.round(input)</code></td>
<td align="left">逐元素四舍五入。</td>
</tr>
<tr>
<td align="left"><code>torch.floor(input)</code></td>
<td align="left">逐元素向下取整。</td>
</tr>
<tr>
<td align="left"><code>torch.ceil(input)</code></td>
<td align="left">逐元素向上取整。</td>
</tr>
</tbody></table>
<hr>
<h3 id="随机数生成"><a href="#随机数生成" class="headerlink" title="随机数生成"></a><strong>随机数生成</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.manual_seed(seed)</code></td>
<td align="left">设置随机种子。</td>
</tr>
<tr>
<td align="left"><code>torch.initial_seed()</code></td>
<td align="left">返回当前随机种子。</td>
</tr>
<tr>
<td align="left"><code>torch.rand(*size)</code></td>
<td align="left">创建均匀分布随机张量（范围 [0, 1)）。</td>
</tr>
<tr>
<td align="left"><code>torch.randn(*size)</code></td>
<td align="left">创建标准正态分布随机张量。</td>
</tr>
<tr>
<td align="left"><code>torch.randint(low, high, size)</code></td>
<td align="left">创建整数随机张量。</td>
</tr>
<tr>
<td align="left"><code>torch.randperm(n)</code></td>
<td align="left">返回 0 到 n-1 的随机排列。</td>
</tr>
</tbody></table>
<hr>
<h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a><strong>线性代数</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.dot(input, other)</code></td>
<td align="left">计算两个向量的点积。</td>
</tr>
<tr>
<td align="left"><code>torch.mm(input, mat2)</code></td>
<td align="left">矩阵乘法。</td>
</tr>
<tr>
<td align="left"><code>torch.bmm(input, mat2)</code></td>
<td align="left">批量矩阵乘法。</td>
</tr>
<tr>
<td align="left"><code>torch.eig(input)</code></td>
<td align="left">计算矩阵的特征值和特征向量。</td>
</tr>
<tr>
<td align="left"><code>torch.svd(input)</code></td>
<td align="left">计算矩阵的奇异值分解。</td>
</tr>
<tr>
<td align="left"><code>torch.inverse(input)</code></td>
<td align="left">计算矩阵的逆。</td>
</tr>
<tr>
<td align="left"><code>torch.det(input)</code></td>
<td align="left">计算矩阵的行列式。</td>
</tr>
<tr>
<td align="left"><code>torch.trace(input)</code></td>
<td align="left">计算矩阵的迹。</td>
</tr>
</tbody></table>
<hr>
<h3 id="设备管理"><a href="#设备管理" class="headerlink" title="设备管理"></a><strong>设备管理</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.cuda.is_available()</code></td>
<td align="left">检查 CUDA 是否可用。</td>
</tr>
<tr>
<td align="left"><code>torch.device(device)</code></td>
<td align="left">创建一个设备对象（如 <code>&#39;cpu&#39;</code> 或 <code>&#39;cuda:0&#39;</code>）。</td>
</tr>
<tr>
<td align="left"><code>torch.to(device)</code></td>
<td align="left">将张量移动到指定设备。</td>
</tr>
</tbody></table>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建张量</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y = torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 数学运算</span></span><br><span class="line">z = torch.add(x, <span class="number">1</span>)  <span class="comment"># 逐元素加 1</span></span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="comment"># 索引和切片</span></span><br><span class="line">mask = x &gt; <span class="number">1</span></span><br><span class="line">selected = torch.masked_select(x, mask)</span><br><span class="line"><span class="built_in">print</span>(selected)</span><br><span class="line"><span class="comment"># 设备管理</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    <span class="built_in">print</span>(x.device)</span><br></pre></td></tr></table></figure>

<h1 id="PyTorch-torch-nn-参考手册"><a href="#PyTorch-torch-nn-参考手册" class="headerlink" title="PyTorch torch.nn 参考手册"></a>PyTorch torch.nn 参考手册</h1><p>PyTorch 的 <code>torch.nn</code> 模块是<strong>构建和训练神经网络的核心模块</strong>，它提供了丰富的类和函数来定义和操作神经网络。</p>
<p>以下是 <code>torch.nn</code> 模块的一些关键组成部分及其功能：</p>
<p><strong>1、nn.Module 类</strong>：</p>
<ul>
<li><code>nn.Module</code> 是<strong>所有自定义神经网络模型的基类</strong>。用户通常会从这个类<strong>派生自己的模型类</strong>，并在其中<strong>定义网络层结构以及前向传播函数（forward pass）</strong>。</li>
</ul>
<p><strong>2、预定义层（Modules）</strong>：</p>
<ul>
<li>包括各种类型的层组件，例如<strong>卷积层（<code>nn.Conv1d</code>, <code>nn.Conv2d</code>, <code>nn.Conv3d</code>）</strong>、<strong>全连接层（<code>nn.Linear</code>）</strong>、**激活函数（<code>nn.ReLU</code>, <code>nn.Sigmoid</code>, <code>nn.Tanh</code>）**等。</li>
</ul>
<p><strong>3、容器类</strong>：</p>
<ul>
<li><code>nn.Sequential</code>：允许将多个层按顺序组合起来，<strong>形成简单的线性堆叠网络</strong>。</li>
<li><code>nn.ModuleList</code> 和 <code>nn.ModuleDict</code>：可以动态地存储和访问子模块，支持可变长度或命名的模块集合。</li>
</ul>
<p><strong>4、损失函数（Loss Functions）</strong>：</p>
<ul>
<li><code>torch.nn</code> 包含了一系列用于衡量模型预测与真实标签之间差异的<strong>损失函数</strong>，例如<strong>均方误差损失（<code>nn.MSELoss</code>）</strong>、**交叉熵损失（<code>nn.CrossEntropyLoss</code>）**等。</li>
</ul>
<p><strong>5、实用函数接口（Functional Interface）</strong>：</p>
<ul>
<li><strong><code>nn.functional</code>（通常简写为 <code>F</code>）</strong>，包含了许多可以<strong>直接作用于张量上</strong>的函数，它们实现了与层对象相同的功能，但不具有参数保存和更新的能力。例如，可以<strong>使用 <code>F.relu()</code> 直接进行 ReLU 操作</strong>，或者 <strong><code>F.conv2d()</code> 进行卷积操作</strong>。</li>
</ul>
<p><strong>6、初始化方法</strong>：</p>
<ul>
<li><code>torch.nn.init</code> 提供了一些常用的<strong>权重初始化策略</strong>，比如 <strong>Xavier 初始化 (<code>nn.init.xavier_uniform_()</code></strong>) 和 <strong>Kaiming 初始化 (<code>nn.init.kaiming_uniform_()</code>)</strong>，这些对于成功训练神经网络至关重要。</li>
</ul>
<hr>
<h2 id="PyTorch-torch-nn-模块参考手册"><a href="#PyTorch-torch-nn-模块参考手册" class="headerlink" title="PyTorch torch.nn 模块参考手册"></a>PyTorch torch.nn 模块参考手册</h2><h3 id="神经网络容器"><a href="#神经网络容器" class="headerlink" title="神经网络容器"></a><strong>神经网络容器</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>类&#x2F;函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.nn.Module</code></td>
<td align="left">所有神经网络模块的基类。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.Sequential(*args)</code></td>
<td align="left">按顺序组合多个模块。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.ModuleList(modules)</code></td>
<td align="left">将子模块存储在列表中。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.ModuleDict(modules)</code></td>
<td align="left">将子模块存储在字典中。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.ParameterList(parameters)</code></td>
<td align="left">将参数存储在列表中。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.ParameterDict(parameters)</code></td>
<td align="left">将参数存储在字典中。</td>
</tr>
</tbody></table>
<hr>
<h3 id="线性层"><a href="#线性层" class="headerlink" title="线性层"></a><strong>线性层</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>类&#x2F;函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.nn.Linear(in_features, out_features)</code></td>
<td align="left">全连接层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.Bilinear(in1_features, in2_features, out_features)</code></td>
<td align="left">双线性层。</td>
</tr>
</tbody></table>
<hr>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a><strong>卷积层</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>类&#x2F;函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.nn.Conv1d(in_channels, out_channels, kernel_size)</code></td>
<td align="left">一维卷积层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size)</code></td>
<td align="left">二维卷积层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.Conv3d(in_channels, out_channels, kernel_size)</code></td>
<td align="left">三维卷积层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size)</code></td>
<td align="left">一维转置卷积层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size)</code></td>
<td align="left">二维转置卷积层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size)</code></td>
<td align="left">三维转置卷积层。</td>
</tr>
</tbody></table>
<hr>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a><strong>池化层</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>类&#x2F;函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.nn.MaxPool1d(kernel_size)</code></td>
<td align="left">一维最大池化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.MaxPool2d(kernel_size)</code></td>
<td align="left">二维最大池化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.MaxPool3d(kernel_size)</code></td>
<td align="left">三维最大池化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.AvgPool1d(kernel_size)</code></td>
<td align="left">一维平均池化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.AvgPool2d(kernel_size)</code></td>
<td align="left">二维平均池化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.AvgPool3d(kernel_size)</code></td>
<td align="left">三维平均池化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.AdaptiveMaxPool1d(output_size)</code></td>
<td align="left">一维自适应最大池化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.AdaptiveAvgPool1d(output_size)</code></td>
<td align="left">一维自适应平均池化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.AdaptiveMaxPool2d(output_size)</code></td>
<td align="left">二维自适应最大池化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.AdaptiveAvgPool2d(output_size)</code></td>
<td align="left">二维自适应平均池化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.AdaptiveMaxPool3d(output_size)</code></td>
<td align="left">三维自适应最大池化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.AdaptiveAvgPool3d(output_size)</code></td>
<td align="left">三维自适应平均池化层。</td>
</tr>
</tbody></table>
<hr>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a><strong>激活函数</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>类&#x2F;函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.nn.ReLU()</code></td>
<td align="left">ReLU 激活函数。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.Sigmoid()</code></td>
<td align="left">Sigmoid 激活函数。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.Tanh()</code></td>
<td align="left">Tanh 激活函数。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.Softmax(dim)</code></td>
<td align="left">Softmax 激活函数。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.LogSoftmax(dim)</code></td>
<td align="left">LogSoftmax 激活函数。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.LeakyReLU(negative_slope)</code></td>
<td align="left">LeakyReLU 激活函数。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.ELU(alpha)</code></td>
<td align="left">ELU 激活函数。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.SELU()</code></td>
<td align="left">SELU 激活函数。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.GELU()</code></td>
<td align="left">GELU 激活函数。</td>
</tr>
</tbody></table>
<hr>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a><strong>损失函数</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>类&#x2F;函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.nn.MSELoss()</code></td>
<td align="left">均方误差损失。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.L1Loss()</code></td>
<td align="left">L1 损失。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.CrossEntropyLoss()</code></td>
<td align="left">交叉熵损失。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.NLLLoss()</code></td>
<td align="left">负对数似然损失。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.BCELoss()</code></td>
<td align="left">二分类交叉熵损失。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.BCEWithLogitsLoss()</code></td>
<td align="left">带 Sigmoid 的二分类交叉熵损失。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.KLDivLoss()</code></td>
<td align="left">KL 散度损失。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.HingeEmbeddingLoss()</code></td>
<td align="left">铰链嵌入损失。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.MultiMarginLoss()</code></td>
<td align="left">多分类间隔损失。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.SmoothL1Loss()</code></td>
<td align="left">平滑 L1 损失。</td>
</tr>
</tbody></table>
<hr>
<h3 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a><strong>归一化层</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>类&#x2F;函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.nn.BatchNorm1d(num_features)</code></td>
<td align="left">一维批归一化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.BatchNorm2d(num_features)</code></td>
<td align="left">二维批归一化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.BatchNorm3d(num_features)</code></td>
<td align="left">三维批归一化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.LayerNorm(normalized_shape)</code></td>
<td align="left">层归一化。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.InstanceNorm1d(num_features)</code></td>
<td align="left">一维实例归一化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.InstanceNorm2d(num_features)</code></td>
<td align="left">二维实例归一化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.InstanceNorm3d(num_features)</code></td>
<td align="left">三维实例归一化层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.GroupNorm(num_groups, num_channels)</code></td>
<td align="left">组归一化。</td>
</tr>
</tbody></table>
<hr>
<h3 id="循环神经网络层"><a href="#循环神经网络层" class="headerlink" title="循环神经网络层"></a><strong>循环神经网络层</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>类&#x2F;函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.nn.RNN(input_size, hidden_size)</code></td>
<td align="left">简单 RNN 层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.LSTM(input_size, hidden_size)</code></td>
<td align="left">LSTM 层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.GRU(input_size, hidden_size)</code></td>
<td align="left">GRU 层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.RNNCell(input_size, hidden_size)</code></td>
<td align="left">简单 RNN 单元。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.LSTMCell(input_size, hidden_size)</code></td>
<td align="left">LSTM 单元。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.GRUCell(input_size, hidden_size)</code></td>
<td align="left">GRU 单元。</td>
</tr>
</tbody></table>
<hr>
<h3 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a><strong>嵌入层</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>类&#x2F;函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.nn.Embedding(num_embeddings, embedding_dim)</code></td>
<td align="left">嵌入层。</td>
</tr>
</tbody></table>
<hr>
<h3 id="Dropout-层"><a href="#Dropout-层" class="headerlink" title="Dropout 层"></a><strong>Dropout 层</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>类&#x2F;函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.nn.Dropout(p)</code></td>
<td align="left">Dropout 层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.Dropout2d(p)</code></td>
<td align="left">2D Dropout 层。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.Dropout3d(p)</code></td>
<td align="left">3D Dropout 层。</td>
</tr>
</tbody></table>
<hr>
<h3 id="实用函数"><a href="#实用函数" class="headerlink" title="实用函数"></a><strong>实用函数</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>函数</strong></th>
<th align="left"><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>torch.nn.functional.relu(input)</code></td>
<td align="left">应用 ReLU 激活函数。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.functional.sigmoid(input)</code></td>
<td align="left">应用 Sigmoid 激活函数。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.functional.softmax(input, dim)</code></td>
<td align="left">应用 Softmax 激活函数。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.functional.cross_entropy(input, target)</code></td>
<td align="left">计算交叉熵损失。</td>
</tr>
<tr>
<td align="left"><code>torch.nn.functional.mse_loss(input, target)</code></td>
<td align="left">计算均方误差损失。</td>
</tr>
</tbody></table>
<hr>
<h3 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># 定义一个简单的神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">20</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建模型和输入</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">output = model(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<h1 id="Transformer-模型"><a href="#Transformer-模型" class="headerlink" title="Transformer 模型"></a>Transformer 模型</h1><p>Transformer 模型是一种<strong>基于注意力机制的深度学习模型</strong>，最初由 Vaswani 等人在 2017 年的论文《Attention is All You Need》中提出。</p>
<p>Transformer <strong>彻底改变了自然语言处理（NLP）领域，并逐渐扩展到计算机视觉（CV）等领域</strong>。</p>
<p>Transformer 的核心思想是<strong>完全摒弃传统的循环神经网络（RNN）结构，仅依赖注意力机制来处理序列数据</strong>，从而实现<strong>更高的并行性</strong>和<strong>更快的训练速度</strong>。</p>
<p>以下是 Transformer 架构图，左边为编码器，右边为解码器。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Transformer_full_architecture.png" alt="img"></p>
<p>Transformer 模型由 编码器（Encoder） 和 解码器（Decoder） 两部分组成，每部分都由多层堆叠的相同模块构成。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/runoob-transformer-1.png" alt="img">编码器（Encoder）</p>
<p>编码器<strong>由 N 层相同的模块堆叠</strong>而成，每层包含两个子层：</p>
<ul>
<li><strong>多头自注意力机制（Multi-Head Self-Attention）：<strong>计算输入序列中每个词与其他词的</strong>相关性</strong>。</li>
<li><strong>前馈神经网络（Feed-Forward Neural Network）：<strong>对每个词进行独立的</strong>非线性变换</strong>。</li>
</ul>
<p>每个子层后面都接有 <strong>残差连接（Residual Connection）</strong> 和 <strong>层归一化（Layer Normalization）</strong>。</p>
<h3 id="解码器（Decoder）"><a href="#解码器（Decoder）" class="headerlink" title="解码器（Decoder）"></a>解码器（Decoder）</h3><p>解码器也由 N 层相同的模块堆叠而成，每层包含三个子层：</p>
<ul>
<li><strong>掩码多头自注意力机制（Masked Multi-Head Self-Attention）：<strong>计算输出序列中每个词与前面词的</strong>相关性（使用掩码防止未来信息泄露）</strong>。</li>
<li><strong>编码器-解码器注意力机制（Encoder-Decoder Attention）：<strong>计算输出序列</strong>与输入序列的相关性</strong>。</li>
<li><strong>前馈神经网络（Feed-Forward Neural Network）：<strong>对每个词进行独立的</strong>非线性变换</strong>。</li>
</ul>
<p>同样，每个子层后面都接有残差连接和层归一化。</p>
<p>在 Transformer 模型出现之前，NLP 领域的主流模型是基于 RNN 的架构，如长短期记忆网络（LSTM）和门控循环单元（GRU）。这些模型通过顺序处理输入数据来捕捉序列中的依赖关系，但存在以下问题：</p>
<ol>
<li><strong>梯度消失问题</strong>：<strong>长距离依赖关系难以捕捉</strong>。</li>
<li><strong>顺序计算的局限性</strong>：无法充分利用现代硬件的并行计算能力，训练效率低下。</li>
</ol>
<p>Transformer 通过引入自注意力机制解决了这些问题，允许模型同时处理整个输入序列，并动态地为序列中的每个位置分配不同的权重。</p>
<hr>
<h2 id="Transformer-的核心思想"><a href="#Transformer-的核心思想" class="headerlink" title="Transformer 的核心思想"></a>Transformer 的核心思想</h2><h3 id="1-自注意力机制（Self-Attention）"><a href="#1-自注意力机制（Self-Attention）" class="headerlink" title="1. 自注意力机制（Self-Attention）"></a>1. 自注意力机制（Self-Attention）</h3><p>自注意力机制是 Transformer 的核心组件。</p>
<p>自注意力机制允许模型在处理序列时，<strong>动态地为每个位置分配不同的权重</strong>，从而捕捉序列中任意两个位置之间的依赖关系。</p>
<ul>
<li><strong>输入表示</strong>：输入序列中的每个词（或标记）<strong>通过词嵌入（Embedding）转换为向量表示</strong>。</li>
<li><strong>注意力权重计算</strong>：通过<strong>计算查询（Query）、键（Key）和值（Value）之间的点积</strong>，得到每个词<strong>与其他词的相关性权重</strong>。</li>
<li><strong>加权求和</strong>：使用注意力权重对值（Value）进行<strong>加权求和</strong>，得到<strong>每个词的上下文表示</strong>。</li>
</ul>
<p>公式如下：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/192f2ceca2b507875d3c7eef6f0367e5.png" alt="192f2ceca2b507875d3c7eef6f0367e5"></p>
<p>其中：</p>
<ul>
<li>Q 是查询矩阵，K 是键矩阵，V是值矩阵。</li>
<li>dk是向量的维度，用于<strong>缩放点积，防止梯度爆炸</strong>。</li>
</ul>
<h3 id="多头注意力（Multi-Head-Attention）"><a href="#多头注意力（Multi-Head-Attention）" class="headerlink" title="多头注意力（Multi-Head Attention）"></a>多头注意力（Multi-Head Attention）</h3><p>为了捕捉更丰富的特征，Transformer 使用<strong>多头</strong>注意力机制。它将<strong>输入分成多个子空间</strong>，每个子空间独立计算注意力，<strong>最后将结果拼接</strong>起来。</p>
<ul>
<li><strong>多头注意力的优势</strong>：允许模型关注序列中不同的部分，例如语法结构、语义关系等。</li>
<li><strong>并行计算</strong>：多个注意力头可以并行计算，提高效率。</li>
</ul>
<h3 id="位置编码（Positional-Encoding）"><a href="#位置编码（Positional-Encoding）" class="headerlink" title="位置编码（Positional Encoding）"></a>位置编码（Positional Encoding）</h3><p>由于 Transformer <strong>没有显式的序列信息（如 RNN 中的时间步）</strong>，<strong>位置编码</strong>被用来<strong>为输入序列中的每个词添加位置信息</strong>。通常<strong>使用正弦和余弦函数生成位置编码</strong>：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/b92014de131411175caef93b06e4006f.png" alt="b92014de131411175caef93b06e4006f"></p>
<h2 id="Transformer-的应用"><a href="#Transformer-的应用" class="headerlink" title="Transformer 的应用"></a>Transformer 的应用</h2><ol>
<li><strong>自然语言处理（NLP）</strong>：<ul>
<li>机器翻译（如 Google Translate）</li>
<li>文本生成（如 GPT 系列模型）</li>
<li>文本分类、问答系统等。</li>
</ul>
</li>
<li><strong>计算机视觉（CV）</strong>：<ul>
<li>图像分类（如 Vision Transformer）</li>
<li>目标检测、图像生成等。</li>
</ul>
</li>
<li><strong>多模态任务</strong>：<ul>
<li>结合文本和图像的任务（如 CLIP、DALL-E）。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="PyTorch-实现-Transformer"><a href="#PyTorch-实现-Transformer" class="headerlink" title="PyTorch 实现 Transformer"></a>PyTorch 实现 Transformer</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, model_dim, num_heads, num_layers, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(input_dim, model_dim)</span><br><span class="line">        <span class="variable language_">self</span>.positional_encoding = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1000</span>, model_dim))  <span class="comment"># 假设序列长度最大为1000</span></span><br><span class="line">        <span class="variable language_">self</span>.transformer = nn.Transformer(d_model=model_dim, nhead=num_heads, num_encoder_layers=num_layers)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(model_dim, output_dim)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        src_seq_length, tgt_seq_length = src.size(<span class="number">1</span>), tgt.size(<span class="number">1</span>)</span><br><span class="line">        src = <span class="variable language_">self</span>.embedding(src) + <span class="variable language_">self</span>.positional_encoding[:, :src_seq_length, :]</span><br><span class="line">        tgt = <span class="variable language_">self</span>.embedding(tgt) + <span class="variable language_">self</span>.positional_encoding[:, :tgt_seq_length, :]</span><br><span class="line">        transformer_output = <span class="variable language_">self</span>.transformer(src, tgt)</span><br><span class="line">        output = <span class="variable language_">self</span>.fc(transformer_output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">input_dim = <span class="number">10000</span>  <span class="comment"># 词汇表大小</span></span><br><span class="line">model_dim = <span class="number">512</span>    <span class="comment"># 模型维度</span></span><br><span class="line">num_heads = <span class="number">8</span>      <span class="comment"># 多头注意力头数</span></span><br><span class="line">num_layers = <span class="number">6</span>     <span class="comment"># 编码器和解码器层数</span></span><br><span class="line">output_dim = <span class="number">10000</span> <span class="comment"># 输出维度（通常与词汇表大小相同）</span></span><br><span class="line"><span class="comment"># 初始化模型、损失函数和优化器</span></span><br><span class="line">model = TransformerModel(input_dim, model_dim, num_heads, num_layers, output_dim)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 假设输入数据</span></span><br><span class="line">src = torch.randint(<span class="number">0</span>, input_dim, (<span class="number">10</span>, <span class="number">32</span>))  <span class="comment"># (序列长度, 批量大小)</span></span><br><span class="line">tgt = torch.randint(<span class="number">0</span>, input_dim, (<span class="number">20</span>, <span class="number">32</span>))  <span class="comment"># (序列长度, 批量大小)</span></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = model(src, tgt)</span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = criterion(output.view(-<span class="number">1</span>, output_dim), tgt.view(-<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 反向传播和优化</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Loss:&quot;</span>, loss.item())</span><br></pre></td></tr></table></figure>

<h1 id="PyTorch-构建-Transformer-模型"><a href="#PyTorch-构建-Transformer-模型" class="headerlink" title="PyTorch 构建 Transformer 模型"></a>PyTorch 构建 Transformer 模型</h1><p>Transformer 是现代机器学习中最强大的模型之一。</p>
<p>Transformer 模型是一种基于自注意力机制（Self-Attention） 的深度学习架构，它彻底改变了自然语言处理（NLP）领域，并成为现代深度学习模型（如 BERT、GPT 等）的基础。</p>
<p>Transformer 是现代 NLP 领域的核心架构，凭借其强大的长距离依赖建模能力和高效的并行计算优势，在语言翻译和文本摘要等任务中超越了传统的 长短期记忆 (LSTM) 网络。</p>
<h2 id="使用-PyTorch-构建-Transformer-模型"><a href="#使用-PyTorch-构建-Transformer-模型" class="headerlink" title="使用 PyTorch 构建 Transformer 模型"></a>使用 PyTorch 构建 Transformer 模型</h2><p><strong>构建 Transformer 模型的步骤如下：</strong></p>
<h3 id="1、导入必要的库和模块"><a href="#1、导入必要的库和模块" class="headerlink" title="1、导入必要的库和模块"></a>1、导入必要的库和模块</h3><p>导入 PyTorch 核心库、神经网络模块、优化器模块、数据处理工具，以及数学和对象复制模块，为定义模型架构、管理数据和训练过程提供支持。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> copy</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<ul>
<li><code>torch</code>：PyTorch 的<strong>核心库</strong>，<strong>用于张量操作和自动求导</strong>。</li>
<li><code>torch.nn</code>：PyTorch 的<strong>神经网络模块</strong>，包含各<strong>种层和损失函数</strong>。</li>
<li><code>torch.optim</code>：<strong>优化算法模块</strong>，如 Adam、SGD 等。</li>
<li><code>math</code>：数学函数库，用于计算平方根等。</li>
<li><code>copy</code>：用于<strong>深度复制对象</strong>。</li>
</ul>
<h3 id="定义基本构建块：多头注意力、位置前馈网络、位置编码"><a href="#定义基本构建块：多头注意力、位置前馈网络、位置编码" class="headerlink" title="定义基本构建块：多头注意力、位置前馈网络、位置编码"></a>定义基本构建块：多头注意力、位置前馈网络、位置编码</h3><p><strong>多头注意力</strong>通过多个”注意力头”计算序列中每对位置之间的关系，能够捕捉输入序列的不同特征和模式。</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Figure_1_Multi_Head_Attention_source_image_created_by_author_653bad32f1.avif" alt="img"></p>
<p><strong>MultiHeadAttention 类封装了 Transformer 模型中常用的多头注意力机制</strong>，负责<strong>将输入拆分成多个注意力头</strong>，对每个注意力头施加注意力，然后将结果组合起来，这样模型就可以在不同尺度上捕捉输入数据中的各种关系，提高模型的表达能力。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % num_heads == <span class="number">0</span>, <span class="string">&quot;d_model必须能被num_heads整除&quot;</span>        </span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model    <span class="comment"># 模型维度（如512）</span></span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads <span class="comment"># 注意力头数（如8）</span></span><br><span class="line">        <span class="variable language_">self</span>.d_k = d_model // num_heads <span class="comment"># 每个头的维度（如64）        </span></span><br><span class="line">        <span class="comment"># 定义线性变换层（无需偏置）</span></span><br><span class="line">        <span class="variable language_">self</span>.W_q = nn.Linear(d_model, d_model) <span class="comment"># 查询变换</span></span><br><span class="line">        <span class="variable language_">self</span>.W_k = nn.Linear(d_model, d_model) <span class="comment"># 键变换</span></span><br><span class="line">        <span class="variable language_">self</span>.W_v = nn.Linear(d_model, d_model) <span class="comment"># 值变换</span></span><br><span class="line">        <span class="variable language_">self</span>.W_o = nn.Linear(d_model, d_model) <span class="comment"># 输出变换       </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算缩放点积注意力</span></span><br><span class="line"><span class="string">        输入形状：</span></span><br><span class="line"><span class="string">            Q: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">            K, V: 同Q</span></span><br><span class="line"><span class="string">        输出形状： (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 计算注意力分数（Q和K的点积）</span></span><br><span class="line">        attn_scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(<span class="variable language_">self</span>.d_k)       </span><br><span class="line">        <span class="comment"># 应用掩码（如填充掩码或未来信息掩码）</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn_scores = attn_scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)     </span><br><span class="line">        <span class="comment"># 计算注意力权重（softmax归一化）</span></span><br><span class="line">        attn_probs = torch.softmax(attn_scores, dim=-<span class="number">1</span>)      </span><br><span class="line">        <span class="comment"># 对值向量加权求和</span></span><br><span class="line">        output = torch.matmul(attn_probs, V)</span><br><span class="line">        <span class="keyword">return</span> output       </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">split_heads</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将输入张量分割为多个头</span></span><br><span class="line"><span class="string">        输入形状: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        输出形状: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, seq_length, d_model = x.size()</span><br><span class="line">        <span class="keyword">return</span> x.view(batch_size, seq_length, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">combine_heads</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将多个头的输出合并回原始形状</span></span><br><span class="line"><span class="string">        输入形状: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">        输出形状: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, _, seq_length, d_k = x.size()</span><br><span class="line">        <span class="keyword">return</span> x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, seq_length, <span class="variable language_">self</span>.d_model)       </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播</span></span><br><span class="line"><span class="string">        输入形状: Q/K/V: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        输出形状: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 线性变换并分割多头</span></span><br><span class="line">        Q = <span class="variable language_">self</span>.split_heads(<span class="variable language_">self</span>.W_q(Q)) <span class="comment"># (batch, heads, seq_len, d_k)</span></span><br><span class="line">        K = <span class="variable language_">self</span>.split_heads(<span class="variable language_">self</span>.W_k(K))</span><br><span class="line">        V = <span class="variable language_">self</span>.split_heads(<span class="variable language_">self</span>.W_v(V))        </span><br><span class="line">        <span class="comment"># 计算注意力</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.scaled_dot_product_attention(Q, K, V, mask)        </span><br><span class="line">        <span class="comment"># 合并多头并输出变换</span></span><br><span class="line">        output = <span class="variable language_">self</span>.W_o(<span class="variable language_">self</span>.combine_heads(attn_output))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<ul>
<li><strong>多头注意力机制</strong>：将输入分割成多个头，每个头独立计算注意力，最后将结果合并。</li>
<li><strong>缩放点积注意力</strong>：计算查询和键的点积，缩放后使用 softmax 计算注意力权重，最后对值进行加权求和。</li>
<li><strong>掩码</strong>：用于屏蔽无效位置（如填充部分）。</li>
</ul>
<h3 id="位置前馈网络（Position-wise-Feed-Forward-Network）"><a href="#位置前馈网络（Position-wise-Feed-Forward-Network）" class="headerlink" title="位置前馈网络（Position-wise Feed-Forward Network）"></a>位置前馈网络（Position-wise Feed-Forward Network）</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFeedForward, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(d_model, d_ff)  <span class="comment"># 第一层全连接</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(d_ff, d_model)  <span class="comment"># 第二层全连接</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()  <span class="comment"># 激活函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 前馈网络的计算</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc2(<span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.fc1(x)))</span><br></pre></td></tr></table></figure>

<p>**前馈网络：**由两个全连接层和一个 ReLU 激活函数组成，用于进一步处理注意力机制的输出。</p>
<h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>位置编码用于注入输入序列中<strong>每个 token 的位置信息</strong>。</p>
<p>使用<strong>不同频率的正弦和余弦函数来生成位置编码</strong>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_seq_length</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        pe = torch.zeros(max_seq_length, d_model)  <span class="comment"># 初始化位置编码矩阵</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_seq_length, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)  <span class="comment"># 偶数位置使用正弦函数</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)  <span class="comment"># 奇数位置使用余弦函数</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe.unsqueeze(<span class="number">0</span>))  <span class="comment"># 注册为缓冲区</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 将位置编码添加到输入中</span></span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.pe[:, :x.size(<span class="number">1</span>)]</span><br></pre></td></tr></table></figure>

<h3 id="构建编码器块（Encoder-Layer）"><a href="#构建编码器块（Encoder-Layer）" class="headerlink" title="构建编码器块（Encoder Layer）"></a>构建编码器块（Encoder Layer）</h3><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Figure_2_The_Encoder_part_of_the_transformer_network_Source_image_from_the_original_paper_b0e3ac40fa.avif" alt="img"></p>
<p><strong>编码器层：<strong>包含</strong>一个自注意力机制和一个前馈网络</strong>，每个子层后接残差连接和层归一化。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads, d_ff, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = MultiHeadAttention(d_model, num_heads)  <span class="comment"># 自注意力机制</span></span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = PositionWiseFeedForward(d_model, d_ff)  <span class="comment"># 前馈网络</span></span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(d_model)  <span class="comment"># 层归一化</span></span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># Dropout       </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="comment"># 自注意力机制</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.self_attn(x, x, x, mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x + <span class="variable language_">self</span>.dropout(attn_output))  <span class="comment"># 残差连接和层归一化     </span></span><br><span class="line">        <span class="comment"># 前馈网络</span></span><br><span class="line">        ff_output = <span class="variable language_">self</span>.feed_forward(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x + <span class="variable language_">self</span>.dropout(ff_output))  <span class="comment"># 残差连接和层归一化</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="构建解码器模块"><a href="#构建解码器模块" class="headerlink" title="构建解码器模块"></a>构建解码器模块</h3><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Figure_3_The_Decoder_part_of_the_Transformer_network_Souce_Image_from_the_original_paper_b90d9e7f66.avif" alt="img"></p>
<p><strong>解码器层：<strong>包含</strong>一个自注意力机制、一个交叉注意力机制和一个前馈网络</strong>，每个子层后接残差连接和层归一化。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads, d_ff, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = MultiHeadAttention(d_model, num_heads)  <span class="comment"># 自注意力机制</span></span><br><span class="line">        <span class="variable language_">self</span>.cross_attn = MultiHeadAttention(d_model, num_heads)  <span class="comment"># 交叉注意力机制</span></span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = PositionWiseFeedForward(d_model, d_ff)  <span class="comment"># 前馈网络</span></span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(d_model)  <span class="comment"># 层归一化</span></span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.norm3 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># Dropout</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, enc_output, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="comment"># 自注意力机制</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.self_attn(x, x, x, tgt_mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x + <span class="variable language_">self</span>.dropout(attn_output))  <span class="comment"># 残差连接和层归一化     </span></span><br><span class="line">        <span class="comment"># 交叉注意力机制</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.cross_attn(x, enc_output, enc_output, src_mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x + <span class="variable language_">self</span>.dropout(attn_output))  <span class="comment"># 残差连接和层归一化  </span></span><br><span class="line">        <span class="comment"># 前馈网络</span></span><br><span class="line">        ff_output = <span class="variable language_">self</span>.feed_forward(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm3(x + <span class="variable language_">self</span>.dropout(ff_output))  <span class="comment"># 残差连接和层归一化</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="构建完整的-Transformer-模型"><a href="#构建完整的-Transformer-模型" class="headerlink" title="构建完整的 Transformer 模型"></a>构建完整的 Transformer 模型</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder_embedding = nn.Embedding(src_vocab_size, d_model)  <span class="comment"># 编码器词嵌入</span></span><br><span class="line">        <span class="variable language_">self</span>.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)  <span class="comment"># 解码器词嵌入</span></span><br><span class="line">        <span class="variable language_">self</span>.positional_encoding = PositionalEncoding(d_model, max_seq_length)  <span class="comment"># 位置编码</span></span><br><span class="line">        <span class="comment"># 编码器和解码器层</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)])</span><br><span class="line">        <span class="variable language_">self</span>.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)])</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(d_model, tgt_vocab_size)  <span class="comment"># 最终的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># Dropout</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_mask</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        <span class="comment"># 源掩码：屏蔽填充符（假设填充符索引为0）</span></span><br><span class="line">        <span class="comment"># 形状：(batch_size, 1, 1, seq_length)</span></span><br><span class="line">        src_mask = (src != <span class="number">0</span>).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 目标掩码：屏蔽填充符和未来信息</span></span><br><span class="line">        <span class="comment"># 形状：(batch_size, 1, seq_length, 1)</span></span><br><span class="line">        tgt_mask = (tgt != <span class="number">0</span>).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">3</span>)</span><br><span class="line">        seq_length = tgt.size(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 生成上三角矩阵掩码，防止解码时看到未来信息</span></span><br><span class="line">        nopeak_mask = (<span class="number">1</span> - torch.triu(torch.ones(<span class="number">1</span>, seq_length, seq_length), diagonal=<span class="number">1</span>)).<span class="built_in">bool</span>()</span><br><span class="line">        tgt_mask = tgt_mask &amp; nopeak_mask  <span class="comment"># 合并填充掩码和未来信息掩码</span></span><br><span class="line">        <span class="keyword">return</span> src_mask, tgt_mask</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        <span class="comment"># 生成掩码</span></span><br><span class="line">        src_mask, tgt_mask = <span class="variable language_">self</span>.generate_mask(src, tgt)</span><br><span class="line">        <span class="comment"># 编码器部分</span></span><br><span class="line">        src_embedded = <span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.positional_encoding(<span class="variable language_">self</span>.encoder_embedding(src)))</span><br><span class="line">        enc_output = src_embedded</span><br><span class="line">        <span class="keyword">for</span> enc_layer <span class="keyword">in</span> <span class="variable language_">self</span>.encoder_layers:</span><br><span class="line">            enc_output = enc_layer(enc_output, src_mask)</span><br><span class="line">        <span class="comment"># 解码器部分</span></span><br><span class="line">        tgt_embedded = <span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.positional_encoding(<span class="variable language_">self</span>.decoder_embedding(tgt)))</span><br><span class="line">        dec_output = tgt_embedded</span><br><span class="line">        <span class="keyword">for</span> dec_layer <span class="keyword">in</span> <span class="variable language_">self</span>.decoder_layers:</span><br><span class="line">            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)        </span><br><span class="line">        <span class="comment"># 最终输出</span></span><br><span class="line">        output = <span class="variable language_">self</span>.fc(dec_output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<ul>
<li><strong>Transformer 模型</strong>：包含编码器和解码器部分，每个部分由多个层堆叠而成。</li>
<li><strong>掩码生成</strong>：用于屏蔽无效位置和未来信息。</li>
<li><strong>前向传播</strong>：依次通过编码器和解码器，最后通过全连接层输出。</li>
</ul>
<p>模型初始化参数说明：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Transformer(nn.Module):</span><br><span class="line">    def __init__(</span><br><span class="line">        self, </span><br><span class="line">        src_vocab_size,  # 源语言词汇表大小（如英文单词数）</span><br><span class="line">        tgt_vocab_size,  # 目标语言词汇表大小（如中文单词数）</span><br><span class="line">        d_model=512,     # 模型维度（每个词向量的长度）</span><br><span class="line">        num_heads=8,     # 多头注意力的头数</span><br><span class="line">        num_layers=6,    # 编码器/解码器的堆叠层数</span><br><span class="line">        d_ff=2048,       # 前馈网络隐藏层维度</span><br><span class="line">        max_seq_length=100, # 最大序列长度（用于位置编码）</span><br><span class="line">        dropout=0.1      # Dropout概率</span><br><span class="line">    ):</span><br></pre></td></tr></table></figure>

<h3 id="训练-PyTorch-Transformer-模型"><a href="#训练-PyTorch-Transformer-模型" class="headerlink" title="训练 PyTorch Transformer 模型"></a>训练 PyTorch Transformer 模型</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">src_vocab_size = <span class="number">5000</span>  <span class="comment"># 源词汇表大小</span></span><br><span class="line">tgt_vocab_size = <span class="number">5000</span>  <span class="comment"># 目标词汇表大小</span></span><br><span class="line">d_model = <span class="number">512</span>  <span class="comment"># 模型维度</span></span><br><span class="line">num_heads = <span class="number">8</span>  <span class="comment"># 注意力头数量</span></span><br><span class="line">num_layers = <span class="number">6</span>  <span class="comment"># 编码器和解码器层数</span></span><br><span class="line">d_ff = <span class="number">2048</span>  <span class="comment"># 前馈网络内层维度</span></span><br><span class="line">max_seq_length = <span class="number">100</span>  <span class="comment"># 最大序列长度</span></span><br><span class="line">dropout = <span class="number">0.1</span>  <span class="comment"># Dropout 概率</span></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)</span><br><span class="line"><span class="comment"># 生成随机数据</span></span><br><span class="line">src_data = torch.randint(<span class="number">1</span>, src_vocab_size, (<span class="number">64</span>, max_seq_length))  <span class="comment"># 源序列</span></span><br><span class="line">tgt_data = torch.randint(<span class="number">1</span>, tgt_vocab_size, (<span class="number">64</span>, max_seq_length))  <span class="comment"># 目标序列</span></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>)  <span class="comment"># 忽略填充部分的损失</span></span><br><span class="line">optimizer = optim.Adam(transformer.parameters(), lr=<span class="number">0.0001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>)</span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line">transformer.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清空梯度，防止累积</span></span><br><span class="line">    <span class="comment"># 输入目标序列时去掉最后一个词（用于预测下一个词）</span></span><br><span class="line">    output = transformer(src_data, tgt_data[:, :-<span class="number">1</span>])      </span><br><span class="line">    <span class="comment"># 计算损失时，目标序列从第二个词开始（即预测下一个词）</span></span><br><span class="line">    <span class="comment"># output形状: (batch_size, seq_length-1, tgt_vocab_size)</span></span><br><span class="line">    <span class="comment"># 目标形状: (batch_size, seq_length-1)</span></span><br><span class="line">    loss = criterion(</span><br><span class="line">        output.contiguous().view(-<span class="number">1</span>, tgt_vocab_size), </span><br><span class="line">        tgt_data[:, <span class="number">1</span>:].contiguous().view(-<span class="number">1</span>)</span><br><span class="line">    )    </span><br><span class="line">    loss.backward()        <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()       <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">transformer.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 生成验证数据</span></span><br><span class="line">val_src_data = torch.randint(<span class="number">1</span>, src_vocab_size, (<span class="number">64</span>, max_seq_length))</span><br><span class="line">val_tgt_data = torch.randint(<span class="number">1</span>, tgt_vocab_size, (<span class="number">64</span>, max_seq_length))</span><br><span class="line"><span class="comment"># 假设输入为一批英文和对应的中文翻译（已转换为索引）</span></span><br><span class="line"><span class="comment"># 示例数据：</span></span><br><span class="line"><span class="comment"># src_data: [[3, 14, 25, ..., 0, 0], ...]  # 英文句子（0为填充符）</span></span><br><span class="line"><span class="comment"># tgt_data: [[5, 20, 36, ..., 0, 0], ...]  # 中文翻译（0为填充符）</span></span><br><span class="line"><span class="comment"># 注意：实际应用中需对文本进行分词、编码、填充等预处理</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    val_output = transformer(val_src_data, val_tgt_data[:, :-<span class="number">1</span>])</span><br><span class="line">    val_loss = criterion(val_output.contiguous().view(-<span class="number">1</span>, tgt_vocab_size), val_tgt_data[:, <span class="number">1</span>:].contiguous().view(-<span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Validation Loss: <span class="subst">&#123;val_loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Python-入门机器学习"><a href="#Python-入门机器学习" class="headerlink" title="Python 入门机器学习"></a>Python 入门机器学习</h1><p>在使用 Python 进行机器学习时，整个过程一般遵循以下步骤：</p>
<ol>
<li><strong>导入必要的库</strong> - 例如，NumPy、Pandas 和 Scikit-learn。</li>
<li><strong>加载和准备数据</strong> - 数据是机器学习的核心。你需要加载数据并进行必要的预处理（例如数据清洗、缺失值填补等）。</li>
<li><strong>选择模型和算法</strong> - 根据任务选择适合的机器学习算法（如线性回归、决策树等）。</li>
<li><strong>训练模型</strong> - 使用训练集数据来训练模型。</li>
<li><strong>评估模型</strong> - 使用测试集评估模型的准确性，并根据评估结果优化模型。</li>
<li><strong>调整模型和超参数</strong> - 根据评估结果调整模型的超参数，进一步优化模型性能。</li>
</ol>
<h2 id="一个简单的机器学习例子：使用-Scikit-learn-做分类"><a href="#一个简单的机器学习例子：使用-Scikit-learn-做分类" class="headerlink" title="一个简单的机器学习例子：使用 Scikit-learn 做分类"></a>一个简单的机器学习例子：使用 Scikit-learn 做分类</h2><p><strong>Scikit-learn（简称 Sklearn）是一个开源的机器学习库</strong>，建立在 NumPy、SciPy 和 matplotlib 这些科学计算库之上，提供了简单高效的数据挖掘和数据分析工具。</p>
<p>Scikit-learn 包含了许多常见的机器学习算法，包括：</p>
<ul>
<li>线性回归、岭回归、Lasso回归</li>
<li>支持向量机（SVM）</li>
<li>决策树、随机森林、梯度提升树</li>
<li>聚类算法（如K-Means、层次聚类、DBSCAN）</li>
<li>降维技术（如PCA、t-SNE）</li>
<li>神经网络</li>
</ul>
<p>接下来我们通过一个简单的分类任务——使用鸢尾花数据集（Iris Dataset）来演示机器学习的流程，鸢尾花数据集是一个经典的数据集，包含 150 个样本，描述了三种不同类型的鸢尾花的花瓣和萼片的长度和宽度。</p>
<h3 id="步骤-1：导入库"><a href="#步骤-1：导入库" class="headerlink" title="步骤 1：导入库"></a>步骤 1：导入库</h3><p>导入需要的 Python 库：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br></pre></td></tr></table></figure>

<h3 id="步骤-2：加载数据"><a href="#步骤-2：加载数据" class="headerlink" title="步骤 2：加载数据"></a>步骤 2：加载数据</h3><p>加载鸢尾花数据集：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转化为 pandas DataFrame</span></span><br><span class="line">X = pd.DataFrame(iris.data, columns=iris.feature_names)  <span class="comment"># 特征数据</span></span><br><span class="line">y = pd.Series(iris.target)  <span class="comment"># 标签数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示前五行数据</span></span><br><span class="line"><span class="built_in">print</span>(X.head())</span><br></pre></td></tr></table></figure>

<p>打印输出数据如下所示：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)</span><br><span class="line"><span class="number">0</span>                <span class="number">5.1</span>               <span class="number">3.5</span>                <span class="number">1.4</span>               <span class="number">0.2</span></span><br><span class="line"><span class="number">1</span>                <span class="number">4.9</span>               <span class="number">3.0</span>                <span class="number">1.4</span>               <span class="number">0.2</span></span><br><span class="line"><span class="number">2</span>                <span class="number">4.7</span>               <span class="number">3.2</span>                <span class="number">1.3</span>               <span class="number">0.2</span></span><br><span class="line"><span class="number">3</span>                <span class="number">4.6</span>               <span class="number">3.1</span>                <span class="number">1.5</span>               <span class="number">0.2</span></span><br><span class="line"><span class="number">4</span>                <span class="number">5.0</span>               <span class="number">3.6</span>                <span class="number">1.4</span>               <span class="number">0.2</span></span><br></pre></td></tr></table></figure>

<h3 id="步骤-3：数据集划分"><a href="#步骤-3：数据集划分" class="headerlink" title="步骤 3：数据集划分"></a>步骤 3：数据集划分</h3><p>将数据集划分为训练集和测试集，<strong>通常使用 70% 训练集和 30% 测试集</strong>的比例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分训练集和测试集（80% 训练集，20% 测试集）</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<h3 id="步骤-4：特征缩放（标准化）"><a href="#步骤-4：特征缩放（标准化）" class="headerlink" title="步骤 4：特征缩放（标准化）"></a>步骤 4：特征缩放（标准化）</h3><p>许多机器学习算法都依赖于特征的尺度，特别是像 K 最近邻算法。为了确保每个特征的均值为 0，标准差为 1，我们使用标准化来处理数据：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标准化特征</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_test = scaler.transform(X_test)</span><br></pre></td></tr></table></figure>

<h3 id="步骤-5：选择模型并训练"><a href="#步骤-5：选择模型并训练" class="headerlink" title="步骤 5：选择模型并训练"></a>步骤 5：选择模型并训练</h3><p>在这个例子中，我们选择 K-Nearest Neighbors（KNN） 算法来进行分类：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 KNN 分类器</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<h3 id="步骤-6：评估模型"><a href="#步骤-6：评估模型" class="headerlink" title="步骤 6：评估模型"></a>步骤 6：评估模型</h3><p>训练完成后，我们使用测试集评估模型的准确性：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测测试集</span></span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;模型准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>完成以上代码，输出结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">模型准确率: 1.00</span><br></pre></td></tr></table></figure>

<h3 id="步骤-7：可视化结果（可选）"><a href="#步骤-7：可视化结果（可选）" class="headerlink" title="步骤 7：可视化结果（可选）"></a>步骤 7：可视化结果（可选）</h3><p>你可以通过可视化来进一步了解模型的表现，尤其是在多维数据集的情况下。例如，你可以用二维图来显示 KNN 分类的结果（不过在这里需要对数据进行降维，<strong>简化为二维</strong>）。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转化为 pandas DataFrame</span></span><br><span class="line">X = pd.DataFrame(iris.data, columns=iris.feature_names)  <span class="comment"># 特征数据</span></span><br><span class="line">y = pd.Series(iris.target)  <span class="comment"># 标签数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集（80% 训练集，20% 测试集）</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化特征</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_test = scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 KNN 分类器</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测测试集</span></span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化 - 这里只是一个简单示例，具体可根据实际情况选择绘图方式</span></span><br><span class="line">plt.scatter(X_test[:, <span class="number">0</span>], X_test[:, <span class="number">1</span>], c=y_pred, cmap=<span class="string">&#x27;viridis&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;KNN Classification Results&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 2&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/knn-python-ml-1.png" alt="img"></p>
<h1 id="机器学习算法"><a href="#机器学习算法" class="headerlink" title="机器学习算法"></a>机器学习算法</h1><p>机器学习算法可以分为监督学习、无监督学习、强化学习等类别。</p>
<p><strong>监督学习算法</strong>：</p>
<ul>
<li><strong>线性回归</strong>（Linear Regression）：用于<strong>回归任务</strong>，预测连续的数值。</li>
<li><strong>逻辑回归</strong>（Logistic Regression）：用于<strong>二分类任务</strong>，预测类别。</li>
<li><strong>支持向量机</strong>（SVM）：用于<strong>分类任务</strong>，构建超平面进行分类。</li>
<li><strong>决策树</strong>（Decision Tree）：<strong>基于树状结构</strong>进行决策的分类或回归方法。</li>
</ul>
<p><strong>无监督学习算法</strong>：</p>
<ul>
<li><strong>K-means 聚类</strong>：<strong>通过聚类中心</strong>将数据分组。</li>
<li><strong>主成分分析（PCA）</strong>：用于<strong>降维</strong>，提取数据的主成分。</li>
</ul>
<p>每种算法都有其适用的场景，在实际应用中，可以根据数据的特征（如是否有标签、数据的维度等）来选择最合适的机器学习算法。</p>
<hr>
<h2 id="监督学习算法"><a href="#监督学习算法" class="headerlink" title="监督学习算法"></a>监督学习算法</h2><h3 id="线性回归（Linear-Regression）"><a href="#线性回归（Linear-Regression）" class="headerlink" title="线性回归（Linear Regression）"></a>线性回归（Linear Regression）</h3><p>线性回归是一种用于回归问题的算法，它通过学习输入特征与目标值之间的线性关系，来预测一个连续的输出。</p>
<p>**应用场景：**预测房价、股票价格等。</p>
<p>线性回归的目标是找到一个最佳的线性方程：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/lg-1-1742965831922-27.png" alt="img"></p>
<ul>
<li>y 是预测值（目标值）。</li>
<li>x1，x2，xn 是输入特征。</li>
<li>w1，w2，wn是待学习的权重（模型参数）。</li>
<li>b 是偏置项。</li>
</ul>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Linear_regression.svg-1742965850608-30.png" alt="img"></p>
<p>接下来我们使用 sklearn 进行简单的房价预测：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个简单的房价数据集</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;面积&#x27;</span>: [<span class="number">50</span>, <span class="number">60</span>, <span class="number">80</span>, <span class="number">100</span>, <span class="number">120</span>],</span><br><span class="line">    <span class="string">&#x27;房价&#x27;</span>: [<span class="number">150</span>, <span class="number">180</span>, <span class="number">240</span>, <span class="number">300</span>, <span class="number">350</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征和标签</span></span><br><span class="line">X = df[[<span class="string">&#x27;面积&#x27;</span>]]</span><br><span class="line">y = df[<span class="string">&#x27;房价&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;预测的房价: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">预测的房价: [180.8411215]</span><br></pre></td></tr></table></figure>

<h3 id="逻辑回归（Logistic-Regression）"><a href="#逻辑回归（Logistic-Regression）" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h3><p>逻辑回归是一种用于分类问题的算法，<strong>尽管名字中包含”回归”，它是用来处理二分类问题的</strong>。</p>
<p>逻辑回归通过学习输入特征与类别之间的关系，来预测一个类别标签。</p>
<p>**应用场景：**垃圾邮件分类、疾病诊断（是否患病）。</p>
<p>逻辑回归的输出是一个概率值，表示样本属于某一类别的概率。</p>
<p>通常使用 Sigmoid 函数：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/881f2480-9f80-448e-a83b-8abfb784d065.png" alt="img"></p>
<p>使用逻辑回归进行二分类任务:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只取前两类做二分类任务</span></span><br><span class="line">X = X[y != <span class="number">2</span>]</span><br><span class="line">y = y[y != <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练逻辑回归模型</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;分类准确率: <span class="subst">&#123;accuracy_score(y_test, y_pred):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">分类准确率: 1.00</span><br></pre></td></tr></table></figure>

<h3 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h3><p>支持向量机是一种常用的分类算法，它通过构造超平面来最大化类别之间的间隔（Margin），使得分类的误差最小。</p>
<p>**应用场景：**文本分类、人脸识别等。</p>
<p>使用 SVM 进行鸢尾花分类任务：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练 SVM 模型</span></span><br><span class="line">model = SVC(kernel=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;SVM 分类准确率: <span class="subst">&#123;accuracy_score(y_test, y_pred):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="决策树（Decision-Tree）"><a href="#决策树（Decision-Tree）" class="headerlink" title="决策树（Decision Tree）"></a>决策树（Decision Tree）</h3><p>决策树是一种基于树结构进行决策的<strong>分类和回归方法</strong>。它通过一系列的”判断条件”来决定一个样本属于哪个类别。</p>
<p>**应用场景：**客户分类、信用评分等。</p>
<p>使用决策树进行分类任务：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练决策树模型</span></span><br><span class="line">model = DecisionTreeClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;决策树分类准确率: <span class="subst">&#123;accuracy_score(y_test, y_pred):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">决策树分类准确率: 1.00</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="无监督学习算法"><a href="#无监督学习算法" class="headerlink" title="无监督学习算法"></a>无监督学习算法</h2><h3 id="K-means-聚类（K-means-Clustering）"><a href="#K-means-聚类（K-means-Clustering）" class="headerlink" title="K-means 聚类（K-means Clustering）"></a>K-means 聚类（K-means Clustering）</h3><p>K-means 是一种<strong>基于中心点的聚类算法</strong>，通过不断<strong>调整簇的中心点</strong>，使每个簇中的数据点尽可能靠近簇中心。</p>
<p>**应用场景：**<strong><strong>客户分群、市场分析、图像压缩。</strong></strong></p>
<p>使用 K-means 进行客户分群:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">from sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 生成一个简单的二维数据集</span><br><span class="line">X, _ = <span class="built_in">make_blobs</span>(n_samples=<span class="number">300</span>, centers=<span class="number">4</span>, cluster_std=<span class="number">0.60</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"># 训练 K-means 模型</span><br><span class="line">model = <span class="built_in">KMeans</span>(n_clusters=<span class="number">4</span>)</span><br><span class="line">model.<span class="built_in">fit</span>(X)</span><br><span class="line"></span><br><span class="line"># 预测聚类结果</span><br><span class="line">y_kmeans = model.<span class="built_in">predict</span>(X)</span><br><span class="line"></span><br><span class="line"># 可视化聚类结果</span><br><span class="line">plt.<span class="built_in">scatter</span>(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_kmeans, s=<span class="number">50</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.<span class="built_in">show</span>()</span><br></pre></td></tr></table></figure>

<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ml-algorithms-1.png" alt="img"></p>
<h3 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h3><p>PCA 是一种<strong>降维技术</strong>，它通过线性变换将数据转换到新的坐标系中，使得大部分的方差集中在前几个主成分上。</p>
<p>**应用场景：**图像降维、特征选择、数据可视化。</p>
<p>使用 PCA 降维并可视化高维数据:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 降维到 2 维</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">X_pca = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line">plt.scatter(X_pca[:, <span class="number">0</span>], X_pca[:, <span class="number">1</span>], c=y, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;PCA of Iris Dataset&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ml-algorithms-2.png" alt="img"></p>
<h1 id="线性回归-Linear-Regression"><a href="#线性回归-Linear-Regression" class="headerlink" title="线性回归 (Linear Regression)"></a>线性回归 (Linear Regression)</h1><p>线性回归（Linear Regression）是机器学习中最基础且广泛应用的算法之一。</p>
<p>线性回归 (Linear Regression) 是一种用于<strong>预测连续值的最基本</strong>的机器学习算法，它<strong>假设</strong>目标变量 <strong>y</strong> 和特征变量 <strong>x</strong> 之间<strong>存在线性</strong>关系，并<strong>试图找到一条最佳拟合直线来描述</strong>这种关系。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = w * x + b</span><br></pre></td></tr></table></figure>

<p>其中：</p>
<ul>
<li><code>y</code> 是预测值</li>
<li><code>x</code> 是特征变量</li>
<li><code>w</code> 是权重 (斜率)</li>
<li><code>b</code> 是偏置 (截距)</li>
</ul>
<p>线性回归的目标是<strong>找到最佳的 <code>w</code> 和 <code>b</code>，使得预测值 <code>y</code> 与真实值之间的误差最小</strong>。常用的<strong>误差函数是均方误差 (MSE)</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MSE = 1/n * Σ(y_i - y_pred_i)^2</span><br></pre></td></tr></table></figure>

<p>其中：</p>
<ul>
<li>y_i 是<strong>实际</strong>值。</li>
<li>y_pred_i 是<strong>预测</strong>值。</li>
<li>n 是数据点的数量。</li>
</ul>
<p>我们的目标是通过<strong>调整 w 和 b ，使得 MSE 最小化。</strong></p>
<h2 id="如何求解线性回归？"><a href="#如何求解线性回归？" class="headerlink" title="如何求解线性回归？"></a>如何求解线性回归？</h2><h3 id="1、最小二乘法"><a href="#1、最小二乘法" class="headerlink" title="1、最小二乘法"></a>1、最小二乘法</h3><p>最小二乘法是一种常用的求解线性回归的方法，它通过求解以下方程来找到最佳的 ( w ) 和 ( b )。</p>
<p>最小二乘法的目标是<strong>最小化残差平方和（RSS）</strong>，其公式为：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/647bf95e98352a445cab7d547a88ede5.png" alt="647bf95e98352a445cab7d547a88ede5"></p>
<p>其中：</p>
<ul>
<li><code>yi</code> 是实际值。</li>
<li><code>y^i</code> 是预测值，由线性回归模型 <code>y^i=wxi+by^i=wxi+b</code> 计算得到。</li>
</ul>
<p>通过<strong>最小化 RSS</strong>，可以得到以下正规方程：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c4cfc420f1878f532910b15d481b66a3.png" alt="c4cfc420f1878f532910b15d481b66a3"></p>
<h3 id="矩阵形式"><a href="#矩阵形式" class="headerlink" title="矩阵形式"></a>矩阵形式</h3><p>将正规方程写成<strong>矩阵形式</strong>：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4b024cba97bb200af12c9907fadae8f0.png" alt="4b024cba97bb200af12c9907fadae8f0"></p>
<h3 id="求解方法"><a href="#求解方法" class="headerlink" title="求解方法"></a>求解方法</h3><p>通过<strong>求解上述矩阵方程</strong>，可以得到最佳的 w 和 <code>b</code>：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/9a03245250f7f00499476096b0df39b2.png" alt="9a03245250f7f00499476096b0df39b2"></p>
<h3 id="2、梯度下降法"><a href="#2、梯度下降法" class="headerlink" title="2、梯度下降法"></a>2、梯度下降法</h3><p>梯度下降法的目标是<strong>最小化损失函数 <code>J(w,b)</code></strong>。对于线性回归问题，通常使用<strong>均方误差（MSE）作为损失函数</strong>：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/836d14994f1d38d667b8f5556328c600.png" alt="836d14994f1d38d667b8f5556328c600"></p>
<p>其中：</p>
<ul>
<li><code>m</code> 是<strong>样本</strong>数量。</li>
<li><code>yi</code> 是实际值。</li>
<li><code>y^i</code> 是预测值，由线性回归模型 <code>y^i=wxi+by^i=wxi+b</code> 计算得到。</li>
</ul>
<p>梯度是<strong>损失函数对参数的偏导数</strong>，表示损失函数<strong>在参数空间中的变化方向</strong>。对于线性回归，梯度计算如下：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4fc791e09f80dc2129464eb7a3791883.png" alt="4fc791e09f80dc2129464eb7a3791883"></p>
<h3 id="参数更新规则"><a href="#参数更新规则" class="headerlink" title="参数更新规则"></a>参数更新规则</h3><p>梯度下降法通过以下规则更新参数 <code>w</code> 和 <code>b</code>：</p>
<p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/83363dfa9d7249a76cf8fb13d3a73467.png" alt="83363dfa9d7249a76cf8fb13d3a73467"></p>
<p>其中：</p>
<ul>
<li><code>α</code> 是学习率（learning rate），控制每次更新的步长。</li>
</ul>
<h4 id="梯度下降法的步骤"><a href="#梯度下降法的步骤" class="headerlink" title="梯度下降法的步骤"></a>梯度下降法的步骤</h4><ol>
<li><strong>初始化参数</strong>：初始化 <code>w</code> 和 <code>b</code> 的值（通常设为 0 或随机值）。</li>
<li><strong>计算损失函数</strong>：计算<strong>当前参数下</strong>的损失函数值 <code>J(w,b)</code>。</li>
<li><strong>计算梯度</strong>：计算损失函数对 <code>w</code> 和 <code>b</code> 的<strong>偏导数</strong>。</li>
<li><strong>更新参数</strong>：<strong>根据梯度更新</strong> <code>w</code> 和 <code>b</code>。</li>
<li><strong>重复迭代</strong>：重复步骤 2 到 4，<strong>直到损失函数收敛或达到最大迭代次数</strong>。</li>
</ol>
<h2 id="使用-Python-实现线性回归"><a href="#使用-Python-实现线性回归" class="headerlink" title="使用 Python 实现线性回归"></a>使用 Python 实现线性回归</h2><p>下面我们通过一个简单的例子来演示如何使用 Python 实现线性回归。</p>
<h3 id="1、导入必要的库"><a href="#1、导入必要的库" class="headerlink" title="1、导入必要的库"></a>1、导入必要的库</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br></pre></td></tr></table></figure>

<h3 id="2、生成模拟数据"><a href="#2、生成模拟数据" class="headerlink" title="2、生成模拟数据"></a>2、生成模拟数据</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一些随机数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">x = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * x + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Generated Data From Runoob&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://RXCCCCCC.github.io">RXCCCCCC</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://rxcccccc.github.io/2025/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">http://rxcccccc.github.io/2025/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/image/touxiang.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comments</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/image/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">RXCCCCCC</div><div class="author-info-description">哦哦哦哦哦哦耶</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B"><span class="toc-number">1.</span> <span class="toc-text">机器学习教程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BC%A0%E7%BB%9F%E7%BC%96%E7%A8%8B%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.1.</span> <span class="toc-text">机器学习与传统编程的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1"><span class="toc-number">1.2.</span> <span class="toc-text">常见机器学习任务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">机器学习常见算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B"><span class="toc-number">2.</span> <span class="toc-text">机器学习简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84%EF%BC%9F"><span class="toc-number">2.1.</span> <span class="toc-text">机器学习是如何工作的？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86"><span class="toc-number">2.1.1.</span> <span class="toc-text">1. 数据收集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">2.1.2.</span> <span class="toc-text">2. 数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.3.</span> <span class="toc-text">3. 选择模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.4.</span> <span class="toc-text">4. 训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.5.</span> <span class="toc-text">5. 评估模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96"><span class="toc-number">2.1.6.</span> <span class="toc-text">6. 模型优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.7.</span> <span class="toc-text">7. 部署模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E5%8F%8D%E9%A6%88%E5%BE%AA%E7%8E%AF"><span class="toc-number">2.1.8.</span> <span class="toc-text">8. 反馈循环</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82"><span class="toc-number">2.1.9.</span> <span class="toc-text">技术细节</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="toc-number">2.2.</span> <span class="toc-text">机器学习的类型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88Supervised-Learning%EF%BC%89"><span class="toc-number">2.2.1.</span> <span class="toc-text">1. 监督学习（Supervised Learning）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88Unsupervised-Learning%EF%BC%89"><span class="toc-number">2.2.2.</span> <span class="toc-text">2. 无监督学习（Unsupervised Learning）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88Reinforcement-Learning%EF%BC%89"><span class="toc-number">2.2.3.</span> <span class="toc-text">3. 强化学习（Reinforcement Learning）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F"><span class="toc-number">2.3.</span> <span class="toc-text">机器学习的应用领域</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%9C%AA%E6%9D%A5"><span class="toc-number">2.4.</span> <span class="toc-text">机器学习的未来</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">机器学习如何工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%98%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E7%A1%80"><span class="toc-number">3.0.1.</span> <span class="toc-text">1. 数据输入：数据是学习的基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%EF%BC%9A%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">3.0.2.</span> <span class="toc-text">2. 模型选择：选择合适的学习算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%9A%E8%AE%A9%E6%A8%A1%E5%9E%8B%E4%BB%8E%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.0.3.</span> <span class="toc-text">3. 训练过程：让模型从数据中学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E9%AA%8C%E8%AF%81%E4%B8%8E%E8%AF%84%E4%BC%B0%EF%BC%9A%E6%B5%8B%E8%AF%95%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD"><span class="toc-number">3.0.4.</span> <span class="toc-text">4. 验证与评估：测试模型的性能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E4%BC%98%E5%8C%96%E4%B8%8E%E8%B0%83%E6%95%B4%EF%BC%9A%E6%8F%90%E9%AB%98%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%B2%BE%E5%BA%A6"><span class="toc-number">3.0.5.</span> <span class="toc-text">5. 优化与调整：提高模型的精度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%8E%E9%A2%84%E6%B5%8B%EF%BC%9A%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8"><span class="toc-number">3.0.6.</span> <span class="toc-text">6. 模型部署与预测：实际应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%A8%A1%E5%9E%8B%E6%9B%B4%E6%96%B0%EF%BC%9A"><span class="toc-number">3.0.7.</span> <span class="toc-text">7. 持续学习与模型更新：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-number">4.</span> <span class="toc-text">机器学习基础概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86%E5%92%8C%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-number">4.0.1.</span> <span class="toc-text">训练集、测试集和验证集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%EF%BC%88Features%EF%BC%89%E5%92%8C%E6%A0%87%E7%AD%BE%EF%BC%88Labels%EF%BC%89"><span class="toc-number">4.0.2.</span> <span class="toc-text">特征（Features）和标签（Labels）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%EF%BC%88Model%EF%BC%89%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%88Algorithm%EF%BC%89"><span class="toc-number">4.0.3.</span> <span class="toc-text">模型（Model）与算法（Algorithm）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.0.4.</span> <span class="toc-text">监督学习、无监督学习和强化学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">4.0.5.</span> <span class="toc-text">过拟合与欠拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95%E8%AF%AF%E5%B7%AE"><span class="toc-number">4.0.6.</span> <span class="toc-text">训练与测试误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">4.0.7.</span> <span class="toc-text">评估指标</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Deep-Learning-%E5%85%A5%E9%97%A8%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">5.</span> <span class="toc-text">深度学习(Deep Learning)入门——基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">5.1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">5.2.</span> <span class="toc-text">基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">5.3.</span> <span class="toc-text">优化算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">5.4.</span> <span class="toc-text">初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%EF%BC%88bias-variance%EF%BC%89"><span class="toc-number">5.5.</span> <span class="toc-text">偏差&#x2F;方差（bias&#x2F;variance）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88regularization%EF%BC%89"><span class="toc-number">5.6.</span> <span class="toc-text">正则化（regularization）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7"><span class="toc-number">5.7.</span> <span class="toc-text">调参技巧</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E6%8A%80%E5%B7%A7"><span class="toc-number">5.8.</span> <span class="toc-text">实现技巧</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80"><span class="toc-number">6.</span> <span class="toc-text">计算机视觉基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A6%82%E8%BF%B0"><span class="toc-number">6.1.</span> <span class="toc-text">一、计算机视觉概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="toc-number">6.2.</span> <span class="toc-text">二、计算机视觉的基本原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F"><span class="toc-number">6.2.1.</span> <span class="toc-text">数字图像</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">6.2.1.1.</span> <span class="toc-text">1、图像分类的定义</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-%E2%80%94-NLP%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97"><span class="toc-number">7.</span> <span class="toc-text">[NLP] 自然语言处理 — NLP入门指南</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#NLP%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">7.1.</span> <span class="toc-text">NLP的预处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC1%E6%AD%A5%EF%BC%9A%E6%94%B6%E9%9B%86%E6%82%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E2%80%94%E8%AF%AD%E6%96%99%E5%BA%93"><span class="toc-number">7.2.</span> <span class="toc-text">第1步：收集您的数据—语料库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A6%BB%E6%95%A3%E5%BC%8F%E8%A1%A8%E7%A4%BA%EF%BC%88Discrete-Representation%EF%BC%89"><span class="toc-number">7.3.</span> <span class="toc-text">离散式表示（Discrete Representation）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA%EF%BC%88Distributed-Representation"><span class="toc-number">7.4.</span> <span class="toc-text">分布式表示（Distributed Representation</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Pytorch"><span class="toc-number">8.</span> <span class="toc-text">Pytorch</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%81%E9%80%82%E5%90%88%E9%98%85%E8%AF%BB%E6%9C%AC%E6%95%99%E7%A8%8B%EF%BC%9F"><span class="toc-number">8.1.</span> <span class="toc-text">谁适合阅读本教程？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%98%85%E8%AF%BB%E6%9C%AC%E6%95%99%E7%A8%8B%E5%89%8D%EF%BC%8C%E6%82%A8%E9%9C%80%E8%A6%81%E4%BA%86%E8%A7%A3%E7%9A%84%E7%9F%A5%E8%AF%86%EF%BC%9A"><span class="toc-number">8.2.</span> <span class="toc-text">阅读本教程前，您需要了解的知识：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B"><span class="toc-number">8.3.</span> <span class="toc-text">实例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E7%AE%80%E4%BB%8B"><span class="toc-number">9.</span> <span class="toc-text">PyTorch 简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-%E7%89%B9%E6%80%A7"><span class="toc-number">9.1.</span> <span class="toc-text">PyTorch 特性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%88Dynamic-Computation-Graph%EF%BC%89"><span class="toc-number">9.1.1.</span> <span class="toc-text">动态计算图（Dynamic Computation Graph）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%EF%BC%88Tensor%EF%BC%89%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%EF%BC%88Autograd%EF%BC%89"><span class="toc-number">9.1.2.</span> <span class="toc-text">张量（Tensor）与自动求导（Autograd）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E4%B8%8E%E8%AE%AD%E7%BB%83"><span class="toc-number">9.1.3.</span> <span class="toc-text">模型定义与训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPU-%E5%8A%A0%E9%80%9F"><span class="toc-number">9.1.4.</span> <span class="toc-text">GPU 加速</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E4%B8%8E%E7%A4%BE%E5%8C%BA%E6%94%AF%E6%8C%81"><span class="toc-number">9.1.5.</span> <span class="toc-text">生态系统与社区支持</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E5%85%B6%E4%BB%96%E6%A1%86%E6%9E%B6%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-number">9.1.6.</span> <span class="toc-text">与其他框架的对比</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E5%9F%BA%E7%A1%80"><span class="toc-number">10.</span> <span class="toc-text">PyTorch 基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%EF%BC%88Tensor%EF%BC%89"><span class="toc-number">10.1.</span> <span class="toc-text">张量（Tensor）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E4%B8%8E%E8%AE%BE%E5%A4%87"><span class="toc-number">10.1.1.</span> <span class="toc-text">张量与设备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E5%92%8C%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">10.1.2.</span> <span class="toc-text">梯度和自动微分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E5%92%8C%E6%80%A7%E8%83%BD"><span class="toc-number">10.1.3.</span> <span class="toc-text">内存和性能</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%EF%BC%88Autograd%EF%BC%89"><span class="toc-number">10.2.</span> <span class="toc-text">自动求导（Autograd）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88Backpropagation%EF%BC%89"><span class="toc-number">10.2.1.</span> <span class="toc-text">反向传播（Backpropagation）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%9C%E6%AD%A2%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">10.2.2.</span> <span class="toc-text">停止梯度计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88nn-Module%EF%BC%89"><span class="toc-number">10.3.</span> <span class="toc-text">神经网络（nn.Module）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E6%8D%9F%E5%A4%B1%E8%AE%A1%E7%AE%97"><span class="toc-number">10.3.1.</span> <span class="toc-text">前向传播与损失计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88Optimizers%EF%BC%89"><span class="toc-number">10.3.2.</span> <span class="toc-text">优化器（Optimizers）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">10.4.</span> <span class="toc-text">训练模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E5%A4%87%EF%BC%88Device%EF%BC%89"><span class="toc-number">10.5.</span> <span class="toc-text">设备（Device）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E5%BC%A0%E9%87%8F%EF%BC%88Tensor%EF%BC%89"><span class="toc-number">11.</span> <span class="toc-text">PyTorch 张量（Tensor）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F"><span class="toc-number">11.1.</span> <span class="toc-text">创建张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B1%9E%E6%80%A7"><span class="toc-number">11.2.</span> <span class="toc-text">张量的属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="toc-number">11.3.</span> <span class="toc-text">张量的操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%EF%BC%9A"><span class="toc-number">11.3.0.1.</span> <span class="toc-text">基础操作：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%A2%E7%8A%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">11.3.0.2.</span> <span class="toc-text">形状操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84-GPU-%E5%8A%A0%E9%80%9F"><span class="toc-number">11.4.</span> <span class="toc-text">张量的 GPU 加速</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E4%B8%8E-NumPy-%E7%9A%84%E4%BA%92%E6%93%8D%E4%BD%9C"><span class="toc-number">11.5.</span> <span class="toc-text">张量与 NumPy 的互操作</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="toc-number">12.</span> <span class="toc-text">PyTorch 神经网络基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%88Neuron%EF%BC%89"><span class="toc-number">12.0.1.</span> <span class="toc-text">神经元（Neuron）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%82%EF%BC%88Layer%EF%BC%89"><span class="toc-number">12.0.2.</span> <span class="toc-text">层（Layer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Feedforward-Neural-Network%EF%BC%8CFNN%EF%BC%89"><span class="toc-number">12.0.3.</span> <span class="toc-text">前馈神经网络（Feedforward Neural Network，FNN）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Recurrent-Neural-Network-RNN%EF%BC%89"><span class="toc-number">12.0.4.</span> <span class="toc-text">循环神经网络（Recurrent Neural Network, RNN）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation-Function%EF%BC%89"><span class="toc-number">12.0.5.</span> <span class="toc-text">激活函数（Activation Function）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88Loss-Function%EF%BC%89"><span class="toc-number">12.0.6.</span> <span class="toc-text">损失函数（Loss Function）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88Optimizer%EF%BC%89"><span class="toc-number">12.0.7.</span> <span class="toc-text">优化器（Optimizer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%88Training-Process%EF%BC%89"><span class="toc-number">12.0.8.</span> <span class="toc-text">训练过程（Training Process）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%AF%84%E4%BC%B0"><span class="toc-number">12.0.9.</span> <span class="toc-text">测试与评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%B1%BB%E5%9E%8B"><span class="toc-number">12.0.10.</span> <span class="toc-text">神经网络类型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">13.</span> <span class="toc-text">PyTorch 第一个神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%A6%E5%A4%96%E4%B8%80%E4%B8%AA%E5%AE%9E%E4%BE%8B"><span class="toc-number">13.1.</span> <span class="toc-text">另外一个实例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-number">13.1.1.</span> <span class="toc-text">1、数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E5%AE%9A%E4%B9%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">13.1.2.</span> <span class="toc-text">2、定义神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">13.1.3.</span> <span class="toc-text">3、定义损失函数和优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">13.1.4.</span> <span class="toc-text">4、训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E6%B5%8B%E8%AF%95%E6%A8%A1%E5%9E%8B%E5%B9%B6%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BB%93%E6%9E%9C"><span class="toc-number">13.1.5.</span> <span class="toc-text">5、测试模型并可视化结果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-number">14.</span> <span class="toc-text">PyTorch 数据处理与加载</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89-Dataset"><span class="toc-number">14.1.</span> <span class="toc-text">自定义 Dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-DataLoader-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">14.2.</span> <span class="toc-text">使用 DataLoader 加载数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">14.3.</span> <span class="toc-text">预处理与数据增强</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">14.3.1.</span> <span class="toc-text">图像数据增强</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">14.4.</span> <span class="toc-text">加载图像数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E6%BA%90%EF%BC%88Multi-source-Dataset%EF%BC%89"><span class="toc-number">14.5.</span> <span class="toc-text">用多个数据源（Multi-source Dataset）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">15.</span> <span class="toc-text">PyTorch 线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-number">15.0.1.</span> <span class="toc-text">数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">15.0.2.</span> <span class="toc-text">定义线性回归模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">15.1.</span> <span class="toc-text">定义损失函数与优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">15.1.1.</span> <span class="toc-text">训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B"><span class="toc-number">15.1.2.</span> <span class="toc-text">评估模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-number">15.1.3.</span> <span class="toc-text">结果分析</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">16.</span> <span class="toc-text">PyTorch 卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="toc-number">16.0.1.</span> <span class="toc-text">卷积神经网络的基本结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation-Function%EF%BC%89"><span class="toc-number">16.0.2.</span> <span class="toc-text">3、激活函数（Activation Function）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA-CNN-%E5%AE%9E%E4%BE%8B"><span class="toc-number">16.1.</span> <span class="toc-text">PyTorch 实现一个 CNN 实例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%AF%BC%E5%85%A5%E5%BF%85%E8%A6%81%E5%BA%93"><span class="toc-number">16.1.1.</span> <span class="toc-text">1、导入必要库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">16.1.2.</span> <span class="toc-text">2、数据加载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E5%AE%9A%E4%B9%89-CNN-%E6%A8%A1%E5%9E%8B"><span class="toc-number">16.1.3.</span> <span class="toc-text">3、定义 CNN 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">16.1.4.</span> <span class="toc-text">4、定义损失函数与优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">16.1.5.</span> <span class="toc-text">5、训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81%E6%B5%8B%E8%AF%95%E6%A8%A1%E5%9E%8B"><span class="toc-number">16.1.6.</span> <span class="toc-text">6、测试模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C%E8%AF%B4%E6%98%8E"><span class="toc-number">16.1.7.</span> <span class="toc-text">运行结果说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7%E3%80%81%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BB%93%E6%9E%9C"><span class="toc-number">16.1.8.</span> <span class="toc-text">7、可视化结果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89"><span class="toc-number">17.</span> <span class="toc-text">PyTorch 循环神经网络（RNN）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN-%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="toc-number">17.0.1.</span> <span class="toc-text">RNN 的基本结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE"><span class="toc-number">17.0.2.</span> <span class="toc-text">RNN 如何处理序列数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-%E4%B8%AD%E7%9A%84-RNN-%E5%9F%BA%E7%A1%80"><span class="toc-number">17.1.</span> <span class="toc-text">PyTorch 中的 RNN 基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84-RNN-%E5%AE%9E%E4%BE%8B"><span class="toc-number">17.1.1.</span> <span class="toc-text">PyTorch 实现一个简单的 RNN 实例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">18.</span> <span class="toc-text">PyTorch 数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch-%E5%86%85%E7%BD%AE%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">18.0.1.</span> <span class="toc-text">PyTorch 内置数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torchvision-%E5%92%8C-torchtext"><span class="toc-number">18.0.2.</span> <span class="toc-text">torchvision 和 torchtext</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-utils-data-Dataset"><span class="toc-number">18.1.</span> <span class="toc-text">torch.utils.data.Dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-utils-data-DataLoader"><span class="toc-number">18.2.</span> <span class="toc-text">torch.utils.data.DataLoader</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%86%85%E7%BD%AE%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">18.3.</span> <span class="toc-text">使用内置数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataset-%E4%B8%8E-DataLoader-%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BA%94%E7%94%A8"><span class="toc-number">18.4.</span> <span class="toc-text">Dataset 与 DataLoader 的自定义应用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2"><span class="toc-number">19.</span> <span class="toc-text">PyTorch 数据转换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2%EF%BC%9F"><span class="toc-number">19.0.1.</span> <span class="toc-text">为什么需要数据转换？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E5%8F%98%E6%8D%A2%E6%93%8D%E4%BD%9C"><span class="toc-number">19.0.2.</span> <span class="toc-text">基础变换操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%93%8D%E4%BD%9C"><span class="toc-number">19.0.3.</span> <span class="toc-text">数据增强操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%84%E5%90%88%E5%8F%98%E6%8D%A2"><span class="toc-number">19.0.4.</span> <span class="toc-text">组合变换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BD%AC%E6%8D%A2"><span class="toc-number">19.0.5.</span> <span class="toc-text">自定义转换</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B-1"><span class="toc-number">19.1.</span> <span class="toc-text">实例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E5%BA%94%E7%94%A8%E8%BD%AC%E6%8D%A2"><span class="toc-number">19.1.1.</span> <span class="toc-text">对图像数据集应用转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E8%BD%AC%E6%8D%A2%E6%95%88%E6%9E%9C"><span class="toc-number">19.1.2.</span> <span class="toc-text">可视化转换效果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Pytorch-torch-%E5%8F%82%E8%80%83%E6%89%8B%E5%86%8C"><span class="toc-number">20.</span> <span class="toc-text">Pytorch torch 参考手册</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-torch-API-%E6%89%8B%E5%86%8C"><span class="toc-number">20.1.</span> <span class="toc-text">PyTorch torch API 手册</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensor-%E5%88%9B%E5%BB%BA"><span class="toc-number">20.1.1.</span> <span class="toc-text">Tensor 创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensor-%E6%93%8D%E4%BD%9C"><span class="toc-number">20.1.2.</span> <span class="toc-text">Tensor 操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-number">20.1.3.</span> <span class="toc-text">数学运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%95%B0%E7%94%9F%E6%88%90"><span class="toc-number">20.1.4.</span> <span class="toc-text">随机数生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-number">20.1.5.</span> <span class="toc-text">线性代数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86"><span class="toc-number">20.1.6.</span> <span class="toc-text">设备管理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-torch-nn-%E5%8F%82%E8%80%83%E6%89%8B%E5%86%8C"><span class="toc-number">21.</span> <span class="toc-text">PyTorch torch.nn 参考手册</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-torch-nn-%E6%A8%A1%E5%9D%97%E5%8F%82%E8%80%83%E6%89%8B%E5%86%8C"><span class="toc-number">21.1.</span> <span class="toc-text">PyTorch torch.nn 模块参考手册</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%B9%E5%99%A8"><span class="toc-number">21.1.1.</span> <span class="toc-text">神经网络容器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%B1%82"><span class="toc-number">21.1.2.</span> <span class="toc-text">线性层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">21.1.3.</span> <span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">21.1.4.</span> <span class="toc-text">池化层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">21.1.5.</span> <span class="toc-text">激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">21.1.6.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82"><span class="toc-number">21.1.7.</span> <span class="toc-text">归一化层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82"><span class="toc-number">21.1.8.</span> <span class="toc-text">循环神经网络层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E5%B1%82"><span class="toc-number">21.1.9.</span> <span class="toc-text">嵌入层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout-%E5%B1%82"><span class="toc-number">21.1.10.</span> <span class="toc-text">Dropout 层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-number">21.1.11.</span> <span class="toc-text">实用函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B-2"><span class="toc-number">21.1.12.</span> <span class="toc-text">实例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer-%E6%A8%A1%E5%9E%8B"><span class="toc-number">22.</span> <span class="toc-text">Transformer 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%EF%BC%88Decoder%EF%BC%89"><span class="toc-number">22.0.1.</span> <span class="toc-text">解码器（Decoder）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer-%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-number">22.1.</span> <span class="toc-text">Transformer 的核心思想</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Self-Attention%EF%BC%89"><span class="toc-number">22.1.1.</span> <span class="toc-text">1. 自注意力机制（Self-Attention）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88Multi-Head-Attention%EF%BC%89"><span class="toc-number">22.1.2.</span> <span class="toc-text">多头注意力（Multi-Head Attention）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88Positional-Encoding%EF%BC%89"><span class="toc-number">22.1.3.</span> <span class="toc-text">位置编码（Positional Encoding）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer-%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">22.2.</span> <span class="toc-text">Transformer 的应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-%E5%AE%9E%E7%8E%B0-Transformer"><span class="toc-number">22.3.</span> <span class="toc-text">PyTorch 实现 Transformer</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E6%9E%84%E5%BB%BA-Transformer-%E6%A8%A1%E5%9E%8B"><span class="toc-number">23.</span> <span class="toc-text">PyTorch 构建 Transformer 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-PyTorch-%E6%9E%84%E5%BB%BA-Transformer-%E6%A8%A1%E5%9E%8B"><span class="toc-number">23.1.</span> <span class="toc-text">使用 PyTorch 构建 Transformer 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%AF%BC%E5%85%A5%E5%BF%85%E8%A6%81%E7%9A%84%E5%BA%93%E5%92%8C%E6%A8%A1%E5%9D%97"><span class="toc-number">23.1.1.</span> <span class="toc-text">1、导入必要的库和模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E5%9F%BA%E6%9C%AC%E6%9E%84%E5%BB%BA%E5%9D%97%EF%BC%9A%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E3%80%81%E4%BD%8D%E7%BD%AE%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C%E3%80%81%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">23.1.2.</span> <span class="toc-text">定义基本构建块：多头注意力、位置前馈网络、位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C%EF%BC%88Position-wise-Feed-Forward-Network%EF%BC%89"><span class="toc-number">23.1.3.</span> <span class="toc-text">位置前馈网络（Position-wise Feed-Forward Network）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">23.1.4.</span> <span class="toc-text">位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E7%BC%96%E7%A0%81%E5%99%A8%E5%9D%97%EF%BC%88Encoder-Layer%EF%BC%89"><span class="toc-number">23.1.5.</span> <span class="toc-text">构建编码器块（Encoder Layer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E8%A7%A3%E7%A0%81%E5%99%A8%E6%A8%A1%E5%9D%97"><span class="toc-number">23.1.6.</span> <span class="toc-text">构建解码器模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E5%AE%8C%E6%95%B4%E7%9A%84-Transformer-%E6%A8%A1%E5%9E%8B"><span class="toc-number">23.1.7.</span> <span class="toc-text">构建完整的 Transformer 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-PyTorch-Transformer-%E6%A8%A1%E5%9E%8B"><span class="toc-number">23.1.8.</span> <span class="toc-text">训练 PyTorch Transformer 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">23.1.9.</span> <span class="toc-text">模型评估</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Python-%E5%85%A5%E9%97%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">24.</span> <span class="toc-text">Python 入门机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BE%8B%E5%AD%90%EF%BC%9A%E4%BD%BF%E7%94%A8-Scikit-learn-%E5%81%9A%E5%88%86%E7%B1%BB"><span class="toc-number">24.1.</span> <span class="toc-text">一个简单的机器学习例子：使用 Scikit-learn 做分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-1%EF%BC%9A%E5%AF%BC%E5%85%A5%E5%BA%93"><span class="toc-number">24.1.1.</span> <span class="toc-text">步骤 1：导入库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-2%EF%BC%9A%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">24.1.2.</span> <span class="toc-text">步骤 2：加载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-3%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86"><span class="toc-number">24.1.3.</span> <span class="toc-text">步骤 3：数据集划分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-4%EF%BC%9A%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%EF%BC%88%E6%A0%87%E5%87%86%E5%8C%96%EF%BC%89"><span class="toc-number">24.1.4.</span> <span class="toc-text">步骤 4：特征缩放（标准化）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-5%EF%BC%9A%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%AE%AD%E7%BB%83"><span class="toc-number">24.1.5.</span> <span class="toc-text">步骤 5：选择模型并训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-6%EF%BC%9A%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B"><span class="toc-number">24.1.6.</span> <span class="toc-text">步骤 6：评估模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-7%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BB%93%E6%9E%9C%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="toc-number">24.1.7.</span> <span class="toc-text">步骤 7：可视化结果（可选）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">25.</span> <span class="toc-text">机器学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">25.1.</span> <span class="toc-text">监督学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88Linear-Regression%EF%BC%89"><span class="toc-number">25.1.1.</span> <span class="toc-text">线性回归（Linear Regression）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89"><span class="toc-number">25.1.2.</span> <span class="toc-text">逻辑回归（Logistic Regression）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89"><span class="toc-number">25.1.3.</span> <span class="toc-text">支持向量机（SVM）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88Decision-Tree%EF%BC%89"><span class="toc-number">25.1.4.</span> <span class="toc-text">决策树（Decision Tree）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">25.2.</span> <span class="toc-text">无监督学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K-means-%E8%81%9A%E7%B1%BB%EF%BC%88K-means-Clustering%EF%BC%89"><span class="toc-number">25.2.1.</span> <span class="toc-text">K-means 聚类（K-means Clustering）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89"><span class="toc-number">25.2.2.</span> <span class="toc-text">主成分分析（PCA）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-Linear-Regression"><span class="toc-number">26.</span> <span class="toc-text">线性回归 (Linear Regression)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%9F"><span class="toc-number">26.1.</span> <span class="toc-text">如何求解线性回归？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="toc-number">26.1.1.</span> <span class="toc-text">1、最小二乘法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F"><span class="toc-number">26.1.2.</span> <span class="toc-text">矩阵形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95"><span class="toc-number">26.1.3.</span> <span class="toc-text">求解方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">26.1.4.</span> <span class="toc-text">2、梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99"><span class="toc-number">26.1.5.</span> <span class="toc-text">参数更新规则</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="toc-number">26.1.5.1.</span> <span class="toc-text">梯度下降法的步骤</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-Python-%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">26.2.</span> <span class="toc-text">使用 Python 实现线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%AF%BC%E5%85%A5%E5%BF%85%E8%A6%81%E7%9A%84%E5%BA%93"><span class="toc-number">26.2.1.</span> <span class="toc-text">1、导入必要的库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E7%94%9F%E6%88%90%E6%A8%A1%E6%8B%9F%E6%95%B0%E6%8D%AE"><span class="toc-number">26.2.2.</span> <span class="toc-text">2、生成模拟数据</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/05/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" title="搭建环境做一个一个备忘">搭建环境做一个一个备忘</a><time datetime="2025-04-05T04:01:34.000Z" title="Created 2025-04-05 12:01:34">2025-04-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/23/%E9%A1%BD%E5%BC%BA%E6%8B%BC%E6%90%8F%E8%AE%B0%E5%BD%95/" title="顽强拼搏记录">顽强拼搏记录</a><time datetime="2025-03-23T08:12:10.000Z" title="Created 2025-03-23 16:12:10">2025-03-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/18/%E7%AE%97%E6%B3%95%E6%AF%94%E8%B5%9B%E5%90%8E%E8%AE%B0/" title="算法比赛后记">算法比赛后记</a><time datetime="2025-03-18T14:14:58.000Z" title="Created 2025-03-18 22:14:58">2025-03-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/17/%E5%94%90b%E6%9D%82%E4%BA%8B/" title="唐b杂事">唐b杂事</a><time datetime="2025-03-17T10:04:06.000Z" title="Created 2025-03-17 18:04:06">2025-03-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/17/%E9%BA%A6%E9%BA%A6-qq%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E9%83%A8%E7%BD%B2%E5%A4%87%E5%BF%98/" title="麦麦--qq聊天机器人部署备忘">麦麦--qq聊天机器人部署备忘</a><time datetime="2025-03-17T05:10:59.000Z" title="Created 2025-03-17 13:10:59">2025-03-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><script src="https://www.example.com/path/to/autoload.js" type="text/javascript"></script><div class="copyright">&copy;2019 - 2025 By RXCCCCCC</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll to Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const initValine = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyValine = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const valineConfig = {
      el: '#vcomment',
      appId: 'RM6CxQZamBGOaLbqapwigHVR-MdYXbMMI',
      appKey: 'W0jx3oFV8keuvguIvlTiy8DQ',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      visitor: false,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || window.location.pathname
    }

    new Valine(valineConfig)
  }

  const loadValine = async (el, path) => {
    if (typeof Valine === 'function') {
      initValine(el, path)
    } else {
      await btf.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js')
      initValine(el, path)
    }
  }

  if (isShuoshuo) {
    'Valine' === 'Valine'
      ? window.shuoshuoComment = { loadComment: loadValine }
      : window.loadOtherComment = loadValine
    return
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/null"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>