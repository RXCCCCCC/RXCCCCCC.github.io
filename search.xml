<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>进行vue的一个一个学</title>
      <link href="/2025/07/24/%E8%BF%9B%E8%A1%8Cvue%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E5%AD%A6/"/>
      <url>/2025/07/24/%E8%BF%9B%E8%A1%8Cvue%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E5%AD%A6/</url>
      
        <content type="html"><![CDATA[<h1 id="什么是-Vue？"><a href="#什么是-Vue？" class="headerlink" title="什么是 Vue？"></a>什么是 Vue？</h1><p>Vue (发音为 &#x2F;vjuː&#x2F;，类似 <strong>view</strong>) 是一款用于构建用户界面的 JavaScript 框架。它基于标准 HTML、CSS 和 JavaScript 构建，并提供了一套声明式的、组件化的编程模型，帮助你高效地开发用户界面。</p><p>下面是一个最基本的示例：</p><p>js</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> &#123; createApp, ref &#125; <span class="keyword">from</span> <span class="string">&#x27;vue&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="title function_">createApp</span>(&#123;</span><br><span class="line">  <span class="title function_">setup</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">      <span class="attr">count</span>: <span class="title function_">ref</span>(<span class="number">0</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;).<span class="title function_">mount</span>(<span class="string">&#x27;#app&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div id=&quot;app&quot;&gt;</span><br><span class="line">  &lt;button @click=&quot;count++&quot;&gt;</span><br><span class="line">    Count is: &#123;&#123; count &#125;&#125;</span><br><span class="line">  &lt;/button&gt;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure><p><strong>结果展示</strong></p><p>上面的示例展示了 Vue 的两个核心功能：</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>面向对象（c艹）一个一个记</title>
      <link href="/2025/07/03/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/"/>
      <url>/2025/07/03/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="面向过程程序设计OPP-Oriented-Procedural-Programming"><a href="#面向过程程序设计OPP-Oriented-Procedural-Programming" class="headerlink" title="面向过程程序设计OPP(Oriented Procedural Programming)"></a>面向过程程序设计OPP(Oriented Procedural Programming)</h2><p>将复杂过程简单的按功能分层从而解决问题<br>编程是<strong>面向操作</strong>的，编程的<strong>单位是函数</strong><br>规范的过程化程序: 过程的功能划分 &#x2F; 编写</p><h4 id="功能与数据分离"><a href="#功能与数据分离" class="headerlink" title="功能与数据分离"></a>功能与数据分离</h4><p>不符合人们对现实世界的认识<br>要保持功能与数据的相容困难</p><h4 id="自顶向下的设计方法"><a href="#自顶向下的设计方法" class="headerlink" title="自顶向下的设计方法"></a>自顶向下的设计方法</h4><p>限制了软件的可重用性，<br>降低开发效率，<br>软件系统难以维护。</p><p><img src="/./%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/4e9f2fabd86809a8bfd9e32c2ea93a51.png" alt="4e9f2fabd86809a8bfd9e32c2ea93a51"></p><p>结合在对象中，按对象组织</p><h2 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h2><p>子类自动共享父类数据和方法的机制，它由<strong>类的派生</strong>体现。一个子类直接继承父类的全部描述，同时<strong>可修改和扩充</strong>，继承是对父类的<strong>重用</strong>机制。</p><p>E.G</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 派生类：圆锥体</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Cone</span> : <span class="keyword">public</span> Circle</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="type">double</span> height; <span class="comment">// 圆锥高度</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="built_in">Cone</span>(<span class="type">double</span> X , <span class="type">double</span> Y , <span class="type">double</span> r , <span class="type">double</span> h = <span class="number">1</span>)</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setHeight</span><span class="params">(<span class="type">double</span> h)</span> </span>;</span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">getHeight</span><span class="params">()</span> <span class="type">const</span> </span>;</span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">calculateArea</span><span class="params">()</span> <span class="type">const</span> </span>;</span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">calculateVolume</span><span class="params">()</span> <span class="type">const</span></span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">printInfo</span><span class="params">()</span> <span class="type">const</span> </span>;</span><br><span class="line">……</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这段代码定义了一个名为<code>Cone</code>（圆锥体）的派生类，它继承自<code>Circle</code>（圆形）基类。这种结构体现了面向对象编程中的<strong>继承特性</strong>，下面详细解释其各部分含义：</p><h3 id="1-类的继承关系"><a href="#1-类的继承关系" class="headerlink" title="1. 类的继承关系"></a>1. 类的继承关系</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Cone</span> : <span class="keyword">public</span> Circle</span><br></pre></td></tr></table></figure><ul><li><code>class Cone</code>：声明一个名为<code>Cone</code>的类（圆锥体）。</li><li><code>: public Circle</code>：表示<code>Cone</code>是<code>Circle</code>的<strong>公有派生类</strong>（<code>public</code>为<strong>继承方式</strong>）。<br>  这意味着：<code>Cone</code>会继承<code>Circle</code>中<strong>所有的非私有成员</strong>（包括<strong>成员变量和成员函数</strong>），可以直接使用基类的功能，同时扩展自己的特性。<br>  （例如：圆形的圆心坐标<code>X,Y</code>、半径<code>r</code>等属性，圆锥体也需要，因此无需重复定义，直接继承即可。）</li></ul><h3 id="2-私有成员变量"><a href="#2-私有成员变量" class="headerlink" title="2. 私有成员变量"></a>2. 私有成员变量</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">double</span> height; <span class="comment">// 圆锥高度</span></span><br></pre></td></tr></table></figure><ul><li><code>private</code>：访问权限修饰符，标识该部分成员<strong>仅能在<code>Cone</code>类内部</strong>使用，外部无法直接访问。</li><li><code>double height</code>：定义了圆锥体特有的成员变量<code>height</code>（高度），这是<code>Cone</code>在基类<code>Circle</code>基础上扩展的属性（圆形没有高度，圆锥有）。</li></ul><h3 id="3-公有成员函数（接口）"><a href="#3-公有成员函数（接口）" class="headerlink" title="3. 公有成员函数（接口）"></a>3. 公有成员函数（接口）</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Cone</span>(<span class="type">double</span> X , <span class="type">double</span> Y , <span class="type">double</span> r , <span class="type">double</span> h = <span class="number">1</span>); <span class="comment">// 构造函数</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">setHeight</span><span class="params">(<span class="type">double</span> h)</span></span>; <span class="comment">// 设置高度</span></span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">getHeight</span><span class="params">()</span> <span class="type">const</span></span>; <span class="comment">// 获取高度</span></span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">calculateArea</span><span class="params">()</span> <span class="type">const</span></span>; <span class="comment">// 计算表面积</span></span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">calculateVolume</span><span class="params">()</span> <span class="type">const</span></span>; <span class="comment">// 计算体积</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">printInfo</span><span class="params">()</span> <span class="type">const</span></span>; <span class="comment">// 打印信息</span></span><br><span class="line">    ……</span><br></pre></td></tr></table></figure><p>这些是<code>Cone</code>类<strong>对外提供的</strong>接口，用于操作和访问类的成员，具体功能如下：</p><ul><li><p><strong>构造函数</strong> <code>Cone(...)</code>(<strong>名称与类名相同</strong>,<strong>无返回类型</strong>,<strong>可重载</strong>,)：<br>  用于初始化圆锥体对象，参数包括：<code>X,Y</code>（圆心 &#x2F; 顶点坐标，继承自<code>Circle</code>）、<code>r</code>（底面半径，继承自<code>Circle</code>）、<code>h</code>（高度，默认值为 1）。<br>  构造函数会<strong>先调用基类<code>Circle</code>的构造函数初始化继承的属性</strong>（如<code>X,Y,r</code>），<strong>再初始化自己的</strong><code>height</code>。</p><h5 id="可初始化成员变量"><a href="#可初始化成员变量" class="headerlink" title="可初始化成员变量"></a><strong>可初始化成员变量</strong></h5>  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> value;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">MyClass</span>(<span class="type">int</span> val) : <span class="built_in">value</span>(val) &#123;&#125; <span class="comment">// 初始化列表（推荐,最直观显式）</span></span><br><span class="line">    <span class="comment">// 或在函数体中赋值：</span></span><br><span class="line">    <span class="comment">// MyClass(int val) &#123; value = val; &#125;</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h5 id="可调用父类构造函数（继承场景）"><a href="#可调用父类构造函数（继承场景）" class="headerlink" title="可调用父类构造函数（继承场景）"></a><strong>可调用父类构造函数（继承场景）</strong></h5><ul><li><p>在派生类的构造函数中，<strong>必须显式调用</strong>父类的构造函数（除非父类有<strong>默认</strong>构造函数(如果类中<strong>未定义任何构造函数</strong>，编译器会自动生成一个<strong>隐式默认构造函数</strong>（无参数）。</p>  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Base</span>(<span class="type">int</span> x) &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Derived</span>(<span class="type">int</span> x, <span class="type">int</span> y) : <span class="built_in">Base</span>(x) &#123;&#125; <span class="comment">// 调用父类构造函数</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li></ul><h5 id="不能被声明为const、virtual或static"><a href="#不能被声明为const、virtual或static" class="headerlink" title="不能被声明为const、virtual或static"></a><strong>不能被声明为<code>const</code>、<code>virtual</code>或<code>static</code></strong></h5><ul><li>构造函数不能是<code>const</code>（因为它<strong>会修改对象状态</strong>）。</li><li>构造函数不能是<code>virtual</code>（虚函数<strong>依赖于对象</strong>的存在，而构造函数<strong>正在创建对象</strong>）。</li><li>构造函数不能是<code>static</code>（<strong>静态</strong>函数<strong>属于类</strong>，而<strong>构造</strong>函数<strong>属于对象</strong>）。</li></ul><h5 id="委托构造函数（C-11-）"><a href="#委托构造函数（C-11-）" class="headerlink" title="委托构造函数（C++11+）"></a><strong>委托构造函数（C++11+）</strong></h5><ul><li><p>构造函数可以<strong>调用同一个类的其他构造函数</strong>，避免代码重复。</p>  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">MyClass</span>(<span class="type">int</span> x) : <span class="built_in">value</span>(x) &#123;&#125;</span><br><span class="line">    <span class="built_in">MyClass</span>() : <span class="built_in">MyClass</span>(<span class="number">0</span>) &#123;&#125; <span class="comment">// 委托给另一个构造函数</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> value;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>setter 和 getter 函数</strong>：</p><ul><li><code>setHeight(double h)</code>：设置圆锥的高度（修改<code>height</code>的值）。</li><li><code>getHeight() const</code>：返回圆锥的高度（读取<code>height</code>的值）。<br>  这是封装特性的体现：<strong>通过函数间接访问私有</strong>变量**(在内部)**，<strong>避免外部直接修改</strong>，保证数据安全性。</li></ul></li><li><p><strong>功能计算函数</strong>：</p><ul><li><code>calculateArea() const</code>：计算圆锥的表面积（基类<code>Circle</code>若有计算圆面积的函数，<strong>则此处重写</strong>为圆锥的表面积）。</li><li><code>calculateVolume() const</code>：计算圆锥的体积（圆锥特有的功能，基类<code>Circle</code>没有）。</li></ul></li><li><p><strong>信息打印函数</strong>：</p><ul><li><code>printInfo() const</code>：打印圆锥的所有信息（如圆心坐标、半径、高度、表面积、体积等），通常会结合继承自基类的信息和自身的信息。</li></ul></li></ul><h3 id="总结：结构的核心意义"><a href="#总结：结构的核心意义" class="headerlink" title="总结：结构的核心意义"></a>总结：结构的核心意义</h3><ul><li><strong>继承复用</strong>：<code>Cone</code>通过继承<code>Circle</code>，直接复用了圆形的属性（如圆心、半径），<strong>无需重复定义，减少代码冗余</strong>。</li><li><strong>扩展功能</strong>：在继承的基础上，<code>Cone</code><strong>增加</strong>了自身特有的属性（高度）和方法（体积计算、表面积计算等），实现了 “圆锥是一种特殊的圆形（带高度）” 的逻辑关系。</li><li><strong>封装接口</strong>：通过公有成员函数对外提供访问接口，<strong>隐藏内部实现细节</strong>（如<code>height</code>的存储方式），符合面向对象的封装原则。</li></ul><p>这种结构使得代码更具扩展性和维护性，例如未来若需要修改圆形的属性（如增加颜色），圆锥体也能自动继承该特性。这段代码定义了一个名为<code>Cone</code>的类，它是从<code>Circle</code>类派生而来的，这意味着<code>Cone</code>继承了<code>Circle</code>的属性和方法。这种继承关系形成了面向对象编程中的<strong>父子类结构</strong>。</p><h3 id="代码结构解析："><a href="#代码结构解析：" class="headerlink" title="代码结构解析："></a>代码结构解析：</h3><ol><li><strong>类定义</strong>：<ul><li><code>class Cone : public Circle</code>：<code>Cone</code>类公开继承自<code>Circle</code>类，因此<code>Cone</code>可以访问<code>Circle</code>的公有成员。</li></ul></li><li><strong>私有成员变量</strong>：<ul><li><code>double height;</code>：圆锥的高度，这是<code>Cone</code>类特有的属性。</li></ul></li><li><strong>构造函数</strong>：<ul><li><code>Cone(double X, double Y, double r, double h = 1)</code>：初始化圆锥的位置（继承自<code>Circle</code>的<code>X</code>和<code>Y</code>）、底面半径（继承自<code>Circle</code>的<code>r</code>）和高度<code>h</code>（默认值为 1）。</li></ul></li><li><strong>成员函数</strong>：<ul><li><code>setHeight(double h)</code>：设置圆锥的高度。</li><li><code>getHeight() const</code>：返回圆锥的高度。</li><li><code>calculateArea() const</code>：计算圆锥的表面积（可能包括底面积和侧面积）。</li><li><code>calculateVolume() const</code>：计算圆锥的体积。</li><li><code>printInfo() const</code>：打印圆锥的信息，可能包括位置、半径、高度、表面积和体积。</li></ul></li></ol><p><code>const</code> 关键字放在函数声明的后面，其作用是表明这个函数属于<strong>常量成员函数</strong>。如果写在<strong>前面</strong>则表示的是其<strong>返回值</strong>是常量</p><h3 id="1-常量成员函数的功能"><a href="#1-常量成员函数的功能" class="headerlink" title="1. 常量成员函数的功能"></a>1. 常量成员函数的功能</h3><ul><li><strong>保护对象状态</strong>：常量成员函数不可以对调用它的对象的非静态数据成员进行修改。</li><li><strong>适配常量对象</strong>：<strong>只有</strong>常量成员函数才能够被常量对象调用。</li></ul><h3 id="2-代码示例与说明"><a href="#2-代码示例与说明" class="headerlink" title="2. 代码示例与说明"></a>2. 代码示例与说明</h3><p>下面是一个包含常量成员函数的类：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Circle</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">double</span> radius;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Circle</span>(<span class="type">double</span> r) : <span class="built_in">radius</span>(r) &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 常量成员函数：不能修改对象状态</span></span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">getRadius</span><span class="params">()</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> radius; <span class="comment">// 允许，因为没有修改成员变量</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 常量成员函数：计算圆的面积</span></span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">calculateArea</span><span class="params">()</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        <span class="comment">// radius = 10.0; // 错误！不可以修改成员变量</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">3.14</span> * radius * radius;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 非常量成员函数：可以修改对象状态</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">setRadius</span><span class="params">(<span class="type">double</span> r)</span> </span>&#123;</span><br><span class="line">        radius = r; <span class="comment">// 允许，因为这不是常量成员函数</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="3-常量对象与函数调用规则"><a href="#3-常量对象与函数调用规则" class="headerlink" title="3. 常量对象与函数调用规则"></a>3. 常量对象与函数调用规则</h3><ul><li><strong>常量对象</strong>：<strong>只能调用常量成员</strong>函数。</li><li><strong>非常量对象</strong>：<strong>既能调用常量成员函数，也能调用非常量成员函数</strong>。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">const</span> Circle <span class="title">c1</span><span class="params">(<span class="number">5.0</span>)</span></span>; <span class="comment">// 常量对象,同时进行了初始化</span></span><br><span class="line"><span class="type">double</span> area = c<span class="number">1.</span><span class="built_in">calculateArea</span>(); <span class="comment">// 正确，calculateArea是常量成员函数</span></span><br><span class="line"><span class="comment">// c1.setRadius(10.0); // 错误，常量对象不能调用非常量成员函数</span></span><br><span class="line"></span><br><span class="line"><span class="function">Circle <span class="title">c2</span><span class="params">(<span class="number">3.0</span>)</span></span>; <span class="comment">// 非常量对象</span></span><br><span class="line">c<span class="number">2.</span><span class="built_in">setRadius</span>(<span class="number">10.0</span>); <span class="comment">// 正确</span></span><br><span class="line">area = c<span class="number">2.</span><span class="built_in">calculateArea</span>(); <span class="comment">// 正确</span></span><br></pre></td></tr></table></figure><h3 id="4-技术原理"><a href="#4-技术原理" class="headerlink" title="4. 技术原理"></a>4. 技术原理</h3><ul><li><p>隐式<code>this</code>指针的类型</p><p>  ：</p><ul><li>在常量成员函数里，<code>this</code> 指针的类型是 <code>const ClassName*</code>。</li><li>在非常量成员函数中，<code>this</code> 指针的类型是 <code>ClassName*</code>。</li></ul></li></ul><h3 id="5-实际应用场景"><a href="#5-实际应用场景" class="headerlink" title="5. 实际应用场景"></a>5. 实际应用场景</h3><ul><li><strong>访问器（Getter）函数</strong>：通常会被声明为常量成员函数，比如 <code>getRadius()</code>。</li><li><strong>不修改对象的计算函数</strong>：像 <code>calculateArea()</code> 就属于这类函数。</li><li><strong>操作符重载</strong>：例如 <code>operator==</code> 通常也会被声明为常量成员函数。</li></ul><h3 id="6-注意要点"><a href="#6-注意要点" class="headerlink" title="6. 注意要点"></a>6. 注意要点</h3><ul><li><strong>函数重载</strong>：常量版本和非常量版本的同一函数可以同时存在。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span>* <span class="title">getData</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> constData; &#125; <span class="comment">// 常量版本</span></span><br><span class="line">    <span class="function"><span class="type">char</span>* <span class="title">getData</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> data; &#125; <span class="comment">// 非常量版本</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">char</span>* data;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* constData;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ul><li><strong>可变数据成员（mutable）</strong>：被 <code>mutable</code> 修饰的数据成员，能够在常量成员函数中被修改。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Counter</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">mutable</span> <span class="type">int</span> accessCount; <span class="comment">// 可变数据成员</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">doSomething</span><span class="params">()</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        accessCount++; <span class="comment">// 允许，因为accessCount是mutable的</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="继承关系说明："><a href="#继承关系说明：" class="headerlink" title="继承关系说明："></a>继承关系说明：</h3><ul><li><strong>父类（基类）</strong>：<code>Circle</code>类（假设包含圆心坐标<code>X</code>、<code>Y</code>和半径<code>r</code>）。</li><li><strong>子类（派生类）</strong>：<code>Cone</code>类通过继承获得了<code>Circle</code>的属性，并添加了自己的属性<code>height</code>。</li></ul><h2 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h2><p>在<strong>继承体系结构</strong>中，同一消息<strong>为不同的对象接受时可产生完全不同的行动</strong></p><p>利用多态性用户可发送一个通用的信息，而将所有的实现细节都留给接受消<br>息的对象自行决定</p><h2 id="template"><a href="#template" class="headerlink" title="template"></a>template<typename T></h2><p>template：声明这是一个模板。<br>typename T：声明一个<strong>类型参数T</strong>，T<strong>可以是任何类型</strong>（如int、double、string等）。<br>typename 也可以用 class 替代（如 template<class T>），两者在模板中<strong>含义相同</strong>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Stack</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    T buffer[<span class="number">100</span>]; <span class="comment">// 假设栈大小为100</span></span><br><span class="line">    <span class="type">int</span> top;       <span class="comment">// 栈顶位置</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Stack</span>() : <span class="built_in">top</span>(<span class="number">-1</span>) &#123;&#125; <span class="comment">// 初始化栈顶为-1，表示空栈</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">push</span><span class="params">(<span class="type">const</span> T&amp; x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (top &gt;= <span class="number">99</span>) <span class="keyword">return</span> <span class="literal">false</span>; <span class="comment">// 栈满</span></span><br><span class="line">        buffer[++top] = x;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">T <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (top &lt; <span class="number">0</span>) <span class="keyword">throw</span> <span class="string">&quot;Stack is empty!&quot;</span>; <span class="comment">// 栈空</span></span><br><span class="line">        <span class="keyword">return</span> buffer[top--];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line">Stack&lt;<span class="type">int</span>&gt; intStack;    <span class="comment">// 存储int的栈</span></span><br><span class="line">Stack&lt;<span class="type">double</span>&gt; doubleStack; <span class="comment">// 存储double的栈</span></span><br><span class="line"></span><br><span class="line">intStack.<span class="built_in">push</span>(<span class="number">10</span>);</span><br><span class="line">doubleStack.<span class="built_in">push</span>(<span class="number">3.14</span>);</span><br></pre></td></tr></table></figure><h2 id="数据结构与数据访问"><a href="#数据结构与数据访问" class="headerlink" title="数据结构与数据访问"></a><strong>数据结构</strong>与数据访问</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> *p1 = (<span class="type">int</span> *)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(<span class="type">int</span>) * length); </span><br><span class="line"><span class="built_in">free</span>(p1);</span><br><span class="line"><span class="type">int</span> *p2 = <span class="keyword">new</span> <span class="type">int</span> [length] ;</span><br><span class="line"><span class="keyword">delete</span> [ ]p2; <span class="comment">// 释放数组用法</span></span><br><span class="line"><span class="keyword">delete</span> p2; <span class="comment">// 释放单个元素</span></span><br></pre></td></tr></table></figure><h2 id="C-函数的新特性"><a href="#C-函数的新特性" class="headerlink" title="C++函数的新特性"></a>C++函数的新特性</h2><h4 id="引用"><a href="#引用" class="headerlink" title="引用"></a><strong>引用</strong></h4><p>对一个数据可以使用引用(reference)的方式声明，引用的作用是<strong>为一个变量起一个别名</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int a ; </span><br><span class="line">int &amp;b = a; // 声明b是int a的引用</span><br><span class="line">b = 20; // a = 20</span><br></pre></td></tr></table></figure><p>在一条语句中声明多个引用时<strong>应逐一声明</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span>&amp; x=a, y = b，z = c ; <span class="comment">//error</span></span><br><span class="line"><span class="type">int</span> &amp;x=a, &amp;y=b, &amp;z=c; </span><br></pre></td></tr></table></figure><p>声明引用变量<strong>必须进行初始化</strong>，引用未定义变量称悬挂引用。<br>将前面声明的引用重新变为另一变量的别名是个<strong>逻辑错误</strong>.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> y =<span class="number">5</span>, z = <span class="number">3</span>; </span><br><span class="line"><span class="type">int</span> &amp;x = z, &amp;x = y</span><br></pre></td></tr></table></figure><p>&amp;在此<strong>不是求地址运算</strong>，而是起<strong>标识</strong>作用。</p><p>引用声明完毕后，相当于目标变量名有<strong>两个名称</strong>，</p><p>声明一个引用，<strong>不是新定义了一个变量</strong>，引用本身<strong>不占存储单元</strong>，系统也不给引用分配存储单元。</p><p>引用即用别名引用这个变量,目的是为了消除指针</p><h4 id="引用传递的特点"><a href="#引用传递的特点" class="headerlink" title="引用传递的特点"></a>引用传递的特点</h4><p><strong>消除了复制大量数据的开销</strong>，有利提高执行效率；<br>在被调用函数中直接使用形参变量，提高可读性；<br>安全性较差，被调用函数能直接访问和修改调用者的数据。<br>fun( const T&amp; value)；<br>若要传递较大的对象，用常量引用参数模拟按值调用．<br>要指定引用常量，在参数声明的类型说明符前面加上const</p><h4 id="内联函数inline-以相应代码代替"><a href="#内联函数inline-以相应代码代替" class="headerlink" title="内联函数inline(以相应代码代替)"></a><strong>内联函数inline(以相应代码代替)</strong></h4><p>C++为<strong>降低小程序调用开销</strong>的一种机制。<br><strong>默认参数值 default parameter value</strong><br>函数参数的默认值使得在函数调用时<strong>可不指定参数。</strong></p><p><strong>建议性声明：不能含有复杂结构控制语句和递归调用</strong></p><h4 id="函数重载"><a href="#函数重载" class="headerlink" title="函数重载"></a><strong>函数重载</strong></h4><p>常用于处理<strong>不同数据类型</strong>而<strong>功能类似</strong>的<strong>同名</strong>函数;</p><h4 id="函数默认参数"><a href="#函数默认参数" class="headerlink" title="函数默认参数"></a>函数默认参数</h4><p>经常需要用<strong>相同的参数调用同一函数时</strong>，简化函数调用。<br>当函数调用时，若实参数个数少于形参数的总数时，<br>则所缺参数自动取函数参数表中设置的缺省值。：<br>当函数声明时，由<strong>右至左指定默认参数</strong>的值</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">volumn</span><span class="params">( <span class="type">int</span> length, <span class="type">int</span> width = <span class="number">1</span>, <span class="type">int</span> highth =<span class="number">1</span>)</span>；</span></span><br><span class="line"><span class="function"><span class="title">volumn</span><span class="params">(<span class="number">2</span>)</span></span>;</span><br><span class="line"><span class="built_in">volumn</span>(<span class="number">2</span>,<span class="number">2</span>); </span><br><span class="line"><span class="built_in">volumn</span>(<span class="number">2</span>, ,<span class="number">2</span>);<span class="comment">//中间参数默认</span></span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">volumn</span><span class="params">( <span class="type">int</span> length, <span class="type">int</span> width = <span class="number">1</span>, <span class="type">int</span> highth =<span class="number">1</span>)</span>；</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">volumn</span><span class="params">( <span class="type">int</span> length, <span class="type">int</span> width )</span>；</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">volumn</span><span class="params">( <span class="type">int</span> length)</span>；</span></span><br><span class="line"><span class="function"><span class="title">volumn</span><span class="params">(<span class="number">1</span>)</span></span>;<span class="comment">//有二义性,回出错</span></span><br><span class="line"><span class="built_in">volumn</span>(<span class="number">2</span>,<span class="number">3</span>); </span><br><span class="line"><span class="built_in">volumn</span>(<span class="number">1</span>,<span class="number">2</span> ,<span class="number">3</span>);</span><br></pre></td></tr></table></figure><h2 id="初识类"><a href="#初识类" class="headerlink" title="初识类"></a>初识类</h2><h2 id="封装-encapsulate"><a href="#封装-encapsulate" class="headerlink" title="封装(encapsulate)"></a>封装(encapsulate)</h2><ul><li>把全部属性和全部行为封装在一起，<br>  形成一个不可分割的独立单位（即对象）。</li><li>信息隐蔽(information hiding)<br>  对象的外部不能直接地存取对象属性，只能通过几个允许外部使用的服务与对象发生联系。</li><li><strong>对象间通发送消息</strong>进行交互.</li></ul><p>类是面向对象编程的<strong>程序基本单位</strong><br>程序<strong>模块</strong>是各种<strong>由类构成的</strong><br>类是<strong>逻辑上相关</strong>数据和函数的<strong>封装</strong><br>类是对问题的<strong>抽象描述</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> 类名 &#123;</span><br><span class="line"><span class="keyword">private</span>：</span><br><span class="line"><span class="comment">//私有数据成员和成员函数；</span></span><br><span class="line"><span class="keyword">protected</span>：</span><br><span class="line"><span class="comment">//保护数据成员和成员函数；</span></span><br><span class="line"><span class="keyword">public</span>：</span><br><span class="line"><span class="comment">//公有数据成员和成员函数；</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="成员函数"><a href="#成员函数" class="headerlink" title="成员函数"></a>成员函数</h4><p>在<strong>类的外部</strong>定义成员函数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">返回类型 类名::成员函数名(参数列表)</span><br><span class="line">&#123;</span><br><span class="line">函数定义体</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">A::show</span><span class="params">( )</span></span>&#123; cout &lt;&lt; m_a &lt;&lt; m_b &lt;&lt;endl;&#125;</span><br></pre></td></tr></table></figure><p>在<strong>类内</strong>直接定义成员函数, <strong>默认创建为内联</strong>函数<br>如果成员函数在类体外定义,要<strong>用inline声明为内联</strong>函数</p><p><strong>域运算符“∷”</strong>，<strong>成员运算符“.”</strong></p><p>在类外定义函数时，应指明成员函数的<strong>作用域</strong></p><p>在成员函数引用本对象的数据成员时，只需<strong>直接写数据成员名，</strong><br>这时C++系统会把它默认为本对象的数据成员。</p><h4 id="保护-protected"><a href="#保护-protected" class="headerlink" title="保护 protected"></a><strong>保护</strong> protected</h4><p><strong>除了类本身的成员函数和说明为友元函数或友元类的成员函数可以访问保</strong></p><p><strong>护成员外，该类的派生类的成员也可以访问</strong><strong>。</strong></p><p><strong>private 在首次出现时可以忽略</strong></p><h2 id="对象的使用"><a href="#对象的使用" class="headerlink" title="对象的使用"></a><strong>对象的使用</strong></h2><p><strong>同类对象之间可以相互赋值</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Time tA, tB;</span><br><span class="line">tA.<span class="built_in">set</span> (<span class="number">15</span>,<span class="number">6</span>,<span class="number">0</span>);</span><br><span class="line">tB = tA;</span><br></pre></td></tr></table></figure><p>成员访问运算符“.” 和**“-&gt;”(对象指针名-&gt;成员名)**</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cout &lt;&lt; t.hour &lt;&lt; pTime -&gt;<span class="built_in">min</span>(相当于访问所指对象的成员min) &lt;&lt; (*pTime).sec;</span><br></pre></td></tr></table></figure><p>软件工程的一个最基本的原则就是<strong>将接口与实现分离</strong>，信息隐蔽是软件工程中一个非常重要的概念。</p><p><strong>自定义类库头文件.h</strong><br>文件中有用户自行设计的类的定义，包括类的外部接口（公有成员函数的原型）。任何需要使用这些类的源程序，只要在文件中包含这些头文件即可。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//point.h</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Point</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">distance</span><span class="params">(Point &amp; p)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setX</span><span class="params">(<span class="type">double</span> i)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setY</span><span class="params">(<span class="type">double</span> j)</span></span>;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="type">double</span> x;</span><br><span class="line"><span class="type">double</span> y;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//point.cpp</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&quot;point.h&quot;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">Point::distance</span><span class="params">(Point &amp; p)</span></span>&#123;<span class="comment">//定义要指明作用域</span></span><br><span class="line"><span class="keyword">return</span> <span class="built_in">sqrt</span>((p.x-x)*(p.x-x)+(p.y-y)*(p.y-y)); &#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Point::setX</span><span class="params">(<span class="type">double</span> i)</span></span>&#123;x=i;&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Point::setY</span><span class="params">(<span class="type">double</span> j)</span></span>&#123;y=j;&#125;</span><br></pre></td></tr></table></figure><p>在面向对象的程序开发中，一般做法是将<strong>类的声明</strong>放在指定的头文件中，用户如果想用该类，只要把有关的头文件包含进来即可，不必在程序中重复书写类的声明,在<strong>程序中</strong>就可以用该类来<strong>定义</strong>对象.为了实现信息隐蔽，对类成员函数的定义一般不放在头文件中，而另外放在一个文件中。</p><h4 id="构造函数与析构函数"><a href="#构造函数与析构函数" class="headerlink" title="构造函数与析构函数"></a>构造函数与析构函数</h4><p>(自定义)<strong>默认构造函数</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;类名&gt;::&lt;默认构造函数名&gt;（）</span><br><span class="line">&#123; &#125;</span><br><span class="line">Time::<span class="built_in">Time</span>( )</span><br><span class="line">&#123; hour=min=sec = <span class="number">0</span>; &#125;</span><br></pre></td></tr></table></figure><p><strong>析构函数</strong></p><p>构造函数的反函数，析构函数是用于<strong>取消对象成员</strong>函数，<br>当一个对象生命期结束时，系统自动调用析构函数。</p><ul><li>析构函数名字为**符号“~”**加类名；<ul><li>析构函数<strong>没有参数和返回值</strong>。<ul><li>一个类中只可能定义一个析构函数，<br>  析构函数<strong>不能重载</strong>。<ul><li>析构函数的作用<br>  进行清除对象，释放内存等；</li></ul></li></ul></li></ul></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">类名::</span><br><span class="line">~默认析构函数名（）</span><br><span class="line">&#123; &#125;<span class="comment">//空函数</span></span><br></pre></td></tr></table></figure><p>自动调用<br>(1) 一个对象当其结束生命周期时 ；<br>(2) 使用<strong>new运算符创建的对象</strong>，<br>在<strong>使用delete运算符释放</strong>该对象时；<br>一般析构函数的<strong>调用顺序</strong>与构造函数<strong>相反</strong>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class Rational&#123;</span><br><span class="line">public:</span><br><span class="line">Rational(int nn=1,int mm=1); //构造函数</span><br><span class="line">void print() ; // 输出化简的 分子/分母</span><br><span class="line">void simple(); // 约分</span><br><span class="line">double getValue(); // 返回分数值</span><br><span class="line">Rational add(Rational &amp; A); // r = r1.add(r2); </span><br><span class="line">void sub(Rational &amp; A , Rational &amp;B); // r.sub(r1,r2);</span><br><span class="line">Rational mul(Rational &amp; A);</span><br><span class="line">void _div(Rational &amp; A, Rational &amp;B); </span><br><span class="line">private:</span><br><span class="line">int m; // 分母</span><br><span class="line">int n; // 分子</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>一般情况下，如果类中的数据<strong>都在栈里</strong>，程序员不需要开发<strong>自定义的拷贝构造函数</strong></p><p><strong>默认拷贝构造函数</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A  b ( a )</span><br></pre></td></tr></table></figure><p>对象复制与对象赋值是<strong>不同的</strong></p><p>静态数据成员的初始化与一般数据成员不同,<strong>外部</strong>静态数据成员初始化的格式如下：</p><p>&lt;类型&gt; &lt;类名&gt;::&lt;静态数据成员&gt; &#x3D; &lt;值&gt;;</p><p>3）在引用静态数据成员时采用格式：  </p><p><strong>&lt;类名&gt;::&lt;静态数据成员&gt;</strong>    &lt;对象名&gt;. &lt;静态数据成员&gt;</p><p>静态数据成员 vs 全局变量     </p><p>有了静态数据成员，<strong>各对象之间(即不依赖于对象使用)<strong>的数据有了沟通的渠道，实现数据共享 。    全局变量</strong>破坏了封装的原则</strong>，不符合面向对象程序的要求。   </p><p>公用静态数据成员与全局变量的作用域不同  </p><p>静态数据成员的作用域<strong>只限于定义该类的作用域内</strong> </p><p>静态成员函数<strong>只能访问静态数据成员、静态成员函数和类以外的函数和数据，不能访问类中的非静态数据成员（因为非静态数据成员只有对象存在时才有意义）</strong>。但静态数据成员和静态成员函数可由任意访问权限许可的函数访问。和一般成员函数类似，静态成员函数也有访问限制，私有静态成员函数不能由外界访问。静态成员函数<strong>没有this指针</strong>，因此，静态成员函数<strong>只能直接访问类中的静态成员</strong>，若要访问类中的非静态成员时，<strong>必须借助对象名或指向对象的指针</strong>。 </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream.h&gt;</span></span></span><br><span class="line"><span class="keyword">class</span>  <span class="title class_">Counter</span></span><br><span class="line">&#123;         <span class="type">static</span>  <span class="type">int</span>  num ;</span><br><span class="line">  <span class="keyword">public</span> :</span><br><span class="line">           <span class="built_in">Counter</span>( )&#123; num++; &#125;</span><br><span class="line">            ～<span class="built_in">Counter</span>()&#123; num--; &#125;</span><br><span class="line">     <span class="function"><span class="type">void</span>  <span class="title">setnum</span> <span class="params">( <span class="type">int</span> i )</span> </span>&#123; num = i ; &#125;</span><br><span class="line">     <span class="function"><span class="type">void</span>  <span class="title">shownum</span><span class="params">()</span> </span>&#123; cout &lt;&lt; num &lt;&lt; <span class="string">&#x27;\t&#x27;</span> ; &#125;</span><br><span class="line">     <span class="function"><span class="type">static</span> <span class="type">int</span> <span class="title">get</span><span class="params">()</span></span>&#123; <span class="keyword">return</span> num ; &#125;</span><br><span class="line">&#125; ;</span><br><span class="line"><span class="type">int</span>  Counter :: num = <span class="number">0</span> ;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">main</span> <span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;  Counter  a ;      a.<span class="built_in">shownum</span>() ;</span><br><span class="line">Counter  b ;      b.<span class="built_in">shownum</span>() ;</span><br><span class="line">cout&lt;&lt;Counter::<span class="built_in">get</span>()&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>static 数据是<strong>类共有</strong>的,static 函数可以类名调用，也可以对象调用,<strong>普通成员函数能访问static数据</strong></p><p>const成员变量<strong>只能由构造函数通过初始化列表对该数据成员进行初始化</strong></p><p>若<strong>成员函数不修改对象</strong>,则声明为const.</p><p>const关键词可以<strong>参与区分重载函数</strong>。</p><p>const 对象<strong>只能调用它的const 成员函数</strong>，而不能调用其他成员函数。</p><p>直接初始化</p><p><strong>分配空间的同时进行初始化</strong>. 一般数组成员较少.</p><p>Box b[3] &#x3D; {Box(1),Box(1,1),Box(1,1,1)};  </p><p>间接初始化</p><p><strong>先分配空间</strong>，<strong>之后完成初始化</strong>. </p><p>Box a[50];&#x2F;&#x2F;先调用默认                      </p><p>for( int i &#x3D; 0; i&lt;50;i++){ a[i].set(i, i, i); }</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Box box [<span class="number">3</span>] ;</span><br><span class="line">Box box [<span class="number">3</span>] = &#123;<span class="built_in">Box</span>(<span class="number">1</span>),<span class="built_in">Box</span>(<span class="number">1</span>,<span class="number">1</span>),<span class="built_in">Box</span>(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)&#125;;  </span><br><span class="line">Box box [<span class="number">3</span>] = &#123;<span class="built_in">Box</span>(),<span class="built_in">Box</span>(),<span class="built_in">Box</span>()&#125;;</span><br><span class="line">Box box [<span class="number">3</span>] = &#123;Box, Box ,Box &#125;;</span><br><span class="line">Box box [<span class="number">3</span>] = &#123;<span class="built_in">Box</span>(<span class="number">1</span>),<span class="built_in">Box</span>(<span class="number">2</span>),  &#125;;</span><br><span class="line">Box box [<span class="number">3</span>] = &#123;<span class="built_in">Box</span>(<span class="number">1</span>),<span class="built_in">Box</span>(<span class="number">1</span>) &#125;;</span><br><span class="line">Box box [<span class="number">3</span>] = &#123;<span class="built_in">Box</span>(<span class="number">1</span>) &#125;;</span><br><span class="line">Box box [<span class="number">3</span>] = &#123; &#125;;</span><br><span class="line">Box box [<span class="number">3</span>] = &#123; <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;;</span><br><span class="line">Box *p =<span class="keyword">new</span> Box[<span class="number">3</span>];</span><br><span class="line">Box *p =<span class="keyword">new</span> Box[<span class="number">3</span>]&#123;<span class="built_in">Box</span>(<span class="number">1</span>),<span class="built_in">Box</span>(<span class="number">1</span>,<span class="number">1</span>),<span class="built_in">Box</span>(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>) &#125;;</span><br></pre></td></tr></table></figure><h2 id="类的组合"><a href="#类的组合" class="headerlink" title="类的组合"></a>类的组合</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Point</span></span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line">      Point（<span class="type">int</span>=<span class="number">0</span>,<span class="type">int</span>=<span class="number">0</span>）;</span><br><span class="line">      <span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">( )</span>  </span>;</span><br><span class="line">      <span class="function"><span class="type">void</span> <span class="title">setX</span><span class="params">(<span class="type">int</span> x)</span></span>;</span><br><span class="line">      <span class="function"><span class="type">void</span> <span class="title">setY</span><span class="params">(<span class="type">int</span> y)</span></span>;</span><br><span class="line">      <span class="function"><span class="type">int</span> <span class="title">getX</span><span class="params">()</span></span>;</span><br><span class="line">      <span class="function"><span class="type">int</span> <span class="title">getY</span><span class="params">()</span></span>;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">     <span class="type">int</span> x; </span><br><span class="line">     <span class="type">int</span> y;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Circle</span>                                   </span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line">      <span class="built_in">Circle</span>(<span class="type">double</span> r, Point p);</span><br><span class="line">      <span class="built_in">Circle</span>(<span class="type">double</span> r, <span class="type">int</span> x, <span class="type">int</span> y)；</span><br><span class="line">       <span class="function"><span class="type">void</span> <span class="title">setCenter</span><span class="params">(<span class="type">int</span> x,<span class="type">int</span> y)</span></span>;</span><br><span class="line">       <span class="function">Point <span class="title">getCenter</span> <span class="params">()</span></span>;</span><br><span class="line">    <span class="keyword">private</span>:</span><br><span class="line">      <span class="type">double</span> radius;</span><br><span class="line">      Point  center;<span class="comment">//类的嵌套    </span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="构造函数-必须首先初始化内嵌对象的数据"><a href="#构造函数-必须首先初始化内嵌对象的数据" class="headerlink" title="构造函数: 必须首先初始化内嵌对象的数据"></a>构造函数: 必须首先初始化内嵌对象的数据</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Circle ::<span class="built_in">Circle</span>(<span class="type">double</span> r, <span class="type">int</span> x, <span class="type">int</span> y):<span class="built_in">radius</span>(r), <span class="built_in">center</span>(x,y)&#123; &#125;</span><br><span class="line">Circle ::<span class="built_in">Circle</span>(<span class="type">double</span> r, Point p):<span class="built_in">radius</span>(r), <span class="built_in">center</span>(p)&#123; &#125;</span><br></pre></td></tr></table></figure><h4 id="成员函数-可以使用内嵌对象调用其函数-注意访问权限控制！"><a href="#成员函数-可以使用内嵌对象调用其函数-注意访问权限控制！" class="headerlink" title="成员函数: 可以使用内嵌对象调用其函数. 注意访问权限控制！"></a>成员函数: 可以使用内嵌对象调用其函数. 注意访问权限控制！</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">setCenter</span><span class="params">(<span class="type">int</span> x,<span class="type">int</span> y)</span></span>&#123;</span><br><span class="line">       center.<span class="built_in">setX</span>(x);  <span class="comment">//center.x = x;   compiler error!</span></span><br><span class="line">       center.<span class="built_in">setY</span>(y);  <span class="comment">//center.y = y;   compiler error!</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="成员对象的初始化"><a href="#成员对象的初始化" class="headerlink" title="成员对象的初始化"></a>成员对象的初始化</h4><p>一个对象如果有**“成员对象”<strong>（即它的成员数据不是普通类型，而是“类”类型的），那么在</strong>实现构造函数时应对“成员对象”进行初始化**)</p><p>方式是在构造函数中<strong>增加构造参数，指明“成员对象”构造的方式</strong></p><p>若没有“成员对象”构造方式的声明，系统<strong>默认调用“成员对象”的无参的构造函数。</strong></p><h4 id="组合关系"><a href="#组合关系" class="headerlink" title="组合关系"></a>组合关系</h4><p>一个类包含另一个类的对象</p><p>描述<strong>整体拥有部分</strong>的关系，即<strong>has-a关系</strong></p><p>该类不与其他类共享对象的引用。即**“整体”端重数只能是1**</p><p>如果这种类的对象生命周期结束，<strong>被包含的对象的生命周期也会结束。</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Textfield</span>;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Botton</span>;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DialogueWindow</span>&#123;</span><br><span class="line">Textfield textfield;   <span class="comment">//data member</span></span><br><span class="line">Botton botton;        <span class="comment">//data member</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DialogueWindow</span>&#123;</span><br><span class="line">Textfield textfield1;  <span class="comment">//data member</span></span><br><span class="line">          Textfield textfield2;  <span class="comment">//data member</span></span><br><span class="line">Botton botton1;          <span class="comment">//data member</span></span><br><span class="line">          Botton botton2;          <span class="comment">//data member</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="依赖关系"><a href="#依赖关系" class="headerlink" title="依赖关系"></a>依赖关系</h4><p>描述两个类对象之间<strong>短暂的相互作用</strong></p><p>依赖关系表示一个类的对象<strong>短暂使用了另一个类对象</strong>，代表类之间**“uses-a”关系**</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Time</span>&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="type">int</span> hour;</span><br><span class="line"><span class="type">int</span> minute;</span><br><span class="line"><span class="type">int</span> second;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">         <span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">()</span></span>&#123;  cout&lt;&lt;hour&lt;&lt;“ ”&lt;&lt;minute&lt;&lt;“ ”&lt;&lt;second&lt;&lt;endl;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Printer</span> &#123;         <span class="comment">// 打印机类</span></span><br><span class="line"><span class="keyword">public</span>: <span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">()</span></span>&#123;...&#125; <span class="comment">// 打印</span></span><br><span class="line">&#125;; </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span>   <span class="comment">// 学生类</span></span><br><span class="line">&#123;  <span class="comment">// 使用打印机</span></span><br><span class="line"><span class="keyword">public</span>: <span class="function"><span class="type">void</span> <span class="title">usePrinter</span><span class="params">(Printer &amp;p)</span></span>&#123;  p.<span class="built_in">print</span>();  &#125;</span><br><span class="line">     ……</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">Printer printer;   Student studnet;</span><br><span class="line">student.<span class="built_in">usePrinter</span>(printer);                </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Student类的成员<strong>不包含打印机Printer的对象或者指针</strong>，即二者<strong>不具有“拥有has-a’关系</strong>。<strong>只有学生对象调用usePrinter( )函数时，学生对象与打印机对象才建立关系</strong>，并且在该<strong>函数执行完毕后，二者关系就结束了</strong>。一种<strong>短暂的”使用关系”</strong>，即“use-a”关系。依赖关系除了<strong>被依赖方作为依赖方的函数参数，还可能作为依赖方的函数中的临时对象</strong>。</p><h2 id="友元"><a href="#友元" class="headerlink" title="友元"></a>友元</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>&#123;</span><br><span class="line"><span class="keyword">private</span>:   <span class="type">int</span> x;</span><br><span class="line">Public:  <span class="built_in">A</span>()&#123;x=<span class="number">1</span>;&#125;</span><br><span class="line">     <span class="keyword">friend</span> <span class="keyword">class</span> <span class="title class_">B</span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>&#123;</span><br><span class="line">Private:  <span class="type">char</span> c;</span><br><span class="line">Public:  <span class="built_in">B</span>() &#123;c=<span class="string">&#x27;c&#x27;</span>;&#125;</span><br><span class="line">              <span class="function"><span class="type">void</span> <span class="title">something</span><span class="params">()</span></span>&#123;</span><br><span class="line">                      A instance;</span><br><span class="line">cout&lt;&lt;instance.x&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">B b_instance;</span><br><span class="line">b_instance.<span class="built_in">something</span>();</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="友元-函数-与友元类"><a href="#友元-函数-与友元类" class="headerlink" title="友元 函数 与友元类"></a>友元 函数 与友元类</h2><h4 id="友元函数和友元类"><a href="#友元函数和友元类" class="headerlink" title="友元函数和友元类"></a>友元函数和友元类</h4><p>可以访问另一个类的<strong>私有和保护（稍后更多）成员(区别于组合)</strong></p><p>友元函数不是类的成员函数</p><p>友元函数<strong>在类范围之外定义</strong></p><h4 id="友元的特性"><a href="#友元的特性" class="headerlink" title="友元的特性"></a>友元的特性</h4><p>友元是**“给予”的，而不是“索要”的**</p><p><strong>非对称</strong>性（如果 B 是 A 的友元，A 不一定是 B 的友元）</p><p><strong>非传递</strong>性（如果 A 是 B 的友元，B 是 C 的友元，A 不一定是 C 的友元） </p><h4 id="友元的主要用途"><a href="#友元的主要用途" class="headerlink" title="友元的主要用途"></a>友元的主要用途</h4><p>提供了一种访问类成员的更方便快捷的途径</p><p>为运算符重载的实现提供了更方便的途径</p><p>友元<strong>可以访问类的任何成员(可不通过成员函数)</strong>,这破环了类的封装性，因此要谨慎使用友元</p><p><strong>有权从类外部更改类的内部状态</strong>。 因此推荐使用成员函数而不是友元来改变状态 </p><h4 id="friend-声明"><a href="#friend-声明" class="headerlink" title="friend 声明"></a>friend 声明</h4><p><strong>friend 函数</strong> </p><p> Keyword friend</p><p>friend int myFunction( int x );</p><p> <strong>声明在类内</strong>，<strong>保证这个函数可以在类外访问类成员</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> m_value;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Accumulator</span>() &#123; m_value = <span class="number">0</span>; &#125; </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">int</span> value)</span> </span>&#123; m_value += value; &#125;</span><br><span class="line">    <span class="comment">// 声明reset() 函数是本类的友元函数</span></span><br><span class="line">    <span class="function"><span class="keyword">friend</span> <span class="type">void</span> <span class="title">reset</span><span class="params">(Accumulator &amp;accumulator)</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// reset() 现在是 Accumulator 的友元</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">reset</span><span class="params">(Accumulator &amp;accumulator)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 可以直接访问Accumulator 对象的任何数据</span></span><br><span class="line">    accumulator.m_value = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="friend-类"><a href="#friend-类" class="headerlink" title="friend 类"></a>friend 类</h4><p>在类名前加 friend <strong>保证该类可以访问类成员</strong>,可以让整个类成为另一个类的友元。 这使<strong>友元类的所有成员函数都可以访问其他类的私有成员</strong>。友元类的<strong>所有函数都是友元函数</strong> </p><p>也可以不把整个类声明为友元, 仅仅只声明一个或多个函数为另一个类的友元函数. 这类似于声明普通函数成为友元，<strong>除了使用包含 className:: 前缀</strong></p><p>友元常用于<strong>定义重载运算符时</strong>。当两个或多个类需要以一种亲密的方式一起工作时，<strong>不常使用友元</strong>。使一个<strong>类</strong>成为友元<strong>只需要作为前向声明</strong>该类存在。 但是，使特定的类的<strong>成员函数</strong>成为友元则需要<strong>首先看到成员函数类的完整声明</strong>.</p><h2 id="继承-不允许继承循环"><a href="#继承-不允许继承循环" class="headerlink" title="继承(不允许继承循环)"></a><strong>继承</strong>(<strong>不允许继承循环</strong>)</h2><h4 id="继承的概念"><a href="#继承的概念" class="headerlink" title="继承的概念"></a><strong>继承的概念</strong></h4><p><strong>派生类具有基类的特性</strong></p><ul><li><p><strong>共享</strong>基类的<strong>成员函数</strong></p></li><li><p>使用基类的数据成员</p></li></ul><p><strong>派生类新增成员(拓展)</strong></p><ul><li><p>定义自己的数据成员</p></li><li><p>定义独特的成员函数</p></li></ul><p><strong>派生类改造基类</strong></p><ul><li><strong>重写基类某些成员函数</strong></li></ul><p>C++中单继承派生类的定义形式如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> 派生类名 : [继承方式] 基类名</span><br><span class="line">&#123;</span><br><span class="line">派生类成员声明;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>继承方式包括：</p><p>public（公有继承）<br>private（私有继承，<strong>默认</strong>）<br>protected（保护继承）(<strong>保护成员在本类与派生类中能直接访问</strong>)</p><p>C++中多重继承派生类的定义形式如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> 派生类名: [继承方式] 基类名<span class="number">1</span>, [继承方式] 基类名<span class="number">2</span>,…, </span><br><span class="line">[继承方式] 基类名n</span><br><span class="line">&#123;</span><br><span class="line">派生类成员声明;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Assistant</span> : <span class="keyword">protected</span> Student, Teacher<span class="comment">//对Teacher默认是私有继承</span></span><br><span class="line">&#123; …… &#125;;</span><br></pre></td></tr></table></figure><p><img src="/./%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/fa7f516ffbe998538989d54213bb5837.png" alt="fa7f516ffbe998538989d54213bb5837"></p><p><strong>继承方式决定了基类成员在派生类中的访问权，</strong></p><p><strong>这种访问来自两个方面</strong>：</p><ul><li><strong>派生类中</strong></li></ul><p>新增函数成员访问从基类继承来的成员</p><ul><li><strong>派生类外部</strong></li></ul><p>通过派生类的对象访问从基类继承的成员</p><p><img src="/./%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/94de02253d18c73529beb0753b328c5d.png" alt="94de02253d18c73529beb0753b328c5d"></p><p><img src="/./%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/98f37468b772cd29ba63bb8ba5dfef81.png" alt="98f37468b772cd29ba63bb8ba5dfef81"></p><h4 id="访问私有继承的成员"><a href="#访问私有继承的成员" class="headerlink" title="访问私有继承的成员"></a>访问私有继承的成员</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span> &#123; <span class="type">int</span> number; <span class="type">char</span> school[<span class="number">10</span>]; </span><br><span class="line"><span class="keyword">protected</span>: <span class="type">char</span> name[<span class="number">10</span>]; <span class="type">char</span> sex; </span><br><span class="line"><span class="keyword">public</span>: <span class="function"><span class="type">void</span> <span class="title">input_data</span><span class="params">( )</span></span>; <span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">()</span></span>; &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CollegeStudent</span> : <span class="keyword">private</span> Student</span><br><span class="line">&#123; <span class="type">char</span> major[<span class="number">10</span>]; <span class="comment">//新增成员:专业</span></span><br><span class="line"><span class="keyword">public</span>: </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">input_major</span><span class="params">( )</span></span>&#123;cin&gt;&gt;major ; &#125; </span><br><span class="line"><span class="comment">// 输入专业</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">( )</span></span>&#123; Student::<span class="built_in">print</span>(); </span><br><span class="line"><span class="comment">// 输出信息</span></span><br><span class="line">cout&lt;&lt;“name:”&lt;&lt;name &lt;&lt;“ sex:”&lt;&lt;sex &lt;&lt;endl; </span><br><span class="line">cout&lt;&lt;<span class="string">&quot;major:&quot;</span>&lt;&lt;major&lt;&lt;endl; &#125; </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span> &#123; …… &#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CollegeStudent</span> : <span class="keyword">private</span> Student&#123;...&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">Student s; s.<span class="built_in">input_data</span>(); s.<span class="built_in">print</span>();</span><br><span class="line">CollegeStudent cs;</span><br><span class="line">cs.<span class="built_in">input_data</span>(); <span class="comment">// 错误</span></span><br><span class="line">cs.<span class="built_in">input_major</span>();</span><br><span class="line">cs.<span class="built_in">print</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>; &#125;</span><br></pre></td></tr></table></figure><p>CollegeStudentl类访问基类,Student的能力没有变化，</p><p>在私有继承的情况下，通过派生类**对象(并非类内)**无法访问基类的任何成员</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span></span><br><span class="line">&#123; <span class="keyword">public</span> :</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">get_XY</span><span class="params">( )</span> </span>; </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">put_XY</span><span class="params">( )</span> </span>; </span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"><span class="type">int</span> x, y;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">A::get_XY</span><span class="params">( )</span></span></span><br><span class="line"><span class="function"></span>&#123; cin &gt;&gt; x &gt;&gt; y ; &#125;</span><br><span class="line"><span class="type">void</span> A:: <span class="built_in">put_XY</span>( )</span><br><span class="line">&#123; cout &lt;&lt; <span class="string">&quot;x = &quot;</span>&lt;&lt; x &lt;&lt; <span class="string">&quot;, y = &quot;</span> &lt;&lt; y &lt;&lt; <span class="string">&#x27;\n&#x27;</span> ; &#125;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span> : <span class="keyword">public</span> A</span><br><span class="line">&#123; <span class="keyword">public</span> :</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">get_S</span><span class="params">()</span> </span>&#123; s = x * y ; <span class="keyword">return</span> s ; &#125; </span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"><span class="type">int</span> s;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">C</span> : <span class="keyword">public</span> B</span><br><span class="line">&#123; <span class="keyword">public</span> : </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">get_H</span><span class="params">()</span> </span>&#123; cin &gt;&gt; h ; &#125; </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">get_V</span><span class="params">()</span> </span>&#123;v = <span class="built_in">get_S</span>() * h ; <span class="keyword">return</span> v ; &#125;</span><br><span class="line"><span class="keyword">protected</span>: </span><br><span class="line"><span class="type">int</span> h, v;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123; A objA ;</span><br><span class="line">B objB ;</span><br><span class="line">C objC ;</span><br><span class="line">objA.<span class="built_in">get_XY</span>() ;</span><br><span class="line">objA.<span class="built_in">put_XY</span>() ;</span><br><span class="line">objB.<span class="built_in">get_XY</span>() ;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;S = &quot;</span> &lt;&lt; objB.<span class="built_in">get_S</span>() &lt;&lt; endl ;</span><br><span class="line">objC.<span class="built_in">get_XY</span>() ;</span><br><span class="line">objC.<span class="built_in">get_H</span>();</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;V = &quot;</span> &lt;&lt; objC.<span class="built_in">get_V</span>() &lt;&lt; endl ;&#125;</span><br></pre></td></tr></table></figure><p><img src="/./%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/a87a6f027be2e1b66c1c2497d61e59f3.png" alt="a87a6f027be2e1b66c1c2497d61e59f3"></p><p><img src="/./%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/4d2fe503e36e73613dfe44ad25b9ed9e.png" alt="4d2fe503e36e73613dfe44ad25b9ed9e"></p><p><strong>三种继承方式的对比</strong></p><p>一般采用不会改变基类成员访问权限的<strong>公有继承</strong>。</p><p><strong>私有继承：</strong></p><p>基类的可被继承的成员都成了其直接派生类的私有成员，</p><p>无法再进一步派生，</p><p>实际上私有继承相当于终止了基类成员的继续派生。</p><p><strong>保护继承</strong>：</p><p>基类的可被继承的成员都成了直接派生类的保护成员，</p><p>保护继承保证了最上层基类的成员依然能被继承树中的</p><p>次级子类所继承。</p><p><strong>访问级别不能升只能降,一层一层来看即可</strong></p><p><img src="/./%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/42f4358cb52c4ca97743131bb9022ee9.png" alt="42f4358cb52c4ca97743131bb9022ee9"></p><h2 id="派生类的构造函数"><a href="#派生类的构造函数" class="headerlink" title="派生类的构造函数"></a><strong>派生类的构造函数</strong></h2><p><strong>派生类的构造与析构函数</strong></p><p>• <strong>创建派生类对象时调用基类的构造函数来初始化基类数据。</strong></p><p>• <strong>执行派生类的析构函数时，基类的析构函数也将被调用。</strong></p><p><img src="/./%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/dcef7a843dbe532605e80a1b95c20b54.png" alt="dcef7a843dbe532605e80a1b95c20b54"></p><p><strong>派生类构造函数的定义方式：</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">派生类名（参数总表）：基类名（基类构造函数参数表<span class="number">1</span>）, 对象成员(参数表<span class="number">2</span>)</span><br><span class="line">&#123;</span><br><span class="line">派生类成员初始化；</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Student</span>(<span class="type">int</span> no, <span class="type">char</span> name[],<span class="type">char</span> sex):<span class="built_in">Person</span>(name,sex)</span><br><span class="line">&#123; id = no; &#125;</span><br><span class="line"><span class="built_in">Student</span>(<span class="type">int</span> no, <span class="type">char</span> name[],<span class="type">char</span> sex):</span><br><span class="line"><span class="built_in">Person</span>(name,sex), <span class="built_in">id</span>( no )&#123; &#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base1</span> &#123; <span class="keyword">public</span>: <span class="built_in">Base1</span>() &#123; cout &lt;&lt; <span class="string">&quot;Base1&quot;</span> &lt;&lt; endl; &#125; &#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Base2</span> &#123; <span class="keyword">public</span>: <span class="built_in">Base2</span>() &#123; cout &lt;&lt; <span class="string">&quot;Base2&quot;</span> &lt;&lt; endl; &#125; &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base2, <span class="keyword">public</span> Base1 &#123;  <span class="comment">// 基类声明顺序：Base2 → Base1</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Derived</span>() : <span class="built_in">Base1</span>(), <span class="built_in">Base2</span>() &#123;  <span class="comment">// 初始化列表顺序：Base1 → Base2（与声明顺序相反）</span></span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Derived&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="1-初始化顺序的决定因素"><a href="#1-初始化顺序的决定因素" class="headerlink" title="1. 初始化顺序的决定因素"></a>1. <strong>初始化顺序的决定因素</strong></h3><ul><li><strong>基类</strong>：按照<strong>派生类定义时基类的声明顺序</strong>初始化（无论初始化列表中如何排列）。</li><li><strong>成员变量</strong>：按照成员在类中<strong>声明的顺序</strong>初始化（<strong>与初始化列表顺序无关</strong>）。</li><li><strong>派生类自身</strong>：<strong>最后执行派生类构造函数</strong>的<strong>函数体</strong>。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base1</span> &#123; <span class="keyword">public</span>: <span class="built_in">Base1</span>() &#123; cout &lt;&lt; <span class="string">&quot;Base1&quot;</span> &lt;&lt; endl; &#125; &#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Base2</span> &#123; <span class="keyword">public</span>: <span class="built_in">Base2</span>() &#123; cout &lt;&lt; <span class="string">&quot;Base2&quot;</span> &lt;&lt; endl; &#125; &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base2, <span class="keyword">public</span> Base1 &#123;  <span class="comment">// 基类声明顺序：Base2 → Base1</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Derived</span>() : <span class="built_in">Base1</span>(), <span class="built_in">Base2</span>() &#123;  <span class="comment">// 初始化列表顺序：Base1 → Base2（与声明顺序相反）</span></span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Derived&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Base2</span><br><span class="line">Base1</span><br><span class="line">Derived</span><br></pre></td></tr></table></figure><h4 id="为什么初始化顺序固定？"><a href="#为什么初始化顺序固定？" class="headerlink" title="为什么初始化顺序固定？"></a><strong>为什么初始化顺序固定？</strong></h4><ul><li><strong>成员变量依赖</strong>：若成员变量的初始化依赖于其他成员的顺序，固定顺序可避免潜在错误。</li><li><strong>基类依赖</strong>：若基类的初始化顺序被用户随意调整，可能导致基类未完全初始化就被使用。</li></ul><h5 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a><strong>最佳实践</strong></h5><ul><li><strong>保持初始化列表顺序与声明顺序一致</strong>：提高代码可读性，避免混淆。</li><li><strong>避免成员间的初始化依赖</strong>：若必须依赖，通过构造函数体或成员函数处理。</li></ul><p>**构造函数执行顺序：**基类-&gt;派生类中对象成员-&gt;派生类</p><h4 id="派生类构造函数的几点说明"><a href="#派生类构造函数的几点说明" class="headerlink" title="派生类构造函数的几点说明"></a>派生类构造函数的几点说明</h4><p>1）派生类构造函数的定义中可<strong>省略对基类构造函数的调用</strong>其条件是<strong>在基类中必须有默认的构造函数或者根本没有定义构造函数。</strong><br>2）当基类的构造函数使用<strong>一个或多个参数</strong>时，则<strong>派生类必须定义构造函数</strong>，提供<strong>将参数传递</strong>给基类构造函数途径。</p><h4 id="继承中的同名成员访问"><a href="#继承中的同名成员访问" class="headerlink" title="继承中的同名成员访问"></a><strong>继承中的同名成员访问</strong></h4><p>多重继承时不同基类成员同名也可以用**类名限定符“::”**来解决。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"><span class="type">int</span> id;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Teacher</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">protected</span> :</span><br><span class="line"><span class="type">int</span> id;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Assistant</span>: <span class="keyword">public</span> Student, <span class="keyword">public</span> Teacher</span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">()</span></span>&#123;</span><br><span class="line">cout&lt;&lt;id&lt;&lt;endl; <span class="comment">//error!访问是二义的</span></span><br><span class="line">cout&lt;&lt;Student::id&lt;&lt;endl; <span class="comment">//访问Student的id</span></span><br><span class="line">cout&lt;&lt;Teacher::id&lt;&lt;endl; <span class="comment">//访问Teacher的id</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>继承时的同名成员隐藏规则</strong><br>派生类定义了与基类相同的成员，此时<strong>基类的同名成员在派生类内不可见</strong>，派生类成员隐藏了同名的基类成员.</p><p>基类成员与派生类成员同名，可以通过类名限定符“::”来解决。其语法为：<strong>类名</strong> <strong>: :</strong> <strong>成员</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span> : <span class="keyword">public</span> Person&#123; <span class="comment">// 派生类</span></span><br><span class="line"><span class="keyword">private</span>: <span class="type">int</span> id; <span class="comment">// 学号</span></span><br><span class="line">……</span><br><span class="line"><span class="keyword">public</span>: <span class="function"><span class="type">void</span> <span class="title">test</span><span class="params">()</span></span>&#123; <span class="comment">//测试函数</span></span><br><span class="line">id=<span class="number">123</span>; <span class="comment">// 访问派生类成员</span></span><br><span class="line">Person::id =<span class="number">456</span>; <span class="comment">// 访问基类成员</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>继承中成员同名有两种情况：<br>1.<strong>基类</strong>成员与<strong>派生类</strong>成员同名<br>2.多重继承时<strong>不同基类</strong>成员同名</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Person</span></span><br><span class="line">&#123; <span class="keyword">protected</span>: <span class="type">int</span> id; &#125;; <span class="comment">// 身份号码</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span> : <span class="keyword">public</span> Person</span><br><span class="line">&#123; <span class="keyword">protected</span>: <span class="type">int</span> id; &#125;; <span class="comment">// 学号</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Teacher</span> : <span class="keyword">public</span> Person</span><br><span class="line">&#123; <span class="keyword">protected</span>: <span class="type">int</span> id;&#125;; <span class="comment">// 职工号</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Assistant</span>: <span class="keyword">public</span> Student, <span class="keyword">public</span> Teacher&#123;</span><br><span class="line"><span class="keyword">public</span>: <span class="function"><span class="type">void</span> <span class="title">test</span><span class="params">()</span></span>&#123; <span class="comment">// 测试</span></span><br><span class="line">Student::id = <span class="number">1001</span>; <span class="comment">// 正确：访问Student类的id</span></span><br><span class="line">Teacher::id = <span class="number">101</span>; <span class="comment">// 正确：访问Teacher类的id</span></span><br><span class="line">Person::id =<span class="number">230</span>×××<span class="number">0001</span>; <span class="comment">// 错误&#125;</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><img src="/./%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/1e35cab60282a49190c05943c754245c.png" alt="1e35cab60282a49190c05943c754245c"></p><h3 id="类族中的赋值兼容"><a href="#类族中的赋值兼容" class="headerlink" title="类族中的赋值兼容"></a>类族中的<strong>赋值兼容</strong></h3><p><strong>公有继承</strong>时，一个<strong>派生类的对象</strong>可用于<strong>基类对象</strong>适用的地方，需要基类对象的任何地方都可以使用派生类对象<strong>替代</strong>。</p><p>赋值兼容规则有三种情况：<br>（1）派生类的对象可以<strong>赋值给基类的对象</strong>。<br>base <em>Obj &#x3D; derived</em> Obj;<br>（2）派生类的对象可以<strong>初始化基类的引用</strong>。<br>base&amp; base_Obj &#x3D; derived_obj;<br>（3）派生类的对象的<strong>地址可以赋给指向基类的指针</strong>。<br>base *pBase &#x3D; &amp;derived_obj;</p><p><img src="/./%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/8b80309614a6abf7fa39b9b15d0abf52.png" alt="8b80309614a6abf7fa39b9b15d0abf52"></p><h2 id="多态-1"><a href="#多态-1" class="headerlink" title="多态"></a><strong>多态</strong></h2><p>指<strong>同样的消息</strong>被<strong>不同类型的对象接收</strong>时,产生不同行为的<strong>现象</strong>。(<strong>同一名字，多种语义</strong>；<strong>同个接口，多种方法</strong>)</p><h4 id="静态多态的概念"><a href="#静态多态的概念" class="headerlink" title="静态多态的概念"></a><strong>静态多态的概念</strong></h4><p>在程序<strong>编译时</strong>系统就能够<strong>确定要调用的是哪个函数</strong>，也被称为<strong>编译时多态</strong>。</p><ul><li><p>函数重载</p></li><li><p>运算符重载</p></li></ul><h4 id="函数重载注意事项"><a href="#函数重载注意事项" class="headerlink" title="函数重载注意事项"></a>函数重载注意事项</h4><p><strong>不能仅靠函数的返回值</strong>来区别重载函数，必须从<strong>形式参数上</strong>区别开来。</p><p><strong>派生类中的同名成员函数</strong></p><p>• <strong>使用</strong> <strong>::</strong> <strong>加以区分</strong></p><p>• <strong>使用对象加以区分</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MonkyKong sun; </span><br><span class="line">sun. <span class="built_in">fly</span> ( )；</span><br><span class="line">sun . Follower :: <span class="built_in">fly</span> ( );</span><br><span class="line">Pig pigsy;</span><br><span class="line">pigsy.<span class="built_in">fly</span>();</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">B</span> :<span class="keyword">public</span> A</span><br><span class="line"><span class="type">char</span> name[<span class="number">10</span>];</span><br><span class="line">A::<span class="built_in">Show</span> ( )</span><br><span class="line"><span class="built_in">Show</span> ( )</span><br><span class="line">Aobj . <span class="built_in">Show</span> ( )；</span><br><span class="line">Bobj . <span class="built_in">Show</span> ( );</span><br><span class="line">Bobj . A :: <span class="built_in">Show</span> ( );</span><br></pre></td></tr></table></figure><h4 id="动态多态性"><a href="#动态多态性" class="headerlink" title="动态多态性"></a><strong>动态多态性</strong></h4><p>指程序在编译时并不能确定要调用的函数，<strong>直到运行时</strong>系统才能动态地确定操作所针对的具体对象，它又被称为<strong>运行时多态</strong></p><p>动态多态是通过<strong>虚函数</strong>（virtual function）实现。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span></span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">show</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123; cout &lt;&lt; <span class="string">&quot;Base&quot;</span>&lt;&lt;endl ; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derv1</span>: <span class="keyword">public</span> Base</span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">show</span><span class="params">()</span></span>&#123; cout &lt;&lt; <span class="string">&quot;Derv1&quot;</span>&lt;&lt;endl ; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derv2</span>: <span class="keyword">public</span> Base</span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">show</span><span class="params">()</span></span>&#123; cout &lt;&lt; <span class="string">&quot;Derv2&quot;</span>&lt;&lt;endl ; &#125;</span><br><span class="line">&#125;;</span><br><span class="line">Derv1 dv1; </span><br><span class="line">Derv2 dv2; </span><br><span class="line">dv<span class="number">1.</span><span class="built_in">show</span>() ;</span><br><span class="line">dv<span class="number">2.</span><span class="built_in">show</span>() ;</span><br><span class="line">Base* pBase; </span><br><span class="line">pBase = &amp;dv1;</span><br><span class="line">pBase-&gt;<span class="built_in">show</span>();</span><br><span class="line">pBase = <span class="keyword">new</span> <span class="built_in">Derv2</span>(); </span><br><span class="line">pBase-&gt;<span class="built_in">show</span>();</span><br><span class="line"><span class="comment">//通过基类指针只能访问从基类继承的成员</span></span><br></pre></td></tr></table></figure><h4 id="虚函数"><a href="#虚函数" class="headerlink" title="虚函数"></a>虚函数</h4><p>C++中的虚函数的作用是<strong>允许在派生类中重新定义与基类同名的函数</strong>，并且可以<strong>通过基类指针或者基类引用来访问这个同名函数(最重要区别)</strong>。虚函数成员声明的语法为：</p><p>1．virtual只能使用<strong>在类定义</strong>的函数<strong>原型声明</strong>中，</p><p>不能在成员函数实现的时候使用，也<strong>不能用来限定类外</strong>的普通函数。</p><p>2．用virtual声明类的<strong>非静态</strong>的成员函数，<strong>只用于类的继承层次结构</strong>中。</p><p>不能将类外的普通函数(友员)和静态成员函数声明成虚函数。</p><p>virtual具有<strong>继承性</strong></p><p>在派生类中<strong>重新定义虚函数</strong>，要求函数名、函数类型、函数参数个数和类型全部与基类的虚函数<strong>完全相同</strong>。<br>否则不能实现多态性, 为<strong>函数重载</strong>.</p><p>虚函数是在基类中冠以关键字 virtual 的非静态成员函数。<br>继承体系：判断成员函数所在的类是否会作为基类；虚函数为类族提供了一种公共接口。<br>重写函数：该函数在类被继承后有无可能被更改功能；允许在派生类中对基类的虚函数重新定义<br>调用形式：是否通过基类指针或引用调用该虚函数；赋值兼容性原则</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Follower</span> <span class="comment">// 徒弟类</span></span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">fly</span><span class="params">( )</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="type">bool</span> <span class="title">fight</span><span class="params">( Ghost&amp;)</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WuKong</span></span><br><span class="line">: <span class="keyword">public</span> Follower</span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">fly</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">fight</span><span class="params">( Ghost* )</span></span>; </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="虚析构函数"><a href="#虚析构函数" class="headerlink" title="虚析构函数"></a>虚析构函数</h4><p><strong>构造函数不能是虚函数</strong></p><p>建立一个派生类对象时，必须从类层次的根开始，沿着继承路径逐个调用基类的构造函数</p><p><strong>析构函数可以是虚函数</strong></p><p>虚析构函数用于<strong>指引 delete 运算符正确析构动态对象</strong></p><p>当<strong>基类的析构函数为虚函数</strong>时，无论指针指的是<strong>同一类族的哪一个类对象</strong>，对象撤销时，系统会采用动态关联，调用<strong>相应的析构函数</strong>，完成该对象的清理工作。</p><p>习惯把析构函数声明为虚函数，即使基类并不需要析构函数，以<strong>确保撤销动态存储空间时能够得到正确的处理</strong>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span></span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line"><span class="built_in">Base</span>( )&#123; cout &lt;&lt; <span class="string">&quot;Base&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">~<span class="built_in">Base</span>( )&#123; cout &lt;&lt; <span class="string">&quot;Desconstruct Base&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derv1</span>: <span class="keyword">public</span> Base</span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line"><span class="built_in">Derv1</span>( )&#123; cout &lt;&lt; <span class="string">&quot;Derv1&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">~<span class="built_in">Derv1</span>( )&#123; cout &lt;&lt; <span class="string">&quot;Desconstruct Derv1&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derv2</span>: <span class="keyword">public</span> Derv1</span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line"><span class="built_in">Derv2</span>( )&#123; cout &lt;&lt; <span class="string">&quot;Derv2&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">~<span class="built_in">Derv2</span>( )&#123; cout &lt;&lt; <span class="string">&quot;Desconstruct Derv2&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line">Base* pBase = <span class="keyword">new</span> <span class="built_in">Base</span>();</span><br><span class="line"><span class="keyword">delete</span> pBase;</span><br><span class="line">Derv1* pDerv1 = <span class="keyword">new</span> <span class="built_in">Derv1</span>();</span><br><span class="line"><span class="keyword">delete</span> pDerv1;</span><br><span class="line">pBase = <span class="keyword">new</span> <span class="built_in">Derv2</span>();</span><br><span class="line"><span class="keyword">delete</span> pBase;<span class="comment">//析构由基类指针建立的派生类对象,没有调用派生类析构函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span></span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line"><span class="built_in">Base</span>( )&#123; cout &lt;&lt; <span class="string">&quot;Base&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line"><span class="keyword">virtual</span> ~<span class="built_in">Base</span>( )&#123; cout &lt;&lt; <span class="string">&quot;Desconstruct Base&quot;</span> &lt;&lt; endl; &#125;<span class="comment">//后面默认都虚,基类指针也可做到连删</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derv1</span>: <span class="keyword">public</span> Base</span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line"><span class="built_in">Derv1</span>( )&#123; cout &lt;&lt; <span class="string">&quot;Derv1&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">~<span class="built_in">Derv1</span>( )&#123; cout &lt;&lt; <span class="string">&quot;Desconstruct Derv1&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derv2</span>: <span class="keyword">public</span> Derv1</span><br><span class="line">&#123; <span class="keyword">public</span>:</span><br><span class="line"><span class="built_in">Derv2</span>( )&#123; cout &lt;&lt; <span class="string">&quot;Derv2&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">~<span class="built_in">Derv2</span>( )&#123; cout &lt;&lt; <span class="string">&quot;Desconstruct Derv2&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>实现动态多态</strong></p><p>• 基类<strong>声明</strong>虚函数</p><p>• 派生类重写虚函数</p><p>• 基类指针或引用调用</p><p>在许多情况下，在基类中<strong>不能给出有意义的虚函数定义</strong>，这时可把它说明成纯虚函数，把它的<strong>定义留给派生类来做</strong>。<br>定义纯虚函数的一般形式为：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> 类名&#123;</span><br><span class="line"><span class="keyword">virtual</span> 返回值类型 函数名(参数表) = <span class="number">0</span>；</span><br><span class="line">&#125;；</span><br></pre></td></tr></table></figure><p>①纯虚函数<strong>没有函数体</strong>；<br>②最后面的“&#x3D;0” 不表示函数返回值为0<br>③这是一个<strong>声明语句</strong>。</p><p>纯虚函数的作用<br>在基类中<strong>为其派生类保留一个函数的名字</strong>，<strong>以便派生类根据需要对它进行定义</strong>, 否则无法实现多态性。</p><h2 id="抽象类的概念"><a href="#抽象类的概念" class="headerlink" title="抽象类的概念"></a><strong>抽象类的概念</strong></h2><p>如果一个类中<strong>至少有一个纯虚函数</strong>，那么这个类被成为<strong>抽象类</strong>（abstract class**）**。</p><p>抽象类必须用作派生其他类的基类,不能作为返回或参数类型，可使用指向抽象类的指针<strong>支持运行时多态性</strong>。而<strong>不能用于直接创建对象实例</strong>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p-&gt;<span class="built_in">getArea</span>(); (*p).<span class="built_in">draw</span>( );<span class="comment">//作指针做对象时的不同写法以区分</span></span><br></pre></td></tr></table></figure><p><strong>派</strong>生类中应<strong>重写</strong>基类中的纯虚函数，否则派生类<strong>仍将被看作</strong>一个<strong>抽象类</strong>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">void test1(TwoDimensionalShape &amp; t)&#123;</span><br><span class="line">t.show( ); t.draw( ); cout&lt;&lt;t.getArea()&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line">void test2(TwoDimensionalShape * p)&#123;</span><br><span class="line">p-&gt;show( ); p-&gt;draw( ); </span><br><span class="line">cout&lt;&lt; (*p).getArea()&lt;&lt;endl; //???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果二维图形又派生出了椭圆形，用于测试的test函数需要需改吗？这有何意义？</p><ol><li><strong>test 函数是否需要修改？</strong><br> <strong>不需要修改</strong>。<br> 若<code>椭圆形（Ellipse）</code>是<code>TwoDimensionalShape</code>的派生类，且正确重写了基类中的<code>show()</code>、<code>draw()</code>、<code>getArea()</code>虚函数（假设这三个函数在<code>TwoDimensionalShape</code>中是<strong>虚函数</strong>），则<code>test1</code>和<code>test2</code>函数可以<strong>直接接收<code>Ellipse</code>对象（或指针 &#x2F; 引用）<strong>并</strong>正确调用派生类</strong>的实现。</li><li><strong>意义：</strong><br> 这体现了<strong>面向对象的多态性</strong>，具体意义如下：<ul><li><strong>代码复用性</strong>：新增派生类（如椭圆形）时，无需修改已有的<code>test1</code>、<code>test2</code>等通用函数，只需<strong>保证派生类遵循基类的接口规范（重写虚函数）</strong>，即可直接使用这些函数进行测试。</li><li><strong>扩展性</strong>：系统可以轻松扩展新的二维图形类型（如椭圆形、三角形等），而不影响原有代码的逻辑，符合 “开闭原则”（对扩展开放，对修改关闭）。</li><li><strong>接口统一性</strong>：通过基类的引用或指针调用派生类的方法，屏蔽了不同派生类的实现差异，使代码更简洁、通用，降低了模块间的耦合度。</li></ul></li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TwoDimensionalShape</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">show</span><span class="params">()</span> </span>= <span class="number">0</span>;       <span class="comment">// 纯虚函数：显示图形信息</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">draw</span><span class="params">()</span> </span>= <span class="number">0</span>;       <span class="comment">// 纯虚函数：绘制图形</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">double</span> <span class="title">getArea</span><span class="params">()</span> </span>= <span class="number">0</span>;  <span class="comment">// 纯虚函数：计算面积</span></span><br><span class="line">    <span class="keyword">virtual</span> ~<span class="built_in">TwoDimensionalShape</span>() &#123;&#125; <span class="comment">// 虚析构函数</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 已有的派生类：圆形</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Circle</span> : <span class="keyword">public</span> TwoDimensionalShape &#123;</span><br><span class="line">    <span class="comment">// 实现show()、draw()、getArea()...</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 新增派生类：椭圆形</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Ellipse</span> : <span class="keyword">public</span> TwoDimensionalShape &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">show</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123; <span class="comment">/* 实现 */</span> &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">draw</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123; <span class="comment">/* 实现 */</span> &#125;</span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">getArea</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123; <span class="comment">/* 计算椭圆面积 */</span> &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Ellipse e;</span><br><span class="line">    <span class="built_in">test1</span>(e);       <span class="comment">// 传入Ellipse对象的引用，正确调用Ellipse的成员函数</span></span><br><span class="line">    <span class="built_in">test2</span>(&amp;e);      <span class="comment">// 传入Ellipse对象的指针，正确调用Ellipse的成员函数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>override</code>是一个<strong>关键字</strong>，用于显式声明派生类中的成员函数<strong>重写（覆盖）</strong> 了基类中的虚函数（<code>virtual</code>函数）。它的主要作用是<strong>增强代码的可读性和安全性</strong>。</p><p><strong>(3)</strong> <strong>在类的层次结构中，顶层或最上面的几层可以是抽象基类。</strong></p><p>抽象基类体现了<strong>本类族中各类的共性</strong>，把各类中共有的成员函数集中在抽象基类中<strong>声明</strong>。</p><p>为什么引⼊多态<br>利⽤多态性可以设计和实现⼀个易于扩展的系统。增强代码的通⽤性</p><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a><strong>区别</strong></h3><table><thead><tr><th><strong>对比维度</strong></th><th><strong>重载（Overload）</strong></th><th><strong>多态（Polymorphism）</strong></th></tr></thead><tbody><tr><td><strong>定义</strong></td><td><strong>同一作用域</strong>内，多个函数名相同但<strong>参数列表（参数类型、个数、顺序）不同</strong>的函数。</td><td><strong>基类与派生类中</strong>，派生类<strong>重写（<code>override</code>）基类的虚函数</strong>，通过<strong>基类指针 &#x2F; 引用调用</strong>时，**根据对象实际类型执行(多种对象对应执行)**对应函数。</td></tr><tr><td><strong>实现阶段</strong></td><td><strong>编译时确定（静态多态）</strong>。编译器根据函数参数列表匹配对应的函数。</td><td><strong>运行时确定（动态多态）</strong>。程序运行时根据对象实际类型调用对应的函数。</td></tr><tr><td><strong>作用范围</strong></td><td><strong>同一类中（或全局函数）</strong>，函数名相同但参数<strong>不同(不完全等同于重写,属于新建,仅名字相同)</strong>。</td><td><strong>继承关系中，基类与派生类之间，函数名、参数列表、返回值完全相同（相同信息重写虚函数带来的不同处理响应）。</strong></td></tr><tr><td><strong>核心依赖</strong></td><td>函数<strong>参数列表的差异</strong>（与返回值无关）。</td><td><strong>基类虚函数、派生类重写、基类指针 &#x2F; 引用能指向相应派生类对象并执行相应的函数。</strong></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><h3 id="二、联系"><a href="#二、联系" class="headerlink" title="二、联系"></a><strong>二、联系</strong></h3><ol><li><strong>都是代码复用的手段</strong><ul><li>重载允许<strong>同一功能（函数名）适配不同参数</strong>，避免为相似功能起不同名字（如<code>printInt</code>、<code>printDouble</code>）。</li><li>多态允许通过<strong>统一接口（基类函数）操作不同派生类对象</strong>，简化代码逻辑（如<strong>用<code>Shape*</code>统一管理圆形、方形</strong>等）。</li></ul></li><li><strong>都体现 “一个接口，多种实现” 的思想</strong><ul><li>重载：同一函数名对应<strong>多种参数组合</strong>的实现。</li><li>多态：同一虚函数接口（基类）对应派生类的<strong>多种重写实现</strong>。</li></ul></li><li><strong>都依赖编译器的处理</strong><ul><li>重载依赖编译器在编译时<strong>根据参数匹配函数（静态绑定）</strong>。</li><li>多态依赖编译器对<strong>虚函数表的处理</strong>，实现<strong>运行时动态绑定</strong>。</li></ul></li></ol><h3 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a><strong>三、总结</strong></h3><ul><li><strong>重载</strong>是 “横向” 的函数扩展（同一类内，同名不同参），解决同一功能的不同参数适配问题，属于<strong>静态多态</strong>。</li><li><strong>多态</strong>是 “纵向” 的函数扩展（继承体系中，重写虚函数），解决不同派生类对象的统一接口调用问题，属于<strong>动态多态</strong>。</li></ul><h2 id="模板"><a href="#模板" class="headerlink" title="模板"></a><strong>模板</strong></h2><p><strong>模板</strong>可以实现<strong>类型参数化(包括新类型)</strong>,C++模板包括 函数模板和类模板两种类型**。**</p><p><strong>函数模板</strong>就解决函数<strong>重载中</strong>多次定义函数的问题。</p><p><strong>类模板</strong>就是对一批<strong>仅仅成员数据类型不同</strong>的类的<strong>抽象</strong>。</p><p><strong>泛型编程（generic programming）</strong><br>模板用于<strong>表达逻辑结构相同</strong>，但<strong>具体数据元素类型不同</strong>的数据对象的通用行为。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">square</span> <span class="params">( <span class="type">int</span> x )</span></span></span><br><span class="line"><span class="function"></span>&#123; <span class="keyword">return</span> x*x ; &#125;</span><br><span class="line"><span class="function">Complex <span class="title">square</span> <span class="params">(Complex x )</span></span></span><br><span class="line"><span class="function"></span>&#123; <span class="keyword">return</span> x*x ; &#125;</span><br><span class="line"><span class="keyword">template</span> &lt; <span class="keyword">typename</span> T &gt;</span><br><span class="line"><span class="function">T <span class="title">square</span> <span class="params">(T x )</span></span></span><br><span class="line"><span class="function"></span>&#123; <span class="keyword">return</span> x*x; &#125;</span><br></pre></td></tr></table></figure><p>通过模板可以产生类或函数的<strong>集合</strong>，使它们操作不同的数据类型，避免需要为每种数据类型产生一个单独的类或函数。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> 类型参数名<span class="number">1</span> ,<span class="keyword">class</span> 类型参数 <span class="number">2</span>，…&gt;</span><br><span class="line">函数返回值类型 函数名(形式参数表)</span><br><span class="line">&#123; 函数体 &#125;</span><br></pre></td></tr></table></figure><p>关键字class也可以使用<strong>关键字typename</strong>；</p><p>在template语句与函数模板定义语句&lt;返回类型&gt;之间<strong>不允许有别的语句</strong></p><p>函数模板允许使用多个类型参数，但在template定义部分的<strong>每个形参</strong>前必须有<strong>关键字typename或class</strong>，</p><p>函数形式参数表中可以使用模板类型参数，也可以使用一般类型参数.</p><p>模板参数说明的每个类型参数必须在函数定义形参表中至<strong>少出现一次</strong>；</p><p>类模板主要用于<strong>数据存储（容器）类</strong>。<strong>表示和算法</strong>不受所包含的元素类型的影响。</p><p>一个类模板在类层次结构中<br>既可以是基类也可以是派生类：<br>Ø 类模板可以从模板类派生<br>Ø 类模板可以从非模板类派生<br>Ø 模板类可以从类模板派生<br>Ø 非模板类可以从类模板派生</p><h1 id="EasyX-基础"><a href="#EasyX-基础" class="headerlink" title="EasyX 基础"></a>EasyX 基础</h1><ol><li>在项目中创建一个.cpp源文件（右键源文件 -&gt; 添加 -&gt; 新建项 -&gt; 设置文件名 first.cpp）</li></ol><p><img src="/./%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%88c%E8%89%B9%EF%BC%89%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/image-20250707160940619.png" alt="image-20250707160940619"></p><h2 id="EasyX-基本概念"><a href="#EasyX-基本概念" class="headerlink" title="EasyX 基本概念"></a>EasyX 基本概念</h2><h3 id="绘图窗口与设备"><a href="#绘图窗口与设备" class="headerlink" title="绘图窗口与设备"></a>绘图窗口与设备</h3><p>initgraph 函数用于<strong>初始化绘图窗口</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">HWND <span class="title">initgraph</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="type">int</span> width,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="type">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="type">int</span> flag = <span class="literal">NULL</span></span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>;</span><br></pre></td></tr></table></figure><p>示例1：创建禁用最小化和关闭按钮的绘图窗口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initgraph(800, 600, EX_NOMINIMIZE | EX_NOCLOSE);</span><br></pre></td></tr></table></figure><p>示例2：窗口开启 EX_SHOWCONSOLE 模式，可以进行代码调试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initgraph(800, 600, EX_SHOWCONSOLE);// 带控制台的图形窗口</span><br></pre></td></tr></table></figure><p>在 EasyX 中，<strong>设备</strong>分两种，一种是默认的<strong>绘图窗口</strong>，另一种是 <strong>IMAGE 对象</strong>。</p><p>通过 SetWorkingImage 函数可以设置当前用于绘图的设备。设置当前用于绘图的设备后，所有的绘图函数都会绘制在该设备上。</p><h3 id="坐标"><a href="#坐标" class="headerlink" title="坐标"></a>坐标</h3><p>在 EasyX 中，坐标分两种：<strong>物理坐标和逻辑坐标</strong>。</p><ul><li><strong>物理坐标</strong></li></ul><p>物理坐标是描述设备的坐标体系。</p><p><strong>坐标原点在设备的左上角，X 轴向右为正，Y 轴向下为正(特点)，度量单位是像素（Pixel）。</strong></p><p>坐标原点、坐标轴方向、缩放比例都不能改变。</p><ul><li><strong>逻辑坐标</strong></li></ul><p>逻辑坐标是在程序中<strong>用于绘图的</strong>坐标体系。</p><p><strong>坐标默认的原点在窗口的左上角，X 轴向右为正，Y 轴向下为正，度量单位是点。</strong></p><p>默认情况下，逻辑坐标<strong>与物理坐标是一一对应</strong>的，<strong>一个逻辑点等于一个物理像素</strong>。</p><p><strong>在 EasyX 中，凡是没有特殊注明的坐标，均指逻辑坐标。</strong></p><p><strong>坐标相关函数</strong></p><table><thead><tr><th>函数用法</th><th>函数说明</th></tr></thead><tbody><tr><td>void <strong>setorigin</strong> ( int x, int y )</td><td>用于设置坐标原点。</td></tr><tr><td>void <strong>setaspectratio</strong> ( float xasp, float yasp )</td><td>通过设置 x 和 y 方向上的<strong>缩放因子</strong>，从而<strong>修改绘图的缩放比例或坐标轴方向</strong>。</td></tr></tbody></table><p>范例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;graphics.h&gt;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">initgraph(600, 600);</span><br><span class="line">setorigin(300, 300);// 将绘图窗口的中心点作为坐标原点 </span><br><span class="line">circle(0, 0, 100);</span><br><span class="line">setorigin(0, 0);// 将绘图窗口的左上角作为坐标原点</span><br><span class="line">setaspectratio(2, 1);// x轴方向的缩放因子为2，y轴方向的缩放因子为1(默认值)</span><br><span class="line">circle(100, 100, 100);</span><br><span class="line">setorigin(0, 600);// 将绘图窗口的左下角作为坐标原点</span><br><span class="line">setaspectratio(1, -1);// 缩放因子为负数，可以实现坐标轴的翻转，此行可使y轴向上为正</span><br><span class="line">circle(100, 100, 100);</span><br><span class="line">system(&quot;pause&quot;);</span><br><span class="line">closegraph();</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="颜色"><a href="#颜色" class="headerlink" title="颜色"></a>颜色</h3><p>EasyX 使用 24bit 真彩色，有四种表示颜色的方法：<a href="https://docs.easyx.cn/zh-cn/color%EF%BC%8C%E9%80%9A%E8%BF%87">https://docs.easyx.cn/zh-cn/color，通过</a> <strong>setlinecolor</strong> 函数可以设置线条颜色</p><ol><li><p>用<strong>预定义常量</strong>表示颜色（ 常量名要大写 ）</p></li><li><p>用<strong>16进制数字表示颜色（ 0xBBGGRR ）</strong>，注意<strong>颜色的顺序与RGB宏相反</strong></p></li><li><p>用 <strong>RGB 宏合成颜色（ RGB(RRGGBB) ）</strong></p></li><li><p>用 HSLtoRGB、HSVtoRGB 转换其他色彩模型到 RGB 颜色</p></li></ol><p>范例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;graphics.h&gt;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">initgraph(800, 600);</span><br><span class="line">setfillcolor(BLUE);// 用预定义常量表示颜色</span><br><span class="line">solidcircle(100, 200, 100);</span><br><span class="line">setfillcolor(0xaa0000);// 用16进制数字表示颜色</span><br><span class="line">solidcircle(300, 200, 100);</span><br><span class="line">setfillcolor(RGB(0, 0, 170));// 用RGB宏合成颜色</span><br><span class="line">solidcircle(500, 200, 100);</span><br><span class="line">setfillcolor(HSLtoRGB(240, 1, 0.33));// 用 HSLtoRGB、HSVtoRGB 转换其他色彩模型到 RGB 颜色</span><br><span class="line">solidcircle(700, 200, 100);</span><br><span class="line">system(&quot;pause&quot;);</span><br><span class="line">closegraph();</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="EasyX-图形绘制函数（33个）"><a href="#EasyX-图形绘制函数（33个）" class="headerlink" title="EasyX 图形绘制函数（33个）"></a>EasyX 图形绘制函数（33个）</h2><p><a href="https://docs.easyx.cn/zh-cn/drawing-func">https://docs.easyx.cn/zh-cn/drawing-func</a></p><table><thead><tr><th align="left">函数用法</th><th>函数说明</th></tr></thead><tbody><tr><td align="left">void <strong>circle</strong> ( int x, int y, int radius )</td><td>画无填充的圆</td></tr><tr><td align="left">fillcircle</td><td>画有边框的填充圆</td></tr><tr><td align="left">solidcircle</td><td>画无边框的填充圆</td></tr><tr><td align="left">clearcircle</td><td>用当前背景色清空圆形区域</td></tr><tr><td align="left"></td><td></td></tr><tr><td align="left">void <strong>ellipse</strong> ( int left, int top, int right, int bottom )</td><td>画无填充的椭圆</td></tr><tr><td align="left">fillellipse</td><td>画有边框的填充椭圆</td></tr><tr><td align="left">solidellipse</td><td>画无边框的填充椭圆</td></tr><tr><td align="left">clearellipse</td><td>用当前背景色清空椭圆区域</td></tr><tr><td align="left"></td><td></td></tr><tr><td align="left">void <strong>pie</strong> ( int left, int top, int right, int bottom, double stangle, double endangle );</td><td>画无填充的扇形</td></tr><tr><td align="left">fillpie</td><td>画有边框的填充扇形</td></tr><tr><td align="left">solidpie</td><td>画无边框的填充扇形</td></tr><tr><td align="left">clearpie</td><td>用当前背景色清空扇形区域</td></tr><tr><td align="left"></td><td></td></tr><tr><td align="left">void <strong>rectangle</strong> ( int left, int top, int right, int bottom )</td><td>画无填充的矩形</td></tr><tr><td align="left">fillrectangle</td><td>画有边框的填充矩形</td></tr><tr><td align="left">solidrectangle</td><td>画无边框的填充矩形</td></tr><tr><td align="left">clearrectangle</td><td>用当前背景色清空矩形区域</td></tr><tr><td align="left"></td><td></td></tr><tr><td align="left">void <strong>roundrect</strong> ( int left, int top, int right, int bottom, int ellipsewidth, int ellipseheight )</td><td>画无填充的圆角矩形</td></tr><tr><td align="left">fillroundrect</td><td>画有边框的填充圆角矩形</td></tr><tr><td align="left">solidroundrect</td><td>画无边框的填充圆角矩形</td></tr><tr><td align="left">clearroundrect</td><td>用当前背景色清空圆角矩形区域</td></tr><tr><td align="left"></td><td></td></tr><tr><td align="left">void <strong>polygon</strong> ( const POINT *points, int num );</td><td>画无填充的多边形</td></tr><tr><td align="left">fillpolygon</td><td>画有边框的填充多边形</td></tr><tr><td align="left">solidpolygon</td><td>画无边框的填充多边形</td></tr><tr><td align="left">clearpolygon</td><td>用当前背景色清空多边形区域</td></tr><tr><td align="left"></td><td></td></tr><tr><td align="left">void <strong>putpixel</strong> ( int x, int y, COLORREF color )</td><td>画点</td></tr><tr><td align="left">void <strong>line</strong> ( int x1, int y1, int x2, int y2 )</td><td>画直线</td></tr><tr><td align="left">void <strong>arc</strong> ( int left, int top, int right, int bottom, double stangle, double endangle )</td><td>画椭圆弧</td></tr><tr><td align="left">void <strong>polyline</strong> ( const POINT *points, int num )</td><td>画多条连续的直线</td></tr><tr><td align="left">void <strong>polybezier</strong> ( const POINT *points, int num )</td><td>画三次方贝塞尔曲线</td></tr><tr><td align="left"></td><td></td></tr><tr><td align="left">void <strong>floodfill</strong> ( int x, int y, COLORREF color, int filltype &#x3D; FLOODFILLBORDER )</td><td>填充区域</td></tr><tr><td align="left"></td><td></td></tr><tr><td align="left">COLORREF <strong>getpixel</strong> ( int x, int y )</td><td>获取坐标点的颜色</td></tr><tr><td align="left">int <strong>getwidth</strong> ( )</td><td>获取绘图区的宽度</td></tr><tr><td align="left">int <strong>getheight</strong> ( )</td><td>获取绘图区的高度</td></tr></tbody></table><h2 id="双缓冲绘图"><a href="#双缓冲绘图" class="headerlink" title="双缓冲绘图"></a>双缓冲绘图</h2><p>双缓冲绘图通过在内存中创建一个与屏幕绘图区域<strong>一致的对象</strong>，先将图形绘制到<strong>内存中的这个对象上</strong>，再<strong>一次性将这个对象上的图形拷贝</strong>到屏幕上，从而<strong>减少对屏幕的直接绘图操作</strong>，<strong>提高绘图效率、消除屏幕闪烁</strong>，广泛应用于游戏开发、图形界面等领域。</p><table><thead><tr><th>函数用法</th><th>函数说明</th></tr></thead><tbody><tr><td>void <strong>BeginBatchDraw</strong> ()</td><td>开始批量绘图</td></tr><tr><td>void <strong>EndBatchDraw</strong> ()<br/>void <strong>EndBatchDraw</strong> ( int left, int top, int right, int bottom )    &#x2F;&#x2F; 指定区域</td><td>结束批量绘制，并执行（指定区域内）未完成的绘制任务</td></tr><tr><td>void <strong>FlushBatchDraw</strong> ()<br/>void <strong>FlushBatchDraw</strong> ( int left, int top, int right, int bottom )    &#x2F;&#x2F; 指定区域</td><td>执行（指定区域内）未完成的绘制任务</td></tr></tbody></table><p><a href="https://docs.easyx.cn/zh-cn/other-func">https://docs.easyx.cn/zh-cn/other-func</a></p><p><strong>自动移动的圆（帧数控制）</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">//include &lt;windows.h&gt;</span><br><span class="line">#include &lt;graphics.h&gt;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">initgraph(640, 480);</span><br><span class="line">BeginBatchDraw();</span><br><span class="line"></span><br><span class="line">setlinecolor(WHITE);</span><br><span class="line">setfillcolor(RED);</span><br><span class="line">for (int i = 50; i &lt; 600; i++)</span><br><span class="line">&#123;</span><br><span class="line">DWORD beginTime = GetTickCount();// 记录循环开始时间</span><br><span class="line"></span><br><span class="line">cleardevice();</span><br><span class="line">circle(i, 100, 40);</span><br><span class="line">floodfill(i, 100, WHITE);</span><br><span class="line">FlushBatchDraw();</span><br><span class="line"></span><br><span class="line">DWORD endTime = GetTickCount();// 记录循环结束时间</span><br><span class="line">DWORD elapsedTime = endTime - beginTime;// 计算循环耗时</span><br><span class="line">if (elapsedTime &lt; 1000 / 60)// 按每秒60帧进行补时</span><br><span class="line">Sleep(1000 / 60 - elapsedTime);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">EndBatchDraw();</span><br><span class="line">closegraph();</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>GetTickCount</strong> 是一个 Windows 系统函数，用于<strong>获取从操作系统启动以来所经过的毫秒数</strong>，通过在代码中的不同位置调用该函数，并计算两次调用之间的差值，可以得知某段代码或某个操作的执行时间。</p><p>注：GetTickCount 的值会在系统启动后约49.7天（(2^32-1) ms）后回绕到0，这是因为其返回值是一个32位无符号整数，<strong>可以使用 GetTickCount64 代替，需添加 windows.h 头文件。</strong></p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h1 id="EasyX-进阶"><a href="#EasyX-进阶" class="headerlink" title="EasyX 进阶"></a>EasyX 进阶</h1><h2 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h2><p><a href="https://docs.easyx.cn/zh-cn/image-func">https://docs.easyx.cn/zh-cn/image-func</a></p><table><thead><tr><th>函数用法</th><th>函数说明</th></tr></thead><tbody><tr><td>void <strong>loadimage</strong> (<br/>IMAGE* pDstImg,&#x2F;&#x2F; 保存图像的 IMAGE 对象指针<br/>LPCTSTR pImgFile,   &#x2F;&#x2F; 图片文件名<br/>int nWidth &#x3D; 0,          &#x2F;&#x2F; 图片的拉伸宽度<br/>int nHeight &#x3D; 0, &#x2F;&#x2F; 图片的拉伸高度<br/>bool bResize &#x3D; false &#x2F;&#x2F;是否调整IMAGE的大小以适应图片<br/>)</td><td>从文件中读取图像。如果pDstImg为NULL，则读取到绘图窗口</td></tr><tr><td>void <strong>putimage</strong> (<br/>    int dstX,    &#x2F;&#x2F; 绘制位置的 x 坐标<br/>    int dstY,    &#x2F;&#x2F; 绘制位置的 y 坐标<br/>    IMAGE *pSrcImg,&#x2F;&#x2F; 要绘制的 IMAGE 对象指针<br/>    DWORD dwRop &#x3D; SRCCOPY&#x2F;&#x2F; 三元光栅操作码<br/>);</td><td>在当前设备上绘制指定图像</td></tr><tr><td>void <strong>putimage</strong> (<br/>    int dstX,    &#x2F;&#x2F; 绘制位置的 x 坐标<br/>    int dstY,    &#x2F;&#x2F; 绘制位置的 y 坐标<br/>    int dstWidth,&#x2F;&#x2F; 绘制的宽度 <br/>    int dstHeight,   &#x2F;&#x2F; 绘制的高度<br/>    IMAGE *pSrcImg,&#x2F;&#x2F; 要绘制的 IMAGE 对象指针<br/>    int srcX,  &#x2F;&#x2F; 绘制内容在 IMAGE 对象中的左上角 x 坐标<br/>    int srcY,  &#x2F;&#x2F; 绘制内容在 IMAGE 对象中的左上角 y 坐标 <br/>    DWORD dwRop &#x3D; SRCCOPY&#x2F;&#x2F; 三元光栅操作码<br/>)</td><td>在当前设备上绘制指定图像（指定宽高和起始位置）</td></tr><tr><td>void <strong>Resize</strong> ( IMAGE* pImg, int width, int height )</td><td>调整指定绘图设备的尺寸，pImg 如果为 NULL 表示默认绘图窗口</td></tr><tr><td>void <strong>rotateimage</strong> (<br/>IMAGE *dstimg,<br/>IMAGE *srcimg,<br/>double radian,<br/>COLORREF bkcolor &#x3D; BLACK,<br/>bool autosize &#x3D; false,<br/>bool highquality &#x3D; true<br/>)</td><td>旋转 IMAGE 中的绘图内容</td></tr><tr><td>void <strong>saveimage</strong> (<br/>LPCTSTR strFileName,<br/>IMAGE* pImg &#x3D; NULL<br/>)</td><td>保存绘图内容至图片文件，支持 bmp &#x2F; gif &#x2F; jpg &#x2F; png &#x2F; tif 格式</td></tr><tr><td>void <strong>SetWorkingImage</strong> ( IMAGE* pImg &#x3D; NULL )</td><td>设定当前的绘图设备，如果参数为 NULL，表示绘图设备为默认绘图窗口</td></tr><tr><td>IMAGE* **GetWorkingImage **()</td><td>获取当前的绘图设备，如果返回值为 NULL，表示当前绘图设备为绘图窗口</td></tr><tr><td>void <strong>getimage</strong> (<br/>IMAGE* pDstImg,   &#x2F;&#x2F; 保存图像的 IMAGE 对象指针<br/>int srcX,   &#x2F;&#x2F; 要获取图像区域左上角 x 坐标<br/>int srcY,       &#x2F;&#x2F; 要获取图像区域的左上角 y 坐标<br/>int srcWidth,  &#x2F;&#x2F; 要获取图像区域的宽度<br/>int srcHeight &#x2F;&#x2F; 要获取图像区域的高度<br/>)</td><td>从当前绘图设备中获取图像</td></tr><tr><td>DWORD* <strong>GetImageBuffer</strong> ( IMAGE* pImg &#x3D; NULL )</td><td>获取绘图设备的显示缓冲区指针，pImg 如果为 NULL，表示默认的绘图窗口</td></tr><tr><td>HDC <strong>GetImageHDC</strong> ( IMAGE* pImg &#x3D; NULL )</td><td>获取绘图设备句柄(HDC)</td></tr></tbody></table><h3 id="IMAGE-类"><a href="#IMAGE-类" class="headerlink" title="IMAGE 类"></a><strong>IMAGE 类</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">class</span> <span class="title">IMAGE</span><span class="params">(<span class="type">int</span> width = <span class="number">0</span>, <span class="type">int</span> height = <span class="number">0</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">公有成员</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">getwidth</span><span class="params">()</span></span>;</span><br><span class="line">返回 IMAGE 对象的宽度，以像素为单位。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">getheight</span><span class="params">()</span></span>;</span><br><span class="line">返回 IMAGE 对象的高度，以像素为单位。</span><br><span class="line"></span><br><span class="line"><span class="keyword">operator</span> =</span><br><span class="line">实现IMAGE对象的直接赋值。该操作仅拷贝源图像的内容，不拷贝源图像的绘图环境。</span><br></pre></td></tr></table></figure><p>在内存中保存图像信息。</p><h3 id="loadimage-函数"><a href="#loadimage-函数" class="headerlink" title="loadimage 函数"></a>loadimage 函数</h3><p>范例1：loadimage <strong>直接读取图片至绘图窗口</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">1400</span>, <span class="number">600</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">loadimage</span>(<span class="literal">NULL</span>, _T(<span class="string">&quot;image\\background.jpg&quot;</span>));<span class="comment">// 第一个参数为NULL时，直接读取图片至绘图窗口</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注：修改窗口大小，可以显示图片部分内容，但<strong>只能从绘图窗口的坐标原点（左上角）开始显示</strong>图片</p><p>范例2：loadimage 直接读取图片至绘图窗口并进行图片或窗口缩放</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">700</span>, <span class="number">300</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">loadimage</span>(<span class="literal">NULL</span>, _T(<span class="string">&quot;image\\background.jpg&quot;</span>), <span class="number">700</span>, <span class="number">300</span>, <span class="literal">false</span>);<span class="comment">// 将图像缩放为700*300在绘图窗口显示</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注1：图片缩放后的尺寸<strong>小于</strong>窗口尺寸，则窗口会有<strong>黑边</strong>；若<strong>大于</strong>窗口尺寸，则图片<strong>显示不全</strong></p><p>注2：<strong>第五个参数若为 true，则会调整窗口以适应图片的大小</strong></p><p>注3：从磁盘中<strong>读取大量图片显示</strong>的情况下，使用 loadimage 直接读取图片至绘图窗口<strong>性能较差</strong></p><p>范例3：loadimage 读取本地图片文件，输出图片宽度和高度</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;graphics.h&gt;</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">initgraph(1000, 600, SHOWCONSOLE);// 初始化绘图窗口并开启终端</span><br><span class="line"></span><br><span class="line">IMAGE img;// 定义图像对象</span><br><span class="line">loadimage(&amp;img, _T(&quot;image\\background.jpg&quot;));// 读取本地图片文件，存入图像对象</span><br><span class="line">printf(&quot;width=%d, height=%d \n&quot;, img.getwidth(), img.getheight());// 输出图像宽度和高度</span><br><span class="line"></span><br><span class="line">system(&quot;pause&quot;);</span><br><span class="line">closegraph();</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注：本例中的图片内容<strong>不会在窗口内显示</strong></p><h3 id="putimage-函数"><a href="#putimage-函数" class="headerlink" title="putimage 函数"></a>putimage 函数</h3><p>范例1：putimage <strong>在绘图窗口显示图像</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">1000</span>, <span class="number">600</span>);</span><br><span class="line"></span><br><span class="line">IMAGE img;</span><br><span class="line"><span class="built_in">loadimage</span>(&amp;img, _T(<span class="string">&quot;image\\background.jpg&quot;</span>));</span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">0</span>, &amp;img);<span class="comment">// 将IMAGE对象显示在绘图窗口的坐标（0,0）处</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>范例2：putimage 截取图像部分内容进行显示</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">900</span>, <span class="number">600</span>);</span><br><span class="line"></span><br><span class="line">IMAGE img;</span><br><span class="line"><span class="built_in">loadimage</span>(&amp;img, _T(<span class="string">&quot;image\\background.jpg&quot;</span>));</span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">900</span>, <span class="number">600</span>, &amp;img, <span class="number">115</span>, <span class="number">0</span>);<span class="comment">// 从图像的(115,0)坐标处截取宽900、高600的部分内容显示在窗口(0,0)处</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>范例3：putimage 三元光栅操作码</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">1000</span>, <span class="number">600</span>);</span><br><span class="line"></span><br><span class="line">IMAGE img;</span><br><span class="line"><span class="built_in">loadimage</span>(&amp;img, _T(<span class="string">&quot;image\\background.jpg&quot;</span>));</span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">0</span>, &amp;img, NOTSRCCOPY);<span class="comment">// 第四个参数是三元光栅操作码</span></span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注：putimage 第四个参数是 <strong>三元光栅操作码</strong> ，它定义了<strong>源图像与目标图像的位合并形式</strong>，默认值为 <strong>SRCCOPY</strong> 详见</p><p><a href="https://docs.easyx.cn/zh-cn/putimage">https://docs.easyx.cn/zh-cn/putimage</a></p><h4 id="透明贴图"><a href="#透明贴图" class="headerlink" title="透明贴图"></a>透明贴图</h4><p>范例1：通过PS制作<strong>原图</strong>的<strong>掩码图</strong>和<strong>前景图</strong>，再进行三元光栅操作叠加而成</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">IMAGE imgGuoqi, imgGuohui, imgGuohuiMask, imgGuohuiFg;</span><br><span class="line"><span class="built_in">loadimage</span>(&amp;imgGuoqi, _T(<span class="string">&quot;image\\guoqi.jpg&quot;</span>), <span class="number">1000</span>, <span class="number">600</span>);<span class="comment">// 加载国旗（背景图）</span></span><br><span class="line"><span class="built_in">loadimage</span>(&amp;imgGuohui, _T(<span class="string">&quot;image\\guohui.jpg&quot;</span>), <span class="number">200</span>, <span class="number">200</span>);<span class="comment">// 加载国徽原图（白色周边）</span></span><br><span class="line"><span class="built_in">loadimage</span>(&amp;imgGuohuiMask, _T(<span class="string">&quot;image\\guohui_mask.jpg&quot;</span>), <span class="number">200</span>, <span class="number">200</span>);<span class="comment">// 加载国徽掩码图（白色周边+黑色内容）</span></span><br><span class="line"><span class="built_in">loadimage</span>(&amp;imgGuohuiFg, _T(<span class="string">&quot;image\\guohui_fg.jpg&quot;</span>), <span class="number">200</span>, <span class="number">200</span>);<span class="comment">// 加载国徽前景图（黑色周边+待显示内容）</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">1000</span>, <span class="number">600</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">0</span>, &amp;imgGuoqi);<span class="comment">// 显示国旗</span></span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">0</span>, &amp;imgGuohui);<span class="comment">// 显示国徽原图</span></span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">200</span>, &amp;imgGuohuiMask);<span class="comment">// 显示国徽掩码图</span></span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">400</span>, &amp;imgGuohuiFg);<span class="comment">// 显示国徽前景图</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 透明贴图</span></span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">200</span>, <span class="number">0</span>, &amp;imgGuohuiMask, SRCAND);<span class="comment">// 显示掩码图（SRCAND：按位与）</span></span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">200</span>, <span class="number">0</span>, &amp;imgGuohuiFg, SRCPAINT);<span class="comment">// 显示前景图（SRCPAINT：按位或）</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>范例2：TransparentBlt 函数实现</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> comment(lib, <span class="string">&quot;MSIMG32.LIB&quot;</span>)<span class="comment">// 链接器在链接过程中包含指定的库文件</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">putimage_alpha</span><span class="params">(IMAGE* dstImg, <span class="type">int</span> x, <span class="type">int</span> y, IMAGE* srcImg, UINT transparentColor)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">HDC dstDC = <span class="built_in">GetImageHDC</span>(dstImg);</span><br><span class="line">HDC srcDC = <span class="built_in">GetImageHDC</span>(srcImg);</span><br><span class="line"><span class="type">int</span> w = srcImg-&gt;<span class="built_in">getwidth</span>();</span><br><span class="line"><span class="type">int</span> h = srcImg-&gt;<span class="built_in">getheight</span>();</span><br><span class="line"><span class="built_in">TransparentBlt</span>(dstDC, x, y, w, h, srcDC, <span class="number">0</span>, <span class="number">0</span>, w, h, transparentColor);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">1000</span>, <span class="number">600</span>);</span><br><span class="line">IMAGE imgGuoqi, imgBaidu;</span><br><span class="line"><span class="built_in">loadimage</span>(&amp;imgGuoqi, _T(<span class="string">&quot;image\\guoqi.jpg&quot;</span>), <span class="number">1000</span>, <span class="number">600</span>);<span class="comment">// 加载国旗（背景图）</span></span><br><span class="line"><span class="built_in">loadimage</span>(&amp;imgBaidu, _T(<span class="string">&quot;image\\baidu.png&quot;</span>));<span class="comment">// 加载百度LOGO（PNG格式）</span></span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">0</span>, &amp;imgGuoqi);<span class="comment">// 显示国旗</span></span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">0</span>, &amp;imgBaidu);<span class="comment">// 显示百度LOGO</span></span><br><span class="line"><span class="built_in">putimage_alpha</span>(<span class="literal">NULL</span>, <span class="number">0</span>, <span class="number">300</span>, &amp;imgBaidu, BLACK);<span class="comment">// 显示百度LOGO（透明贴图）</span></span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>TransparentBlt 是 Windows GDI（Graphics Device Interface）中的一个函数，用于在绘制位图时支持透明效果。</p><p>函数说明：第1个参数为目标设备，第2、3个参数是输出目标矩形左上角坐标，第4、5个参数是目标矩形的宽和高，参数6-10与1-5类似，第11个参数是<strong>透明底色</strong>（若图片是透明图片，默认为BLACK）</p><p><strong>注：此方法只支持 PNG 格式的图片</strong></p><p>范例3：AlphaBlend 函数实现（<strong>推荐</strong>）</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> comment(lib, <span class="string">&quot;MSIMG32.LIB&quot;</span>)<span class="comment">// 链接器在链接过程中包含指定的库文件</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">putimage_alpha</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y, IMAGE* img)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="type">int</span> w = img-&gt;<span class="built_in">getwidth</span>();</span><br><span class="line"><span class="type">int</span> h = img-&gt;<span class="built_in">getheight</span>();</span><br><span class="line"><span class="built_in">AlphaBlend</span>(<span class="built_in">GetImageHDC</span>(<span class="literal">NULL</span>), x, y, w, h, <span class="built_in">GetImageHDC</span>(img), <span class="number">0</span>, <span class="number">0</span>, w, h, &#123; AC_SRC_OVER, <span class="number">0</span>, <span class="number">255</span>, AC_SRC_ALPHA &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">1000</span>, <span class="number">600</span>);</span><br><span class="line"></span><br><span class="line">IMAGE imgGuoqi, imgBaidu;</span><br><span class="line"><span class="built_in">loadimage</span>(&amp;imgGuoqi, _T(<span class="string">&quot;image\\guoqi.jpg&quot;</span>), <span class="number">1000</span>, <span class="number">600</span>);<span class="comment">// 加载国旗（背景图）</span></span><br><span class="line"><span class="built_in">loadimage</span>(&amp;imgBaidu, _T(<span class="string">&quot;image\\baidu.png&quot;</span>));<span class="comment">// 加载百度LOGO（PNG格式）</span></span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">0</span>, &amp;imgGuoqi);<span class="comment">// 显示国旗</span></span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">0</span>, &amp;imgBaidu);<span class="comment">// 显示百度LOGO</span></span><br><span class="line"><span class="built_in">putimage_alpha</span>(<span class="number">0</span>, <span class="number">300</span>, &amp;imgBaidu);<span class="comment">// 显示百度LOGO（透明贴图）</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>AlphaBlend 是 Windows GDI 中用于实现 <strong>Alpha 混合（透明&#x2F;半透明）</strong> 绘制的函数，比 TransparentBlt 更强大，支持 <strong>逐像素透明度（Alpha 通道）</strong> 和 <strong>整体透明度（全局 Alpha）</strong>。</p><p><strong>注：此方法只支持 PNG 格式的图片</strong></p><h4 id="图片动画"><a href="#图片动画" class="headerlink" title="图片动画"></a>图片动画</h4><p>图片动画的核心是<strong>一系列静态的图像（动画帧）</strong>。每一帧都是一张静态的图片，但它们之间略有不同，通常表现为物体的位置、形状或颜色的微小变化。这些帧按照特定的顺序排列，并以一定的速度连续播放，使得观者感受到运动的效果。</p><p>范例：角色动画</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> comment(lib, <span class="string">&quot;MSIMG32.LIB&quot;</span>)</span></span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> WINDOW_WIDTH = <span class="number">1000</span>;<span class="comment">//窗口宽度</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> WINDOW_HEIGHT = <span class="number">600</span>;<span class="comment">//窗口高度</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> FRAME = <span class="number">60</span>;<span class="comment">//帧数</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> INTERVAL_MS = <span class="number">15</span>;<span class="comment">//动画帧间隔</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> IMAGE_NUM = <span class="number">13</span>;<span class="comment">//动画图片数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//显示透明图片</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">putimage_alpha</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y, IMAGE* img)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="type">int</span> w = img-&gt;<span class="built_in">getwidth</span>();</span><br><span class="line"><span class="type">int</span> h = img-&gt;<span class="built_in">getheight</span>();</span><br><span class="line"><span class="built_in">AlphaBlend</span>(<span class="built_in">GetImageHDC</span>(<span class="literal">NULL</span>), x, y, w, h, <span class="built_in">GetImageHDC</span>(img), <span class="number">0</span>, <span class="number">0</span>, w, h, &#123; AC_SRC_OVER, <span class="number">0</span>, <span class="number">255</span>, AC_SRC_ALPHA &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="type">bool</span> running = <span class="literal">true</span>;<span class="comment">//主循环控制</span></span><br><span class="line">ExMessage msg;<span class="comment">//键鼠消息</span></span><br><span class="line">IMAGE imgBackground;<span class="comment">//背景图片对象</span></span><br><span class="line">IMAGE imgPEA[<span class="number">13</span>];<span class="comment">//玩家动画图片</span></span><br><span class="line">TCHAR imgPath[<span class="number">256</span>];<span class="comment">//动画图片文件路径</span></span><br><span class="line"><span class="type">int</span> imgIndex = <span class="number">0</span>;<span class="comment">//动画帧索引</span></span><br><span class="line"><span class="type">static</span> <span class="type">int</span> timer = <span class="number">0</span>;<span class="comment">//动画计时器</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">loadimage</span>(&amp;imgBackground, _T(<span class="string">&quot;image\\background.jpg&quot;</span>));<span class="comment">//加载背景图片</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; IMAGE_NUM; i++)<span class="comment">//加载动画图片</span></span><br><span class="line">&#123;</span><br><span class="line">_stprintf_s(imgPath, _T(<span class="string">&quot;image\\pea\\%d.png&quot;</span>), i + <span class="number">1</span>);<span class="comment">//动画图片路径（格式转换）</span></span><br><span class="line"><span class="built_in">loadimage</span>(&amp;imgPEA[i], imgPath);<span class="comment">//加载动画图片</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">initgraph</span>(WINDOW_WIDTH, WINDOW_HEIGHT);</span><br><span class="line"><span class="built_in">BeginBatchDraw</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">//主循环</span></span><br><span class="line"><span class="keyword">while</span> (running)</span><br><span class="line">&#123;</span><br><span class="line">DWORD beginTime = <span class="built_in">GetTickCount</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">//消息处理</span></span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">peekmessage</span>(&amp;msg))</span><br><span class="line">&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//数据处理</span></span><br><span class="line">timer += <span class="number">5</span>;</span><br><span class="line"><span class="keyword">if</span> (timer &gt; INTERVAL_MS)<span class="comment">//定时器超过预定的时间间隔时切换下一张图片</span></span><br><span class="line">&#123;</span><br><span class="line">imgIndex = (imgIndex + <span class="number">1</span>) % IMAGE_NUM;<span class="comment">//循环切换图片：索引值0-12</span></span><br><span class="line">timer = <span class="number">0</span>;<span class="comment">//重置计时器</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//绘图</span></span><br><span class="line"><span class="built_in">cleardevice</span>();</span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">0</span>, &amp;imgBackground);<span class="comment">//绘制背景图片</span></span><br><span class="line"><span class="built_in">putimage_alpha</span>(<span class="number">500</span>, <span class="number">300</span>, &amp;imgPEA[imgIndex]);<span class="comment">//绘制豌豆图片</span></span><br><span class="line"><span class="built_in">FlushBatchDraw</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">//帧延时处理</span></span><br><span class="line">DWORD endTime = <span class="built_in">GetTickCount</span>();</span><br><span class="line">DWORD elapsedTime = endTime - beginTime;</span><br><span class="line"><span class="keyword">if</span> (elapsedTime &lt; <span class="number">1000</span> / FRAME)</span><br><span class="line"><span class="built_in">Sleep</span>(<span class="number">1000</span> / FRAME - elapsedTime);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">EndBatchDraw</span>();</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Resize-函数"><a href="#Resize-函数" class="headerlink" title="Resize 函数"></a>Resize 函数</h3><h3 id="GetImageBuffer-函数"><a href="#GetImageBuffer-函数" class="headerlink" title="GetImageBuffer 函数"></a>GetImageBuffer 函数</h3><p>范例1：GetImageBuffer 通过<strong>直接操作显示缓冲区绘制渐变的蓝色</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">600</span>, <span class="number">400</span>);</span><br><span class="line"></span><br><span class="line">DWORD* pMem = <span class="built_in">GetImageBuffer</span>();<span class="comment">// 获取当前窗口所指图像缓冲区的指针</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">600</span> * <span class="number">400</span>; i++)</span><br><span class="line">pMem[i] = <span class="built_in">BGR</span>(<span class="built_in">RGB</span>(<span class="number">0</span>, <span class="number">0</span>, i * <span class="number">256</span> / (<span class="number">600</span> * <span class="number">400</span>)));<span class="comment">// 直接对图像缓冲区每个坐标像素赋值（颜色）</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>范例2：图像翻转</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 图像翻转</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">flip_image</span><span class="params">(IMAGE* srcImg, IMAGE* dstImg)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="type">int</span> w = srcImg-&gt;<span class="built_in">getwidth</span>();<span class="comment">// 获取源图像宽度</span></span><br><span class="line"><span class="type">int</span> h = srcImg-&gt;<span class="built_in">getheight</span>();<span class="comment">// 获取源图像高度</span></span><br><span class="line"><span class="built_in">Resize</span>(dstImg, w, h);<span class="comment">// 设置目标图像与源图像宽高一致</span></span><br><span class="line">DWORD* src_buffer = <span class="built_in">GetImageBuffer</span>(srcImg);<span class="comment">// 获取源图像缓冲区指针</span></span><br><span class="line">DWORD* dst_buffer = <span class="built_in">GetImageBuffer</span>(dstImg);<span class="comment">// 获取目标图像缓冲区指针</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> y = <span class="number">0</span>; y &lt; h; y++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> x = <span class="number">0</span>; x &lt; w; x++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> idx_src = y * w + x;</span><br><span class="line"><span class="type">int</span> idx_dst = y * w + (w - x - <span class="number">1</span>);</span><br><span class="line">dst_buffer[idx_dst] = src_buffer[idx_src];<span class="comment">// 交换对应坐标像素的颜色值</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">1400</span>, <span class="number">600</span>);</span><br><span class="line"></span><br><span class="line">IMAGE img1, img2;</span><br><span class="line"><span class="built_in">loadimage</span>(&amp;img1, _T(<span class="string">&quot;image\\background.jpg&quot;</span>));</span><br><span class="line"><span class="built_in">flip_image</span>(&amp;img1, &amp;img2);</span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">0</span>, &amp;img2);</span><br><span class="line"></span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="消息处理"><a href="#消息处理" class="headerlink" title="消息处理"></a>消息处理</h2><p><a href="https://docs.easyx.cn/zh-cn/msg-func">https://docs.easyx.cn/zh-cn/msg-func</a></p><p><strong>消息缓冲区</strong>可以<strong>缓冲 63 个未处理的消息</strong>。每次获取消息时，将从消息缓冲区<strong>取出一个最早发生的</strong>消息。</p><table><thead><tr><th>函数用法</th><th>函数说明</th></tr></thead><tbody><tr><td>ExMessage <strong>getmessage</strong> ( BYTE filter &#x3D; -1 )<br/>void <strong>getmessage</strong> ( ExMessage *msg, BYTE filter &#x3D; -1 )</td><td>从消息缓冲区获取一个消息。如果缓冲区中没有消息，则程序会一直等待（阻塞式）</td></tr><tr><td>bool <strong>peekmessage</strong> ( ExMessage *msg, BYTE filter &#x3D; -1, bool removemsg &#x3D; true)</td><td>从消息缓冲区获取一个消息，并立即返回</td></tr><tr><td>void <strong>flushmessage</strong> ( BYTE filter &#x3D; -1 )</td><td>清空消息缓冲区</td></tr></tbody></table><p><strong>参数说明：</strong></p><ul><li><strong>msg</strong>：指向消息结构体 ExMessage 的指针，用来保存获取到的消息。</li><li><strong>filter</strong>：指定要获取的消息范围，默认 -1 获取所有类别的消息。可以用以下值或值的组合获取指定类别的消息</li></ul><table><thead><tr><th>标志</th><th>描述</th></tr></thead><tbody><tr><td>EX_MOUSE</td><td><strong>鼠标</strong>消息。</td></tr><tr><td>EX_KEY</td><td><strong>按键</strong>消息。</td></tr><tr><td>EX_CHAR</td><td>字符消息。</td></tr><tr><td>EX_WINDOW</td><td><strong>窗口</strong>消息。</td></tr></tbody></table><ul><li><strong>removemsg</strong>：在 peekmessage 处理完消息后，<strong>是否将其从消息队列中移除。</strong></li></ul><h3 id="ExMessage-结构体"><a href="#ExMessage-结构体" class="headerlink" title="ExMessage 结构体"></a><strong>ExMessage 结构体</strong></h3><p><a href="https://docs.easyx.cn/zh-cn/exmessage">https://docs.easyx.cn/zh-cn/exmessage</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">struct ExMessage</span><br><span class="line">&#123;</span><br><span class="line">USHORT message;// 消息标识</span><br><span class="line">union</span><br><span class="line">&#123;</span><br><span class="line">// 鼠标消息的数据</span><br><span class="line">struct</span><br><span class="line">&#123;</span><br><span class="line">bool ctrl:1;// Ctrl 键是否按下</span><br><span class="line">bool shift:1;// Shift 键是否按下</span><br><span class="line">bool lbutton:1;// 鼠标左键是否按下</span><br><span class="line">bool mbutton:1;// 鼠标中键是否按下</span><br><span class="line">bool rbutton:1;// 鼠标右键</span><br><span class="line">short x;// 鼠标的 x 坐标</span><br><span class="line">short y;// 鼠标的 y 坐标</span><br><span class="line">short wheel;// 鼠标滚轮滚动值，为 120 的倍数</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">// 按键消息的数据</span><br><span class="line">struct</span><br><span class="line">&#123;</span><br><span class="line">BYTE vkcode;// 按键的虚拟键码</span><br><span class="line">BYTE scancode;// 按键的扫描码（依赖于 OEM）</span><br><span class="line">bool extended:1;// 按键是否是扩展键</span><br><span class="line">bool prevdown:1;// 按键的前一个状态是否按下</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">// 字符消息的数据</span><br><span class="line">TCHAR ch;</span><br><span class="line"></span><br><span class="line">// 窗口消息的数据</span><br><span class="line">struct</span><br><span class="line">&#123;</span><br><span class="line">WPARAM wParam;</span><br><span class="line">LPARAM lParam;</span><br><span class="line">&#125;;</span><br><span class="line">&#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>message</strong> ：可以分为四大类：<strong>EX_MOUSE</strong>（鼠标11项）、<strong>EX_KEY</strong>（键盘2项）、<strong>EX_CHAR</strong>（字符1项）、<strong>EX_WINDOW</strong>（窗口3项）</p><p><strong>union</strong> ：共用体中存储具体消息的数据</p><h3 id="鼠标消息"><a href="#鼠标消息" class="headerlink" title="鼠标消息"></a>鼠标消息</h3><p>范例：跟随鼠标移动的圆</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Ball</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="type">int</span> posX;</span><br><span class="line"><span class="type">int</span> posY;</span><br><span class="line"><span class="type">int</span> radius;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="built_in">Ball</span>(<span class="type">int</span> x = <span class="number">100</span>, <span class="type">int</span> y=<span class="number">100</span>, <span class="type">int</span> r=<span class="number">50</span>) : <span class="built_in">posX</span>(x), <span class="built_in">posY</span>(y), <span class="built_in">radius</span>(r) &#123;&#125;<span class="comment">// 构造方法</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">getX</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> posX; &#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">getY</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> posY; &#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">getRadius</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> radius; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setX</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123; posX = x; &#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setY</span><span class="params">(<span class="type">int</span> y)</span> </span>&#123; posY = y; &#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setRadius</span><span class="params">(<span class="type">int</span> r)</span> </span>&#123; radius = r; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="type">bool</span> running = <span class="literal">true</span>;<span class="comment">// 主循环控制参数</span></span><br><span class="line">ExMessage msg;<span class="comment">// 消息对象</span></span><br><span class="line"><span class="function">Ball <span class="title">ball</span><span class="params">(<span class="number">400</span>, <span class="number">300</span>, <span class="number">50</span>)</span></span>;<span class="comment">// 待绘制对象</span></span><br><span class="line"><span class="comment">//Ball ball;</span></span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">800</span>, <span class="number">600</span>);<span class="comment">// 初始化绘图窗口</span></span><br><span class="line"><span class="built_in">BeginBatchDraw</span>();<span class="comment">// 开启批量绘图</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 主循环</span></span><br><span class="line"><span class="keyword">while</span> (running) &#123;</span><br><span class="line"><span class="comment">// 消息处理</span></span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">peekmessage</span>(&amp;msg)) &#123;</span><br><span class="line"><span class="keyword">if</span> (msg.message == WM_MOUSEMOVE) &#123;<span class="comment">// 圆的位置随鼠标位置变化</span></span><br><span class="line">ball.<span class="built_in">setX</span>(msg.x);</span><br><span class="line">ball.<span class="built_in">setY</span>(msg.y);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (msg.message == WM_LBUTTONDOWN) &#123;<span class="comment">// 左键按下圆变红色</span></span><br><span class="line"><span class="built_in">setfillcolor</span>(RED);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (msg.message == WM_LBUTTONUP) &#123;<span class="comment">// 左键松开圆变白色</span></span><br><span class="line"><span class="built_in">setfillcolor</span>(WHITE);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (msg.message == WM_RBUTTONDOWN) &#123;<span class="comment">// 右键按下结束主循环</span></span><br><span class="line">running = <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 绘图</span></span><br><span class="line"><span class="built_in">cleardevice</span>();<span class="comment">// 清除屏幕</span></span><br><span class="line"><span class="built_in">solidcircle</span>(ball.<span class="built_in">getX</span>(), ball.<span class="built_in">getY</span>(), ball.<span class="built_in">getRadius</span>());<span class="comment">// 绘制当前帧内容</span></span><br><span class="line"><span class="built_in">FlushBatchDraw</span>();<span class="comment">// 刷新批量绘图</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">EndBatchDraw</span>();<span class="comment">// 关闭批量绘图</span></span><br><span class="line"><span class="built_in">closegraph</span>();<span class="comment">// 关闭绘图窗口</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="键盘消息"><a href="#键盘消息" class="headerlink" title="键盘消息"></a>键盘消息</h3><p>范例1：用键盘控制小球</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 用结构体封装小球属性</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">Ball</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> x;<span class="comment">// 小球圆心坐标x</span></span><br><span class="line"><span class="type">int</span> y;<span class="comment">// 小球圆心坐标y</span></span><br><span class="line"><span class="type">int</span> r;<span class="comment">// 小球半径</span></span><br><span class="line"><span class="type">int</span> dx;<span class="comment">// 小球在x轴方向移动的增量</span></span><br><span class="line"><span class="type">int</span> dy;<span class="comment">// 小球在y轴方向移动的增量</span></span><br><span class="line">COLORREF color;<span class="comment">// 小球颜色</span></span><br><span class="line">&#125; Ball;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="type">bool</span> running = <span class="literal">true</span>;</span><br><span class="line">ExMessage msg;</span><br><span class="line">Ball ball = &#123; <span class="number">300</span>, <span class="number">300</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">5</span>, YELLOW &#125;;<span class="comment">// 创建小球并初始化</span></span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">600</span>, <span class="number">600</span>);</span><br><span class="line"><span class="built_in">BeginBatchDraw</span>();</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (running)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">peekmessage</span>(&amp;msg))</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (msg.message == WM_KEYDOWN)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">switch</span> (msg.vkcode)<span class="comment">// 判断虚拟键代码</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;w&#x27;</span>:<span class="comment">// 上键：小球Y坐标减少</span></span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;W&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> VK_UP:</span><br><span class="line">ball.y -= ball.dy;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;s&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;S&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> VK_DOWN:<span class="comment">// 下键：小球Y坐标增加</span></span><br><span class="line">ball.y += ball.dy;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;a&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;A&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> VK_LEFT:<span class="comment">// 右键：小球X坐标减少</span></span><br><span class="line">ball.x -= ball.dx;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;d&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;D&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> VK_RIGHT:<span class="comment">// 右键：小球X坐标增加</span></span><br><span class="line">ball.x += ball.dx;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> VK_ESCAPE:<span class="comment">// ESC键：结束主循环</span></span><br><span class="line">running = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cleardevice</span>();<span class="comment">// 清除屏幕</span></span><br><span class="line"><span class="built_in">setfillcolor</span>(ball.color);<span class="comment">// 设置填充颜色</span></span><br><span class="line"><span class="built_in">solidcircle</span>(ball.x, ball.y, ball.r);<span class="comment">// 绘制无边框填充圆;</span></span><br><span class="line"><span class="built_in">FlushBatchDraw</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">EndBatchDraw</span>();</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>虚拟键代码 <a href="https://learn.microsoft.com/zh-cn/windows/win32/inputdev/virtual-key-codes">https://learn.microsoft.com/zh-cn/windows/win32/inputdev/virtual-key-codes</a></p><p><strong>优化后</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> WIN_WIDTH 600</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> WIN_HEIGHT 600</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 用结构体封装小球属性</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">Ball</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> x;<span class="comment">// 小球圆心坐标x</span></span><br><span class="line"><span class="type">int</span> y;<span class="comment">// 小球圆心坐标y</span></span><br><span class="line"><span class="type">int</span> r;<span class="comment">// 小球半径</span></span><br><span class="line"><span class="type">int</span> dx;<span class="comment">// 小球在x轴方向移动的增量</span></span><br><span class="line"><span class="type">int</span> dy;<span class="comment">// 小球在y轴方向移动的增量</span></span><br><span class="line">COLORREF color;<span class="comment">// 小球颜色</span></span><br><span class="line"><span class="type">bool</span> isMoveUp = <span class="literal">false</span>;<span class="comment">// 小球是否向四个方向移动</span></span><br><span class="line"><span class="type">bool</span> isMoveDown = <span class="literal">false</span>;</span><br><span class="line"><span class="type">bool</span> isMoveLeft = <span class="literal">false</span>;</span><br><span class="line"><span class="type">bool</span> isMoveRight = <span class="literal">false</span>;</span><br><span class="line">&#125; Ball;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="type">bool</span> running = <span class="literal">true</span>;</span><br><span class="line">ExMessage msg;</span><br><span class="line">Ball ball = &#123; <span class="number">300</span>, <span class="number">300</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">5</span>, YELLOW &#125;;<span class="comment">// 创建小球并初始化</span></span><br><span class="line"><span class="built_in">initgraph</span>(WIN_WIDTH, WIN_HEIGHT);</span><br><span class="line"><span class="built_in">BeginBatchDraw</span>();</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (running)</span><br><span class="line">&#123;</span><br><span class="line">DWORD beginTime = <span class="built_in">GetTickCount</span>();<span class="comment">// 记录循环开始时间</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 消息处理</span></span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">peekmessage</span>(&amp;msg))</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (msg.message == WM_KEYDOWN)<span class="comment">// 按下按键处理</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">switch</span> (msg.vkcode)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;w&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;W&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> VK_UP:</span><br><span class="line">ball.isMoveUp = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;s&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;S&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> VK_DOWN:</span><br><span class="line">ball.isMoveDown = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;a&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;A&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> VK_LEFT:</span><br><span class="line">ball.isMoveLeft = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;d&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;D&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> VK_RIGHT:</span><br><span class="line">ball.isMoveRight = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> VK_ESCAPE:</span><br><span class="line">running = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (msg.message == WM_KEYUP)<span class="comment">// 松开按键处理</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">switch</span> (msg.vkcode)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;w&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;W&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> VK_UP:</span><br><span class="line">ball.isMoveUp = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;s&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;S&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> VK_DOWN:</span><br><span class="line">ball.isMoveDown = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;a&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;A&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> VK_LEFT:</span><br><span class="line">ball.isMoveLeft = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;d&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;D&#x27;</span>:</span><br><span class="line"><span class="keyword">case</span> VK_RIGHT:</span><br><span class="line">ball.isMoveRight = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 斜向移动：计算不同方向(包括同时)按下时的速度增量</span></span><br><span class="line"><span class="type">int</span> directX = ball.isMoveRight - ball.isMoveLeft;</span><br><span class="line"><span class="type">int</span> directY = ball.isMoveDown - ball.isMoveUp;</span><br><span class="line"><span class="type">double</span> directXY = <span class="built_in">sqrt</span>(directX * directX + directY * directY);</span><br><span class="line"><span class="keyword">if</span> (directXY != <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="type">double</span> factorX = directX / directXY;<span class="comment">//计算X、Y方向的标准化分量</span></span><br><span class="line"><span class="type">double</span> factorY = directY / directXY;</span><br><span class="line">ball.x += (<span class="type">int</span>)ball.dx * factorX;<span class="comment">//小球坐标 = 方向增速 * 方向的标准化分量</span></span><br><span class="line">ball.y += (<span class="type">int</span>)ball.dy * factorY;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 边缘检测</span></span><br><span class="line"><span class="keyword">if</span> (ball.y - ball.r &lt;= <span class="number">0</span>)<span class="comment">// 上</span></span><br><span class="line">ball.y = ball.r;</span><br><span class="line"><span class="keyword">if</span> (ball.y + ball.r &gt;= WIN_HEIGHT)<span class="comment">// 下</span></span><br><span class="line">ball.y = WIN_HEIGHT - ball.r - <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span> (ball.x - ball.r &lt;= <span class="number">0</span>)<span class="comment">// 左</span></span><br><span class="line">ball.x = ball.r;</span><br><span class="line"><span class="keyword">if</span> (ball.x + ball.r &gt;= WIN_WIDTH)<span class="comment">// 右</span></span><br><span class="line">ball.x = WIN_WIDTH - ball.r - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 绘图</span></span><br><span class="line"><span class="built_in">cleardevice</span>();<span class="comment">// 清除屏幕</span></span><br><span class="line"><span class="built_in">setfillcolor</span>(ball.color);<span class="comment">// 设置填充颜色</span></span><br><span class="line"><span class="built_in">solidcircle</span>(ball.x, ball.y, ball.r);<span class="comment">// 绘制无边框填充圆;</span></span><br><span class="line"><span class="built_in">FlushBatchDraw</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 帧延时</span></span><br><span class="line">DWORD endTime = <span class="built_in">GetTickCount</span>();<span class="comment">// 记录循环结束时间</span></span><br><span class="line">DWORD elapsedTime = endTime - beginTime;<span class="comment">// 计算循环耗时</span></span><br><span class="line"><span class="keyword">if</span> (elapsedTime &lt; <span class="number">1000</span> / <span class="number">60</span>)<span class="comment">// 按每秒60帧进行补时</span></span><br><span class="line"><span class="built_in">Sleep</span>(<span class="number">1000</span> / <span class="number">60</span> - elapsedTime);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">EndBatchDraw</span>();</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="其它函数"><a href="#其它函数" class="headerlink" title="其它函数"></a>其它函数</h2><h3 id="设置窗口标题"><a href="#设置窗口标题" class="headerlink" title="设置窗口标题"></a>设置窗口标题</h3><p>范例：使用 <strong>GetHWnd</strong> 和 <strong>SetWindowText</strong> 函数设置窗口标题</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">600</span>, <span class="number">600</span>);</span><br><span class="line"></span><br><span class="line">HWND hWnd = <span class="built_in">GetHWnd</span>();<span class="comment">// 获得窗口句柄</span></span><br><span class="line"><span class="built_in">SetWindowText</span>(hWnd, _T(<span class="string">&quot;植物大战僵尸&quot;</span>));<span class="comment">// 使用 Windows API 修改窗口名称</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="弹窗消息"><a href="#弹窗消息" class="headerlink" title="弹窗消息"></a>弹窗消息</h3><p>在Visual C++（VC）中，MessageBox 函数是一个常用的 Windows API 函数，用于显示一个模态对话框，其中包含文本、标题、图标和按钮等。以下是函数的详细用法：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">MessageBox</span><span class="params">(  </span></span></span><br><span class="line"><span class="params"><span class="function">  HWND   hWnd,           <span class="comment">// 父窗口句柄。如果为NULL，则消息框没有父窗口  </span></span></span></span><br><span class="line"><span class="params"><span class="function">  LPCTSTR lpText,         <span class="comment">// 要显示的消息文本  </span></span></span></span><br><span class="line"><span class="params"><span class="function">  LPCTSTR lpCaption,      <span class="comment">// 消息框的标题  </span></span></span></span><br><span class="line"><span class="params"><span class="function">  UINT    uType           <span class="comment">// 指定消息框的内容和行为的标志  </span></span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>;</span><br></pre></td></tr></table></figure><p><strong>参数说明</strong></p><ol><li><strong>hWnd</strong>：指定消息框的父窗口句柄。如果此参数为NULL，则消息框没有父窗口，且作为顶级窗口显示。</li><li><strong>lpText</strong>：要在消息框中显示的文本。</li><li><strong>lpCaption</strong>：消息框的标题。如果此参数为NULL，则默认标题为“Error”。</li><li><strong>uType</strong>：用于指定消息框的内容和行为的标志。这可以是一个或多个以下常量的组合：<ul><li>MB_OK：消息框包含一个“确定”按钮。</li><li>MB_OKCANCEL：消息框包含“确定”和“取消”按钮。</li><li>MB_YESNO：消息框包含“是”和“否”按钮。</li><li>MB_YESNOCANCEL：消息框包含“是”、“否”和“取消”按钮。</li><li>MB_ICONEXCLAMATION、MB_ICONWARNING、MB_ICONINFORMATION、MB_ICONQUESTION、MB_ICONERROR等：用于指定消息框中显示的图标。</li></ul></li></ol><p><strong>返回值</strong></p><p>函数返回一个整数值，表示用户点击的按钮。例如：</p><ul><li><strong>IDOK</strong>：用户点击了“确定”按钮。</li><li><strong>IDCANCEL</strong>：用户点击了“取消”按钮。</li><li><strong>IDYES</strong>：用户点击了“是”按钮。</li><li><strong>IDNO</strong>：用户点击了“否”按钮。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">1000</span>, <span class="number">600</span>);</span><br><span class="line"></span><br><span class="line">HWND hWnd = <span class="built_in">GetHWnd</span>();</span><br><span class="line"><span class="built_in">MessageBox</span>(hWnd, _T(<span class="string">&quot;你被僵尸吃掉了！&quot;</span>), _T(<span class="string">&quot;游戏结束&quot;</span>), MB_OK | MB_ICONERROR);</span><br><span class="line"></span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注：在使用 MessageBox 函数之前，需要包含 <strong>windows.h</strong> 头文件（<strong>如果已经包含了 graphics.h 头文件则可以省略</strong>）</p><h3 id="播放音频"><a href="#播放音频" class="headerlink" title="播放音频"></a>播放音频</h3><p><strong>mciSendString</strong>  是 Windows API 中的一个函数，用于向媒体控制接口（Media Control Interface，MCI）设备发送命令字符串。这个函数常用于控制多媒体设备，如音频和视频播放，支持 MPEG, AVI, WAV, MP3 等多种格式。</p><p>范例：播放背景音乐</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;graphics.h&gt;</span></span></span><br><span class="line"><span class="comment">//#include &lt;windows.h&gt;// 此项在导入graphics.h头文件后可以省略</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> comment(lib, <span class="string">&quot;winmm.lib&quot;</span>)<span class="comment">// 加载多媒体静态库</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">initgraph</span>(<span class="number">1000</span>, <span class="number">600</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">mciSendString</span>(_T(<span class="string">&quot;open audio\\bg.mp3 alias BGM&quot;</span>), <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);<span class="comment">// 打开音乐文件，alias指定别名</span></span><br><span class="line"><span class="built_in">mciSendString</span>(_T(<span class="string">&quot;play BGM repeat&quot;</span>), <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);<span class="comment">// 使用别名播放音乐，repeat重复播放</span></span><br><span class="line"></span><br><span class="line">IMAGE img;</span><br><span class="line"><span class="built_in">loadimage</span>(&amp;img, _T(<span class="string">&quot;image\\background.jpg&quot;</span>));</span><br><span class="line"><span class="built_in">putimage</span>(<span class="number">0</span>, <span class="number">0</span>, &amp;img);</span><br><span class="line"></span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="built_in">closegraph</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="随堂练习"><a href="#随堂练习" class="headerlink" title="随堂练习"></a>随堂练习</h1><h3 id="练习1：鼠标-键盘绘图"><a href="#练习1：鼠标-键盘绘图" class="headerlink" title="练习1：鼠标+键盘绘图"></a>练习1：鼠标+键盘绘图</h3><p>功能要求：</p><ol><li><p>鼠标按下左键画 10*10 的正方形、按下右键画半径为 10 的圆（鼠标点击的位置为图形中心）</p></li><li><p>同时按下 Ctrl 键和鼠标左、右键，分别画 20*20 的正方形和半径为 20 的圆</p></li><li><p>按下键盘 C 键清屏、R&#x2F;G&#x2F;B&#x2F;W 键分别修改画图颜色为红、绿、蓝、白，ESC 键退出</p></li></ol><h3 id="练习2：弹球"><a href="#练习2：弹球" class="headerlink" title="练习2：弹球"></a>练习2：弹球</h3><p>功能要求：</p><ol><li>一个自由运动的小球，碰到左、右、上边界会反弹，碰到下边界结束程序</li><li>一个键盘控制的挡板，挡板不能超出屏幕范围，小球碰到后会反弹</li><li>按P键可以切换画面暂停&#x2F;继续</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>海思赛道嵌赛开发一个一个日记</title>
      <link href="/2025/05/29/%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/"/>
      <url>/2025/05/29/%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>海思赛道嵌赛开发一个一个日记,玩一玩试试水.</p><h1 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题:"></a>常见问题:</h1><ul><li>osal_print<strong>k(不是f)</strong></li></ul><h1 id="全国大学生嵌入式芯片与系统设计竞赛-芯片应用赛道选题指南"><a href="#全国大学生嵌入式芯片与系统设计竞赛-芯片应用赛道选题指南" class="headerlink" title="全国大学生嵌入式芯片与系统设计竞赛(芯片应用赛道选题指南)"></a><strong>全国大学生嵌入式芯片与系统设计竞赛(<strong>芯片应用赛道选题指南</strong>)</strong></h1><p><strong>星闪物联网应用平台</strong></p><p>以下仅介绍WS63E相关</p><p><strong>关键特性</strong>:高性能 32bit 微处理器、<strong>2.4GHz WiFi 6、SLE、BLE三模</strong>、丰富的外设接口，其中增强款WS63E 支持2.4GHz 的<strong>雷达人体活动检测</strong>功能</p><p><strong>应用场景</strong>:智慧家居、雷达感知、星闪网关、星闪中控屏、星闪手柄等</p><p><strong>注：<strong>星闪（NearLink），是中国原生的新一代</strong>无线短距通信</strong>技术。与传统短距传输技术方案相比，星闪在功耗、速度、覆盖范围和连接性能全面领先，可以在智能终端、智能家居、智能汽车、智能制造等各类细分场景下实现更极致的用户体验。</p><p>1.操作系统可使用 LiteOS 或 OpenHarmony 版本。</p><p>2、如采用星闪 WS63 或 WS63E 或 BS21，并<strong>发挥星闪技术特性</strong>可酌情加分。</p><p>3、本选题重点考察参赛选手的<strong>嵌入式系统开发能力</strong>, SLE&#x2F;BLE&#x2F;WiFi <strong>多端互联能力</strong>。</p><p>4、参赛选手须具备基础的 C 语言编码能力，了解物联网技术及应用相关知识。</p><p>5、本选题学习资料可参考《2025 年嵌入式竞赛海思赛道学习入口》，并知晓开发环境要求及套件功能限制。</p><h1 id="学长思路总结"><a href="#学长思路总结" class="headerlink" title="学长思路总结"></a>学长思路总结</h1><p><strong>主控选型上</strong>:一定要根据你的作品功能参数需求而选择正确的主控,最好要利用到星闪的功能</p><p><strong>作品创新性上</strong>:利用他们的芯片（这个很重要）做一些让他们眼前一亮的设计，利用<strong>芯片自身的一些特性和外设</strong>来做一些全新的应用（最好是能<strong>贴近产品</strong>的）。很重要,值得好好考量.可能会比较喜欢一些自动化功能,尽量一键完成那种</p><p><strong>功能实现度上</strong>:不是说功能越高大上，越多就越好,做了就正常展示,一定要符合<strong>产品使用实际</strong>.想好<strong>什么场景用什么协议</strong>，每个<strong>通信协议的缺点和优点</strong>要清楚.并且，尽量使用<strong>芯片板载的</strong>一些硬件外设，能全部用上最好（工作量体现）.</p><p>另外，毕竟是物联网赛道，而且是华为海思的关系，尽量加入<strong>一些互联功能（用华为云最好）</strong>，比如利用<strong>小程序控制</strong>，或者<strong>手机 NFC 与板子交互</strong>，<strong>星闪交互</strong>等等。</p><p>学习之初先根据 <strong>gitee 上的开源案例</strong>进行实现并改进学习（案例链接：doc&#x2F;物联网技术及应用实验指导手册.mdHiSpark&#x2F;hi3861_hdu_iot_application - Gitee.com）。在海思<strong>社区</strong>(<a href="https://developer.hisilicon.com/forum/all)%E4%B8%8A**%E5%AF%BB%E6%89%BE%E7%96%91%E9%9A%BE%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3">https://developer.hisilicon.com/forum/all)上**寻找疑难问题的解</a>**</p><p><strong>决方案</strong>。提前熟悉<strong>华为云平台</strong>的使用.学习制作<strong>简易的微信小程序或手机 app</strong>.</p><h1 id="代码开源提交流程"><a href="#代码开源提交流程" class="headerlink" title="代码开源提交流程"></a>代码开源提交流程</h1><ul><li><p>2025年嵌入式大赛海思赛道代码开源提交流程</p><h2 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h2><p>  此仓库主要用于2025年嵌入式大赛学生作品代码开源，欢迎将自己的作品提交到本仓库。相关提交请遵守git的具体操作，并满足各个仓库的具体规范要求。</p><h2 id="2、仓库目录介绍"><a href="#2、仓库目录介绍" class="headerlink" title="2、仓库目录介绍"></a>2、仓库目录介绍</h2><ul><li>在2025_hisilicon_embedded_competition目录下，一共有两个文件夹，其中AIOT文件夹是海思赛道 AI端侧智能方向的队伍提交代码的目录，IOT文件夹是海思赛道 星闪物联网方向的队伍提交代码的目录。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/1711003179840-1752135035476-85.png" alt="1711003179840"></p><ul><li>参赛队伍请根据自己的比赛方向，进入对应的文件夹中，然后 用不同的文件夹来区分不同的组别，文件命名规则为团队编号_作品名称</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/1711003320865-1752135035476-86.png" alt="1711003320865"></p><ul><li>进到每个团队的目录下，我们需要将关键代码上传到code 目录，还需要撰写README.md 文档，README.md的主要内容是关于你们作品的大致描述以及对整个code目录中的文件进行介绍。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/1711003361709-1752135035476-87.png" alt="1711003361709"></p><h2 id="3、-git简介"><a href="#3、-git简介" class="headerlink" title="3、 git简介"></a>3、 git简介</h2><ul><li><p>关于git的历史和原理，笔者找了一个介绍比较好的文章，大家可以参考<a href="https://gitee.com/link?target=https://zhuanlan.zhihu.com/p/66506485">git原理</a> 对于初学者而言，下面的图来理解更直观。 <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/git%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B.png" alt="img"></p></li><li><p>1.fork：指的是从官网仓库中复制一份拷贝到自己的账号仓库下，在这个时间节点下两者的内容一致；后续需要不断的手动完成同步；</p></li><li><p>2.clone:指的是从自己的账号仓库下下载到本地端;</p></li><li><p>3.commit:指的是将克隆的代码，根据需要修改更正某些内容或者增加新内容、删除冗余内容，形成记录。</p></li><li><p>4.push:指的是将自己的修改提交到本人账号仓库下；</p></li><li><p>5.pr:指的是将自己的修改从自己的账号仓库下提交到官方账号仓库下；</p></li><li><p>6.merge:指的是官方账号仓库的commiter接受了你的修改；</p></li><li><p>7.fetch:指的是将官方账号仓库的内容拉取到本地。</p></li></ul><h2 id="4、提交作品流程实操"><a href="#4、提交作品流程实操" class="headerlink" title="4、提交作品流程实操"></a>4、提交作品流程实操</h2><h3 id="4-1、创建gitee账号"><a href="#4-1、创建gitee账号" class="headerlink" title="4.1、创建gitee账号"></a>4.1、创建gitee账号</h3><ul><li>首先你准备一个自己的手机以及邮箱，为方便后续的操作方便，该手机号码以及邮箱没有和gitee平台发生关联。</li><li>在<a href="https://gitee.com/">gitee官网</a>完成注册 <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/gitee%E5%AE%98%E7%BD%91.png" alt="img"></li><li>根据自己情况设置信息，图里所示个人空间地址很重要，它就是你的用户名。姓名可以是相同的，但用户名是唯一的。 <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/%E6%B3%A8%E5%86%8C%E4%BF%A1%E6%81%AF%E5%A1%AB%E5%86%99.png" alt="img"></li><li><strong>如无意外则创建成功</strong> <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/%E8%B4%A6%E5%8F%B7%E5%88%9B%E5%BB%BA%E6%88%90%E5%8A%9F.png" alt="img"></li></ul><h3 id="4-2、gitee账号绑定邮箱"><a href="#4-2、gitee账号绑定邮箱" class="headerlink" title="4.2、gitee账号绑定邮箱"></a>4.2、gitee账号绑定邮箱</h3><ul><li>在注册账号的时候，有些信息我们是没有补充的，基本信息基本只有电话号和密码，能够满足我们的登录并使用该账号“发言”。那么问题来了，为了让我们后续发言更有说服力，我们需要补充相关的信息。邮箱作为互联网的比较典型的通用信息，需要补充完善</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230703180601641-1752135035476-88.png" alt="image-20230703180601641"></p><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230703180824689-1752135035476-89.png" alt="image-20230703180824689"></p><ul><li>按照图示补充邮箱信息，并设置自己的提交邮箱。为自己后续的打怪升级做准备，人民会记住你的贡献的。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230703180908605-1752135035476-90.png" alt="image-20230703180908605"></p><h3 id="4-3、Fork官方仓库"><a href="#4-3、Fork官方仓库" class="headerlink" title="4.3、Fork官方仓库"></a>4.3、Fork官方仓库</h3><ul><li>所谓Fork，就是把官方仓库当前时间点内容搬迁到自己账号下面，直接在网页上操作即可完成。如我们把赛事活动仓库Fork到自己账号下面。<a href="https://gitee.com/HiSpark/2025_hisilicon_embedded_competition/tree/master">活动仓库的官方地址。</a></li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/1711003809247-1752135035476-91.png" alt="1711003809247"></p><ul><li>fork之后，在我们的gitee账号就可以看到这个仓库啦。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/1711003918652-1752135035476-92.png" alt="1711003918652"></p><h3 id="4-4、安装GIT客户端工具：gitbash"><a href="#4-4、安装GIT客户端工具：gitbash" class="headerlink" title="4.4、安装GIT客户端工具：gitbash"></a>4.4、安装GIT客户端工具：gitbash</h3><ul><li>Windows 环境下建议大家使用命令行的工具，如果你是MACOS或者Linux,我相信你使用起来会更简单，此处不表，本文仅仅以WINDOWS环境介绍。 可以从<a href="https://gitee.com/link?target=https://git-scm.com/download/win">git bash下载地址</a>下载git bash工具并安装。 安装完毕之后，在你的工作目录下右键点击即可出现git bash here。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705160602737-1752135035476-93.png" alt="image-20230705160602737"></p><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230703203430552-1752135035476-94.png" alt="image-20230703203430552"></p><ul><li>点击启动 git bash 之后会进入一个linux终端的界面，这就是我们后续将修改内容从本地上传到</li><li>Gitee上的个人仓库的主要战场了。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230703203500743-1752135035476-95.png" alt="image-20230703203500743"></p><h3 id="4-5、设置git客户端"><a href="#4-5、设置git客户端" class="headerlink" title="4.5、设置git客户端"></a>4.5、设置git客户端</h3><ul><li>使用SSH公钥可以让你在你的电脑和 Gitee 通讯的时候使用安全连接。 那么怎么获取到我们PC的SSH公钥呢？在桌面右键打开git bash</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230703203430552-1752135035476-94.png" alt="image-20230703203430552"></p><ul><li>输入<strong>ssh-keygen.exe</strong> 并回车，再次回车，然后输入y，继续回车两次，这样即可生成个人的SSH公钥保持文件。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230703203644726-1752135035476-96.png" alt="image-20230703203644726"></p><ul><li>git无法直接ctrl+c&#x2F;v实现复制粘贴，但可以鼠标选中ssh公钥保持文件（即Your public key has been saved in 后面的内容）然后右键Copy复制，Paste粘贴实现这个功能。</li><li>使用cat命令查看生成的id_rsa.pub文件，输入cat （右键Paste粘贴ssh公钥保持文件）回车即可查看具体信息。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230703203733019-1752135035476-97.png" alt="image-20230703203733019"></p><ul><li>从ssh-rsa开始，整段选中然后复制，打开gitee官网在设置里面找到ssh公钥，粘贴确定即可将公钥添加到我们的gitee账号中。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230703203809351-1752135035476-98.png" alt="image-20230703203809351"></p><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230703203840145-1752135035476-99.png" alt="image-20230703203840145"></p><h3 id="4-6、配置个人信息"><a href="#4-6、配置个人信息" class="headerlink" title="4.6、配置个人信息"></a>4.6、配置个人信息</h3><ul><li>我们向gitee个人仓库提交修改内容，需要告知大家这些修改内容是谁发起提交的，不然大家怎么知道是哪位英雄好汉为开源社区出了力。所以为了避免每次都重复输入一些提交信息（个人账号信息），我们需要使用git bash统一配置一下提交信息。</li><li>首先，先记住自己的个人空间地址，在个人主页的网页链接上可看到，我的用户名就是 “wgm2022”</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/1711004003189-1752135035477-100.png" alt="1711004003189"></p><ul><li>打开git bash，依次输入以下命令并回车，前两个命令没有反应就证明配置成功。</li></ul>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;xxxx&quot;   （配置用户名，xxxx为账号用户名，即个人空间地址）</span><br><span class="line"></span><br><span class="line">git config --global user.email &quot;xxxxxx@xxx&quot;   // 与你的gitee 账号邮箱和你签署DCO 的邮箱保持一致即可  </span><br><span class="line"></span><br><span class="line">git config --list         （查看配置情况）</span><br></pre></td></tr></table></figure><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230703204144932-1752135035477-101.png" alt="image-20230703204144932"></p><h3 id="4-7、克隆仓库内容到本地"><a href="#4-7、克隆仓库内容到本地" class="headerlink" title="4.7、克隆仓库内容到本地"></a>4.7、克隆仓库内容到本地</h3><ul><li>到个人账号点击并进入这个<strong>2025_hisilicon_embedded_competition</strong>仓库，点击克隆&#x2F;下载按钮，复制clone的链接地址。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/1711004092314-1752135035477-102.png" alt="1711004092314"></p><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/1711004068507-1752135035477-103.png" alt="1711004068507"></p><ul><li>在git bash工具下面使用git clone命令完成clone动作。</li></ul>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://gitee.com/RXCCCCCC/2025_hisilicon_embedded_competition.git --depth=1</span><br></pre></td></tr></table></figure><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/1711004982366-1752135035477-104.png" alt="1711004982366"></p><ul><li>–depth&#x3D;1意思是只clone当前仓库最新版本，省去一些历史log，避免仓库历史记录过于庞大花费太多clone时间。需要注意的是开发者需要克隆自己账号下的仓库，原则上这个地址构成如下.</li></ul>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://gitee.com/账号名/仓库名.git  --depth=1</span><br></pre></td></tr></table></figure><ul><li>clone完毕之后，即可在本地目录下看到这个clone的仓库。补充说明一下，本地目录所在位置是根据git bash的位置决定的，比如你在桌面启动git bash，则clone的仓库会出现在桌面。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/1711005091142-1752135035477-105.png" alt="1711005091142"></p><h3 id="4-8、添加或修改文件"><a href="#4-8、添加或修改文件" class="headerlink" title="4.8、添加或修改文件"></a>4.8、添加或修改文件</h3><ul><li>按照要求增加目录、文件，或者修改部分文件内容。</li><li>首先在2025_hisilicon_embedded_competition目录下，按照文件命名规则：<strong>团队编号_作品名称</strong> 新建你们团队的个人文件夹</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/1711003320865-1752135035476-86.png" alt="1711003320865"></p><ul><li>进到每个团队的目录下，你们需要将关键代码上传进来，还需要撰写README.md 文档，README.md的主要内容是关于你们作品的大致描述以及对整个code目录中的文件进行介绍。</li><li>比如说，我的队伍编号是99999，然后我们的作品是手势识别，那我创建的文件的名字为99999_hand_classify，然后我Taurus开发板 只修改了ai_sample的hand_classify文件夹里面的代码，以及smp目录下的代码。Pegasus开发板只使用并修改了uart_demo里面的代码。因此我提供的主要代码如下图所示。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705162657919-1752135035477-106.png" alt="image-20230705162657919"></p><ul><li>readme.md里面的内容，如下图所示进行描述即可。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705163903688-1752135035477-107.png" alt="image-20230705163903688"></p><h3 id="4-9、开始提交"><a href="#4-9、开始提交" class="headerlink" title="4.9、开始提交"></a>4.9、开始提交</h3><h4 id="1、查看修改变更后的文件"><a href="#1、查看修改变更后的文件" class="headerlink" title="1、查看修改变更后的文件"></a>1、查看修改变更后的文件</h4>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd   2025_hisilicon_embedded_competition</span><br><span class="line"></span><br><span class="line">git status</span><br></pre></td></tr></table></figure><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705164304336-1752135035477-108.png" alt="image-20230705164304336"></p><h4 id="2、将变更文件加入到暂存区"><a href="#2、将变更文件加入到暂存区" class="headerlink" title="2、将变更文件加入到暂存区"></a>2、将变更文件加入到暂存区</h4>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add *</span><br></pre></td></tr></table></figure><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705164633348-1752135035477-109.png" alt="image-20230705164633348"></p><h4 id="3、将暂存区内容签名并提交到本地"><a href="#3、将暂存区内容签名并提交到本地" class="headerlink" title="3、将暂存区内容签名并提交到本地"></a>3、将暂存区内容签名并提交到本地</h4>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -s -m  &quot; add:12563_Intelligent_Environmental_Sensing_Car init&quot;</span><br></pre></td></tr></table></figure><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705164815165-1752135035477-110.png" alt="image-20230705164815165"></p><ul><li>最后我们再用git status查看一下，可看到已没有修改变更内容存在了。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705164909408-1752135035477-111.png" alt="image-20230705164909408"></p><h4 id="4、推送本地修改到账号仓库"><a href="#4、推送本地修改到账号仓库" class="headerlink" title="4、推送本地修改到账号仓库"></a>4、推送本地修改到账号仓库</h4><ul><li>现在我们需要将本地仓库的修改内容推送到gitee上的个人仓库，使用git push命令来完成这个动作。origin指的是自己的仓库对应的原始远程服务器地址；master标识的是想要提交的分支</li></ul>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin master</span><br></pre></td></tr></table></figure><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705165054837-1752135035477-112.png" alt="image-20230705165054837"></p><h3 id="4-10、开始提交PR到海思仓库"><a href="#4-10、开始提交PR到海思仓库" class="headerlink" title="4.10、开始提交PR到海思仓库"></a>4.10、开始提交PR到海思仓库</h3><h4 id="1、检查更新"><a href="#1、检查更新" class="headerlink" title="1、检查更新"></a>1、检查更新</h4><ul><li>进入我们的账号下面，我们查看这个仓库，发现已经发生了变化</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705165318270-1752135035477-113.png" alt="image-20230705165318270"></p><ul><li>进入到你创建的文件夹目录下，并检查一下我们需要提交的代码和文档是否都在里面。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705165549580-1752135035477-114.png" alt="image-20230705165549580"></p><h4 id="2、从个人账号仓库下先海思官方仓库提交PR"><a href="#2、从个人账号仓库下先海思官方仓库提交PR" class="headerlink" title="2、从个人账号仓库下先海思官方仓库提交PR"></a>2、从个人账号仓库下先海思官方仓库提交PR</h4><ul><li>进入个人账号的该仓库下，点击增加<strong>Pull Request</strong>即可开始提交PR。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705165844868-1752135035477-115.png" alt="image-20230705165844868"></p><ul><li>在标题框内输入 你们团队编号+你们作品的描述，然后再点击 Pull Request按钮，提交PR。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705170048187-1752135035477-116.png" alt="image-20230705170048187"></p><ul><li>提交成功之后，等待海思工程师审查。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705170115055-1752135035477-117.png" alt="image-20230705170115055"></p><ul><li>如果你提交完后，出现下面的提示需要你签署贡献值协议的话，就点击此超链接，按照提示进行操作。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20250704173534873-1752135035477-118.png" alt="image-20250704173534873"></p><ul><li>海思工程师审查通过之后，就可以看到如下图所示，会显示已合并，并且审查和测试的状态会显示已完成</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705172643773-1752135035477-119.png" alt="image-20230705172643773"></p><ul><li>进入到海思的<a href="https://gitee.com/HiSpark/2025_hisilicon_embedded_competition">官网代码仓</a>，然后进入到属于你们队伍的那个文件夹中。</li><li>此时复制浏览器网址中的内容。</li></ul><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705172836323-1752135035477-120.png" alt="image-20230705172836323"></p><h3 id="3、复制海思官网gitee代码仓中属于你们队伍提交的代码的网址到组委会的作品上传的位置"><a href="#3、复制海思官网gitee代码仓中属于你们队伍提交的代码的网址到组委会的作品上传的位置" class="headerlink" title="3、复制海思官网gitee代码仓中属于你们队伍提交的代码的网址到组委会的作品上传的位置"></a>3、复制海思官网gitee代码仓中属于你们队伍提交的代码的网址到组委会的作品上传的位置</h3><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705173808500-1752135035477-121.png" alt="image-20230705173808500"></p><h3 id="注意：-如果我们没有及时合并你的代码到我们代码仓，你可以将自己代码仓的路径复制到组委会的作品上传位置也是可以的。"><a href="#注意：-如果我们没有及时合并你的代码到我们代码仓，你可以将自己代码仓的路径复制到组委会的作品上传位置也是可以的。" class="headerlink" title="**注意：**如果我们没有及时合并你的代码到我们代码仓，你可以将自己代码仓的路径复制到组委会的作品上传位置也是可以的。"></a>**注意：**如果我们没有及时合并你的代码到我们代码仓，你可以将自己代码仓的路径复制到组委会的作品上传位置也是可以的。</h3><p>  <img src="/./%E6%B5%B7%E6%80%9D%E8%B5%9B%E9%81%93%E5%B5%8C%E8%B5%9B%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%97%A5%E8%AE%B0/image-20230705173808501-1752135035477-122.png" alt="image-20230705173808500"></p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>进行linux命令行大全的一个一个读</title>
      <link href="/2025/05/15/%E8%BF%9B%E8%A1%8Clinux%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%A4%A7%E5%85%A8%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AF%BB/"/>
      <url>/2025/05/15/%E8%BF%9B%E8%A1%8Clinux%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%A4%A7%E5%85%A8%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p><strong>Ubuntu apt</strong> <strong>常用命令</strong></p><p>列出所有可更新的软件清单命令：<strong>sudo apt update</strong></p><p>升级软件包：<strong>sudo apt upgrade</strong></p><p>列出可更新的软件包及版本信息：<strong>apt list –upgradeable</strong></p><p>升级软件包，升级前先删除需要更新软件包：<strong>sudo apt full-upgrade</strong></p><p>安装指定的软件命令：<strong>sudo apt install <package_name></strong></p><p>安装多个软件包：<strong>sudo apt install <package1> <package2> <package_3></strong></p><p>更新指定的软件命令：<strong>sudo apt update <package_name></strong></p><p>显示软件包具体信息, 例如：版本号，安装大小，依赖关系等等：<strong>sudo apt</strong></p><p><strong>show <package_name></strong></p><p>删除软件包命令：<strong>sudo apt remove <package_name></strong></p><p>清理不再使用的依赖和库文件: <strong>sudo apt autoremove</strong></p><p>移除软件包及配置文件: <strong>sudo apt purge <package_name></strong></p><p>查找软件包命令：<strong>sudo apt search</strong></p><p>列出所有已安装的包：<strong>apt list –installed</strong></p><p>列出所有已安装的包的版本信息：<strong>apt list –all-versions</strong></p><h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 | 引言"></a><strong>1</strong> <strong>|</strong> 引言</h1><h2 id="1-1-为什么使用命令行"><a href="#1-1-为什么使用命令行" class="headerlink" title="1.1 为什么使用命令行"></a><strong>1.1</strong> 为什么使用命令行</h2><p>现在，大多数的计算机用户只是熟悉图形用户界面（GUI），命令行界面（CLI）是过去使用的一种很恐怖的东西。</p><p>这本书共分为五部分，每一部分讲述了不同方面的命令行知识。除了第一部分，也就是你正在阅读的这一部分，这本书还包括：</p><p>• 第二部分—学习 shell 开始探究命令行基本语言，包括命令组成结构，文件系统浏览，编写命令行，查找命令帮助文档。</p><p>• 第三部分—配置文件及环境讲述了如何编写配置文件，通过配置文件，用命令行来操控计算机。</p><p>• 第四部分—常见任务及主要工具探究了许多命令行经常执行的普通任务。类似于 Unix 的操作系统，例如 Linux, 包括许多经典的命令行程序，这些程序可以用来对数据进行强大的操作。</p><p>• 第五部分—编写 Shell 脚本介绍了 shell 编程，一个无可否认的基本技能，能够自动化许多常见的计算任务，很容易学。通过学习 shell 编程，你会逐渐熟悉一些关于编程语言方面的概念，这些概念也适用于其他的编程语言。</p><p>从技术层面讲，Linux 只是操作系统的内核名字，没别的含义。当然内核非常重要，因为有它，操作系统才能运行起来，但它并不能构成一个完备的操作系统。</p><h1 id="2-什么是-shell"><a href="#2-什么是-shell" class="headerlink" title="2 | 什么是 shell"></a><strong>2</strong> <strong>|</strong> 什么是 <strong>shell</strong></h1><p>一说到命令行，我们真正指的是 shell。<strong>shell 就是一个程序</strong>，它<strong>接受从键盘输入的命令</strong>，然后把命令传递给操作系统去执行。几乎所有的 Linux 发行版都提供一个<strong>名为 bash 的来自 GNU项目的 shell 程序</strong>。<strong>“bash”是“Bourne Again SHell”的首字母缩写</strong>，所指的是这样一个事实，bash 是最初 Unix 上由 Steve Bourne 写成 shell 程序 <strong>sh 的增强版</strong>。</p><h2 id="2-1-终端仿真器"><a href="#2-1-终端仿真器" class="headerlink" title="2.1 终端仿真器"></a><strong>2.1</strong> 终端仿真器</h2><p>当<strong>使用图形用户界面时</strong>，我们需要另一个<strong>和 shell 交互的叫做终端仿真器的程序</strong>。如果我们浏览一下桌面菜单，可能会找到一个。虽然在菜单里它可能都被简单地称为**“terminal”**，但是 KDE用的是 konsole , 而 GNOME 则使用 gnome-terminal。还有其他一些终端仿真器可供 Linux 使用，但基本上，它们都完成同样的事情，让我们能访问 shell。也许，你可能会因为附加的一系列花俏功能而喜欢上某个终端。</p><h2 id="2-2-第一次按键"><a href="#2-2-第一次按键" class="headerlink" title="2.2 第一次按键"></a><strong>2.2</strong> 第一次按键</h2><p>好，开始吧。启动终端仿真器！一旦它运行起来，我们应该看到一行像这样的文字：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[me@linuxbox ~]$</span><br></pre></td></tr></table></figure><p>这叫做 <strong>shell 提示符</strong>，无论何时当 shell 准备好了去接受输入时，它就会出现。然而，它可能会以各种各样的面孔显示，这则取决于不同的 Linux 发行版，它通常<strong>包括你的用户名 @ 主机名</strong>，<strong>紧接着当前工作目录（稍后会有更多介绍）和一个美元符号</strong>。如果提示符的最后一个字符<strong>是“#”, 而不是“$”</strong>, 那么这个终端会话就有<strong>超级用户权限</strong>。</p><p>这意味着，我们或者是<strong>以 root 用户的身份登录</strong>，或者是我们选择的终端仿真器提供超级用户（管理员）权限。</p><h2 id="2-3-命令历史"><a href="#2-3-命令历史" class="headerlink" title="2.3 命令历史"></a><strong>2.3</strong> 命令历史</h2><p>如果按下上箭头按键，我们会看到刚才输入的命令“kaekfjaeifj”重新出现在提示符之后。这就</p><p>叫做命令历史。许多 Linux 发行版默认保存最后输入的 500 个命令。</p><h2 id="2-4-移动光标"><a href="#2-4-移动光标" class="headerlink" title="2.4 移动光标"></a><strong>2.4</strong> 移动光标</h2><p>可借助<strong>上箭头按键</strong>，来获得<strong>上次输入</strong>的命令。现在试着使用左右箭头按键。看一下怎样把光标定位到命令行的任意位置？通过使用箭头按键，使编辑命令变得轻松些。</p><h2 id="2-5-关于鼠标和光标"><a href="#2-5-关于鼠标和光标" class="headerlink" title="2.5 关于鼠标和光标"></a><strong>2.5</strong> 关于鼠标和光标</h2><p>虽然，shell 是和键盘打交道的，但你也<strong>可以在终端仿真器里使用鼠标</strong>。X 窗口系统（使 GUI工作的底层引擎）内建了一种机制，支持<strong>快速拷贝和粘贴</strong>技巧。</p><p><strong>注意：不要在一个终端窗口里使用 Ctrl-c 和 Ctrl-v 快捷键</strong>来执行拷贝和粘贴操作。它们不起作用。对于 shell 来说，这两个控<strong>制代码有着不同的含义</strong>，它们在早于 Microsoft Windows（定义复制粘贴的含义）许多年之前就赋予了不同的意义。</p><p>你的图形桌面环境（像 KDE 或 GNOME），努力想和 Windows 一样，可能会把它的聚焦策略设置成“单击聚焦”。这意味着，为了<strong>让窗口聚焦（变成活动窗口）你需要单击</strong>它。这与“<strong>聚焦跟随着鼠标</strong>”的<strong>传统 X 行为</strong>不同，传统 X 行为是指只要把鼠标移动到一个窗口的上方。它能接受输入，但是直到你单击窗口之前它都<strong>不会成为前端窗口</strong>。设置聚焦策略为**“聚焦跟随着**</p><p><strong>鼠标”，可以使拷贝和粘贴更方便易用</strong>。尝试一下。我想如果你试了一下你会喜欢上它的。你能在窗口管理器的配置程序中找到这个设置。</p><h2 id="试试运行一些简单命令"><a href="#试试运行一些简单命令" class="headerlink" title="试试运行一些简单命令"></a>试试运行一些简单命令</h2><p>现在，我们学习了怎样输入命令，那我们执行一些简单的命令吧。第一个命令是 date。这个命令显示系统当前时间和日期。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[me@linuxbox ~]$ <span class="built_in">date</span></span><br><span class="line">Thu Oct 25 13:51:54 EDT 2007</span><br></pre></td></tr></table></figure><p>一个相关联的命令，cal，它默认显示当前月份的日历</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[me@linuxbox ~]$ cal</span><br><span class="line">October 2007</span><br><span class="line">Su Mo Tu We Th Fr Sa</span><br><span class="line">1 2 3 4 5 6</span><br><span class="line">7 8 9 10 11 12 13</span><br><span class="line">14 15 16 17 18 19 20</span><br><span class="line">21 22 23 24 25 26 27</span><br><span class="line">28 29 30 31</span><br></pre></td></tr></table></figure><p>查看磁盘剩余空间的数量，输入 df:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[me@linuxbox ~]$ <span class="built_in">df</span></span><br><span class="line">Filesystem 1K-blocks Used Available Use% Mounted on</span><br><span class="line">/dev/sda2 15115452 5012392 9949716 34% /</span><br><span class="line">/dev/sda5 59631908 26545424 30008432 47% /home</span><br><span class="line">/dev/sda1 147764 17370 122765 13% /boot</span><br><span class="line">tmpfs 256856 0 256856 0% /dev/shm</span><br></pre></td></tr></table></figure><p>同样地，显示空闲内存的数量，输入命令 free。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[me@linuxbox ~]$ free</span><br><span class="line">total used free shared buffers cached</span><br><span class="line">Mem: 2059676 846456 1213220 0</span><br><span class="line">44028 360568</span><br><span class="line">-/+ buffers/cache: 441860 1617816</span><br><span class="line">Swap: 1042428 0 1042428</span><br></pre></td></tr></table></figure><h2 id="结束终端会话"><a href="#结束终端会话" class="headerlink" title="结束终端会话"></a>结束终端会话</h2><p>我们可以通过关闭终端仿真器窗口，或者是在 shell 提示符下输入 exit 命令来终止一个终端会话:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[me@linuxbox ~]$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure><h1 id="3-文件系统中跳转"><a href="#3-文件系统中跳转" class="headerlink" title="3 | 文件系统中跳转"></a><strong>3</strong> <strong>|</strong> 文件系统中跳转</h1><p>• pwd —打印出当前工作目录名</p><p>• cd —更改目录</p><p>• ls —列出目录内容</p><h2 id="理解文件系统树"><a href="#理解文件系统树" class="headerlink" title="理解文件系统树"></a>理解文件系统树</h2><p> Linux总是只有一个<strong>单一的文件系统树</strong>，<strong>不管有多少个磁盘</strong>或者存储设备连接</p><p>到计算机上。根据负责维护系统安全的系统管理员的兴致，<strong>存储设备连接</strong>到（或着更精确些，是挂载到）<strong>目录树的各个节点</strong>上。</p><p>我们所在的目录则称为当前工作目录。我们使用 **pwd（print working directory(的缩写)）**命令，来显示当前工作目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[me@linuxbox ~]$ <span class="built_in">pwd</span></span><br><span class="line">/home/me</span><br></pre></td></tr></table></figure><p>当我们首次登录系统（或者启动终端仿真器会话）后，当前工作目录是我们的家目录。每个用户都有他自己的家目录，当用户以普通用户的身份操控系统时，家目录是唯一允许用户写入文件的地方。</p><h2 id="列出目录内容"><a href="#列出目录内容" class="headerlink" title="列出目录内容"></a>列出目录内容</h2><p>列出一个目录包含的文件及子目录，使用 ls 命令。</p><h2 id="更改当前工作目录"><a href="#更改当前工作目录" class="headerlink" title="更改当前工作目录"></a>更改当前工作目录</h2><p>要更改工作目录（此刻，我们站在树形迷宫里面），我们用 cd 命令。输入 cd, 然后输入你想要</p><p>去的工作目录的路径名。路径名就是沿着目录树的分支到达想要的目录期间所经过的路线。路</p><p>径名可通过两种方式来指定，一种是绝对路径，另一种是相对路径。</p><h2 id="绝对路径"><a href="#绝对路径" class="headerlink" title="绝对路径"></a>绝对路径</h2><p>系统中有一个目录，大多数系统程序都安装在这个目录下。这个目录的路径名是 &#x2F;usr&#x2F;bin。它意味着<strong>从根目录（用开头的“&#x2F;“表示）开始</strong>，有一个叫”usr” 的目录包含了目录 “bin”。</p><h2 id="相对路径"><a href="#相对路径" class="headerlink" title="相对路径"></a>相对路径</h2><p>在<strong>文件系统树中用一对特殊符号来表示相对位置</strong>。这对特殊符号是 “.” (点)和 “..” (点点)。符号 <strong>“.” 指的是工作目录</strong>，<strong>”..” 指的是工作目录的父目录</strong>。</p><p>更改工作目录到 &#x2F;usr&#x2F;bin 的父目录 &#x2F;usr,使用相对路径:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[me@linuxbox bin]$ <span class="built_in">cd</span> ..</span><br><span class="line">[me@linuxbox usr]$ <span class="built_in">pwd</span></span><br><span class="line">/usr</span><br></pre></td></tr></table></figure><p>在几乎所有的情况下，可以<strong>省略 “.&#x2F;”。它是隐含的。</strong></p><h2 id="有用的快捷键"><a href="#有用的快捷键" class="headerlink" title="有用的快捷键"></a>有用的快捷键</h2><p><img src="/./%E8%BF%9B%E8%A1%8Clinux%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%A4%A7%E5%85%A8%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AF%BB/040d287dafbc0134068ebbf400a8b0da.png" alt="040d287dafbc0134068ebbf400a8b0da"></p><h2 id="关于文件名的重要规则"><a href="#关于文件名的重要规则" class="headerlink" title="关于文件名的重要规则"></a>关于文件名的重要规则</h2><ol><li><p>以 “.” 字符开头的文件名是<strong>隐藏文件</strong>。这仅表示，ls 命令不能列出它们，用 <strong>ls-a</strong> 命令就可以了。当你创建帐号后，几个配置帐号的隐藏文件被放置在你的家目录下。稍后，我们会仔细研究一些隐藏文件，来定制你的系统环境。另外，一些应用程序也会把它们的配置文件以隐藏文件的形式放在你的家目录下面。</p></li><li><p>文件名和命令名是<strong>大小写敏感</strong>的。文件名“File1”和“file1”是指两个不同的</p></li></ol><p>文件名。</p><p>   3.Linux 没有“文件扩展名”的概念，不像其它一些系统。可以用你<strong>喜欢的任何</strong></p><p><strong>名字</strong>来给文件起名。文件内容或用途由其它方法来决定。虽然类 Unix 的操作</p><p>系统，不用文件扩展名来决定文件的内容或用途，但是有些应用程序会。</p><ol start="4"><li>虽然 Linux 支持长文件名，文件名可能包含空格，标点符号，但标点符号仅限使用“.”，“－”，下划线。最重要的是，<strong>不要在文件名中使用空格</strong>。如果你想表示词与词间的空格，<strong>用下划线</strong>字符来代替。过些时候，你会感激自己这样做。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>轻轻进行一个一个网安的学习</title>
      <link href="/2025/04/29/%E8%BD%BB%E8%BD%BB%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E7%BD%91%E5%AE%89%E7%9A%84%E5%AD%A6%E4%B9%A0/"/>
      <url>/2025/04/29/%E8%BD%BB%E8%BD%BB%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E7%BD%91%E5%AE%89%E7%9A%84%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>看着朋友玩感觉挺有意思的，就稍微去了解一下</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>进行一个一个blender的学</title>
      <link href="/2025/04/21/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AAblender%E7%9A%84%E5%AD%A6/"/>
      <url>/2025/04/21/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AAblender%E7%9A%84%E5%AD%A6/</url>
      
        <content type="html"><![CDATA[<p>进行一个一个blender的学习哦哦哦</p><h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><p>旋转视角:鼠标中键按住拖动</p><p>平移视角:shift+鼠标中键按住拖动</p><p>快速切换各个正交视角:alt+鼠标中键按住拖动</p><p>用这个快捷键组合的话，每切换一次，需要按住放开鼠标中键一次，即<strong>按住alt — 按住中键拖动 — 放开中键，完成一次切换</strong>，要再切换的话，需要重复上述操作，觉得麻烦的话，可以用<strong>神键~</strong>替代。(<strong>按住「~」之后不松开，把鼠标指向想切换的视角按钮上，然后松开「~」</strong>，即可完成切换，这样可以少了一次鼠标点击)</p><p>新建物体:shift+A</p><p>(新建物体之后，左下角会有<strong>一个物体的详细设置</strong>，如果需要调整的话需要在这个时候调整，不然后期需要修改的话，就只能重新新建了。)</p><h4 id="Blender界「三大操作」"><a href="#Blender界「三大操作」" class="headerlink" title="Blender界「三大操作」"></a>Blender界「三大操作」</h4><p>x&#x2F;y&#x2F;z </p><p><strong>滚(G)、绕(R)、缩(S)</strong></p><p>G +</p><p>可以看到，滚绕缩三大操作，都<strong>可以配合XYZ来规定变换方向</strong>，一定要谨记，因为会对规范物体制作有很大的帮助~</p><p><strong>在按下G、R、S之后，直接按数字</strong></p><p>物体就会根据数字来严格变换,也可以配合XYZ来使用哦<br>例如你按了S，再按2</p><p>物体的长宽高就都放大成2倍</p><p>A 全选所有物体</p><p>X 删除物体</p><p>shitf+d 快速复制物体</p><p>Tab 编辑模式</p><p><strong>文本物体的物体数据属性面板上</strong>有很多可以玩的参数设置</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AAblender%E7%9A%84%E5%AD%A6/image-20250421101544936.png" alt="image-20250421101544936"></p><p>编辑模式下，可以编辑各种物体的形状</p><p>一个非常常用的功能—— <strong>细分</strong></p><p>细分可以按照两个顶点中间生成新顶点的方式，让物体有更多的顶点，这样物体会有更多操作的可能性。</p><p>属性编辑器，就是打开Blender后</p><p>右下角的这一块面版</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AAblender%E7%9A%84%E5%AD%A6/v2-dd9441d7884c3f8eaa667da400680edb_1440w.jpg" alt="img"></p><p><strong>物体属性</strong></p><p>这里主要是设置物体的基本变换属性</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AAblender%E7%9A%84%E5%AD%A6/v2-c8e109a2d7b43b832d13ac89dd2d6818_1440w.jpg" alt="img"></p><p><strong>物体数据属性</strong></p><p>我们在新建一个球体之后</p><p>可以通过<strong>右键—平滑着色</strong>来让它看上去变圆滑</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AAblender%E7%9A%84%E5%AD%A6/v2-3a88205a827009929b0ac8fa256459eb_1440w.jpg" alt="img"></p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AAblender%E7%9A%84%E5%AD%A6/v2-2707a47af25e698aa61dfbd2995be71a_1440w.jpg" alt="img"></p><p>这个时候「物体数据属性」可以登场了</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AAblender%E7%9A%84%E5%AD%A6/v2-b46cf4a02ee967b96dc614ce11236661_1440w.jpg" alt="img"></p><p>在勾选这个**法向下的「自动光滑」**之后</p><p>柱体的平滑着色就会变得合理~</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AAblender%E7%9A%84%E5%AD%A6/v2-369a536b87d3a0aaa76bdf929e019653_1440w.jpg" alt="img"></p><p>修改器属性</p><p>修改器也是一个<strong>庞大的可玩性极高的功能</strong></p><p>而且绝大部分修改器<br>都是<strong>保证不改变原物体形状的前提下来产生效果</strong></p><p>非常实用</p><p><strong>倒角修改器</strong></p><p>可以给物体生成圆润的边缘：</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AAblender%E7%9A%84%E5%AD%A6/v2-1d33320a3cafc15aae820e40eecb58ce_1440w.jpg" alt="img"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>计科相关自感知识点的一个一个补充</title>
      <link href="/2025/04/14/%E8%AE%A1%E7%A7%91%E7%9B%B8%E5%85%B3%E8%87%AA%E6%84%9F%E7%9F%A5%E8%AF%86%E7%82%B9%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%A1%A5%E5%85%85/"/>
      <url>/2025/04/14/%E8%AE%A1%E7%A7%91%E7%9B%B8%E5%85%B3%E8%87%AA%E6%84%9F%E7%9F%A5%E8%AF%86%E7%82%B9%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%A1%A5%E5%85%85/</url>
      
        <content type="html"><![CDATA[<h1 id="小知识点"><a href="#小知识点" class="headerlink" title="小知识点"></a>小知识点</h1><h2 id="dll文件"><a href="#dll文件" class="headerlink" title="dll文件"></a>dll文件</h2><p>.dll文件是<strong>Dynamic Link Library（<a href="https://zhida.zhihu.com/search?content_id=229567508&content_type=Article&match_order=1&q=%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5%E5%BA%93&zhida_source=entity">动态链接库</a>）文件的缩写</strong>，它是一种共享库文件，包含了程序所需的代码和数据。与静态链接库不同，动态链接库可以在<strong>程序运行时动态加载</strong>，使得程序的内存占用更小，同时也方便了程序的更新和维护。</p><h2 id="鲁棒性"><a href="#鲁棒性" class="headerlink" title="鲁棒性"></a>鲁棒性</h2><p>鲁棒性是 robustness 的音译，在中文中常常也被表达为健壮性和强壮性，总体来说其可以用于反映<strong>一个系统在面临着内部结构或外部环境的改变时也能够维持其功能稳定运行的能力</strong> 。</p><p>计算机系统：鲁棒性是一个系统或组件在出现不正确的或矛盾的输入时能够正确运行的程度。</p><p>人类语言技术：语言的鲁棒性（识别和分解等）是指人类即使在信息不完全、意思模糊或不断的变化情况下，仍然能够实现沟通的能力。</p><p>实用非线性控制：鲁棒性是一个系统在遇到了设计中所没有考虑过的情况时不受到影响的程度。</p><p>生物系统：鲁棒性是那些具有恢复、自动修复、自控制、自组装、自复制能力的系统所具有的特性。</p><p>面向对象的软件构造：鲁棒性是软件在非正常环境下（也就是在规范外的环境下，包括新的平台、网络超载、内存故障等）做出适当反应的能力。</p><h2 id="句柄（HANDLE）"><a href="#句柄（HANDLE）" class="headerlink" title="句柄（HANDLE）"></a>句柄（HANDLE）</h2><p>句柄是一个<strong>整数</strong>，单独的看它只是数字。</p><p>但这个整数是<strong>进程句柄表数组的下标</strong>，有了这个下标，操作系统就可以找到其索引的数据结构，并能找到数据结构里面的指针，然后根据这个指针获取内核里的某个对象。</p><h1 id="大知识点"><a href="#大知识点" class="headerlink" title="大知识点"></a>大知识点</h1><h2 id="缓冲（buffer）"><a href="#缓冲（buffer）" class="headerlink" title="缓冲（buffer）"></a>缓冲（buffer）</h2><p>缓冲区是<strong>内存空间的一部分</strong>。也就是说，在内存空间中<strong>预留</strong>了一定的存储空间，这些存储空间用来<strong>缓冲输入或输出的数据</strong>，这部分预留的空间就叫做缓冲区。缓冲区根据其对应的是输入设备还是输出设备，分为输入缓冲区和输出缓冲区。</p><h4 id="为什么要引入缓冲区"><a href="#为什么要引入缓冲区" class="headerlink" title="为什么要引入缓冲区"></a>为什么要引入缓冲区</h4><p>比如我们从磁盘里取信息，我们先把读出的数据放在缓冲区，<strong>计算机再直接从缓冲区中取数据</strong>，等缓冲区的数据取完后再去磁盘中读取，这样就可以<strong>减少磁盘的读写次数</strong>，再加上计算机对缓冲区的操作<strong>速度大大快于</strong>对磁盘的操作，故应用缓冲区可大大<strong>提高计算机的运行速度。</strong></p><p>又比如，我们使用打印机打印文档，由于打印机的打印速度相对较慢，我们先把文档输出到打印机相应的缓冲区，打印机再自行逐步打印，这时我们的<strong>CPU可以处理别的事情</strong>。缓冲区就是一块内存区，它<strong>用在输入输出设备和CPU之间</strong>，用来缓存数据。它使得低速的输入输出设备和高速的CPU能够<strong>协调工作</strong>，避免低速的输入输出设备占用CPU，<strong>解放出CPU，使其能够高效率工作</strong>。</p><h2 id="缓存（cache）"><a href="#缓存（cache）" class="headerlink" title="缓存（cache）"></a>缓存（cache）</h2><p>cache是一个非常大的概念。</p><h3 id="一、CPU的Cache"><a href="#一、CPU的Cache" class="headerlink" title="一、CPU的Cache"></a><strong>一、CPU的Cache</strong></h3><p>CPU的Cache，它中文名称是高速缓冲存储器，读写速度很快，几乎与CPU一样。由于CPU的运算速度太快，<strong>内存的数据存取速度无法跟上CPU的速度</strong>，所以在cpu与内存间设置了cache为cpu的数据快取区。当计算机执行程序时，数据与地址管理部件会<strong>预测</strong>可能要用到的数据和指令，并将这些数据和指令<strong>预先从内存中读出送到Cache</strong>。一旦需要时，<strong>先检查Cache</strong>，若有就从Cache中读取，若无再访问内存，现在的CPU还有一级cache，二级cache。简单来说，Cache就是用来<strong>解决CPU与内存之间速度不匹配的问题</strong>，避免内存与辅助内存频繁存取数据，这样就提高了系统的执行效率。</p><h3 id="二、磁盘的Cache"><a href="#二、磁盘的Cache" class="headerlink" title="二、磁盘的Cache"></a><strong>二、磁盘的Cache</strong></h3><p>磁盘也有cache，硬盘的cache作用就类似于CPU的cache，它解决了总线接口的高速需求和读写硬盘的矛盾以及对某些扇区的反复读取。</p><h3 id="三、浏览器的Cache"><a href="#三、浏览器的Cache" class="headerlink" title="三、浏览器的Cache"></a><strong>三、浏览器的Cache</strong></h3><p><a href="https://so.csdn.net/so/search?q=%E6%B5%8F%E8%A7%88%E5%99%A8%E7%BC%93%E5%AD%98&spm=1001.2101.3001.7020">浏览器缓存</a>（Browser Caching）是为了节约网络的资源加速浏览，浏览器在用户磁盘上对最近请求过的文档进行存储，当访问者再次请求这个页面时，浏览器就可以从本地磁盘显示文档，这样就可以加速页面的阅览，并且可以减少服务器的压力。游览器的缓存的数据只是短时间保存，可以人为的清空</p><h4 id="缓存（cache）与缓冲-buffer-的主要区别"><a href="#缓存（cache）与缓冲-buffer-的主要区别" class="headerlink" title="缓存（cache）与缓冲(buffer)的主要区别"></a>缓存（cache）与缓冲(buffer)的主要区别</h4><p>Buffer的核心作用是用来缓冲，缓和冲击（对输出设备的冲击，包括磁盘、打印机、显示器）。比如你每秒要写100次硬盘，对系统冲击很大，<strong>浪费了大量时间</strong>在忙着<strong>处理</strong>开始写和结束写这两件事嘛。用个buffer暂存起来，变成每10秒写一次硬盘，对系统的冲<strong>击就很小</strong>，写入效率高了，日子过得爽了。极大缓和了冲击。</p><p>Cache的核心作用是加快取用的速度（加快读取速度，包括CPU读内存、内存读磁盘、用户通过浏览器请求资源）。比如你一个很复杂的计算做完了，下次还要用结果，就把结果放手边一个<strong>好拿</strong>的地方存着，下次不用再算了。加快了数据取用的速度。</p><p>简单来说就是buffer偏重于写，而cache偏重于读。</p><h2 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h2><p>JSON: JavaScript Object Notation JS对象简谱 , 是一种轻量级的数据交换格式.</p><p>2.JSON对象格式<br>我们通过java，js,xml和json这几种不同的语言来描述一个对象<br>对象是book，它有两个属性，分别是name和info</p><p>java格式</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">class</span> <span class="title class_">Book</span>&#123; </span><br><span class="line">&gt; 　　　<span class="keyword">private</span> String name; </span><br><span class="line">&gt; 　　　<span class="keyword">private</span> String info;</span><br><span class="line">&gt; 　　　get/set... </span><br><span class="line">&gt; 　　　&#125;</span><br><span class="line">&gt;  　　<span class="type">Book</span> <span class="variable">b</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Book</span>();</span><br><span class="line">&gt;   　　b.setName(“金苹果”); </span><br><span class="line">&gt;   　　b.setInfo(“种苹果”); </span><br><span class="line">&gt;      　　...</span><br></pre></td></tr></table></figure><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">var</span> b = <span class="keyword">new</span> <span class="title class_">Object</span>(); </span><br><span class="line">&gt; b.<span class="property">name</span> = <span class="string">&quot;金苹果&quot;</span>; </span><br><span class="line">&gt; b.<span class="property">info</span> = <span class="string">&quot;种苹果&quot;</span>;</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">book</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>金苹果<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">info</span>&gt;</span>种苹果<span class="tag">&lt;/<span class="name">info</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span> </span><br><span class="line"><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;金苹果&quot;</span><span class="punctuation">,</span> </span><br><span class="line"><span class="attr">&quot;info&quot;</span><span class="punctuation">:</span><span class="string">&quot;种苹果&quot;</span> </span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p><strong>一个对象, 由一个大括号表示.</strong><br>         括号中 描述对象的属性 .<br>         通过<strong>键值对来描述对象的属性</strong> (可以理解为, 大括号中, 包含的是一个个的键值对.)<br>         格式<br>                  键与值之间使用冒号连接, 多个键值对之间使用<strong>逗号分隔.</strong><br>                  键值对的键 应使用<strong>引号引住</strong> (通常Java解析时, 键不使用引号会报错. 而JS能正确解 析.)                   键值对的值, 可以是JS中的<strong>任意类型</strong>的数据</p><p>数组格式</p><blockquote><p>在JSON格式中可以与对象<strong>互相嵌套</strong><br>[元素1,元素2…]</p></blockquote><p><strong>案例</strong></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span> </span><br><span class="line"><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;小陈&quot;</span><span class="punctuation">,</span> </span><br><span class="line"><span class="attr">&quot;age&quot;</span><span class="punctuation">:</span><span class="number">20</span><span class="punctuation">,</span> </span><br><span class="line"><span class="attr">&quot;pengyou&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;张三&quot;</span><span class="punctuation">,</span><span class="string">&quot;李四&quot;</span><span class="punctuation">,</span><span class="string">&quot;王二&quot;</span><span class="punctuation">,</span><span class="string">&quot;麻子&quot;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span> </span><br><span class="line"><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;小明&quot;</span><span class="punctuation">,</span> </span><br><span class="line"><span class="attr">&quot;info&quot;</span><span class="punctuation">:</span><span class="string">&quot;像彭于晏一样帅气的男人&quot;</span> </span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span> </span><br><span class="line"><span class="attr">&quot;heihei&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span> </span><br><span class="line"><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;大长刀&quot;</span><span class="punctuation">,</span> </span><br><span class="line"><span class="attr">&quot;length&quot;</span><span class="punctuation">:</span><span class="string">&quot;40m&quot;</span> </span><br><span class="line"><span class="punctuation">&#125;</span> </span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>3.JSON数据解析<br>下面开始讲解如何<strong>使用IDEA将Java对象快速转换成JSON数据</strong>，和如何将JSON数据转换成Java对象</p><p>将Java中的对象 快速的<strong>转换为 JSON格式的字符串.</strong><br>将JSON格式的字符串, 转换为Java的对象.</p><p>注意：</p><p>1.以下的导jar包操作如果不会，请去参考一些导jar包操作<br>2.在文章的底部提供了有关的jar包，需要自取<br><strong>GSON解析</strong></p><ul><li><p>将对象转换为JSON字符串</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">转换JSON字符串的步骤: </span><br><span class="line"><span class="number">1.</span> 引入JAR包 </span><br><span class="line"><span class="number">2.</span> 在需要转换JSON字符串的位置编写如下代码即可: </span><br><span class="line"><span class="type">String</span> <span class="variable">json</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Gson</span>().toJSON(要转换的对象); </span><br><span class="line">案例:</span><br><span class="line"><span class="comment">//1. 创建Gson类型的对象</span></span><br><span class="line">        <span class="type">Gson</span> <span class="variable">g</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Gson</span>();</span><br><span class="line">        <span class="comment">//2. 转换</span></span><br><span class="line">        <span class="comment">//优化   可以使用匿名对象</span></span><br><span class="line">        <span class="type">book</span> <span class="variable">b</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">book</span>(<span class="string">&quot;100&quot;</span>,<span class="string">&quot;金苹果&quot;</span>,<span class="string">&quot;种植苹果的故事&quot;</span>);</span><br><span class="line">        <span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> g.toJson(b);</span><br><span class="line">        System.out.println(s);</span><br></pre></td></tr></table></figure><p>  将JSON字符串转换为对象</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> 引入JAR包 </span><br><span class="line"><span class="number">2.</span> 在需要转换Java对象的位置, 编写如下代码:</span><br><span class="line"><span class="comment">//1. 创建Gson类型的对象</span></span><br><span class="line">        <span class="type">Gson</span> <span class="variable">g</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Gson</span>();</span><br><span class="line">        <span class="comment">//2. 转换 &#123;&quot;id&quot;:&quot;100&quot;,&quot;name&quot;:&quot;金苹果&quot;,&quot;info&quot;:&quot;种植苹果的故事&quot;,&quot;page&quot;:[&quot;远赴人间惊鸿晏&quot;,&quot;一度人间盛世颜&quot;,&quot;致hdd&quot;]&#125;</span></span><br><span class="line">        <span class="comment">//2.1  返回book类型</span></span><br><span class="line">        <span class="type">book</span> <span class="variable">b</span> <span class="operator">=</span> g.fromJson(<span class="string">&quot;&#123;\&quot;id\&quot;:\&quot;100\&quot;,\&quot;name\&quot;:\&quot;金苹果\&quot;,\&quot;info\&quot;:\&quot;种植苹果的故事\&quot;&#125;&quot;</span>, book.class);</span><br><span class="line">        System.out.println(b.getId());</span><br><span class="line">        <span class="comment">//2.2 返回MAP类型，键值对形式</span></span><br><span class="line">        <span class="type">HashMap</span> <span class="variable">hm</span> <span class="operator">=</span> g.fromJson(<span class="string">&quot;&#123;\&quot;id\&quot;:\&quot;100\&quot;,\&quot;name\&quot;:\&quot;金苹果\&quot;,\&quot;info\&quot;:\&quot;种植苹果的故事\&quot;&#125;&quot;</span>, HashMap.class);</span><br><span class="line">        System.out.println(hm.get(<span class="string">&quot;id&quot;</span>));</span><br><span class="line">        <span class="comment">//2.3 MAP类型中值是一个数组形式</span></span><br><span class="line">        <span class="type">HashMap</span> <span class="variable">data</span> <span class="operator">=</span> g.fromJson(<span class="string">&quot;&#123;\&quot;id\&quot;:\&quot;100\&quot;,\&quot;name\&quot;:\&quot;金苹果\&quot;,\&quot;info\&quot;:\&quot;种植苹果的故事\&quot;,\&quot;page\&quot;:[\&quot;远赴人间惊鸿晏\&quot;,\&quot;一度人间盛世颜\&quot;,\&quot;致hdd\&quot;]&#125;&quot;</span>, HashMap.class);</span><br><span class="line">        <span class="type">List</span> <span class="variable">list</span> <span class="operator">=</span> (List) data.get(<span class="string">&quot;page&quot;</span>);</span><br><span class="line">        System.out.println(list.get(<span class="number">1</span>));</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>进行一个一个算法知识点的复习</title>
      <link href="/2025/04/11/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E7%82%B9%E7%9A%84%E5%A4%8D%E4%B9%A0/"/>
      <url>/2025/04/11/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E7%82%B9%E7%9A%84%E5%A4%8D%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>本篇为方便博主笨比时及时进行算法知识点的一个一个快速回顾</p><h1 id="赛前提醒"><a href="#赛前提醒" class="headerlink" title="赛前提醒"></a>赛前提醒</h1><ul><li><p>能用long long和double就用,记得宏替换endl</p></li><li><p>像什么<strong>从0看</strong>什么<strong>从1看</strong>这样的状态设置,<strong>怎么方便就怎么来(不一定就要统一,但若要输出记得还原)</strong>,但<strong>一定一定要在开做前就区分好了</strong>,再去实现,不然后面会<strong>容易混淆的</strong>.不过,这些<strong>初步设置成啥样也只是实现难度的问题,并不会影响最终结果</strong>的话,就<strong>切忌一直在这方面思考优化</strong>了,既<strong>没啥大用</strong>还有<strong>可能导致自己一直左右手互博而一直浪费不必要的时间</strong>,题目进展却一点没有(</p></li><li><p>真<strong>不要吝啬于多处理几个边界</strong>情况,要求稳,<strong>不用追求一个写法</strong>处理好全局情况(也可能<strong>本来就不能仅一个写法</strong>)</p></li><li><p>稳定心态,减少失误,能得就得,至少能把自己能力范围内的分100%拿到</p></li><li><p>先通看一遍,有个整体把握,<strong>可以边看边把模板写好</strong>.</p></li><li><p>三思而后行(思考对了才好实现),一定要花足够多的时间去读题和构思(花多点时间读题思考肯定<strong>比在一个错误的思路上进行要好</strong>的多!<strong>多一分钟好的思考也许能减少五分钟的码量</strong>)</p><ul><li><p>仔细读题,确保每个字眼都读到读懂,<strong>题目理解是一定要注意有没有问题的,直接就决定了开头方向对不对,写不写得出来!</strong></p><ul><li>数据范围及分布</li><li>极端&#x2F;边界情况</li></ul></li><li><p>构思代码</p><ul><li>多用稿纸辅助思考,推算思路</li><li>不要尝试用不熟练的算法,就靠已学尽力做就好</li></ul></li><li><p>实现代码</p><ul><li>一遍实现一遍思考极端,边界等情况(极端小数据 ,边界数据等)思路是否仍正确</li></ul></li><li><p>检查</p><ul><li>低级错误(变量名,数组清空,初始化赋值,符号,数据类型)</li><li>提交前多多测试几个数据,自己捏也好,generate一些也好</li></ul><p>  如果怎么找思路上和实现上的错误都找不出来,那么一定是<strong>很细微的铸币低级错误,就再重新确认一遍题目要求(最好是一开始就能确定好,如果做题的根基都错了就烷基八蛋了,但碰上这种情况也再去看一遍(你也不知道你第一遍认为的是不是真的就跟题意切合,再揣摩好)),挑着低级错误细细去看,并且把看过的部分就收起来,不要再重复看了!!!那是无意义的浪费时间(除非自己检查的标准有了新变动,那就应该用新标准再去看一遍)</strong>  </p></li><li><p>有时间的话,对于<strong>没有把握</strong>的题,不要吝啬去蒙去猜结论,肯定是稳赚不赔的</p><ul><li><strong>时间不多没啥好思路</strong>,就先暴力搜索+分段(先处理小的,对大的单独优化),再优化(剪枝,记忆化),<strong>赌数据弱去骗分</strong></li><li>加上贪心</li><li>打表</li><li>找规律</li></ul></li></ul></li><li><p>除非还能有把握,不然最后10分钟就全面检查(中间就注意保存),看看好还能不能救(命名,数组大小等)</p></li></ul><h1 id="小知识点"><a href="#小知识点" class="headerlink" title="小知识点"></a>小知识点</h1><ul><li>传入常数时<strong>注意使用lld强转</strong></li><li>一秒认定为<strong>2e8</strong></li><li><strong>堆上256MB</strong>最多可开<strong>5e7个int最好不超1e7(所以开大数组时再慎用lld)</strong></li><li>erase会返回所删除元素的下一个元素的迭代器,赋值即可</li><li>对于递推式,一般而言若<strong>由子状态递推</strong>,要<strong>根据子状态所处位置选择正确的循环顺序</strong>.</li><li></li></ul><h1 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h1><ul><li>在main()里面用solve()函数,方便运行至中间就结束</li><li>学会多去<strong>使用空间记录来简化</strong>代码量**(本来也就用不完)**</li><li>对于二维的状态压缩,可以<strong>先用一维的去存储(将二维拆分为一维)</strong>,在<strong>处理时再转化为二维</strong>的</li><li>若有多次求取而<strong>有可能出现重复</strong>,则<strong>使用记忆化数组记下第一次求的值</strong></li><li><strong>根据题目情况</strong>,确定好什么<strong>从0看</strong>什么<strong>从1看</strong>,<strong>最好统一</strong>来看更方便,但一定要<strong>明确好什么变量怎么看,才方便做题</strong></li><li>把<strong>bitset当作二进制数串来看即可</strong></li></ul><h1 id="铸币小错"><a href="#铸币小错" class="headerlink" title="铸币小错"></a>铸币小错</h1><ul><li><strong>实数!&#x3D;整数!!!要用lf!!!</strong></li><li>调用stl函数放入是<strong>从0开始</strong>的!</li><li><strong>unorder_<strong>map</strong>不存在pair的hash</strong>,而map用key存储无影响</li><li>使用stl会略增大空间使用和运行效率,<strong>大数据下且卡得死的话</strong>不如数组模拟效率高</li><li>使用pair,map等<strong>需要排序占用时间</strong>部分需要留意(尤其时间卡的死的时候,把不必要的时间消耗部分去掉)</li></ul><h1 id="基础算法"><a href="#基础算法" class="headerlink" title="基础算法"></a>基础算法</h1><h4 id="贪心"><a href="#贪心" class="headerlink" title="贪心"></a>贪心</h4><p>只看<strong>目前何种选择最好</strong>就去选(局部的,对于全局来说不一定选择这个就最好,可能现在差点会对全局更好)</p><p>证明其正确性即证明<strong>局部最优解就是全局最优解</strong>(局部最好<strong>就对全局最好</strong>,一般要<strong>独立的或能共同好</strong>的)</p><h4 id="差分-不同于用树状数组-维护的是变化数组-并非差分"><a href="#差分-不同于用树状数组-维护的是变化数组-并非差分" class="headerlink" title="差分(不同于用树状数组,维护的是变化数组,并非差分)"></a>差分(不同于用树状数组,维护的是变化数组,并非差分)</h4><p>当需要**多次修改区间O(1)但仅少量查询O(n)**时可用</p><h4 id="二维前缀和"><a href="#二维前缀和" class="headerlink" title="二维前缀和"></a>二维前缀和</h4><p>注意维护好边界情况,看好是否要减去或是加上(<strong>写成区块去看</strong>)</p><p>向右下<strong>加和,块+块-重叠+单点</strong></p><p>求取 <strong>块-(块+块-重叠)+(边界)</strong></p><h4 id="二维差分"><a href="#二维差分" class="headerlink" title="二维差分"></a>二维差分</h4><p>差分&lt;-&gt;原数组&lt;-&gt;前缀和三者之间都可<strong>运算互化</strong></p><p>二维差分定义式 <strong>b[i][j]&#x3D;a[i][j]-a[i-1][j]-a[i][j-1]+a[i-1][j-1]</strong></p><p><strong>修改时注意b[x2+1][y2+1]处也要+1</strong></p><h4 id="二进制枚举"><a href="#二进制枚举" class="headerlink" title="二进制枚举"></a>二进制枚举</h4><p>表示用一个<strong>二进制数(虽然短,但其实能对应很多种情况,所以也是独特的)来表示一个子集(相对于总体集合,每个元素选与不选)</strong>,从而共可能有<strong>2的n次方种情况</strong></p><p>即为<strong>状态压缩</strong>,用一个二进制<strong>数特别地表示出一个状态</strong></p><h4 id="单调队列"><a href="#单调队列" class="headerlink" title="单调队列"></a>单调队列</h4><p>元素<strong>入队前</strong>把<strong>队首会破坏区间性</strong>和<strong>队尾会破坏单调性</strong>的元素去掉(队列<strong>保存下标即可</strong>,<strong>调用原数组读取值</strong>,否则要用pair)</p><h4 id="单调栈"><a href="#单调栈" class="headerlink" title="单调栈"></a>单调栈</h4><p>每个元素最多出入一次</p><p>元素<strong>入栈前</strong>把<strong>栈顶</strong>会破坏单调性的元素去掉</p><h1 id="数论"><a href="#数论" class="headerlink" title="数论"></a>数论</h1><p><strong>gcd*lcm&#x3D;a*b</strong></p><h4 id="组合数思想"><a href="#组合数思想" class="headerlink" title="组合数思想"></a>组合数思想</h4><p>互补性质<img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E7%82%B9%E7%9A%84%E5%A4%8D%E4%B9%A0/189b3a44f8bcf6b0e687730f596d413d.png" alt="189b3a44f8bcf6b0e687730f596d413d"></p><p>杨辉三角</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E7%82%B9%E7%9A%84%E5%A4%8D%E4%B9%A0/7e32652740f3f8403d4fea252c15d9ed.png" alt="7e32652740f3f8403d4fea252c15d9ed"></p><p>可用于<strong>线性递推二项式系数(先乘后除)</strong><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E7%82%B9%E7%9A%84%E5%A4%8D%E4%B9%A0/69c41febe0e1fd9d41106447ed434f31.png" alt="69c41febe0e1fd9d41106447ed434f31"></p><p>直观解释</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E7%82%B9%E7%9A%84%E5%A4%8D%E4%B9%A0/2536917b2042b0806ac454700cb38442.png" alt="2536917b2042b0806ac454700cb38442"></p><p>Cmn的写法</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">lld <span class="title">C</span><span class="params">(lld m,lld n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n==m||m==<span class="number">0</span>) ret <span class="number">1</span>;<span class="comment">//边界情况</span></span><br><span class="line">    <span class="built_in">ret</span> (<span class="built_in">C</span>(m<span class="number">-1</span>,n<span class="number">-1</span>)+<span class="built_in">C</span>(m,n<span class="number">-1</span>));    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h1><p>求回文串(接近线性)</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ull unsigned long long</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">2000010</span>, P = <span class="number">131</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">char</span> s[N];</span><br><span class="line">ull h1[N], h2[N], p[N];</span><br><span class="line"></span><br><span class="line"><span class="function">ull <span class="title">get</span><span class="params">(ull h[], ull l, ull r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> h[r] - h[l - <span class="number">1</span>] * p[r - l + <span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>, s + <span class="number">1</span>) &amp;&amp; <span class="built_in">strcmp</span>(s + <span class="number">1</span>, <span class="string">&quot;END&quot;</span>))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n = <span class="built_in">strlen</span>(s + <span class="number">1</span>) * <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = n; i; i -= <span class="number">2</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            s[i] = s[i / <span class="number">2</span>];</span><br><span class="line">            s[i - <span class="number">1</span>] = <span class="string">&#x27;z&#x27;</span> + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        s[ ++ n] = <span class="string">&#x27;z&#x27;</span> + <span class="number">1</span>;</span><br><span class="line">        p[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>, j = n; i &lt;= n; i ++ , j -- )</span><br><span class="line">        &#123;</span><br><span class="line">            h1[i] = h1[i - <span class="number">1</span>] * P + s[i] - <span class="string">&#x27;a&#x27;</span> + <span class="number">1</span>;</span><br><span class="line">            h2[i] = h2[i - <span class="number">1</span>] * P + s[j] - <span class="string">&#x27;a&#x27;</span> + <span class="number">1</span>;</span><br><span class="line">            p[i] = p[i - <span class="number">1</span>] * P;</span><br><span class="line">        &#125;</span><br><span class="line">        ull ans = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            ull r = <span class="built_in">min</span>(i - <span class="number">1</span>, n - i);</span><br><span class="line">            <span class="keyword">if</span> (ans &gt;= r || <span class="built_in">get</span>(h1, i - ans, i - <span class="number">1</span>) != <span class="built_in">get</span>(h2, n - (i + ans) + <span class="number">1</span>, n - i)) <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">while</span> (ans &lt;= r &amp;&amp; <span class="built_in">get</span>(h1, i - ans, i - <span class="number">1</span>) == <span class="built_in">get</span>(h2, n - (i + ans) + <span class="number">1</span>, n - i)) ans ++ ;</span><br><span class="line">            ans -- ;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case %d: %d\n&quot;</span>, ++ cnt, ans);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><h4 id="树状数组"><a href="#树状数组" class="headerlink" title="树状数组"></a>树状数组</h4><p>若用于二维就<strong>多套一层循环</strong>即可</p><h1 id="dp"><a href="#dp" class="headerlink" title="dp"></a>dp</h1><p><strong>阶段:从起始阶段转移到目标阶段的若干个子问题序列,每个是解决某个子问题以进行状态转移的解题过程,在一系列的阶段中逐步推进(每个阶段都依赖于前面阶段的结果,需要满足避免重复计算,无后效性)</strong></p><p><strong>状态表示f[i][j]</strong>:<strong>子问题的特征</strong></p><p><strong>决策</strong>:<strong>状态转移方程</strong>,可以<strong>多根据状态转移的限制来思考</strong>出状态转移方程</p><p><strong>状态设置要求</strong></p><p><strong>只有dp的初始状态按要求设置好了,才能够确保是由初始状态递推而来的答案.</strong></p><p>要设置成互无影响的,不重不漏的状态,能划分而<strong>确定好局部与全局的关系(部分好能推得全局好,否则换一种状态表示)</strong>.要确保当使用子问题结果递推时<strong>子问题已经完全解决(或者不完全解决也不影响结果,是互相独立的,可以叠加)<strong>才</strong>可用于递推更新状态</strong></p><p>此后,别以为可能会互相囊括冲突,其实因为<strong>有维度i,j的限制</strong>,每个值都是<strong>要看作两两独立的(只是讲求取时不会互相有囊括冲突影响)</strong></p><p>满足三个条件：最优子结构，无后效性和子问题重叠。</p><h3 id="最优子结构"><a href="#最优子结构" class="headerlink" title="最优子结构"></a>最优子结构</h3><p>具有最优子结构也可能是<strong>适合用贪心</strong>的方法求解。</p><p>注意要确保我们<strong>考察了最优解中用到的所有子问题</strong>。</p><ol><li>证明问题最优解的第一个组成部分是做出一个选择；</li><li>对于一个给定问题，在其可能的第一步选择中，<strong>假定你已经知道哪种选择才会得到最优解(假设要是求得的就会是最优(但其实现在还没求得),那么也一定是由求得的子问题的最优解得到的(即肯定会是由最优继承,不可能有漏解或非最优继承影响))</strong>,现在并不用关心这种选择具体是如何得到的.</li><li>给定可获得的最优解的选择后，确定<strong>这次选择会产生哪些子问题</strong>，以及<strong>如何最好地刻画子问题空间</strong>；</li><li>证明<strong>在这一种决策下</strong>,<strong>作为构成原问题最优解的组成部分</strong>，每个<strong>子问题</strong>的解<strong>就是它本身的最优解</strong>。方法是<strong>反证法</strong>，考虑加入某个子问题的解不是其自身的最优解，那么就可以从原问题的解中用该子问题的最优解替换掉当前的非最优解，从而得到原问题的一个更优的解，从而与原问题最优解的假设矛盾。</li></ol><p>要保持子问题空间尽量简单，只在必要时扩展。</p><p>最优子结构的不同体现在两个方面：</p><ol><li>原问题的最优解中涉及多少个子问题；</li><li>确定最优解使用哪些子问题时，需要考察多少种选择。</li></ol><p>子问题图中每个定点对应一个子问题，而需要考察的选择对应关联至子问题顶点的边。</p><h3 id="无后效性"><a href="#无后效性" class="headerlink" title="无后效性"></a>无后效性</h3><p>已经求解的子问题，<strong>不会再受到后续决策的影响</strong>。</p><h3 id="子问题重叠"><a href="#子问题重叠" class="headerlink" title="子问题重叠"></a>子问题重叠</h3><p>如果有<strong>大量的重叠子问题</strong>，可以<strong>记忆化避免重复求解(在当前阶段解决该阶段的总体的子问题需要解决若干个子问题的子问题,而部分子问题的子问题在之前的阶段已经求过了)<strong>相同的子问题，从而提升效率(方便</strong>由先前状态得出的最优值</strong>,判断性继承)。</p><h4 id="图论"><a href="#图论" class="headerlink" title="图论"></a>图论</h4><p>如果需要<strong>将边按边权排序</strong>,需要<strong>多次建图（如建一遍原图，建一遍反图）<strong>可以将边直接存下来，需要</strong>重新建图</strong>时<strong>利用直接存下的边</strong>来建图,<strong>就用vector<int> e</strong></p><p>邻接矩阵<strong>只适用于没有重边（或重边可以忽略,如求最短路,就只要看最小的那条边就足够）的情况</strong>。(或者<strong>能够特殊处理</strong>)</p><p>其最显著的优点是可以 <strong>O(1) 查询一条边是否存在</strong>。一般只会在<strong>稠密图上</strong>使用</p><p><strong>邻接表vector<int> e[N]</strong>,存<strong>各种图</strong>都很适合除非有特殊需求（如需要<strong>快速查询</strong>一条边是否存在，且点数较少，可以使用邻接矩阵）。</p><p><strong>链式前向星</strong>,不能快速查询一条边是否存在，也<strong>不能方便地对一个点的出边进行排序</strong>。优点是<strong>边是带编号</strong>的.而且<strong>如果 <code>cnt</code> 的初始值为奇数</strong>，存<strong>双向边时 <code>i ^ 1</code> 即是 <code>i</code> 的反边(且一定这么关联)</strong>（常用于 <a href="https://oi-wiki.org/graph/flow/">网络流</a>）</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>进行一个一个算法的学</title>
      <link href="/2025/04/10/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E7%AE%97%E6%B3%95%E7%9A%84%E5%AD%A6/"/>
      <url>/2025/04/10/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E7%AE%97%E6%B3%95%E7%9A%84%E5%AD%A6/</url>
      
        <content type="html"><![CDATA[<p>25.4.10</p><h1 id="闫氏dp分析法-从集合角度分析dp问题"><a href="#闫氏dp分析法-从集合角度分析dp问题" class="headerlink" title="闫氏dp分析法(从集合角度分析dp问题)"></a>闫氏dp分析法(从集合角度分析dp问题)</h1><p>干瞪眼看做题只是自设难度,多动手列(哪怕很简单)辅助思考才能简化而做题(如列竖式)</p><p>所有dp问题都是<strong>有限集</strong>中的最值问题</p><p>可能状态为指数级别,因而要用dp去优化,最优化.</p><h4 id="为什么可以优化"><a href="#为什么可以优化" class="headerlink" title="为什么可以优化?"></a>为什么可以优化?</h4><p>一般分析要经过两个阶段:</p><h4 id="1-化零为整"><a href="#1-化零为整" class="headerlink" title="1.化零为整"></a>1.化零为整</h4><p><strong>状态表示f[i](一般都必须做过类似的题目才会好想出来)</strong></p><p>划归,不是一个一个去枚举,而是<strong>一堆一堆</strong>去枚举,再去细化.<strong>把一堆有相似点(所有满足相同限制条件)的元素划为一个子集,然后用某一个状态来表示</strong></p><ul><li><strong>维度</strong>表示<strong>一个或一类东西(集合</strong>)的<strong>一种限制条件</strong></li><li><strong>f[i]的值</strong>表示<strong>属性(最值,数量等)</strong></li></ul><h4 id="2-化整为零"><a href="#2-化整为零" class="headerlink" title="2.化整为零"></a>2.化整为零</h4><p><strong>状态计算</strong></p><p>把f[i]划分成**若干个子集(不重(可能)不漏(必须))**来求</p><p>划分依据:<strong>寻找最后一个不同点</strong></p><h4 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h4><p><strong>01背包</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>比赛相关内容的一个一个记</title>
      <link href="/2025/04/10/%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/"/>
      <url>/2025/04/10/%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>本篇对各个比赛的一个一个印象记录</p><h1 id="蓝桥杯"><a href="#蓝桥杯" class="headerlink" title="蓝桥杯"></a>蓝桥杯</h1><h4 id="题型"><a href="#题型" class="headerlink" title="题型"></a>题型</h4><ul><li><p>填空 (2-4) 手动计算(日期,如闰年;几何,数学) <strong>保证正确率</strong></p></li><li><p>编程 (省6国8) (搜索,dp,贪心) 有部分分 <strong>先暴力,后优化</strong></p></li></ul><p><strong>模板为王</strong></p><p>必须在代码结尾 <strong>return 0</strong>，否则一定为零分</p><p>检查好自己提交的代码是否为**去除多余东西,(只含必要输出成果)**有必要东西的代码</p><h1 id="全国大学生嵌入式芯片与系统设计竞赛-芯片应用赛道选题指南"><a href="#全国大学生嵌入式芯片与系统设计竞赛-芯片应用赛道选题指南" class="headerlink" title="全国大学生嵌入式芯片与系统设计竞赛(芯片应用赛道选题指南)"></a><strong>全国大学生嵌入式芯片与系统设计竞赛(<strong>芯片应用赛道选题指南</strong>)</strong></h1><p><strong>星闪物联网应用平台</strong></p><p>以下仅介绍WS63相关</p><p><strong>关键特性</strong>:高性能 32bit 微处理器、2.4GHz WiFi 6、SLE、BLE三模、丰富的外设接口，其中增强款WS63E 支持2.4GHz 的雷达人体活动检测功能</p><p><strong>应用场景</strong>:智慧家居、雷达感知、星闪网关、星闪中控屏、星闪手柄等</p><p>**注：**星闪（NearLink），是中国原生的新一代无线短距通信技术。与传统短距传输技术方案相比，星闪在功耗、速度、覆盖范围和连接性能全面领先，可以在智能终端、智能家居、智能汽车、智能制造等各类细分场景下实现更极致的用户体验。</p><p><strong>选题方向一：星闪物联网应用方向（参赛组别：本科生）</strong></p><p>1、本选题参赛作品的主控要求优先使用星闪 WS63 或 WS63E 或 BS21，</p><p>Hi3861V100 也可使用，操作系统可使用 LiteOS 或 OpenHarmony 版本。</p><p>2、如采用星闪 WS63 或 WS63E 或 BS21，并发挥星闪技术特性可酌情加分。</p><p>3、本选题重点考察参赛选手的嵌入式系统开发能力, SLE&#x2F;BLE&#x2F;WiFi 多端互联能力。</p><p>4、参赛选手须具备基础的 C 语言编码能力，了解物联网技术及应用相关知识。</p><p>5、本选题学习资料可参考《2025 年嵌入式竞赛海思赛道学习入口》，并知晓开发环境要求及套件功能限制。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>搭建环境做一个一个备忘</title>
      <link href="/2025/04/05/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
      <url>/2025/04/05/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>为临时需要再搭建环境时做一个一个备忘</p><p>记得先切换电脑自带的快捷键,如切换<strong>语言标点</strong></p><h1 id="c-类型"><a href="#c-类型" class="headerlink" title="c++类型"></a>c++类型</h1><h2 id="关于GCC"><a href="#关于GCC" class="headerlink" title="关于GCC"></a>关于GCC</h2><h2 id="C-11"><a href="#C-11" class="headerlink" title="C++11"></a>C++11</h2><p>完全支持</p><p>从GCC4.8.1版本完全支持</p><p>-std&#x3D;c++11 or std&#x3D;gnu++11</p><h2 id="C-14"><a href="#C-14" class="headerlink" title="C++14"></a>C++14</h2><p>完全支持</p><p>从GCC6.1版本开始完全支持，从6.1-10(包括)的默认模式</p><p>-std&#x3D;c++14 or std&#x3D;gnu++14</p><h2 id="C-17"><a href="#C-17" class="headerlink" title="C++17"></a>C++17</h2><p>完全支持</p><p>从GCC 5版本开始支持，到GCC 7版本已完全支持，是GCC 11到13版本的默认模式</p><p>-std&#x3D;c++17 or std&#x3D;gnu++17</p><h2 id="C-17-1"><a href="#C-17-1" class="headerlink" title="C++17"></a>C++17</h2><p>完全支持</p><p>从GCC 5版本开始支持，到GCC 7版本已完全支持，是GCC 11到13版本的默认模式</p><p>-std&#x3D;c++17 or std&#x3D;gnu++17</p><p>C++20</p><p>未完全支持</p><p>从GCC 8版本开始支持</p><p>-std&#x3D;c++20 or</p><p>std&#x3D;gnu++20（GCC 9以及之前版本使用**-std&#x3D;c++2a**）</p><h1 id="CPH"><a href="#CPH" class="headerlink" title="CPH"></a>CPH</h1><p>抓取洛谷题目需要<strong>右键选取LuoguProblemParser</strong></p><h1 id="Devc"><a href="#Devc" class="headerlink" title="Devc"></a>Devc</h1><p>最高支持c++11标准</p><p>安装时按默认(包括语言),可改安装到那个盘</p><h3 id="选择语言"><a href="#选择语言" class="headerlink" title="选择语言"></a><strong>选择语言</strong></h3><p>在这里选择简体中文。</p><p><img src="/./%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/903788e3cc733503d1b25dd3be391ac9.png" alt="图片"></p><h2 id="关于调试"><a href="#关于调试" class="headerlink" title="关于调试"></a>关于调试</h2><p>工具-&gt;编译选项-&gt;代码生成优化-&gt;代码生成-&gt;语言标准(根据需要修改)</p><p>​  -&gt; 代码性能-&gt;生成代码性能信息</p><p>​  -&gt;连接器-&gt;产生调试信息</p><p>工具-&gt;环境选项-&gt;鼠标查看变量值</p><p><strong>有改动,重启都一定要先编译再调试</strong></p><p>快捷键   F5   <strong>开始</strong>调试</p><p>快捷键   F6   <strong>停止</strong>调试</p><p>快捷键   F7   单步调试（运行下一步）</p><p>快捷键   F8   单步<strong>进入函数</strong>调试</p><p>如果你调试前设置查看的话，变量会**“Execute to evaluate”**</p><p>遇见 <code>endl</code> 会卡死！调试查看 STL 里面参数会卡死！</p><p><strong>看stl去调试的话一定会卡死,只能打印出来,或者(赋值给一个变量，查看变量)(注意取消鼠标看变量)</strong></p><p><img src="/./%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/600d17661c6b3c7b5f944514539744ae.png" alt="查看"></p><h2 id="自动整理代码"><a href="#自动整理代码" class="headerlink" title="自动整理代码"></a>自动整理代码</h2><p>按下Ctrl+<strong>Shift</strong>+A整理</p><h4 id="调整界面风格"><a href="#调整界面风格" class="headerlink" title="调整界面风格"></a><strong>调整界面风格</strong></h4><p>工具 -&gt; 编辑器选项 -&gt; 基本 -&gt;去掉高亮显示当前行的√</p><p>​-&gt; 语法-&gt;预设-&gt;obsidian</p><h2 id="手动补全-自己用过的变量"><a href="#手动补全-自己用过的变量" class="headerlink" title="手动补全(自己用过的变量)"></a>手动补全(自己用过的变量)</h2><p>打开工具 -&gt; 快捷键选项，找到最下面的Show code completion，把快捷键改成别的 （我选择的是Ctrl+Enter，就是选中你要更改的那项，然后直接按下你想要的组合键，自动就改了），然后确定后退出。<br><img src="/./%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/3eac3cf5dc10ab53b736de9936e7d5d6.png" alt="只要鼠标点击到上面就可以按下要更改的快捷键"></p><p>它只能提示出它<strong>已经缓存好了的头文件的内容</strong>，(不如<strong>去查api文档</strong>)如果你在安装的时候只缓存了常用头文件的话， 你包含<br>以下为收集的一些可以提示和不可以提示的总结：</p><p>已经写上的头文件里面的函数<br>函数形参列表<br>你定义的变量、函数名<br>对象实例的方法、构造方法<br>宏定义</p><p>看上去可以，实际上不能提示的有一些，但不限于：</p><p>语言保留字（一些关键字），比如break、return、continue、sizeof、malloc……<br>头文件（就是#include后不能像visual studio一样出现一些头文件的提示）<br>预处理语句</p><p>如果你想让它提示保留字出来的话，你可以这样解决，那就是写这么一个宏定义（以continue为例）：</p><p>#define continue continue<br>然后continue就能被作为一个宏而提示出来了，但是完全没必要</p><h1 id="小熊猫devc"><a href="#小熊猫devc" class="headerlink" title="小熊猫devc"></a>小熊猫devc</h1><h2 id="配置及使用"><a href="#配置及使用" class="headerlink" title="配置及使用"></a>配置及使用</h2><p>**ctrl+R(非H) **替换(<strong>蓝块为当前</strong>)</p><p>可以<strong>自行调整各栏间距</strong>以方便观察</p><p>F10无功能</p><p>F11为编译并运行(如果有改动会编译,挺智能的)</p><p>F12全部重编译(<strong>改动编译器设置时必须用</strong>)</p><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>自带类似cph插件,在下栏试题处,添加即可</p><p>编译完<strong>确认好是报错还是警告</strong></p><h2 id="调试-可能需要多按几次换行-所以最好输入时就自带"><a href="#调试-可能需要多按几次换行-所以最好输入时就自带" class="headerlink" title="调试(可能需要多按几次换行,所以最好输入时就自带)"></a>调试(可能需要多按几次换行,所以最好输入时就自带)</h2><p>如果<strong>要调试必须把输入输出优化先注释掉,不然不会提前出结果</strong>.</p><p><strong>调试窗口</strong>会<strong>吞复制内容</strong>,<strong>必须再次复制</strong>才行</p><p>ctrl+F5 执行到光标处(中间<strong>有断点就暂停</strong>)</p><p>F6 停止调试</p><h4 id="单步执行"><a href="#单步执行" class="headerlink" title="单步执行"></a>单步执行</h4><p>顾名思义，单步执行就是让程序执行一步后暂停。问题在于，到底多少程序算”一步“？在小熊猫C++的运行菜单中，有这么几种单步执行：</p><ul><li>单步跳过F8：一行程序算一步。执行完当前行后暂停。</li><li>单步进入F7：如果当前行不包含函数调用，则一行程序算一步；如果这行程序中包含对函数的调用，会在进入函数后暂停；如果找不到该函数的符号信息，则在执行完该函数后暂停。</li><li>单步跳出ctrl+F8：退出当前函数后暂停。</li></ul><h4 id="继续执行F4"><a href="#继续执行F4" class="headerlink" title="继续执行F4"></a>继续执行F4</h4><p>程序暂停后，通过”运行“菜单或者调试工具栏选择”继续执行”，程序就<strong>会继续以调试方式运行</strong>，<strong>直到遇到下一个断点(包括在循环内再次遇到自己这行)</strong>，或者程序运行结束为止。</p><h4 id="监视变量"><a href="#监视变量" class="headerlink" title="监视变量"></a>监视变量</h4><p>如果变量在<strong>当前作用域</strong>无效会直接无,尽量开全局看全局,但若同名也只优先显示当前作用域的值</p><p>添加完之后改不了名字,但是<strong>双击可以修改其值</strong></p><p>*(a+3)@10监视数组部分(注意下标也会随+3变化)</p><p>下方栏调试-局部变量可<strong>自动</strong>看当前函数作用域中的所有局部变量（包括函数参数）</p><p>gdb（小熊猫C++使用的调试器）支持监视<strong>任何合法的C、C++表达式(包括函数,这样递归函数时方便直接看返回值了)</strong>。但请保证在<strong>表达式中不会出现无穷递归、无限循环等错误</strong>，否则gdb调试器<strong>会卡死</strong>无法正常使用。</p><h4 id="调用栈视图"><a href="#调用栈视图" class="headerlink" title="调用栈视图"></a>调用栈视图</h4><p>我们按照<strong>自顶向下或者模块化</strong>的思路设计程序时，会<strong>以函数为单位</strong>来组织和实现的程序的功能。在调试程序时，我们经常需要知道，函数现在<strong>正被谁调用</strong>？调用者的状态是怎样的？</p><p>调试面板中的调用栈视图为我们<strong>提供了程序调用栈（Call Stack）的信息</strong>。从下图中我们可以看出，当前程序执行到isPrime函数中，它是在main函数的第30行被调用的。</p><p><img src="/./%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/v2-cd058dfc523f80464f665e4296eb09eb_1440w.jpg" alt="img"></p><p>在调用栈视图中双击某一行，小熊猫C++就会自动跳转到对应的程序位置。</p><p><img src="/./%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/v2-6cf7f15962e80b2d0f4b6c63f5505a22_1440w.jpg" alt="img"></p><h4 id="求值工具"><a href="#求值工具" class="headerlink" title="求值工具"></a>求值工具</h4><p>除了监视和局部变量之外，我们还可以使用求值工具<strong>来快速计算某个表达式</strong>。</p><p><img src="/./%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/v2-2e2ab8da54241be35d7b47a496861217_1440w.jpg" alt="img"></p><p>注意，求值工具可以执行任意合法的C&#x2F;C++表达式，包括赋值表达式！其<strong>效果和在程序中执行该表达式的作用是相同的(别冲突了)</strong>。例如，我们在求值输入框中输入n&#x3D;500，就可以将变量n的值<strong>改为500</strong>。</p><h4 id="输入重定向"><a href="#输入重定向" class="headerlink" title="输入重定向"></a>输入重定向</h4><p><strong>打开gdb server调试模式</strong></p><p>调试的数据重定向功能需要打开gdb server调试模式。从“工具”菜单“选项”打开“选项”对话框，在“<a href="https://zhida.zhihu.com/search?content_id=189947528&content_type=Article&match_order=1&q=%E8%B0%83%E8%AF%95%E5%99%A8&zhida_source=entity">调试器</a>”-&gt;“通用”选项页中可以找到“使用gdb server调试“选项，将其勾选上，然后确定即可。</p><p><img src="/./%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/v2-385e5ac602415c34da1cc9ba0bc15bd4_1440w.jpg" alt="img"></p><h4 id="指定数据文件-从而可以直接只看输出"><a href="#指定数据文件-从而可以直接只看输出" class="headerlink" title="指定数据文件(从而可以直接只看输出)"></a>指定数据文件(从而可以直接只看输出)</h4><p>通过工具栏或者**”运行“菜单中的”运行参数…“按钮，打开”运行参数“选项页**</p><p>在选项页中，勾选”将程序的标准输入重定向到下面的文件“，并选择<strong>数据所在的文件</strong>（本例中使用F盘根目录下的test.txt文件，即F:\test.txt作为输入文件），然后确定即可。</p><p><img src="/./%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/v2-96d0c56a476cc963e80c828d9394fb00_1440w.jpg" alt="img"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>顽强拼搏记录</title>
      <link href="/2025/03/23/%E9%A1%BD%E5%BC%BA%E6%8B%BC%E6%90%8F%E8%AE%B0%E5%BD%95/"/>
      <url>/2025/03/23/%E9%A1%BD%E5%BC%BA%E6%8B%BC%E6%90%8F%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>本篇记录博主自己顽强拼搏的时刻以供回念,以激励博主继续又菜又爱玩.<strong>(没有含金量,全是刺激性)</strong></p><p>2025.3.23</p><h1 id="大工之星第二场"><a href="#大工之星第二场" class="headerlink" title="大工之星第二场"></a>大工之星第二场</h1><p>开头ev录屏忘开了,中间又花了近1h去做了个志愿活动(雾</p><p><img src="/./%E9%A1%BD%E5%BC%BA%E6%8B%BC%E6%90%8F%E8%AE%B0%E5%BD%95/ab0a68dd90d0f528e83e053c6fc546fb.png" alt="ab0a68dd90d0f528e83e053c6fc546fb"></p><p>2025.4.14</p><h1 id="大工之星第五场"><a href="#大工之星第五场" class="headerlink" title="大工之星第五场"></a>大工之星第五场</h1><p>最后发现数据<strong>没取模连lld都爆了</strong>(INT128好像爆不了),急救了一波.</p><p><img src="/./%E9%A1%BD%E5%BC%BA%E6%8B%BC%E6%90%8F%E8%AE%B0%E5%BD%95/2d0d43ec394364fe67b27d4ab4ec41a7.png" alt="2d0d43ec394364fe67b27d4ab4ec41a7"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>算法比赛后记</title>
      <link href="/2025/03/18/%E7%AE%97%E6%B3%95%E6%AF%94%E8%B5%9B%E5%90%8E%E8%AE%B0/"/>
      <url>/2025/03/18/%E7%AE%97%E6%B3%95%E6%AF%94%E8%B5%9B%E5%90%8E%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>唐b杂事</title>
      <link href="/2025/03/17/%E5%94%90b%E6%9D%82%E4%BA%8B/"/>
      <url>/2025/03/17/%E5%94%90b%E6%9D%82%E4%BA%8B/</url>
      
        <content type="html"><![CDATA[<p>[toc]</p><h1 id="算法学习方面"><a href="#算法学习方面" class="headerlink" title="算法学习方面"></a>算法学习方面</h1><p>1.vscode调试跟coderunner插件配置文件是分开来的,要想用vscode自带调试设置为c++20,必须在tasks.json文件中的args里面添加一个”-std&#x3D;c++2<strong>a”,</strong>(注意逗号,双引号和a).</p><p>2.玛德用<strong>宏定义</strong>(纯文本替换卧槽忘了<strong>运算顺序</strong>)被干死了,再用宏定义我就是煞笔.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>麦麦--qq聊天机器人部署备忘</title>
      <link href="/2025/03/17/%E9%BA%A6%E9%BA%A6-qq%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E9%83%A8%E7%BD%B2%E5%A4%87%E5%BF%98/"/>
      <url>/2025/03/17/%E9%BA%A6%E9%BA%A6-qq%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E9%83%A8%E7%BD%B2%E5%A4%87%E5%BF%98/</url>
      
        <content type="html"><![CDATA[<h1 id="重启电脑后如何启动"><a href="#重启电脑后如何启动" class="headerlink" title="重启电脑后如何启动"></a>重启电脑后如何启动</h1><h2 id="启动QQ"><a href="#启动QQ" class="headerlink" title="启动QQ"></a>启动QQ</h2><h2 id="打开compass，启动数据库"><a href="#打开compass，启动数据库" class="headerlink" title="打开compass，启动数据库"></a>打开compass，启动数据库</h2><h2 id="打开bot文件夹，启动终端"><a href="#打开bot文件夹，启动终端" class="headerlink" title="打开bot文件夹，启动终端"></a>打开bot文件夹，启动终端</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bot\\Scripts\\activate</span><br><span class="line">nb run</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>身为一个小白,有了这次经历后,感觉真不该嗯看个几把教程,身为新手,无法分辨教程是不是全对,又搜不到,该问的时候就正确的去发问,真几把别不好意思.</strong></p><p>多看看新的教程,评价好的,完整的教程,甚至是视频教程(虽然慢点但是最稳,能看见所有步)</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>算法学习有感</title>
      <link href="/2025/03/13/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%9C%89%E6%84%9F/"/>
      <url>/2025/03/13/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>一定要有能自主配好<strong>成熟</strong>环境的能力,总会遇到<strong>到新环境要调整</strong>的时候</p><p><strong>多多去通过问问题(学长,ai)等去学习理解问题而非总是自己死磕(自个儿而看都卡了,还光重复多看怎么可能高效学习的了呢?自主学习不是纯靠自己学习,故步自封),要避免不必要的麻烦</strong></p><ul><li><p>一定要能独立完成(<strong>包括不看资料(除非纯生涩那种)</strong>)编码——可以参考别人的代码，但是<strong>写的时候要独立</strong>。凡是参考过别人代码的题，隔一段时间之后<strong>要重写</strong></p></li><li><p>多问几个为什么，不要在知识点和代码里遗留那些：好像这样就能过了的问题，AC不是一个题做完的标志，<strong>想明白的才是</strong>。</p></li><li><p>写题解——题解应该包含的内容：<strong>英文题的大概意思</strong>、大体思路、<strong>“我是哪里没想到”，产生错误提交的原因(别只会总结对的不会纠正错的,那样遇到错遇到不会又出错误思路怎么办)</strong></p></li></ul><h1 id="关键问题：先看题解还是先做题？"><a href="#关键问题：先看题解还是先做题？" class="headerlink" title="关键问题：先看题解还是先做题？"></a>关键问题：先看题解还是先做题？</h1><h2 id="分场景策略"><a href="#分场景策略" class="headerlink" title="分场景策略"></a>分场景策略</h2><table><thead><tr><th>情况</th><th>建议方法</th><th>示例</th></tr></thead><tbody><tr><td><strong>完全无</strong>思路</td><td><strong>先看题解</strong>理解算法思想，<strong>再独立实现</strong>代码</td><td>初次接触“状态压缩DP”时，参考题解学习状态设计技巧。</td></tr><tr><td><strong>有思路</strong>但代码写不出</td><td>尝试手写<strong>伪代码</strong>，再<strong>对比题解优化</strong>逻辑</td><td>BFS迷宫问题中，自己设计队列结构后对比标准实现。</td></tr><tr><td>能<strong>通过部</strong>分测试用例</td><td>先<strong>自行调试（打印中间变量!!!）</strong>，<strong>参考题解找错误</strong></td><td>动态规划题中n&#x3D;0或n&#x3D;1的边界情况处理。</td></tr></tbody></table><h2 id="学习步骤建议"><a href="#学习步骤建议" class="headerlink" title="学习步骤建议"></a>学习步骤建议</h2><ol><li>自主思考（出思路前<strong>10分钟最多</strong>）：尝试建立问题<strong>模型</strong>，写出<strong>伪代码或暴力解法</strong>。  </li><li>代码复现：关闭题解，<strong>独立写出并能记忆下</strong>通过代码，<strong>确保理解</strong>每一行逻辑。</li></ol><h1 id="3-高效刷题技巧"><a href="#3-高效刷题技巧" class="headerlink" title="3. 高效刷题技巧"></a>3. 高效刷题技巧</h1><h2 id="错题本整理"><a href="#错题本整理" class="headerlink" title="错题本整理"></a>错题本整理</h2><ul><li>记录题目<strong>链接</strong>、<strong>错误原因</strong>（如越界、逻辑错误）、正确解法。  </li><li><strong>定期重做高重要度错题</strong>（建议每周复盘一次）。</li></ul><p><img src="/./%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%9C%89%E6%84%9F/image-20250313181037086.png" alt="image-20250313181037086"></p><p><img src="/./%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%9C%89%E6%84%9F/image-20250313181112291.png" alt="image-20250313181112291"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>一个一个计算机网络笔记</title>
      <link href="/2025/03/08/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
      <url>/2025/03/08/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="一个一个计算机网络的笔记"><a href="#一个一个计算机网络的笔记" class="headerlink" title="一个一个计算机网络的笔记"></a>一个一个计算机网络的笔记</h1><p>[TOC]</p><h2 id="TCP-IP-网络模型有哪几层？"><a href="#TCP-IP-网络模型有哪几层？" class="headerlink" title="TCP&#x2F;IP 网络模型有哪几层？"></a>TCP&#x2F;IP 网络模型有哪几层？</h2><p>问大家，为什么要有 TCP&#x2F;IP 网络模型？</p><p>对于同一台设备上的进程间通信，有很多种方式，比如有管道、消息队列、共享内存、信号等方式，而对于不同设备上的进程间通信，就需要网络通信，而设备是多样性的，所以<strong>要兼容多种多样的设备</strong>，就协商出了一套通用的网络协议。</p><p>这个网络协议是<strong>分层</strong>的，每一层都有各自的作用和职责，接下来就根据「 TCP&#x2F;IP 网络模型」分别对每一层进行介绍。</p><h2 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h2><p>最上层的，也是我们能<strong>直接接触到的</strong>就是应用层（Application Layer），我们电脑或手机使用的应用软件都是在应用层实现。那么，当两个不同设备的应用需要通信的时候，应用就把应用数据传给下一层，也就是<strong>传输层</strong>。</p><p>所以，应用层只需要<strong>专注于为用户提供应用功能</strong>，比如 HTTP、FTP、Telnet、DNS、SMTP等。</p><p>应用层是不用去关心数据是如何传输的，就类似于，我们寄快递的时候，只需要把包裹交给快递员，由他负责运输快递，我们不需要关心快递是如何被运输的。</p><p>而且应用层是工作在操作系统中的<strong>用户态</strong>，传输层及以下则工作在<strong>内核态</strong>。</p><h2 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h2><p>应用层的数据包会传给传输层，传输层（Transport Layer）是为应用层提供网络支持的。</p><p>在传输层会有两个传输协议，分别是 TCP 和 UDP。</p><p>TCP 的全称叫<strong>传输控制协议（Transmission Control Protocol）</strong>，大部分应用使用的正是 TCP <strong>传输层</strong>协议，比如 <strong>HTTP 应用层协议</strong>。TCP 相比 UDP 多了很多特性，比如流量控制、超时重传、拥塞控制等，这些都是为了保证数据包能可靠地传输给对方。</p><p>UDP 相对来说就很简单，简单到只负责发送数据包，不保证数据包是否能抵达对方，但它实时性相对更好，传输效率也高。当然，UDP 也可以实现可靠传输，把 TCP 的特性在应用层上实现就可以，不过要实现一个商用的可靠 UDP 传输协议，也不是一件简单的事情。</p><p>应用需要传输的数据可能会非常大，如果直接传输就不好控制，因此当传输层的数据包大小<strong>超过 MSS（TCP 最大报文段长度）</strong> ，就要将<strong>数据包分块</strong>，这样即使中途有一个分块丢失或损坏了，只需要重新发送这一个分块，而不用重新发送整个数据包。在 TCP 协议中，我们把每个分块称为一个 <strong>TCP 段（TCP Segment）</strong>。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP%E6%AE%B5.png" alt="img"></p><p>当设备作为接收方时，传输层则要负责<strong>把数据包传给应用</strong>，但是一台设备上可能会有<strong>很多应用在接收或者传输</strong>数据，因此需要<strong>用一个编号将应用区分</strong>开来，这个编号就是<strong>端口</strong>。</p><p>比如 <strong>80 端口通常是 Web 服务器</strong>用的，<strong>22 端口通常是远程登录服务器</strong>用的。而对于浏览器（客户端）中的<strong>每个标签栏都是一个独立的进程</strong>，操作系统会为这些进程<strong>分配临时的端口号</strong>。</p><p>由于传输层的<strong>报文</strong>中会<strong>携带端口号</strong>，因此接收方可以识别出该报文是发送给哪个应用。</p><h2 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h2><p>传输层可能大家刚接触的时候，会认为它负责将数据从一个设备传输到另一个设备，事实上它<strong>并不负责</strong>。</p><p>实际场景中的网络环节是错综复杂的，中间有各种各样的线路和分叉路口，如果一个设备的数据要传输给另一个设备，就需要在<strong>各种各样的路径和节点进行选择</strong>，而传输层的设计理念是简单、高效、专注，如果传输层还负责这一块功能就有点违背设计原则了。</p><p>也就是说，我们不希望传输层协议处理太多的事情，只需要<strong>服务好应用</strong>即可，让其作为应用间数据传输的媒介，帮助实现<strong>应用到应用的通信</strong>，而<strong>实际的传输功能</strong>就交给<strong>下一</strong>层，也就是<strong>网络层</strong>（Internet Layer）。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E5%B1%82.png" alt="img"></p><p><strong>网络层</strong>最常使用的是 <strong>IP 协议（Internet Protocol）</strong>，IP 协议会<strong>将传输层的报文作为数据部分</strong>，再<strong>加上 IP 包头组装成 IP 报文</strong>，如果 <strong>IP 报文大小超过 MTU（以太网中一般为 1500 字节）<strong>就会再次进行</strong>分片</strong>，得到一个即将发送到网络的 IP 报文。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/12.jpg" alt="img"></p><p>网络层负责将数据从一个设备<strong>传输到另一个</strong>设备，世界上那么多设备，又该如何找到对方呢？因此，网络层需要有<strong>区分设备的编号</strong>。</p><p>我们一般用 IP 地址给设备进行编号，对于 IP<strong>v4</strong> 协议， IP 地址共 <strong>32 位</strong>，<strong>分成了四段</strong>（比如，192.168.100.1），<strong>每段是 8 位</strong>。只有一个单纯的 IP 地址虽然做到了区分设备，但是寻址起来就特别麻烦，全世界那么多台设备，难道一个一个去匹配？这显然不科学。</p><p>因此，需要将 IP 地址分成两种意义：</p><p>一个是<strong>网络号</strong>，负责标识该 IP 地址是属于<strong>哪个「子网」<strong>的；<br>一个是</strong>主机号</strong>，负责标识<strong>同一「子网」下的不同主机</strong>；<br>怎么分的呢？这需要<strong>配合子网掩码</strong>才能算出 IP 地址 的网络号和主机号。</p><p>举个例子，比如 10.100.122.0&#x2F;24，后面的**&#x2F;24表示就是 255.255.255.0 子网掩码**，<strong>255.255.255.0 二进制是「11111111-11111111-11111111-00000000」</strong>，大家数数一共多少个1？不用数了，是 24 个1，为了<strong>简化子网掩码的表示，用&#x2F;24代替255.255.255.0</strong>。</p><p>知道了子网掩码，该怎么计算出网络地址和主机地址呢？</p><p>将 10.100.122.2 和 255.255.255.0 进行<strong>按位与</strong>运算，就可以<strong>得到网络号</strong>，如下图：</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/16.jpg" alt="img"></p><p>将 255.255.255.0 <strong>取反后</strong>与IP地址进行进行<strong>按位与运算</strong>，就可以得到<strong>主机号</strong>。</p><p> 那么在寻址的过程中，先匹配到<strong>相同的网络号</strong>（表示要找到<strong>同一个子网</strong>），才会去找对应的主机。</p><p>除了<strong>寻址能力</strong>， IP 协议还有另一个重要的能力就是<strong>路由</strong>。实际场景中，两台设备并不是用一条网线连接起来的，而是<strong>通过很多网关、路由器、交换机等众多网络设备连接</strong>起来的，那么就会形成很多条网络的路径，因此当数据包到达一个网络节点，就需要<strong>通过路由算法</strong>决定下一步走哪条路径。</p><p>路由器寻址工作中，就是要<strong>找到目标地址的子网</strong>，找到后<strong>进而把数据包转发</strong>给对应的网络内。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/17.jpg" alt="IP地址的网络号"></p><p>所以，IP 协议的寻址作用是告诉我们<strong>去往下一个目的地该朝哪个方向</strong>走，路由则是<strong>根据「下一个目的地」选择路径</strong>。寻址更像在导航，路由更像在操作方向盘。</p><h2 id="网络接口层"><a href="#网络接口层" class="headerlink" title="网络接口层"></a>网络接口层</h2><p>生成了 <strong>IP 头部</strong>之后，接下来要交给<strong>网络接口层（Link Layer）<strong>在 IP 头部的前面</strong>加上 MAC 头部</strong>，并<strong>封装成数据帧（Data frame）发送到网络上</strong>。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3%E5%B1%82.png" alt="img"></p><p>IP 头部中的接收方 IP 地址表示<strong>网络包的目的地</strong>，通过这个地址我们就可以判断要将包发到哪里，但在以太网的世界中，这个思路是行不通的。</p><p>什么是以太网呢？<strong>电脑上的以太网接口，Wi-Fi接口，以太网交换机、路由器上的千兆，万兆以太网口，还有网线，它们都是以太网的组成部分。<strong>以太网就是一种</strong>在「局域网」内</strong>，把附近的设备连接起来，使它们之间可以进行通讯的技术。</p><p>以太网在判断网络包目的地时和 IP 的方式不同，因此<strong>必须采用相匹配的方式</strong>才能在以太网中将包发往目的地，而 MAC 头部就是干这个用的，所以，<strong>在以太网进行通讯要用到 MAC 地址</strong>。</p><p>MAC 头部是以太网使用的头部，它<strong>包含了接收方和发送方的 MAC 地址</strong>等信息，我们可以<strong>通过 ARP 协议获取对方</strong>的 MAC 地址。</p><p>所以说，网络接口层主要<strong>为网络层提供「链路级别」传输的服务</strong>，负责在以太网、WiFi 这样的<strong>底层网络上发送</strong>原始数据包，<strong>工作在网卡这个层次</strong>，使用 MAC 地址来标识网络上的设备。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>综上所述，TCP&#x2F;IP 网络通常是由上到下分成 4 层，分别是应用层，传输层，网络层和网络接口层。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/tcpip%E5%8F%82%E8%80%83%E6%A8%A1%E5%9E%8B.drawio.png" alt="img"></p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E5%B0%81%E8%A3%85.png" alt="img"></p><p>网络接口层的传输单位是<strong>帧（frame）</strong>，IP 层的传输单位是<strong>包（packet）</strong>，TCP 层的传输单位是<strong>段（segment）</strong>，HTTP 的传输单位则是<strong>消息或报文（message）</strong>。但这些名词并没有什么本质的区分，可以<strong>统称为数据包</strong>。</p><h2 id="2-2-键入网址到网页显示，期间发生了什么？"><a href="#2-2-键入网址到网页显示，期间发生了什么？" class="headerlink" title="2.2 键入网址到网页显示，期间发生了什么？"></a>2.2 键入网址到网页显示，期间发生了什么？</h2><p>以下图较简单的<strong>网络拓扑模型</strong>作为例子，探究探究其间发生了什么</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/2.jpg" alt="简单的网络模型"></p><h2 id="孤单小弟-——-HTTP"><a href="#孤单小弟-——-HTTP" class="headerlink" title="孤单小弟 —— HTTP"></a>孤单小弟 —— HTTP</h2><blockquote><p>浏览器做的第一步工作是解析 URL</p></blockquote><p>首先浏览器做的第一步工作就是要<strong>对 URL 进行解析</strong>，从而<strong>生成发送给 Web 服务器的请求信息</strong>。</p><p>让我们看看一条长长的 URL 里的各个元素的代表什么，见下图：</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/3.jpg" alt="URL 解析"></p><p>所以图中的长长的 URL 实际上是<strong>请求服务器</strong>里的文件资源。</p><h4 id="要是上图中的蓝色部分-URL-元素都省略了，那应该是请求哪个文件呢？"><a href="#要是上图中的蓝色部分-URL-元素都省略了，那应该是请求哪个文件呢？" class="headerlink" title="要是上图中的蓝色部分 URL 元素都省略了，那应该是请求哪个文件呢？"></a>要是上图中的蓝色部分 URL 元素都省略了，那应该是请求哪个文件呢？</h4><p>当<strong>没有路径名时</strong>，就代表<strong>访问根目录下</strong>事先设置的默认文件，也就是 &#x2F;index.html 或者 &#x2F;default.html 这些文件，这样就不会发生混乱了。</p><h4 id="生产-HTTP-请求信息"><a href="#生产-HTTP-请求信息" class="headerlink" title="生产 HTTP 请求信息"></a>生产 HTTP 请求信息</h4><p>对 URL 进行解析之后，浏览器<strong>确定了 Web 服务器和文件名</strong>，接下来就是根<strong>据这些信息来生成 HTTP 请求</strong>消息了。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/4.jpg" alt="HTTP 的消息格式"></p><p>一个孤单 HTTP 数据包表示：“我这么一个小小的数据包，没亲没友，直接发到浩瀚的网络，谁会知道我呢？谁能载我一程呢？谁能保护我呢？我的目的地在哪呢？”。充满各种疑问的它，没有停滞不前，依然踏上了征途！</p><h2 id="真实地址查询-——-DNS"><a href="#真实地址查询-——-DNS" class="headerlink" title="真实地址查询 —— DNS"></a>真实地址查询 —— DNS</h2><p>通过浏览器解析 URL 并生成 HTTP 消息后，需要<strong>委托操作系统将消息发送给 Web 服务器</strong>。</p><p>但在发送之前，还有一项工作需要完成，那就是<strong>查询服务器域名对应的 IP 地址</strong>，因为委托操作系统发送消息时，必须提供通信对象的 IP 地址。</p><p>比如我们打电话的时候，必须要知道对方的电话号码，但由于电话号码难以记忆，所以通常我们会将对方电话号 + 姓名保存在通讯录里。</p><p>所以，<strong>有一种服务器就专门保存了 Web 服务器域名与 IP 的对应关系</strong>，它就是 <strong>DNS 服务器</strong>。</p><h4 id="域名的层级关系"><a href="#域名的层级关系" class="headerlink" title="域名的层级关系"></a>域名的层级关系</h4><p>DNS 中的域名都是用句点来分隔的，比如 <a href="http://www.server.com,这里的句点代表了**不同层次之间的界限**./">www.server.com，这里的句点代表了**不同层次之间的界限**。</a></p><p>在域名中，<strong>越靠右的位置表示其层级越高</strong>。</p><p>毕竟域名是外国人发明，所以思维和中国人相反，比如说一个城市地点的时候，<strong>外国喜欢从小到大的方式</strong>顺序说起（如 XX 街道 XX 区 XX 市 XX 省），而中国则喜欢从大到小的顺序（如 XX 省 XX 市 XX 区 XX 街道）。</p><p>实际上域名最后还有一个点，比如 <a href="http://www.server.com.,这个最后的一个点代表**根域名**./">www.server.com.，这个最后的一个点代表**根域名**。</a></p><p>也就是，. 根域是在最顶层，它的下一层就是 .com 顶级域，再下面是 server.com。</p><p>所以域名的层级关系类似一个<strong>树状结构</strong>：</p><p><strong>根 DNS 服务器</strong>（.）<br><strong>顶级域 DNS 服务器</strong>（.com）<br><strong>权威 DNS 服务器</strong>（server.com）</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/5.jpg" alt="DNS 树状结构"></p><p><strong>根域</strong>的 DNS 服务器信息保存在<strong>互联网中所有的 DNS 服务器中</strong>。</p><p>这样一来，任何 DNS 服务器就都可以<strong>找到并访问根域 DNS 服务器</strong>了。</p><p>因此，客户端只要能够找到<strong>任意一台 DNS 服务器</strong>，就可以通过它找到根域 DNS 服务器，然后再一路顺藤摸瓜找到位于下层的某台目标 DNS 服务器。</p><h4 id="域名解析的工作流程"><a href="#域名解析的工作流程" class="headerlink" title="域名解析的工作流程"></a>域名解析的工作流程</h4><p>客户端首先会发出一个 <strong>DNS 请求</strong>，问 <a href="http://www.server.com/">www.server.com</a> 的 <strong>IP 是啥</strong>，并发给<strong>本地 DNS 服务器</strong>（也就是客户端的 <strong>TCP&#x2F;IP 设置中填写的 DNS 服务器地址</strong>）。<br>本地域名服务器收到客户端的请求后，如果<strong>缓存里</strong>的表格能找到 <a href="http://www.server.com,则它**直接返回/">www.server.com，则它**直接返回</a>** IP 地址。如果没有，本地 DNS 会去问它的根域名服务器：“老大， 能告诉我 <a href="http://www.server.com/">www.server.com</a> 的 IP 地址吗？” 根域名服务器是<strong>最高层次</strong>的，它不直接用于域名解析，但能指明一条道路。<br>根 DNS 收到来自本地 DNS 的请求后，发现后置是 .com，说：“<a href="http://www.server.com/">www.server.com</a> 这个域名归 .com 区域管理”，我给你 .com 顶级域名服务器地址给你，你去问问它吧。”<br>本地 DNS 收到顶级域名服务器的地址后，发起请求问“老二， 你能告诉我 <a href="http://www.server.com/">www.server.com</a> 的 IP 地址吗？”<br>顶级域名服务器说：“我给你负责 <a href="http://www.server.com/">www.server.com</a> 区域的权威 DNS 服务器的地址，你去问它应该能问到”。<br>本地 DNS 于是转向问权威 DNS 服务器：“老三，<a href="http://www.server.com对应的IP是啥呀？”">www.server.com对应的IP是啥呀？”</a> server.com 的权威 DNS 服务器，它是<strong>域名解析结果的原出处</strong>。为啥叫权威呢？就是我的域名我做主。<br>权威 DNS 服务器<strong>查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS</strong>。<br>本地 DNS <strong>再将 IP 地址返回客户端</strong>，客户端和目标建立连接。<br>至此，我们完成了 DNS 的解析过程。现在总结一下，整个过程我画成了一个图。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/6.jpg" alt="域名解析的工作流程"></p><p>DNS 域名解析的过程蛮有意思的，整个过程就和我们日常生活中找人问路的过程类似，<strong>只指路不带路</strong>。</p><p>那是不是每次解析域名都要经过那么多的步骤呢？</p><p>当然不是了，还有缓存这个东西的嘛。</p><p>浏览器会先看自身有没有对这个域名的缓存，如果有，就直接返回，如果没有，就去问<strong>操作系统</strong>，操作系统也会去看自己的缓存，如果有，就直接返回，如果没有，再去 <strong>hosts 文件</strong>看，也没有，<strong>才会去问「本地 DNS 服务器」</strong>。</p><p>数据包表示：“DNS 老大哥厉害呀，找到了目的地了！我还是很迷茫呀，我要发出去，接下来我需要谁的帮助呢?”</p><h2 id="指南好帮手-——-协议栈"><a href="#指南好帮手-——-协议栈" class="headerlink" title="指南好帮手 —— 协议栈"></a>指南好帮手 —— 协议栈</h2><p>通过 DNS 获取到 IP 后，就可以把 HTTP 的<strong>传输</strong>工作交给操作系统中的<strong>协议栈</strong>。</p><p>协议栈的内部分为几个部分，分别承担不同的工作。上下关系是有一定的规则的，上面的部分会向下面的部分委托工作，下面的部分收到委托的工作并执行。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/7.jpg" alt="img"></p><p>应用程序（浏览器）通过<strong>调用 Socket 库</strong>，来委托协议栈工作。协议栈的上半部分有两块，分别是<strong>负责收发数据的 TCP 和 UDP 协议</strong>，这两个传输协议会接受应用层的<strong>委托执行收发数据</strong>的操作。</p><p>协议栈的下面一半是用 <strong>IP 协议控制网络包收发操作</strong>，在互联网上传数据时，数据会被切分成一块块的网络包，而将网络包<strong>发送给对方</strong>的操作就是由 <strong>IP 负责</strong>的。</p><p>此外 IP 中还包括 ICMP 协议和 ARP 协议。</p><p>ICMP 用于告知网络包传送过程中产生的<strong>错误以及各种控制信息</strong>。<br>ARP 用于<strong>根据 IP 地址查询相应的以太网 MAC 地址</strong>。</p><p>IP 下面的网卡驱动程序负责控制网卡硬件，而最下面的网卡则负责完成<strong>实际的收发操作</strong>，也就是对网线中的信号执行发送和接收操作。</p><p>数据包看了这份指南表示：“原来我需要那么多大佬的协助啊，那我先去找找 TCP 大佬！”</p><h2 id="可靠传输-——-TCP"><a href="#可靠传输-——-TCP" class="headerlink" title="可靠传输 —— TCP"></a>可靠传输 —— TCP</h2><p><strong>HTTP</strong> 是<strong>基于 TCP 协议传输</strong>的，所以在这我们先了解下 TCP 协议。</p><h4 id="TCP-包头格式"><a href="#TCP-包头格式" class="headerlink" title="TCP 包头格式"></a>TCP 包头格式</h4><p>TCP <strong>报文头部</strong>的格式</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/8.jpg" alt="TCP 包头格式"></p><p>首先，<strong>源端口号</strong>和<strong>目标端口号</strong>是不可少的，如果没有这两个端口号，数据就不知道应该发给哪个应用。</p><p>接下来有<strong>包的序号</strong>，这个是为了解决包乱序的问题。</p><p>还有应该有的是<strong>确认号</strong>，目的是确认发出去对方是否有收到。如果<strong>没有收到就应该重新发送，直到送达</strong>，这个是为了<strong>解决丢包的问题</strong>。</p><p>接下来还有一些<strong>状态位</strong>。例如 <strong>SYN 是发起一个连接</strong>，<strong>ACK 是回复</strong>，<strong>RST 是重新连接</strong>，<strong>FIN 是结束连接</strong>等。TCP 是面向连接的，因而双方要维护连接的状态，这些带状态位的包的发送，会引起双方的状态变更。</p><p>还有一个重要的就是<strong>窗口大小</strong>。TCP 要做流量控制，通信双方<strong>各声明一个窗口（缓存大小）</strong>，标识自己<strong>当前能够的处理能力</strong>，别发送的太快，撑死我，也别发的太慢，饿死我。</p><p>除了做流量控制以外，TCP还会做<strong>拥塞控制</strong>，对于真正的通路堵车不堵车，它无能为力，唯一能做的就是控制自己，也即<strong>控制发送的速度</strong>。不能改变世界，就改变自己嘛。</p><h4 id="TCP-传输数据之前，要先三次握手建立连接"><a href="#TCP-传输数据之前，要先三次握手建立连接" class="headerlink" title="TCP 传输数据之前，要先三次握手建立连接"></a>TCP 传输数据之前，要先三次握手建立连接</h4><p>在 <strong>HTTP 传输数据</strong>之前，首先需要 <strong>TCP 建立连接</strong>，TCP 连接的建立，通常称为<strong>三次握手</strong>。</p><p>这个所谓的「连接」，只是双方计算机里维护一个状态机，在连接建立的过程中，双方的状态变化时序图就像这样。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.drawio.png" alt="TCP 三次握手"></p><p>一开始，客户端和服务端<strong>都处于 CLOSED 状态</strong>。<strong>先是服务端主动监听某个端口</strong>，处于 <strong>LISTEN 状态</strong>。</p><p>然后<strong>客户端主动发起连接 SYN</strong>，之后<strong>处于 SYN-SENT 状态</strong>。</p><p>服务端收到发起的连接，<strong>返回 SYN</strong>，并且 <strong>ACK 客户端的 SYN</strong>，之后处于 <strong>SYN-RCVD 状态</strong>。</p><p>客户端收到<strong>服务端发送的 SYN 和 ACK</strong> 之后，<strong>发送对 SYN 确认的 ACK</strong>，之后<strong>处于 ESTABLISHED 状态</strong>，因为它<strong>一发一收成功</strong>了。</p><p>服务端收到 <strong>ACK 的 ACK</strong> 之后，<strong>处于 ESTABLISHED 状态</strong>，因为它也<strong>一发一收</strong>了。</p><p>所以三次握手目的是<strong>保证双方都有发送和接收</strong>的能力。</p><h4 id="如何查看-TCP-的连接状态？"><a href="#如何查看-TCP-的连接状态？" class="headerlink" title="如何查看 TCP 的连接状态？"></a>如何查看 TCP 的连接状态？</h4><p>TCP 的连接状态查看，在 Linux 可以通过 <strong>netstat -napt 命令</strong>查看。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/10.jpg" alt="TCP 连接状态查看"></p><h4 id="TCP-分割数据"><a href="#TCP-分割数据" class="headerlink" title="TCP 分割数据"></a>TCP 分割数据</h4><p>如果 HTTP 请求消息比较长，超过了 MSS 的长度，这时 TCP 就需要把 HTTP 的数据拆解成一块块的数据发送，而不是一次性发送所有数据。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/11.jpg" alt="MTU 与 MSS"></p><p>MTU：一个<strong>网络包的最大长度</strong>，以太网中一般为 1500 字节。<br>MSS：<strong>除去 IP 和 TCP 头部之后</strong>，一个网络包所能容纳的 TCP 数据的最大长度。<br>数据会被以 MSS 的长度为单位进行拆分，拆分出来的每一块数据都会被放进<strong>单独的网络包中</strong>。也就是在每个被拆分的数据加上 TCP 头信息，然后交给 IP 模块来发送数据。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/12-1741610449767-37.jpg" alt="数据包分割"></p><h4 id="TCP-报文生成"><a href="#TCP-报文生成" class="headerlink" title="TCP 报文生成"></a>TCP 报文生成</h4><p>TCP 协议里面会有两个端口，一个是<strong>浏览器监听的端口</strong>（通常是<strong>随机生成</strong>的），一个是 <strong>Web 服务器监听的端口</strong>（<strong>HTTP</strong> 默认端口号是 <strong>80</strong>， <strong>HTTPS</strong> 默认端口号是 <strong>443</strong>）。</p><p>在双方建立了连接后，<strong>TCP 报文</strong>中的<strong>数据部分</strong>就是存放 <strong>HTTP 头部 + 数据</strong>，组装好 TCP 报文之后，就需交给下面的网络层处理。</p><p>至此，网络包的报文如下图。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/13.jpg" alt="TCP 层报文"></p><p>此时，遇上了 TCP 的 数据包激动表示：“太好了，碰到了可靠传输的 TCP 传输，它给我加上 TCP 头部，我不再孤单了，安全感十足啊！有大佬可以保护我的可靠送达！但我应该往哪走呢？”</p><h2 id="远程定位-——-IP"><a href="#远程定位-——-IP" class="headerlink" title="远程定位 —— IP"></a>远程定位 —— IP</h2><p>TCP 模块在执行<strong>连接、收发、断开</strong>等各阶段操作时，<strong>都需要委托 IP 模块</strong>将数据封装成网络包发送给通信对象。</p><h4 id="IP-包头格式"><a href="#IP-包头格式" class="headerlink" title="IP 包头格式"></a>IP 包头格式</h4><p>IP 报文头部的格式：</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/14.jpg" alt="IP 包头格式"></p><p>在 IP 协议里面需要有<strong>源地址 IP</strong> 和 <strong>目标地址 IP</strong>：</p><p>源地址IP，即是<strong>客户端输出</strong>的 IP 地址；<br>目标地址，即通过 DNS 域名解析得到的 <strong>Web 服务器 IP</strong>。<br>因为 HTTP 是经过 TCP 传输的，所以在 <strong>IP 包头的协议号</strong>，要填<strong>写为 06（十六进制）</strong>，<strong>表示协议为 TCP</strong>。</p><p>假设客户端有<strong>多个网卡</strong>，就会有<strong>多个 IP 地址</strong>，那 IP 头部的<strong>源地址</strong>应该选择哪个 IP 呢？</p><p>当存在多个网卡时，在填写源地址 IP 时，就需要判断到底应该填写哪个地址。这个判断相当于在多块网卡中判断<strong>应该使用哪个一块网卡</strong>来发送包。</p><p>这个时候就需要<strong>根据路由表规则</strong>，来判断哪一个网卡作为源地址 IP。</p><p>在 Linux 操作系统，我们可以使用 route -n 命令查看当前系统的路由表。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/15.jpg" alt="路由表"></p><p>举个例子，根据上面的路由表，我们假设 Web 服务器的目标地址是 192.168.10.200。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/16-1741610779790-46.jpg" alt="路由规则判断"></p><p>1.首先先和<strong>第一条目的子网掩码（Genmask）<strong>进行 <strong>与运算</strong>，得到结果为 192.168.10.0，但是第一个条目的 Destination 是 192.168.3.0，两者不一致所以匹配失败。<br>2.再与第二条目的子网掩码进行 与运算，得到的结果为 192.168.10.0，与第二条目的 Destination 192.168.10.0 匹配成功，所以将使用 eth1 网卡的 IP 地址作为 <strong>IP 包头的源地址</strong>。<br>那么假设 Web 服务器的目标地址是 10.100.20.100，那么依然</strong>依照上面的路由表规则判断</strong>，判断后的结果是和第三条目匹配。</p><p>第三条目比较特殊，它<strong>目标地址和子网掩码都是 0.0.0.0</strong>，这表示<strong>默认网关</strong>，如果其他所有条目都无法匹配，就会<strong>自动匹配这一行</strong>。并且后续就把包发给路由器，<strong>Gateway 即是路由器的 IP 地址</strong>。</p><p>至此，网络包的报文如下图。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/17-1741680701446-49.jpg" alt="IP 层报文"></p><p>此时，加上了 IP 头部的数据包表示 ：“有 IP 大佬给我指路了，感谢 IP 层给我加上了 IP 包头，让我有了<strong>远程定位的能力</strong>！不会害怕在浩瀚的互联网迷茫了！可是目的地好远啊，我下一站应该去哪呢？”</p><h2 id="两点传输-——-MAC"><a href="#两点传输-——-MAC" class="headerlink" title="两点传输 —— MAC"></a>两点传输 —— MAC</h2><p>生成了 IP 头部之后，接下来网络包还需要<strong>在 IP 头部的前面加上 MAC 头部</strong></p><h4 id="MAC-包头格式"><a href="#MAC-包头格式" class="headerlink" title="MAC 包头格式"></a>MAC 包头格式</h4><p>MAC 头部<strong>是以太网使用的头部</strong>，它包含了<strong>接收方和发送方的 MAC 地址</strong>等信息。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/18.jpg" alt="MAC 包头格式"></p><p>在 MAC 包头里需要发送方 MAC 地址和接收方目标 MAC 地址，用于两点之间的传输。</p><p>一般在 TCP&#x2F;IP 通信里，<strong>MAC 包头的协议类型</strong>只使用：</p><p><strong>0800 ： IP 协议</strong><br><strong>0806 ： ARP 协议</strong></p><h4 id="MAC-发送方和接收方如何确认"><a href="#MAC-发送方和接收方如何确认" class="headerlink" title="MAC 发送方和接收方如何确认?"></a>MAC 发送方和接收方如何确认?</h4><p><strong>发送方</strong>的 MAC 地址获取就比较简单了，MAC 地址是<strong>在网卡生产时写入到 ROM 里</strong>的，只要将这个值读取出来写入到 MAC 头部就可以了。</p><p><strong>接收方</strong>的 MAC 地址就有点复杂了，只要告诉以太网对方的 MAC 的地址，以太网就会帮我们把包发送过去，那么很显然这里应该填写对方的 MAC 地址。</p><p>所以先得搞清楚应该把包发给谁，这个只要查一下路由表就知道了。在路由表中找到相匹配的条目，然后把包发给 Gateway 列中的 IP 地址就可以了。</p><h4 id="既然知道要发给谁，该如何获取对方的-MAC-地址呢？"><a href="#既然知道要发给谁，该如何获取对方的-MAC-地址呢？" class="headerlink" title="既然知道要发给谁，该如何获取对方的 MAC 地址呢？"></a>既然知道要发给谁，该如何获取对方的 MAC 地址呢？</h4><p>不知道对方 MAC 地址？不知道就喊呗。</p><p>此时就需要 ARP 协议帮我们找到路由器的 MAC 地址。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/19.jpg" alt="ARP 广播"></p><p>ARP 协议会在以太网中以广播的形式，对以太网所有的设备喊出：“<strong>这个 IP 地址是谁的</strong>？请把你的 <strong>MAC 地址告诉我”</strong>。</p><p>然后就会有人回答：“这个 IP 地址是我的，我的 MAC 地址是 XXXX”。</p><p>如果对方和自己<strong>处于同一个子网中</strong>，那么通过上面的操作就可以得到对方的 MAC 地址。然后，我们将这个 MAC 地址写入 MAC 头部，MAC 头部就完成了。</p><h4 id="好像每次都要广播获取，这不是很麻烦吗？"><a href="#好像每次都要广播获取，这不是很麻烦吗？" class="headerlink" title="好像每次都要广播获取，这不是很麻烦吗？"></a>好像每次都要广播获取，这不是很麻烦吗？</h4><p>放心，在后续操作系统会把本次查询结果放到一块叫做 <strong>ARP 缓存的内存空间</strong>留着以后用，不过<strong>缓存的时间就几分钟</strong>。</p><p>也就是说，在发包时：</p><p><strong>先查询 ARP 缓存</strong>，如果其中已经保存了对方的 MAC 地址，就不需要发送 ARP 查询，直接使用 ARP 缓存中的地址。<br>而当 ARP 缓存中不存在对方 MAC 地址时，则<strong>发送 ARP 广播</strong>查询。</p><h4 id="查看-ARP-缓存内容"><a href="#查看-ARP-缓存内容" class="headerlink" title="查看 ARP 缓存内容"></a>查看 ARP 缓存内容</h4><p>在 Linux 系统中，我们可以使用 arp -a 命令来查看 ARP 缓存的内容。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/20.jpg" alt="ARP 缓存内容"></p><h4 id="MAC-报文生成"><a href="#MAC-报文生成" class="headerlink" title="MAC 报文生成"></a>MAC 报文生成</h4><p>至此，网络包的报文如下图。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/21.jpg" alt="MAC 层报文"></p><p>此时，加上了 MAC 头部的数据包万分感谢，说道 ：“感谢 MAC 大佬，我知道我下一步要去哪了！我现在有很多头部兄弟，相信我可以到达最终的目的地！”。 带着众多头部兄弟的数据包，终于准备要出门了。</p><h2 id="出口-——-网卡"><a href="#出口-——-网卡" class="headerlink" title="出口 —— 网卡"></a>出口 —— 网卡</h2><p>网络包只是<strong>存放在内存中的一串二进制数字</strong>信息，没有办法直接发送给对方。因此，我们需要将<strong>数字信息转换为电信号</strong>，才能<strong>在网线上传输</strong>，也就是说，这才是真正的数据发送过程。</p><p><strong>负责执行这一操作的是网卡</strong>，要控制网卡还需要靠网卡驱动程序。</p><p>网卡驱动获取网络包之后，会将其<strong>复制到网卡内的缓存区中</strong>，接着会在其<strong>开头加上报头和起始帧分界符</strong>，在<strong>末尾加上用于检测错误的帧校验序列</strong>。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E6%95%B0%E6%8D%AE%E5%8C%85.drawio.png" alt="数据包"></p><ul><li>起始帧分界符是一个用来表示包起始位置的标记</li><li>末尾的 FCS（帧校验序列）用来检查包传输过程是否有损坏</li></ul><p>最后<strong>网卡会将包转为电信号</strong>，通过网线发送出去。</p><p>唉，真是不容易，发一个包，真是历经千辛万苦。致此，一个带有许多头部的数据终于踏上寻找目的地的征途了！</p><h2 id="送别者-——-交换机"><a href="#送别者-——-交换机" class="headerlink" title="送别者 —— 交换机"></a>送别者 —— 交换机</h2><p>下面来看一下包是如何通过交换机的。交换机的设计是<strong>将网络包原样转发到目的地</strong>。<strong>交换机工作在 MAC 层</strong>，也称为二层网络设备。</p><h4 id="交换机的包接收操作"><a href="#交换机的包接收操作" class="headerlink" title="交换机的包接收操作"></a>交换机的包接收操作</h4><p>首先，电信号到达网线接口，交换机里的模块进行接收，接下来<strong>交换机里的模块将电信号转换为数字信号</strong>。</p><p>然后<strong>通过包末尾的 FCS 校验错误</strong>，如果没问题则放到缓冲区。这部分操作基本和计算机的网卡相同，但交换机的工作方式和网卡不同。</p><p>计算机的网卡本身具有 MAC 地址，并通过核对收到的包的接收方 MAC 地址判断<strong>是不是发给自己的</strong>，如果不是发给自己的则丢弃；相对地，<strong>交换机的端口不核对接收方 MAC 地址</strong>，而是<strong>直接接收所有的包并存放到缓冲区中</strong>。因此，和网卡不同，交换机的<strong>端口</strong>不具有 MAC 地址。</p><p>将包存入缓冲区后，接下来需要查询一下<strong>这个包的接收方 MAC 地址是否已经在 MAC 地址表中有记录</strong>了。</p><p><strong>交换机的 MAC 地址</strong>表主要包含两个信息：</p><ul><li>一个是<strong>设备的 MAC 地址</strong>，</li><li>另一个是该设备<strong>连接在交换机的哪个端口</strong>上。</li></ul><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/23.jpg" alt="交换机的 MAC 地址表"></p><p>举个例子，如果<strong>收到的包的接收方 MAC 地址</strong>为 00-02-B3-1C-9C-F9，则<strong>与图中表中的第 3 行匹配，根据端口列的信息</strong>，可知这个地址位于 3 号端口上，然后就可以<strong>通过交换电路将包发送到相应的端口</strong>了。</p><p>所以，<strong>交换机根据 MAC 地址表查找 MAC 地址，然后将信号发送到相应的端口</strong>。</p><h4 id="当-MAC-地址表找不到指定的-MAC-地址会怎么样？"><a href="#当-MAC-地址表找不到指定的-MAC-地址会怎么样？" class="headerlink" title="当 MAC 地址表找不到指定的 MAC 地址会怎么样？"></a>当 MAC 地址表找不到指定的 MAC 地址会怎么样？</h4><p>地址表中找不到指定的 MAC 地址。这可能是因为具有该地址的设备还没有向交换机发送过包，或者这个设备一段时间没有工作导致地址被从地址表中删除了。</p><p>这种情况下，交换机无法判断应该把包转发到哪个端口，只能将包转<strong>发到除了源端口之外的所有端口上</strong>，<strong>无论该设备连接在哪个端口上都能收到这个包</strong>。</p><p>这样做不会产生什么问题，因为以太网的设计本来就是将包发送到整个网络的，然后<strong>只有相应的接收者才接收包，而其他设备则会忽略这个包</strong>。</p><p>有人会说：“这样做会发送多余的包，会不会造成网络拥塞呢？”</p><p>其实完全不用过于担心，因为发送了包之后目标设备会作出响应，只要返回了响应包，交换机就可以将它的地址写入 MAC 地址表，下次也就不需要把包发到所有端口了。</p><p>局域网中<strong>每秒可以传输上千个包，多出一两个包并无大碍</strong>。</p><p>此外，如果接收方 MAC 地址是一个<strong>广播地址</strong>，那么交换机会将包发送到除源端口之外的所有端口。</p><p><strong>以下两个属于广播地址：</strong></p><p><strong>MAC 地址中的 FF:FF:FF:FF:FF:FF</strong><br><strong>IP 地址中的 255.255.255.255</strong><br>数据包通过交换机转发抵达了路由器，准备要<strong>离开土生土长的子网</strong>了。此时，数据包和交换机离别时说道：“感谢交换机兄弟，帮我转发到出境的大门，我要出远门啦！”</p><h4 id="出境大门-——-路由器"><a href="#出境大门-——-路由器" class="headerlink" title="出境大门 —— 路由器"></a>出境大门 —— 路由器</h4><p>路由器与交换机的区别</p><p>网络包经过交换机之后，现在<strong>到达了路由器，并在此被转发到下一个路由器或目标设备</strong>。</p><p>这一步转发的工作原理和交换机类似，也是通过<strong>查表判断包转发的目标</strong>。</p><p>不过在具体的操作过程上，路由器和交换机是有区别的。</p><p>因为<strong>路由器是基于 IP 设计</strong>的，俗称三层网络设备，<strong>路由器的各个端口都具有 MAC 地址和 IP 地址</strong>；<br>而<strong>交换机是基于以太网设计</strong>的，俗称二层网络设备，交换机的<strong>端口不具有 MAC 地址</strong>。</p><h4 id="路由器基本原理"><a href="#路由器基本原理" class="headerlink" title="路由器基本原理"></a>路由器基本原理</h4><p>路由器的<strong>端口具有 MAC 地址</strong>，因此它<strong>就能够成为以太网的发送方和接收方</strong>；同时<strong>还具有 IP 地址</strong>，从这个意义上来说，它和计算机的网卡是一样的。</p><p>当转发包时，首先路由器端口会接收发给自己的以太网包，然后路由表查询转发目标，再由相应的端口作为发送方将<strong>以太网包</strong>发送出去。</p><h4 id="路由器的包接收操作"><a href="#路由器的包接收操作" class="headerlink" title="路由器的包接收操作"></a>路由器的包接收操作</h4><p>首先，<strong>电信号到达网线接口</strong>部分，路由器中的模块会<strong>将电信号转成数字信号</strong>，然后通过<strong>包末尾的 FCS 进行错误校验</strong>。</p><p>如果<strong>没问题则检查 MAC 头部中的接收方 MAC 地址</strong>，看看<strong>是不是发给自己的包</strong>，如果是就放到接收缓冲区中，否则就丢弃这个包。</p><p>总的来说，路由器的端口都具有 MAC 地址，只接收与自身地址匹配的包，遇到不匹配的包则直接丢弃。</p><h4 id="查询路由表确定输出端口"><a href="#查询路由表确定输出端口" class="headerlink" title="查询路由表确定输出端口"></a>查询路由表确定输出端口</h4><p>完成包接收操作之后，路由器就会<strong>去掉包开头的 MAC 头部</strong>。</p><p><strong>MAC 头部的作用就是将包送达路由器</strong>，其中的<strong>接收方 MAC 地址就是路由器端口的 MAC 地址</strong>。因此，当包到达路由器之后，MAC 头部的任务就完成了，于是 MAC 头部就会被丢弃。</p><p>接下来，路由器会根据 MAC 头部后方的 <strong>IP 头部中的内容进行包的转发操作</strong>。</p><p>转发操作分为几个阶段，<strong>首先是查询路由表</strong>判断转发目标。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/24.jpg" alt="路由器转发"></p><p>具体的工作流程根据上图，举个例子。</p><p>假设地址为 10.10.1.101 的计算机要向地址为 192.168.1.100 的服务器发送一个包，这个包先到达图中的路由器。</p><p>判断转发目标的第一步，就是<strong>根据包的接收方 IP 地址查询路由表中的目标地址栏</strong>，以找到相匹配的记录。</p><p>路由匹配和前面讲的一样，<strong>每个条目的子网掩码和 192.168.1.100 IP 做 &amp; 与运算后</strong>，得到的结果与<strong>对应条目的目标地址</strong>进行<strong>匹配</strong>，如果匹配就会<strong>作为候选转发目标</strong>，如果不匹配就继续与下个条目进行路由匹配。</p><p>如第二条目的子网掩码 255.255.255.0 与 192.168.1.100 IP 做 &amp; 与运算后，得到结果是 192.168.1.0 ，这与第二条目的目标地址 192.168.1.0 匹配，该第二条目记录就会被作为转发目标。</p><p>实在找不到匹配路由时，就会<strong>选择默认路由</strong>，路由表中<strong>子网掩码为 0.0.0.0 的记录表示「默认路由」</strong>。</p><h5 id="路由器的发送操作"><a href="#路由器的发送操作" class="headerlink" title="路由器的发送操作"></a>路由器的发送操作</h5><p>接下来就会进入包的发送操作。</p><p>首先，我们需要<strong>根据路由表的网关列判断对方的地址</strong>。</p><p>如果网关是一个 IP 地址，则<strong>这个IP 地址就是我们要转发到的目标地址</strong>，还未抵达终点，还需继续需要路由器转发。<br>如果网关为空，则 <strong>IP 头部中的接收方 IP 地址</strong>就是要转发到的目标地址，也是就终于找到 IP 包头里的目标地址了，说明<strong>已抵达终点</strong>。<br>知道对方的 IP 地址之后，接下来需要<strong>通过 ARP 协议根据 IP 地址查询 MAC 地址</strong>，并将<strong>查询的结果作为接收方 MAC 地址</strong>。</p><p><strong>路由器也有 ARP 缓存</strong>，因此首先会在 ARP 缓存中查询，如果找不到则发送 ARP 查询请求。</p><p>接下来是<strong>发送方 MAC 地址字段</strong>，这里<strong>填写输出端口的 MAC 地址</strong>。还有一个<strong>以太类型字段，填写 0800 （十六进制）表示 IP 协议</strong>。</p><p>网络包完成后，接下来会将其转换成电信号并通过端口发送出去。这一步的工作过程和计算机也是相同的。</p><p>发送出去的网络包会<strong>通过交换机到达下一个路由器</strong>。由于<strong>接收方 MAC 地址</strong>就是下一个路由器的地址，所以<strong>交换机会根据这一地址将包传输到下一个路由器</strong>。</p><p>接下来，下一个路由器会将包转发给再下一个路由器，经过层层转发之后，网络包就到达了最终的目的地。</p><p>不知你发现了没有，在网络包传输的过程中**，源 IP 和目标 IP 始终是不会变<strong>的，<strong>一直变化的是 MAC 地址</strong>，因为</strong>需要 MAC 地址在以太网内进行两个设备之间的包传输**。</p><p>数据包通过多个路由器道友的帮助，在网络世界途经了很多路程，最终抵达了目的地的城门！城门值守的路由器，发现了这个小兄弟数据包原来是找城内的人，于是它就将数据包送进了城内，再经由城内的交换机帮助下，最终转发到了目的地了。数据包感慨万千的说道：“多谢这一路上，各路大侠的相助！”</p><h2 id="互相扒皮-——-服务器-与-客户端"><a href="#互相扒皮-——-服务器-与-客户端" class="headerlink" title="互相扒皮 —— 服务器 与 客户端"></a>互相扒皮 —— 服务器 与 客户端</h2><p>数据包抵达了服务器，服务器肯定高兴呀，正所谓有朋自远方来，不亦乐乎？</p><p>服务器高兴的不得了，于是开始扒数据包的皮！就好像你收到快递，能不兴奋吗？</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/25.jpg" alt="网络分层模型"></p><p>数据包抵达服务器后，服务器会先扒开数据包的 MAC 头部，查看<strong>是否和服务器自己的 MAC 地址符合</strong>，符合就将包收起来。</p><p>接着继续扒开数据包的 IP 头，发现 IP 地址符合，根据 IP 头中协议项，知道自己上层是 TCP 协议。</p><p>于是**，扒开 TCP 的头，里面有序列号**，需要看一看这个<strong>序列包是不是我想要的，如果是就放入缓存中然后返回一个 ACK，如果不是就丢弃</strong>。<strong>TCP头部里面还有端口号， HTTP 的服务器正在监听这个端口号</strong>。</p><p>于是，<strong>服务器</strong>自然就<strong>知道是 HTTP 进程想要这个包，于是就将包发给 HTTP 进程</strong>。</p><p>服务器的 HTTP 进程看到，原来这个请求是要访问一个页面，于是就把这个网页封装在 HTTP 响应报文里。</p><p>HTTP 响应报文也需要穿上 TCP、IP、MAC 头部，不过这次是<strong>源地址是服务器 IP 地址，目的地址是客户端 IP 地址</strong>。</p><p>穿好头部衣服后，从网卡出去，交由交换机转发到出城的路由器，路由器就把<strong>响应数据包</strong>发到了下一个路由器，就这样跳啊跳。</p><p>最后跳到了客户端的城门把守的路由器，路由器扒开 IP 头部发现是要找城内的人，于是又把包发给了城内的交换机，再由交换机转发到客户端。</p><p>客户端收到了服务器的响应数据包后，同样也非常的高兴，客户能拆快递了！</p><p>于是，客户端开始扒皮，把收到的数据包的<strong>皮扒剩 HTTP 响应报文后，交给浏览器去渲染页面</strong>，一份特别的数据包快递，就这样显示出来了！</p><p>最后，<strong>客户端要离开了，向服务器发起了 TCP 四次挥手</strong>，至此双方的连接就断开了。</p><h2 id="读者问答"><a href="#读者问答" class="headerlink" title="读者问答"></a>读者问答</h2><p>读者问：“笔记本的是自带交换机的吗？交换机现在我还不知道是什么”</p><p>笔记本不是交换机，<strong>交换机通常是2个网口以上</strong>。</p><p>现在家里的路由器其实有了交换机的功能了。交换机可以简单理解成<strong>一个设备</strong>，三台电脑网线接到这个设备，这三台电脑就可以互相通信了，交换机嘛，交换数据这么理解就可以。</p><p>读者问：“如果知道你电脑的mac地址，我可以直接给你发消息吗？”</p><p>Mac地址<strong>只能是两个设备之间传递时使用</strong>的，如果你要从大老远给我发消息，是<strong>离不开 IP</strong> 的。</p><p>读者问：“请问公网服务器的 Mac 地址是在什么时机通过什么方式获取到的？我看 arp 获取Mac地址只能获取到内网机器的 Mac 地址吧？”</p><p>在发送数据包时，如果<strong>目标主机不是本地局域网，填入的MAC地址是路由器</strong>，也就是把数据包转发给路由器，路由器一直转发下一个路由器，<strong>直到转发到目标主机的路由器</strong>，发现 IP 地址是<strong>自己局域网内的主机</strong>，就会 arp 请求<strong>获取目标主机的 MAC 地址，从而转发到这个服务器主机</strong>。</p><p>转发的过程中，<strong>源IP地址和目标IP地址是不会变的（前提：没有使用 NAT 网络的）</strong>，<strong>源 MAC 地址和目标 MAC 地址是会变化的</strong>。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>wordle,启动!</title>
      <link href="/2025/02/27/wordle/"/>
      <url>/2025/02/27/wordle/</url>
      
        <content type="html"><![CDATA[<p><strong>我上传了一个wordle小游戏,在右上角的游戏栏目下.</strong></p><p><strong>也可点此游玩</strong></p><html lang="en"><head>    <meta charset="UTF-8">    <meta name="viewport" content="width=device-width, initial-scale=1.0">    <style>        /* 定义全局 CSS 变量 --green，设置为绿色 */        :root {            --green: #2ecc71;        }        /* 设置 #wordle-link 元素的样式 */        #wordle-link {            font-size: larger;            display: block;            margin: 30px auto -20px;            width: fit-content;            padding: 18px 26px;            border: none;            border-radius: 40px;            background-color: var(--green);            color: white;            font-weight: bold;            cursor: pointer;            transition: background-color 0.2s;            position: relative;            top: -20px;            text-align: center;            text-decoration: none;        }        /* 当鼠标悬停在 #wordle-link 元素上时，更改背景颜色 */        #wordle-link:hover {            background-color: #558850;        }    </style></head><body>    <!-- 创建一个指向 Wordle 游戏的链接 -->    <a href="https://rxcccccc.github.io/game/word-games/" id="wordle-link">wordle,启动!</a></body></html>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>进行一个一个比赛的补</title>
      <link href="/2025/02/16/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/"/>
      <url>/2025/02/16/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/</url>
      
        <content type="html"><![CDATA[<p>[toc]</p><p>本页面记录了本人在<strong>赛时</strong>和赛后的思路和相关问题以及对应解法,总结和反思</p><h1 id="cf"><a href="#cf" class="headerlink" title="cf"></a>cf</h1><h2 id="2025cccc选拔赛"><a href="#2025cccc选拔赛" class="headerlink" title="2025cccc选拔赛"></a>2025cccc选拔赛</h2><p>后记:一定要用成熟的版本,不要临时求变,一定要<strong>巩固好最稳定的做法</strong>.前几题考不到什么大的知识点的,主要都是侧重思维,因此一定要<strong>注意</strong>自己解法<strong>在思维上的投入(太少太暴力模拟肯定不对)</strong>,肯定<strong>不能过于直观</strong>,一般一定是错的,要不在此基础上优化,要不就全部推翻用<strong>新思路</strong>.<strong>确定了绝对不可能</strong>的思路那就一定要<strong>及时完全跳出</strong>来,不要再被拖累了,每次都是先想到了巨麻烦的做法但是又总不即时抛弃掉,<strong>脑子跳不出来想</strong>.</p><p><strong>B 放烟花</strong></p><p>Bob喜欢放烟花，他购买了两个烟花发射装置和大量的发射炮弹。</p><p>两个装置同时开启。第一个装置每隔 $a$ 分钟（即开启后 $a, 2 \cdot a, 3 \cdot a, \dots$ 分钟）发射一次烟花。第二个装置每隔 $b$ 分钟（即开启后 $b, 2 \cdot b, 3 \cdot b, \dots$ 分钟）发射一次烟花。</p><p>每个烟花在发射后的 $m + 1$ 分钟内都可以在天空中看到，也就是说，如果一个烟花是在装置开启后的 $x$ 分钟后发射的，那么从 $x$ 到 $x + m$ （包括首尾两分钟）的每一分钟都可以看到该烟花。如果一个烟花在另一个烟花 $m$ 分钟后发射，则两个烟花都将在一分钟内可见。</p><p>天空中最多可以同时看到多少枚烟花？</p><p>第一行包含一个整数 $t$ $(1 \le t \le 10^4)$ 代表测试用例数。</p><p>每个测试用例的第一行也是唯一一行包含整数 $a$, $b$, $m$ $(1 \le a, b, m \le 10^{18})$ 代表第一个装置、第二个装置的发射频率和烟花在天空中可见的时间。</p><p>赛时思路:画图直观枚举叠加,发现数据量过大暴力不行,又想到差分(?),根据样例分析直接盲猜*2做差的结论(?),后面才发现只需求出各个区间内最多后再相加即可</p><p>正确思路:由于<strong>时间无限</strong>,直接求出<strong>各个区间内最多</strong>后再相加即可,总有重叠的时候.</p><p>反思:不要直观画图了真就完全只按直观的来,肯定要在图上思考优化的,甚至是推翻图重做,思路一定要打开,别把前面的题就想得那么难直接<strong>硬套太多大知识点</strong>.可以根据样例分析而思考相应做法,但样例分析肯定不会完全透出正解甚至可能完全跟正解做法不相关(<strong>纯暴力模拟</strong>),绝对<strong>不能完全依靠据此猜得的各种结论和做法</strong>.</p><p><strong>C 自然语言处理</strong></p><p>Alice 觉得无聊，于是决定用五个字母 $\texttt{a}$ , $\texttt{b}$ , $\texttt{c}$ , $\texttt{d}$ , $\texttt{e}$ 创造一种简单的语言。字母分为两类：</p><ul><li>元音 — 字母 $\texttt{a}$ 和 $\texttt{e}$ 。用 $\textsf{V}$ 表示。</li><li>辅音 — 字母 $\texttt{b}$ , $\texttt{c}$ 和 $\texttt{d}$ 。用 $\textsf{C}$ 表示。</li></ul><p>这种语言中有两种类型的音节： $\textsf{CV}$ （辅音后接元音）或 $\textsf{CVC}$ （元音前后均有辅音）。例如， $\texttt{ba}$ , $\texttt{ced}$ , $\texttt{bab}$ 是音节，但 $\texttt{aa}$ , $\texttt{eda}$ , $\texttt{baba}$ 不是。</p><p>语言的单词由音节序列构成。Alice 写了一个单词，但她不知道如何将其分割为音节。请帮助她分割单词。</p><p>例如，给定单词 $\texttt{bacedbab}$ ，应分割为 $\texttt{ba.ced.bab}$ （点 $\texttt{.}$ 表示音节边界）。</p><p><strong>Input</strong></p><p>输入包含多个测试用例。</p><p>第一行包含一个整数 $t$ $(1 \leq t \leq 100)$ 代表测试用例数量。接下来描述每个测试用例。</p><p>每个测试用例的第一行包含一个整数 $n$ $(1 \leq n \leq 2 \cdot 10^5)$ 代表单词长度。</p><p>每个测试用例的第二行包含一个由 $n$ 个小写拉丁字母组成的字符串，代表要分割的单词。</p><p>所有给定的单词均为合法单词，即仅使用字母 $\texttt{a}$, $\texttt{b}$, $\texttt{c}$, $\texttt{d}$, $\texttt{e}$，且每个单词由若干音节构成。</p><p>保证所有测试用例的 $n$ 之和不超过 $2 \cdot 10^5$。</p><p><strong>Output</strong></p><p>对每个测试用例，输出一个字符串，通过在相邻音节间插入点 .. 来表示分割后的结果。</p><p>若存在多种可能的分割方式，输出任意一种即可。输入保证至少存在一种有效分割。</p><p>赛时思路:一个个找出来再分割,不同的分割情况相应深搜再剪枝叶(?),后面发现数据量太大不可能,随后观察题意,才发现有元音处一定有分割,于是想先找出所有元音位置后再进行分割然后再深搜剪枝(?),发现肯定也太麻烦,后面对比两种形式后才发现元音前一定是辅音,只要在辅音前加.则一定会满足题意(题目保证了一定有一种解法,这样一定最优,无论哪种分割都不会漏点)</p><p>正解思路:对比两种形式后才发现元音前一定是辅音,**只要在辅音前加.**则一定会满足题意(<strong>题目保证</strong>了一定有一种解法,这样一定最优,无论哪种分割都不会漏点)</p><p>反思:要学会观察题意对比,找出<strong>共通点</strong>从而形成<strong>简单而符合题意的思路</strong>.</p><p>代码问题:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">repp</span>(i,<span class="number">0</span>,s.<span class="built_in">length</span>()<span class="number">-1</span>)<span class="comment">//循环中用了s.length,由于字符串被修改且长度变化,这里循环次数也是会变化的</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(s[i]==<span class="string">&#x27;a&#x27;</span>||s[i]==<span class="string">&#x27;e&#x27;</span>)</span><br><span class="line">            &#123;</span><br><span class="line">            s.<span class="built_in">insert</span>(i<span class="number">-1</span>,<span class="string">&quot;.&quot;</span>);<span class="comment">//就在该位置插入,然后把后面字符串往后推</span></span><br><span class="line">            i++;<span class="comment">//关键一步,如果插入了,则肯定要再向后推(s长度变化了),不然这里指向的位置就不再是原来的a/e</span></span><br><span class="line">        &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        s.<span class="built_in">erase</span>(s.<span class="built_in">begin</span>());<span class="comment">//前面肯定会有一个多的点,要删除掉</span></span><br></pre></td></tr></table></figure><p>D 数学平均数</p><p>Bob 有一个由 $n$ 个整数组成的数组 $a$。让我们用 $k$ 表示这些元素的数学平均数（注意 $k$ 有可能不是整数）。</p><p>由 $n$ 个元素组成的数组的数学平均数是所有元素之和除以这些元素的个数（即和除以 $n$）。</p><p>Bob 希望从 $a$ 中恰好删除两个元素，这样剩下的 $(n - 2)$ 个元素的数学平均数仍然等于 $k$。</p><p>你的任务是计算有多少对位置 $[i, j]$ (i &lt; j)$，使得删除这些位置上的两个元素后，剩下的 $(n - 2)$ 个元素的数学平均数仍然等于 $k$（即等于原数组 $a中 n个元素的数学平均数）。</p><p>赛时思路:求出所有i-n的数的出现次数,一开始想用二分,但发现<strong>并非有序</strong>,但发现<strong>空间太大</strong>,无法完成,又尝试用map映射,但发现也是空间太大,实现不了**(本质都没变)**,</p><p>正解思路:先处理好1-n的数的出现次数,再倒序遍历,对于遍历到每个数时减去一次该数相应求平均数对象的出现次数,再加和<strong>即为1-i-1的数的出现次数</strong>.</p><p>反思:使用任何方法时要<strong>注意限制条件再使用</strong>,换方法时一定要注意自己<strong>本质变没变</strong>.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> a[(<span class="type">int</span>)<span class="number">2e5</span><span class="number">+3</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> t;cin&gt;&gt;t;</span><br><span class="line">    <span class="keyword">while</span>(t--)&#123;</span><br><span class="line">        <span class="type">int</span> n;cin&gt;&gt;n;</span><br><span class="line">        unordered_map&lt;<span class="type">int</span>,<span class="type">int</span>&gt;s;<span class="comment">//记得每次都要重新开,得hu</span></span><br><span class="line">        lld sum=<span class="number">0</span>;lld ans=<span class="number">0</span>;</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">1</span>,n)&#123;</span><br><span class="line">            cin&gt;&gt;a[i];</span><br><span class="line">            sum+=a[i];</span><br><span class="line">            s[a[i]]++;</span><br><span class="line">        &#125;</span><br><span class="line">        lf k0=(sum*<span class="number">2.0</span>/n);lld k=(sum*<span class="number">2.0</span>/n);</span><br><span class="line">        <span class="keyword">if</span>(k0!=k)</span><br><span class="line">        &#123;</span><br><span class="line">            cout&lt;&lt;<span class="number">0</span>&lt;&lt;endl;ct;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">repm</span>(i,n,<span class="number">1</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="type">int</span> a0=k-a[i];s[a[i]]--;</span><br><span class="line">                ans+=(s[a0]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="牛客"><a href="#牛客" class="headerlink" title="牛客"></a>牛客</h1><h2 id="2025-牛客寒假训练1"><a href="#2025-牛客寒假训练1" class="headerlink" title="2025 牛客寒假训练1"></a>2025 牛客寒假训练1</h2><p><strong>G调整出1-n序列(贪心+排序)</strong></p><p>题意：给一个数组，每次操作可以使一个元素加1，另一个元素减1，问变成排列的最小操作次数。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">lld a[<span class="number">100003</span>],sum,ans;<span class="type">bool</span> biao[<span class="number">100003</span>];vi dai,que;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="comment">//int t;cin&gt;&gt;t;</span></span><br><span class="line">    <span class="comment">//while(t--)&#123; &#125;</span></span><br><span class="line">    lld n;cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        cin&gt;&gt;a[i];</span><br><span class="line">        sum+=a[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(sum!=(n*(n<span class="number">+1</span>))/<span class="number">2</span>)<span class="comment">//加减操作不应影响整个的和</span></span><br><span class="line">    &#123;cout&lt;&lt;<span class="number">-1</span>;ret <span class="number">0</span>;&#125;</span><br><span class="line">    <span class="built_in">sort</span>(a<span class="number">+1</span>,a<span class="number">+1</span>+n);</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        ans+=<span class="built_in">abs</span>(a[i]-i);<span class="comment">//对应操作</span></span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;ans/<span class="number">2</span>;<span class="comment">//取一半即可,加减操作次数相同</span></span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>E 双生双宿之错</strong></p><p>题意：给定一个数组，每次操作可以使得一个元素加1或者减1，问最小操作几次可以变成双生数组，即元素种类数为2、且出现次数相同。</p><p><strong>中位数定理</strong>：给定一个数组，每次操作<strong>加1或者减1</strong>，将所有元素<strong>变成相同</strong>的<strong>最小操作次数</strong>则是<strong>将所有元素变成中位数即可。</strong>(出现浮点数时整左右<strong>哪个整数都行</strong>,因为左右两半的<strong>数个数是相同的</strong>)</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/2a4d15a25658e929758cd4a0a0afd4d2.png" alt="2a4d15a25658e929758cd4a0a0afd4d2"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">lld a[<span class="number">100003</span>],x,y,t,n;</span><br><span class="line"><span class="function">lld <span class="title">check</span><span class="params">(lld xx,lld yy)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    lld sum=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(xx!=yy)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">1</span>,t)</span><br><span class="line">        sum+=<span class="built_in">abs</span>(a[i]-xx);</span><br><span class="line">        <span class="built_in">repp</span>(i,t<span class="number">+1</span>,n)</span><br><span class="line">        sum+=<span class="built_in">abs</span>(a[i]-yy);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span><span class="comment">//如果相等分类讨论取最小</span></span><br><span class="line">    &#123;</span><br><span class="line">        lld sum1=<span class="number">0</span>;lld sum2=<span class="number">0</span>;</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">1</span>,t)</span><br><span class="line">        sum1+=<span class="built_in">abs</span>(a[i]-(xx<span class="number">-1</span>));</span><br><span class="line">        <span class="built_in">repp</span>(i,t<span class="number">+1</span>,n)</span><br><span class="line">        sum1+=<span class="built_in">abs</span>(a[i]-yy);</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">1</span>,t)</span><br><span class="line">        sum2+=<span class="built_in">abs</span>(a[i]-xx);</span><br><span class="line">        <span class="built_in">repp</span>(i,t<span class="number">+1</span>,n)</span><br><span class="line">        sum2+=<span class="built_in">abs</span>(a[i]-(yy<span class="number">+1</span>));</span><br><span class="line">        sum=<span class="built_in">min</span>(sum1,sum2);</span><br><span class="line">    &#125;</span><br><span class="line">    ret sum;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="type">int</span> k;cin&gt;&gt;k;<span class="comment">//记得把t换了</span></span><br><span class="line">    <span class="keyword">while</span>(k--)&#123; </span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        cin&gt;&gt;a[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">sort</span>(a<span class="number">+1</span>,a<span class="number">+1</span>+n);<span class="comment">//排好序</span></span><br><span class="line">    t=n&gt;&gt;<span class="number">1</span>;<span class="comment">//拆一半</span></span><br><span class="line">    <span class="keyword">if</span>(t&amp;<span class="number">1</span>)<span class="comment">//如何取中位数</span></span><br><span class="line">    &#123;</span><br><span class="line">        x=a[(t<span class="number">+1</span>)/<span class="number">2</span>];</span><br><span class="line">        y=a[(n+t<span class="number">+1</span>)/<span class="number">2</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        x=a[t/<span class="number">2</span>];<span class="comment">//不统一方向取,尽量防止相等</span></span><br><span class="line">        y=a[(n+t<span class="number">+1</span>)/<span class="number">2</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;<span class="built_in">check</span>(x, y)&lt;&lt;endl;<span class="comment">//输出处理结果</span></span><br><span class="line">    &#125;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>M 数值膨胀之美</strong></p><p><strong>STL、排序，枚举</strong></p><p>要使得数组极差变小，显然需要先让最小值乘以 2 ，然后是<strong>次小值</strong>，……，以此类推。</p><p>新的区间一定会<strong>包含</strong>这个区间.我们需要在<strong>每次乘以 2 操作后计算</strong>数组的极差。</p><p>我们需要一个容器：每次操作可以在容器中<strong>删除</strong>一个数，<strong>并插入</strong>一个数。符合条件的容器有许多，我们选择的是 <strong>multiset</strong></p><p>时间复杂度 O(nlogn)。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">a</span><span class="params">(n + <span class="number">1</span>)</span></span>;<span class="comment">//用于读取</span></span><br><span class="line">    vector&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; b;<span class="comment">//用于形成映射并排序</span></span><br><span class="line">    multiset&lt;<span class="type">int</span>&gt; st;<span class="comment">//用于读取两个最值</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">        cin &gt;&gt; a[i];</span><br><span class="line">        b.<span class="built_in">push_back</span>(&#123;a[i], i&#125;);<span class="comment">//用&#123;&#125;也能造pair</span></span><br><span class="line">        st.<span class="built_in">insert</span>(a[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">sort</span>(b.<span class="built_in">begin</span>(),b.<span class="built_in">end</span>());<span class="comment">//排序,从而读取原序列第k小值位置</span></span><br><span class="line">    multiset&lt;<span class="type">int</span>&gt;::iterator it;<span class="comment">//先定义迭代器及其类型(auto 在 if里面不给用)</span></span><br><span class="line">    <span class="keyword">auto</span> [l, r] = b[<span class="number">0</span>];<span class="comment">//读取pair的一种方法</span></span><br><span class="line">    <span class="keyword">if</span>(( it=st.<span class="built_in">find</span>(l))!=st.<span class="built_in">end</span>())st.<span class="built_in">erase</span>(it);<span class="comment">//只能删除一个数一次,不然还得数多少个,麻烦</span></span><br><span class="line">    st.<span class="built_in">insert</span>(l * <span class="number">2</span>);</span><br><span class="line">    <span class="type">int</span> ans = *st.<span class="built_in">rbegin</span>() - *st.<span class="built_in">begin</span>();<span class="comment">//逆向即末尾数</span></span><br><span class="line">    l = r;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;[_, i] : b)&#123;<span class="comment">//当用不上一个元素时,可用_代替(记得打上&amp;)</span></span><br><span class="line">        <span class="keyword">if</span>(i &gt;= l &amp;&amp; i &lt;= r) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = l - <span class="number">1</span>; j &gt;= i; j--)&#123;</span><br><span class="line">            <span class="keyword">if</span>((it=st.<span class="built_in">find</span>(a[j]))!=st.<span class="built_in">end</span>())st.<span class="built_in">erase</span>(it);</span><br><span class="line">            st.<span class="built_in">insert</span>(a[j] * <span class="number">2</span>);</span><br><span class="line">            ans = <span class="built_in">min</span>(ans, *st.<span class="built_in">rbegin</span>() - *st.<span class="built_in">begin</span>());</span><br><span class="line">        &#125;</span><br><span class="line">        l = <span class="built_in">min</span>(l, i);</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = r + <span class="number">1</span>; j &lt;= i; j++)&#123;</span><br><span class="line">            <span class="keyword">if</span>((it=st.<span class="built_in">find</span>(a[j]))!=st.<span class="built_in">end</span>())st.<span class="built_in">erase</span>(it);</span><br><span class="line">            st.<span class="built_in">insert</span>(a[j] * <span class="number">2</span>);</span><br><span class="line">            ans = <span class="built_in">min</span>(ans, *st.<span class="built_in">rbegin</span>() - *st.<span class="built_in">begin</span>());</span><br><span class="line">        &#125;</span><br><span class="line">        r = <span class="built_in">max</span>(r, i);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="H-井然有序之窗"><a href="#H-井然有序之窗" class="headerlink" title="H 井然有序之窗"></a>H 井然有序之窗</h2><p><strong>构造、贪心</strong></p><p><strong>从小到大考虑(后面选择合理的前提条件)<strong>1-n的数.在</strong>符合要求(l&lt;i,此时选择l更大的或者更小的都不会影响最终结果,因为都是符合这个要求的了)<strong>的</strong>所有</strong>区间中选择 **r 最小的区间给使用掉(限制最严的)**一定不会使得答案变劣。</p><p>可以使用<strong>优先队列维护右端点最小的区间</strong>，对区间<strong>先按左端点排序后</strong>，可以从前往后将左端点小于i的<strong>区间加入优先队列(所以这个优先队列里的一直都能用,因为i是从小到大考虑的,i增大后放进来的数也肯定还满足l&lt;i)</strong>，然后<strong>取出右端点最小</strong>的区间。</p><p>若右端点小于i ，理论上应该将这个区间丢弃，找到第一个满足右端点大于等于i的区间，但由<strong>于区间不能浪费(据题意每个区间内都要有一个数)</strong>，因此此时一定无解。若<strong>优先队列为空，同样无解</strong>。</p><p>我的写法</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">st qujian&#123;</span><br><span class="line">    <span class="type">int</span> l;<span class="type">int</span> r;<span class="type">int</span> hao;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">cmp</span><span class="params">(qujian a,qujian b)</span><span class="comment">//用于sort比较的</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(a.l!=b.l)</span><br><span class="line">    <span class="built_in">ret</span>(a.l&lt;b.l);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">ret</span>(a.r&lt;b.r);</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">bool</span> <span class="keyword">operator</span> &lt;(qujian a,qujian b)<span class="comment">//用于pq特殊比较的</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(a.l!=b.l)</span><br><span class="line">    <span class="built_in">ret</span>(a.l&gt;b.l);<span class="comment">//记得反过来</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">ret</span>(a.r&gt;b.r);</span><br><span class="line">&#125;</span><br><span class="line">lld n,a[M];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">signed</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="number">0</span>);cin.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin&gt;&gt;n;<span class="type">int</span> nowhao=<span class="number">0</span>;<span class="comment">//标记第几个区间</span></span><br><span class="line">    pq&lt;qujian&gt;q;</span><br><span class="line">    deque&lt;qujian&gt;<span class="built_in">s</span>(n);<span class="comment">//提前预定好空间,从而下面可借助引用&amp;读取输入</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;[l, r, i] : s)&#123;</span><br><span class="line">        cin &gt;&gt; l &gt;&gt; r;</span><br><span class="line">        i = ++nowhao;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">sort</span>(s.<span class="built_in">begin</span>(),s.<span class="built_in">end</span>(),cmp);</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(s.<span class="built_in">size</span>()&amp;&amp;s.<span class="built_in">front</span>().l&lt;=i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">auto</span> [l,r,hao]=s.<span class="built_in">front</span>();s.<span class="built_in">pop_front</span>();<span class="comment">//因为不可以浪费,所以这里直接弹出就好</span></span><br><span class="line">            q.<span class="built_in">push</span>(&#123;r,l,hao&#125;);<span class="comment">//注意只在这边反着存入,读取时仍然按原定义的l,r顺序读取</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(q.<span class="built_in">empty</span>())&#123;<span class="comment">//特判空的1情况</span></span><br><span class="line">            cout&lt;&lt;<span class="number">-1</span>;ret <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">auto</span> tt=q.<span class="built_in">top</span>();</span><br><span class="line">        <span class="keyword">if</span>(i&gt;tt.l)<span class="comment">//肯定不符合要求了</span></span><br><span class="line">        &#123;</span><br><span class="line">            cout&lt;&lt;<span class="number">-1</span>;</span><br><span class="line">            ret <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            a[tt.hao]=i;q.<span class="built_in">pop</span>();<span class="comment">//在a的指定位置填入i</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)cout&lt;&lt;a[i]&lt;&lt;<span class="string">&#x27; &#x27;</span>;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>佬写的</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    deque&lt;array&lt;<span class="type">int</span>, 3&gt;&gt; <span class="built_in">a</span>(n);</span><br><span class="line">    <span class="type">int</span> pt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;[l, r, i] : a)&#123;</span><br><span class="line">        cin &gt;&gt; l &gt;&gt; r;</span><br><span class="line">        i = ++pt;</span><br><span class="line">    &#125;</span><br><span class="line">    ranges::<span class="built_in">sort</span>(a);</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">ans</span><span class="params">(n + <span class="number">1</span>)</span></span>;</span><br><span class="line">    set&lt;array&lt;<span class="type">int</span>, 3&gt;&gt; st;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">        <span class="keyword">while</span>(a.<span class="built_in">size</span>() &amp;&amp; a[<span class="number">0</span>][<span class="number">0</span>] &lt;= i)&#123;</span><br><span class="line">            <span class="keyword">auto</span> [l, r, i] = a[<span class="number">0</span>];</span><br><span class="line">            st.<span class="built_in">insert</span>(&#123;r, l, i&#125;);</span><br><span class="line">            a.<span class="built_in">pop_front</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(!st.<span class="built_in">size</span>())&#123;</span><br><span class="line">            cout &lt;&lt; <span class="number">-1</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">auto</span> [r, l, j] = *st.<span class="built_in">begin</span>();</span><br><span class="line">        st.<span class="built_in">erase</span>(st.<span class="built_in">begin</span>());</span><br><span class="line">        <span class="keyword">if</span>(r &lt; i)&#123;</span><br><span class="line">            cout &lt;&lt; <span class="number">-1</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ans[j] = i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">        cout &lt;&lt; ans[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="J-硝基甲苯之袭"><a href="#J-硝基甲苯之袭" class="headerlink" title="J 硝基甲苯之袭"></a>J 硝基甲苯之袭</h2><p><strong>因数分解、打表、质数筛</strong></p><p><strong>先x 的因子 t,再枚举 x</strong>  ，那么 y&#x3D;x⊕t，再检查 gcd(x,y)&#x3D;t 是否合法(逆向思维)**，若合法统计答案。</p><p>时间复杂度 O(nsqrt(n)) ，使用质数筛大概可以优化到 O(nlogn)) 。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ci M = (<span class="type">int</span>)<span class="number">2e5</span><span class="number">+10</span>;</span><br><span class="line">lld n,a[M],maxn,ans;</span><br><span class="line"><span class="function"><span class="type">signed</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">        cin&gt;&gt;n;unordered_map&lt;lld,lld&gt;s;<span class="comment">//标记出现次数</span></span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">1</span>,n)&#123;</span><br><span class="line">cin&gt;&gt;a[i];s[a[i]]++;maxn=<span class="built_in">max</span>(maxn,a[i]);&#125;</span><br><span class="line"><span class="built_in">repp</span>(i,<span class="number">1</span>,maxn)<span class="comment">//先枚举x的因子</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span>(lld j=i;j&lt;=maxn;j+=i)<span class="comment">//再枚举数x</span></span><br><span class="line">&#123;</span><br><span class="line">lld k=i^j;</span><br><span class="line"><span class="keyword">if</span>(j&lt;k&amp;&amp;__gcd(j,k)==i&amp;&amp;k&lt;=maxn)<span class="comment">//需满足的条件</span></span><br><span class="line">ans+=s[j]*s[k];<span class="comment">//如果两个数都存在才会叠加</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">cout&lt;&lt;ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="2025牛客寒假训练营"><a href="#2025牛客寒假训练营" class="headerlink" title="2025牛客寒假训练营"></a>2025牛客寒假训练营</h1><h2 id="D-字符串里串"><a href="#D-字符串里串" class="headerlink" title="D 字符串里串"></a>D 字符串里串</h2><p>思维、贪心</p><p>有一个比较容易想到的结论，如果从前往后第i&gt;1个字符在i之后的第j个位置还出现过，那么选择这[1,i]]前缀作为子串，子序列就把[1,i-1]和[j,j]拼在一起，就得到了两个不一样的子串。</p><p>然后把字符串翻转一下，再跑一次即可（<strong>从前往后和从后往前都是可以的不要只有一个方向</strong>）。</p><p>注意 “aba” 的<strong>答案是 0 ，而不是 1</strong> ，之前 std 也错了，不过数据里没有这个，所以不考虑这一个也能过。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    string s;</span><br><span class="line">    cin &gt;&gt; s;</span><br><span class="line">    s = <span class="string">&quot; &quot;</span> + s;</span><br><span class="line">    set&lt;<span class="type">int</span>&gt; st;</span><br><span class="line">    <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = n; i &gt;= <span class="number">2</span>; i--)&#123;</span><br><span class="line">        <span class="keyword">if</span>(st.<span class="built_in">count</span>(s[i]))&#123; ans = <span class="built_in">max</span>(ans, i);<span class="keyword">break</span>;&#125;<span class="comment">//看后面有没有这个字母,仅在此可break</span></span><br><span class="line">        st.<span class="built_in">insert</span>(s[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    st = <span class="built_in">set</span>&lt;<span class="type">int</span>&gt;();<span class="comment">//可以清空</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n - <span class="number">1</span>; i++)&#123;<span class="comment">//另一个方向</span></span><br><span class="line">        <span class="keyword">if</span>(st.<span class="built_in">count</span>(s[i])) ans = <span class="built_in">max</span>(ans, n - i + <span class="number">1</span>);</span><br><span class="line">        st.<span class="built_in">insert</span>(s[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="C-字符串外串"><a href="#C-字符串外串" class="headerlink" title="C 字符串外串"></a>C 字符串外串</h2><p>构造</p><p>首先要知道D题的结论，那我们需要构造一个字符串使得<strong>从前往后看和从后往前看可爱度都是</strong>m的字符串。</p><p>那就思考一下<strong>类似回文串的构造</strong>方法。</p><p>如果 n超过了m的两倍，意味着前m个和后m个字母<strong>对称</strong>（”abcdefcba”），<strong>中间n-2m</strong>个字母在<strong>整个字符串中全都只能出现一次</strong>，总共需要的字母种类是m+n-2m&#x3D;n-m，很显然字母种类不能超过26个。</p><p>如果 n不超过m的两倍，意味着前 n-m个和后n-m个字母对称（”abczzzcba”），<strong>中间字母任意</strong>，总共需要的字母种类是n-m，很显然字母种类不能超过26个。</p><p>按上述情况<strong>分类讨论构造</strong>一下即可。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> m,n;</span><br><span class="line">string s;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="type">int</span> t;</span><br><span class="line">    cin&gt;&gt;t;</span><br><span class="line">    <span class="keyword">while</span>(t--)</span><br><span class="line">    &#123;</span><br><span class="line">        cin&gt;&gt;n&gt;&gt;m;string ans=<span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="keyword">if</span>(n-m&gt;<span class="number">26</span>||n&lt;=m)&#123;</span><br><span class="line">            cout&lt;&lt;<span class="string">&quot;NO&quot;</span>&lt;&lt;endl;ct;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            cout&lt;&lt;<span class="string">&quot;YES&quot;</span>&lt;&lt;endl;</span><br><span class="line">            <span class="keyword">if</span>(n&gt;=<span class="number">2</span>*m)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">repp</span>(i,<span class="number">0</span>,m<span class="number">-1</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    ans+=(<span class="string">&#x27;a&#x27;</span>+i);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="built_in">repp</span>(i,m,n-m<span class="number">-1</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    ans+=(<span class="string">&#x27;a&#x27;</span>+i);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="built_in">repm</span>(i,m<span class="number">-1</span>,<span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    ans+=(<span class="string">&#x27;a&#x27;</span>+i);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">repp</span>(i,<span class="number">0</span>,n-m<span class="number">-1</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    ans+=(<span class="string">&#x27;a&#x27;</span>+i);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="built_in">repp</span>(i,<span class="number">1</span>,<span class="number">2</span>*m-n)</span><br><span class="line">                &#123;</span><br><span class="line">                    ans+=(<span class="string">&#x27;a&#x27;</span>+n-m<span class="number">-1</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="built_in">repm</span>(i,n-m<span class="number">-1</span>,<span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    ans+=(<span class="string">&#x27;a&#x27;</span>+i);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="H-一起画很大的圆！"><a href="#H-一起画很大的圆！" class="headerlink" title="H 一起画很大的圆！"></a>H 一起画很大的圆！</h2><p><strong>构造、计算几何，贪心</strong></p><p>三个不共线的点确定一个圆。</p><p>如果这三个点<strong>越接近一条直线，这个圆最大(三点共线的时候这个最大)</strong>。</p><p>那么我们需要在边界上找三个点使得最接近一条直线，<strong>猜一下有一个点会在边角</strong>，那剩下的点就不难确定了。</p><p><strong>横着</strong>可以找到一个答案是<img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/9d846c00bf84543b285250dadf59bd12.png" alt="9d846c00bf84543b285250dadf59bd12">，但如果<img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/c32aae6022cbfb5e60918fcde1845b07.png" alt="c32aae6022cbfb5e60918fcde1845b07"> 的话，就应该<strong>竖着</strong>找。</p><p>显然<strong>在长边取两个点越接近直线斜边也越长</strong></p><p>我们应当使得斜边尽可能的大，同时斜边所对的角尽可能的接近0 或 <img src="https://www.nowcoder.com/equation?tex=180%20%5E%7B%5Ccirc%7D" alt="180 ^{\circ}">。前者很好实现，令长边上的一点<strong>位于角落</strong>（如图中点A，短边上点尽可能靠近角落（如图中点C)，此时可使得AB所对的锐角角度最小(相切那个是<strong>角度最大</strong>)。</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/D2B5CA33BD970F64A6301FA75AE2EB22.png" alt="alt"></p><p>再<strong>移动一位</strong>取B</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/D2B5CA33BD970F64A6301FA75AE2EB22-1742044851694-89.png" alt="alt"></p><p>那么关键就在于<strong>找出长边</strong>了.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记</title>
      <link href="/2025/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2025/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习教程"><a href="#机器学习教程" class="headerlink" title="机器学习教程"></a>机器学习教程</h1><p>[TOC]</p><p><strong>ps：</strong></p><p>1.py3.13版本下可能需要切换后端才能跑通</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="comment">#matplotlib.use(&#x27;Agg&#x27;)  # 无头模式，用于服务器或无图形界面环境</span></span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)  <span class="comment"># 用于显示交互式窗口（需 GUI 支持）</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 后续绘图代码</span></span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Machine-learning-logo-1.webp" alt="img"></p><p>机器学习（Machine Learning）是人工智能（AI）的一个<strong>分支</strong>，它使计算机系统能够<strong>利用数据和算法自动学习和改进</strong>其性能。</p><p>机器学习是让**机器通过经验（数据）**来做决策和预测。</p><p>机器学习已经广泛应用于许多领域，包括推荐系统、图像识别、语音识别、金融分析等。</p><p>举个例子，通过机器学习，汽车可以学习如何识别交通标志、行人和障碍物，以实现自动驾驶。</p><h2 id="机器学习与传统编程的区别"><a href="#机器学习与传统编程的区别" class="headerlink" title="机器学习与传统编程的区别"></a>机器学习与传统编程的区别</h2><p>在传统的编程方法中，程序员会编写一系列规则或指令，告诉计算机如何执行任务。而在机器学习中，程序员并不是直接编写所有规则，而是<strong>训练计算机</strong>从数据中<strong>自动</strong>学习和推断模式。具体的差异可以总结如下：</p><ul><li><strong>传统编程：</strong> 程序员<strong>定义明确</strong>的规则和逻辑，计算机根据这些规则执行任务。</li><li><strong>机器学习：</strong> 计算机<strong>通过数据”学习”<strong>模式，<strong>生成模型</strong>并</strong>基于这些模式</strong>进行<strong>预测</strong>或<strong>决策</strong>。</li></ul><p>举个简单的例子，假设我们要训练一个模型来识别猫和狗的图片。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/68747470733a2f2f6d69726f25674a6b50587563386f672e676966.gif" alt="img"></p><p>在传统编程中，程序员需要手动定义哪些特征可以区分猫和狗（如耳朵形状、鼻子形状等），而在机器学习中，程序员只需要提供大量带标签的图片数据，计算机会<strong>自动学习</strong>如何区分猫和狗。</p><hr><h2 id="常见机器学习任务"><a href="#常见机器学习任务" class="headerlink" title="常见机器学习任务"></a>常见机器学习任务</h2><ul><li><strong>回归问题</strong>：<strong>预测连续</strong>值，例如房价预测。</li><li><strong>分类问题</strong>：将样本<strong>分为不同类别</strong>，例如垃圾邮件检测。</li><li><strong>聚类问题</strong>：将数据<strong>自动分组</strong>，例如客户细分。</li><li><strong>降维问题</strong>：将数据<strong>降到低维</strong>度，例如主成分分析（PCA）。</li></ul><hr><h2 id="机器学习常见算法"><a href="#机器学习常见算法" class="headerlink" title="机器学习常见算法"></a>机器学习常见算法</h2><p><strong>监督学习：</strong></p><ul><li><strong>线性</strong>回归（Linear Regression）</li><li><strong>逻辑</strong>回归（Logistic Regression）</li><li>支持向量机（SVM）</li><li>K-近邻算法（KNN）</li><li>决策树（Decision Tree）</li><li>随机森林（Random Forest）</li></ul><p><strong>无监督学习：</strong></p><ul><li>K-均值聚类（K-Means Clustering）</li><li>主成分分析（PCA）</li></ul><p><strong>深度学习：</strong></p><ul><li>神经网络（Neural Networks）</li><li>卷积神经网络（CNN）</li><li>循环神经网络（RNN）</li></ul><h1 id="机器学习简介"><a href="#机器学习简介" class="headerlink" title="机器学习简介"></a>机器学习简介</h1><h2 id="机器学习是如何工作的？"><a href="#机器学习是如何工作的？" class="headerlink" title="机器学习是如何工作的？"></a>机器学习是如何工作的？</h2><p>机器学习通过让计算机<strong>从大量数据</strong>中学习模式和规律来做出决策和预测。</p><ul><li>首先，<strong>收集并准备数据</strong>，然后选择一个<strong>合适的算法</strong>来训练模型。</li><li>然后，模型通过<strong>不断优化参数</strong>，<strong>最小化预测错误</strong>，直到能准确地对新数据进行预测。</li><li>最后，模型<strong>部署</strong>到实际应用中，<strong>实时</strong>做出预测或决策，并根据新的数据进行更新。</li></ul><p>机器学习是一个<strong>迭代</strong>过程，可能需要<strong>多次调整</strong>模型参数和特征选择，以提高模型的性能。</p><p>下面这张图展示了机器学习的基本流程：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/how-does-machine-learning-work.png" alt="img"></p><ol><li><strong>Labeled Data（标记数据）：</strong>：图中蓝色区域显示了标记数据，这些数据包括了不同的几何形状（如六边形、正方形、三角形）。</li><li><strong>Model Training（模型训练）：</strong>：在这个阶段，机器学习<strong>算法分析</strong>数据的特征，并学习如何根据这些特征来预测标签。</li><li><strong>Test Data（测试数据）：</strong>：图中深绿色区域显示了测试数据，包括一个正方形和一个三角形。</li><li><strong>Prediction（预测）：</strong>：模型使用从训练数据中学到的规则来预测测试数据的标签。在图中，模型预测了测试数据中的正方形和三角形。</li><li><strong>Evaluation（评估）：</strong>：预测结果与测试数据的真实标签进行<strong>比较</strong>，以评估模型的准确性。</li></ol><p>机器学习的工作流程可以大致分为以下几个步骤：</p><h3 id="1-数据收集"><a href="#1-数据收集" class="headerlink" title="1. 数据收集"></a>1. 数据收集</h3><ul><li><strong>收集数据</strong>：这是机器学习项目的第一步，涉及收集相关数据。数据可以来自数据库、文件、网络或实时数据流。</li><li><strong>数据类型</strong>：可以是结构化数据（如表格数据）或非结构化数据（如文本、图像、视频）。</li></ul><h3 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h3><ul><li><strong>清洗数据</strong>：处理缺失值、异常值、错误和重复数据。</li><li><strong>特征工程</strong>：<strong>选择</strong>有助于模型学习的最相关特征，可能包括创建新特征或转换现有特征。</li><li><strong>数据标准化&#x2F;归一化</strong>：调整数据的尺度，使其在<strong>同一范围</strong>内，有助于某些算法的性能。</li></ul><h3 id="3-选择模型"><a href="#3-选择模型" class="headerlink" title="3. 选择模型"></a>3. 选择模型</h3><ul><li><strong>确定问题类型</strong>：根据<strong>问题的性质</strong>（分类、回归、聚类等）<strong>选择合适</strong>的机器学习<strong>模型</strong>。</li><li><strong>选择算法</strong>：基于问题类型和数据特性，选择<strong>一个或多个算法</strong>进行实验。</li></ul><h3 id="4-训练模型"><a href="#4-训练模型" class="headerlink" title="4. 训练模型"></a>4. 训练模型</h3><ul><li><strong>划分数据集</strong>：将数据分为<strong>训练</strong>集、<strong>验证</strong>集和<strong>测试</strong>集。</li><li><strong>训练</strong>：使用训练集上的数据来训练模型，调整模型参数以<strong>最小化损失函数</strong>。</li><li><strong>验证</strong>：使用验证集来调整模型参数，<strong>防止过拟合</strong>。</li></ul><h3 id="5-评估模型"><a href="#5-评估模型" class="headerlink" title="5. 评估模型"></a>5. 评估模型</h3><ul><li><strong>性能指标</strong>：使用<strong>测试集来评估</strong>模型的性能，常用的指标包括<strong>准确</strong>率、<strong>召回</strong>率、<strong>F1分数</strong>等。</li><li><strong>交叉验证</strong>：一种评估模型泛化能力的技术，通过将数据<strong>分成多个子集</strong>进行训练和验证。</li></ul><h3 id="6-模型优化"><a href="#6-模型优化" class="headerlink" title="6. 模型优化"></a>6. 模型优化</h3><ul><li><strong>调整超参数</strong>：超参数是<strong>学习过程之前设置的</strong>参数，如学习率、树的深度等，可以通过网格搜索、随机搜索或贝叶斯优化等方法来调整。</li><li><strong>特征选择</strong>：可能需要<strong>重新</strong>评估和选择特征，以提高模型性能。</li></ul><h3 id="7-部署模型"><a href="#7-部署模型" class="headerlink" title="7. 部署模型"></a>7. 部署模型</h3><ul><li><strong>集成到应用</strong>：将训练好的模型集成到实际应用中，如网站、移动应用或软件中。</li><li><strong>监控和维护</strong>：持续监控模型的性能，并根据新数据更新模型。</li></ul><h3 id="8-反馈循环"><a href="#8-反馈循环" class="headerlink" title="8. 反馈循环"></a>8. 反馈循环</h3><ul><li><strong>持续学习</strong>：机器学习模型可以设计为随着时间的推移自动从新数据中学习，以适应变化。</li></ul><h3 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h3><ul><li><strong>损失函数</strong>：一个衡量模型<strong>预测与实际结果差异</strong>的函数，模型训练的<strong>目标是最小化</strong>这个函数。</li><li><strong>优化算法</strong>：如梯度下降，用于找到最小化损失函数的参数值。</li><li><strong>正则化</strong>：一种技术，通过<strong>添加惩罚项</strong>来防止模型过拟合。</li></ul><p>机器学习的工作流程是迭代的，可能需要多次调整和优化以达到最佳性能。此外，随着数据的积累和算法的发展，机器学习模型可以变得更加精确和高效。</p><h2 id="机器学习的类型"><a href="#机器学习的类型" class="headerlink" title="机器学习的类型"></a>机器学习的类型</h2><p>机器学习主要分为以下三种类型：</p><h3 id="1-监督学习（Supervised-Learning）"><a href="#1-监督学习（Supervised-Learning）" class="headerlink" title="1. 监督学习（Supervised Learning）"></a>1. <strong>监督学习（Supervised Learning）</strong></h3><ul><li><strong>定义：</strong> 监督学习是指使用<strong>带标签的数据</strong>进行训练，模型通过学习输入数据与标签之间的<strong>关系</strong>，来做出预测或分类。</li><li><strong>应用：</strong> <strong>分类</strong>（如垃圾邮件识别）、<strong>回归</strong>（如房价预测）。</li><li><strong>例子：</strong> 线性回归、决策树、支持向量机（SVM）。</li></ul><h3 id="2-无监督学习（Unsupervised-Learning）"><a href="#2-无监督学习（Unsupervised-Learning）" class="headerlink" title="2. 无监督学习（Unsupervised Learning）"></a>2. <strong>无监督学习（Unsupervised Learning）</strong></h3><ul><li><strong>定义：</strong> 无监督学习使用<strong>没有标签</strong>的数据，模型试图在数据中发现潜在的结构或模式。</li><li><strong>应用：</strong> <strong>聚类</strong>（如客户分群）、<strong>降维</strong>（如数据可视化）。</li><li><strong>例子：</strong> K-means 聚类、主成分分析（PCA）。</li></ul><h3 id="3-强化学习（Reinforcement-Learning）"><a href="#3-强化学习（Reinforcement-Learning）" class="headerlink" title="3. 强化学习（Reinforcement Learning）"></a>3. <strong>强化学习（Reinforcement Learning）</strong></h3><ul><li><strong>定义：</strong> 强化学习通过与环境互动，智能体<strong>在试错中学习</strong>最佳策略，以最大化长期回报。每次行动后，系统会<strong>收到奖励或惩罚</strong>，来指导行为的改进。</li><li><strong>应用：</strong> 游戏AI（如AlphaGo）、自动驾驶、机器人控制。</li><li><strong>例子：</strong> Q-learning、深度Q网络（DQN）。</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/The-main-types-of-machine-learning-Main-approaches-include-classification-and-regression.png" alt="img"></p><p>这三种机器学习类型各有其应用场景和优势，监督学习适用于<strong>有明确标签的数据</strong>，无监督学习适用于<strong>探索数据内在结构</strong>，而强化学习适用于需要通过试错来学习最优策略的场景。</p><h2 id="机器学习的应用领域"><a href="#机器学习的应用领域" class="headerlink" title="机器学习的应用领域"></a>机器学习的应用领域</h2><ul><li><strong>推荐系统：</strong> 例如，抖音推荐你可能感兴趣的视频，淘宝推荐你可能会购买的商品，网易云音乐推荐你喜欢的音乐。</li><li><strong>自然语言处理（NLP）：</strong> 机器学习在语音识别、机器翻译、情感分析、聊天机器人等方面的应用。例如，Google 翻译、Siri 和智能客服等。</li><li><strong>计算机视觉：</strong> 机器学习在图像识别、物体检测、面部识别、自动驾驶等领域有广泛应用。例如，自动驾驶汽车通过摄像头和传感器识别周围的障碍物，识别行人和其他车辆。</li><li><strong>金融分析：</strong> 机器学习在股市预测、信用评分、欺诈检测等金融领域具有重要应用。例如，银行利用机器学习检测信用卡交易中的欺诈行为。</li><li><strong>医疗健康：</strong> 机器学习帮助医生诊断疾病、发现药物副作用、预测病情发展等。例如，IBM 的 Watson 系统帮助医生分析患者的病历数据，提供诊断和治疗建议。</li><li><strong>游戏和娱乐：</strong> 机器学习不仅用于游戏中的智能对手，还应用于游戏设计、动态难度调整等方面。例如，AlphaGo 使用深度学习技术战胜了围棋世界冠军。</li></ul><h2 id="机器学习的未来"><a href="#机器学习的未来" class="headerlink" title="机器学习的未来"></a>机器学习的未来</h2><p>随着数据量的爆炸式增长和计算能力的提升，机器学习的应用将继续扩展，带来更加智能和高效的系统。例如：</p><ul><li><strong>强化学习：</strong> 使计算机能够在没有明确指导的情况下通过试错来解决复杂问题。例如，AlphaGo 和 OpenAI 的 Dota 2 游戏 AI 都使用了强化学习。</li><li><strong>自监督学习：</strong> 目前的机器学习模型通常<strong>需要大量带标签</strong>的数据来进行训练，而自监督学习则能够在没有标签的数据下学习更有效的表示。</li><li><strong>深度学习：</strong> <strong>深度</strong>学习是机器学习中的一个分支，主要关注<strong>神经网络的应用</strong>，它已经在图像识别、自然语言处理等方面取得了突破性进展。未来，深度学习将继续推动人工智能的发展。</li></ul><h1 id="机器学习如何工作"><a href="#机器学习如何工作" class="headerlink" title="机器学习如何工作"></a>机器学习如何工作</h1><p>机器学习（Machine Learning, ML）的核心思想是让计算机能够通过<strong>数据学习</strong>，并从中推断出规律或模式，而不依赖于显式编写的规则或代码。</p><p>简单来说，机器学习的工作流程是让机器<strong>通过历史数据自动改进</strong>其决策和预测能力。</p><p>机器学习的工作流程可以简化为以下几个步骤：</p><ol><li><strong>收集数据</strong>：准备包含特征和标签的数据。</li><li><strong>选择模型</strong>：根据任务选择合适的机器学习算法。</li><li><strong>训练模型</strong>：让模型通过数据学习模式，最小化误差。</li><li><strong>评估与验证</strong>：通过测试集评估模型性能，并进行优化。</li><li><strong>部署模型</strong>：将训练好的模型应用到实际场景中进行预测。</li><li><strong>持续改进</strong>：随着新数据的产生，模型需要定期更新和优化。</li></ol><p>这个过程能够让计算机从经验中自动学习，并在各种任务中做出越来越准确的预测。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/machine-learning-how-machine-learning-work.png" alt="img"></p><p>我们可以从以下几个方面来理解机器学习是如何工作的：</p><h3 id="1-数据输入：数据是学习的基础"><a href="#1-数据输入：数据是学习的基础" class="headerlink" title="1. 数据输入：数据是学习的基础"></a>1. 数据输入：数据是学习的基础</h3><p>机器学习的<strong>第一步是数据收集</strong>。没有数据，机器学习模型无法进行训练。数据通常包括”输入<strong>特征</strong>“和”<strong>标签</strong>“：</p><ul><li><strong>输入特征（Features）：</strong> 这些是模型用来做预测或分类的信息。例如，在房价预测问题中，输入特征可以是房子的面积、地理位置、卧室数量等。</li><li><strong>标签（Labels）：</strong> 标签是我们<strong>想要预测或分类的结果</strong>，通常是一个数字或类别。例如，在房价预测问题中，标签是房子的价格。</li></ul><p>机器学习模型的目标是从数据中找出输入特征与标签之间的关系，基于这些关系做出预测。</p><h3 id="2-模型选择：选择合适的学习算法"><a href="#2-模型选择：选择合适的学习算法" class="headerlink" title="2. 模型选择：选择合适的学习算法"></a>2. 模型选择：选择合适的学习算法</h3><p>机器学习<strong>模型</strong>（<strong>也叫做算法</strong>）是帮助计算机学习数据并进行预测的工具。根据数据的性质和任务的不同，常见的机器学习模型包括：</p><ul><li><strong>监督学习模型：</strong> 给定带有标签的数据，模型通过学习输入和标签之间的关系来做预测。例如，<strong>线性回归</strong>、<strong>逻辑回归</strong>、<strong>支持向量机（SVM）</strong> 和 <strong>决策树</strong>。</li><li><strong>无监督学习模型：</strong> 没有标签的数据，模型通过探索数据中的结构或模式来进行学习。例如，<strong>K-means 聚类</strong>、<strong>主成分分析（PCA）</strong>。</li><li><strong>强化学习模型：</strong> 模型在与环境互动的过程中，通过奖励和惩罚来学习最佳行为。例如，<strong>Q-learning</strong>、<strong>深度强化学习</strong>（Deep Q-Networks, DQN）。</li></ul><h3 id="3-训练过程：让模型从数据中学习"><a href="#3-训练过程：让模型从数据中学习" class="headerlink" title="3. 训练过程：让模型从数据中学习"></a>3. 训练过程：让模型从数据中学习</h3><p>在训练阶段，模型通过历史数据”学习”输入和标签之间的关系，通常通过最小化一个损失函数（Loss Function）来优化模型的参数。训练过程可以概括为以下步骤：</p><ul><li><strong>初始状态：</strong> 模型从<strong>随机值开始</strong>。比如，神经网络的权重是随机初始化的。</li><li><strong>计算预测：</strong> 对于每个输入，模型会做出一个预测。这是通过将输入数据传递给模型，计算得到输出。</li><li><strong>计算误差（损失）：</strong> 误差是指模型预测的输出与实际标签之间的差异。例如，对于回归问题，误差可以通过均方误差（MSE）来衡量。</li><li><strong>优化模型：</strong> 通过反向传播（在神经网络中）或梯度下降等优化算法，<strong>不断调整模型的参数</strong>（如神经网络的权重），使得误差最小化。这个过程就是<strong>训练</strong>，直到模型能够在训练数据上做出比较准确的预测。</li></ul><h3 id="4-验证与评估：测试模型的性能"><a href="#4-验证与评估：测试模型的性能" class="headerlink" title="4. 验证与评估：测试模型的性能"></a>4. 验证与评估：测试模型的性能</h3><p>训练过程完成后，我们需要<strong>评估</strong>模型的性能。为了<strong>避免模型过度拟合</strong>训练数据，我们将数据分为<strong>训练集</strong>和<strong>测试集</strong>，其中：</p><ul><li><strong>训练集：</strong> 用于训练模型的部分数据。</li><li><strong>测试集：</strong> 用于评估模型性能的部分数据，通常不参与训练过程。</li></ul><p>常见的评估指标包括：</p><ul><li><strong>准确率（Accuracy）：</strong> 分类问题中正确分类的比例。</li><li><strong>均方误差（MSE）：</strong> <strong>回归</strong>问题中，预测值与真实值差的<strong>平方的平均值</strong>。</li><li><strong>精确率（Precision）与召回率（Recall）：</strong> 用于二分类问题，尤其是<strong>类别不平衡</strong>时。</li><li><strong>F1分数：</strong> <strong>精确率</strong>与<strong>召回率</strong>的<strong>调和平均数</strong>，综合考虑分类器的表现。</li></ul><h3 id="5-优化与调整：提高模型的精度"><a href="#5-优化与调整：提高模型的精度" class="headerlink" title="5. 优化与调整：提高模型的精度"></a>5. 优化与调整：提高模型的精度</h3><p>如果模型在测试集上的表现不理想，可能需要进一步优化。这通常包括：</p><ul><li><strong>调整超参数（Hyperparameters）：</strong> 比如<strong>学习率</strong>、<strong>正则化系数</strong>、<strong>树的深度</strong>等。这些超参数影响模型的学习能力。</li><li><strong>模型选择与融合：</strong> 尝试不同的模型或模型融合（比如集成学习方法，如随机森林、XGBoost 等）来提高精度。</li><li><strong>数据增强：</strong> <strong>扩展</strong>训练数据集，比如对图像进行旋转、翻转等操作，帮助模型提高泛化能力。</li></ul><h3 id="6-模型部署与预测：实际应用"><a href="#6-模型部署与预测：实际应用" class="headerlink" title="6. 模型部署与预测：实际应用"></a>6. 模型部署与预测：实际应用</h3><p>一旦模型在训练和测试数据上表现良好，就可以将模型部署到实际应用中：</p><ul><li><strong>模型部署：</strong> 将训练好的模型嵌入到应用程序、网站、服务器等系统中，供用户使用。</li><li><strong>实时预测：</strong> 在实际环境中，新的数据输入到模型中，模型根据之前学习到的模式进行实时预测或分类。</li></ul><h3 id="7-持续学习与模型更新："><a href="#7-持续学习与模型更新：" class="headerlink" title="7. 持续学习与模型更新："></a>7. 持续学习与模型更新：</h3><p>机器学习系统通常不是一次性完成的。在实际应用中，随着时间的推移，新的数据会不断产生，因此，模型需要定期更新和再训练，以保持其预测能力。这可以通过<strong>在线学习</strong>、<strong>迁移学习</strong>等方法来实现。</p><h1 id="机器学习基础概念"><a href="#机器学习基础概念" class="headerlink" title="机器学习基础概念"></a>机器学习基础概念</h1><p>在学习机器学习时，理解其核心基础概念至关重要。</p><p>这些基础概念帮助我们理解数据如何输入到模型中、模型如何学习、以及如何评估模型的表现。</p><p>接下来，我们将详细讲解几个机器学习中的基本概念：</p><ul><li><strong>训练集、测试集和验证集</strong>：帮助训练、评估和调优模型。</li><li><strong>特征与标签</strong>：特征是<strong>输入</strong>，标签是模型预测的<strong>目标</strong>。</li><li><strong>模型与算法</strong>：模型是<strong>通过算法训练得到</strong>的，算法帮助模型<strong>学习</strong>数据中的<strong>模式</strong>。</li><li><strong>监督学习、无监督学习和强化学习</strong>：三种常见的学习方式，分别用于不同的任务。</li><li><strong>过拟合与欠拟合</strong>：两种常见的问题，<strong>影响</strong>模型的<strong>泛化</strong>能力。</li><li><strong>训练误差与测试误差</strong>：反映模型是否能适应数据，并进行有效预测。</li><li><strong>评估指标</strong>：衡量模型好坏的标准，根据任务选择合适的指标。</li></ul><p>这些基础概念是理解和应用机器学习的基础，掌握它们是进一步学习的关键。</p><h3 id="训练集、测试集和验证集"><a href="#训练集、测试集和验证集" class="headerlink" title="训练集、测试集和验证集"></a>训练集、测试集和验证集</h3><ul><li><strong>训练集（Training Set）：</strong> 训练集是用于训练机器学习模型的数据集，它包含输入特征和对应的标签（在监督学习中）。模型通过学习训练集中的数据来调整参数，逐步提高预测的准确性。</li><li><strong>测试集（Test Set）：</strong> 测试集用于<strong>评估</strong>训练好的模型的<strong>性能</strong>。测试集中的数据不参与模型的训练，模型使用它来进行预测，并与真实标签进行比较，帮助我们了解模型在<strong>未见过</strong>的数据上的<strong>表现</strong>。</li><li><strong>验证集（Validation Set）：</strong> 验证集用于在训练过程中<strong>调整</strong>模型的<strong>超参数</strong>（如学习率、正则化参数等）。它通常被用于模型<strong>调优</strong>，帮助选择<strong>最佳的模型参数</strong>，<strong>避免过</strong>拟合。验证集的作用是对模型进行<strong>监控和调试</strong>。</li></ul><p><strong>总结：</strong></p><ul><li>训练集用于训练模型。</li><li>测试集用于<strong>评估</strong>模型的<strong>最终</strong>性能。</li><li>验证集用于模型<strong>调优</strong>。</li></ul><h3 id="特征（Features）和标签（Labels）"><a href="#特征（Features）和标签（Labels）" class="headerlink" title="特征（Features）和标签（Labels）"></a>特征（Features）和标签（Labels）</h3><ul><li><strong>特征（Features）：</strong> 特征是<strong>输入数据的不同属性</strong>，模型使用这些特征来做出预测或分类。例如，在房价预测中，特征可能包括房子的面积、地理位置、卧室数量等。</li><li><strong>标签（Labels）：</strong> 标签是机器学习任务中的<strong>目标变量</strong>，模型要预测的结果。对于监督学习任务，标签通常是已知的。例如，在房价预测中，标签就是房子的实际价格。</li></ul><p><strong>总结：</strong></p><ul><li>特征是模型输入的数据。</li><li>标签是模型需要预测的输出。</li></ul><h3 id="模型（Model）与算法（Algorithm）"><a href="#模型（Model）与算法（Algorithm）" class="headerlink" title="模型（Model）与算法（Algorithm）"></a>模型（Model）与算法（Algorithm）</h3><ul><li><strong>模型（Model）：</strong> 模型是通过学习数据中的模式而构建的<strong>数学结构</strong>。它接受输入特征，经过一系列计算和转化，输出一个预测结果。常见的模型有线性回归、决策树、神经网络等。</li><li><strong>算法（Algorithm）：</strong> 算法是实现机器学习的<strong>步骤或规则</strong>，它定义了模型如何从数据中学习。常见的算法有梯度下降法、随机森林、K近邻算法等。算法帮助模型调整其参数以最小化预测误差。</li></ul><p><strong>总结：</strong></p><ul><li>模型是<strong>学习到的结果</strong>，它可以用来进行预测。</li><li>算法是<strong>训练模型的过程</strong>，帮助模型从数据中学习。</li></ul><h3 id="监督学习、无监督学习和强化学习"><a href="#监督学习、无监督学习和强化学习" class="headerlink" title="监督学习、无监督学习和强化学习"></a>监督学习、无监督学习和强化学习</h3><ul><li><strong>监督学习（Supervised Learning）：</strong> 在监督学习中，训练数据包含<strong>已知的标签</strong>。模型通过学习输入特征与标签之间的关系来进行预测或分类。监督学习的目标是最小化预测错误，使模型能够在新数据上做出准确的预测。<ul><li><strong>例子：</strong> 线性回归、逻辑回归、支持向量机（SVM）、决策树。</li></ul></li><li><strong>无监督学习（Unsupervised Learning）：</strong> 无监督学习中，训练数据没有标签，模型通过分析输入数据中的结构或模式来进行学习。目标是发现数据的<strong>潜在规律</strong>，常见的任务包括聚类、降维等。<ul><li><strong>例子：</strong> K-means 聚类、主成分分析（PCA）。</li></ul></li><li><strong>强化学习（Reinforcement Learning）：</strong> 强化学习是让智能体（Agent）通过与环境（Environment）的互动，采取行动并<strong>根据奖励或惩罚来学习最优</strong>策略。智能体的目标是通过最大化长期奖励来优化行为。<ul><li><strong>例子：</strong> AlphaGo、自动驾驶、游戏AI。</li></ul></li></ul><p><strong>总结：</strong></p><ul><li>监督学习：有标签的训练数据，任务是预测或分类。</li><li>无监督学习：没有标签的训练数据，任务是发现数据中的模式或结构。</li><li>强化学习：通过与环境互动，智能体根据奖励和惩罚进行学习。</li></ul><h3 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h3><ul><li><strong>过拟合（Overfitting）：</strong> 过拟合是指模型<strong>在训练数据上</strong>表现非常好，但<strong>在测试数据上</strong>表现很差。这通常发生在模型<strong>复杂度过高</strong>、<strong>参数过多</strong>，导致模型”记住”了<strong>训练数据中的</strong>噪声或偶然性，而<strong>不具备泛化</strong>能力。过拟合的模型<strong>无法有效应对新</strong>数据。</li><li><strong>欠拟合（Underfitting）：</strong> 欠拟合是指模型在训练数据上和测试数据上<strong>都表现不佳</strong>，通常是因为模型<strong>过于简单</strong>，无法捕捉数据中的复杂模式。欠拟合的模型无法从数据中学习到有用的规律。</li></ul><p><strong>解决方法：</strong></p><ul><li>过拟合：可以通过<strong>简化</strong>模型、<strong>增加</strong>训练数据或使用<strong>正则化</strong>等方法来缓解。</li><li>欠拟合：可以通过增加模型复杂度或使用更复杂的算法来改进。</li></ul><h3 id="训练与测试误差"><a href="#训练与测试误差" class="headerlink" title="训练与测试误差"></a>训练与测试误差</h3><ul><li><strong>训练误差（Training Error）：</strong> 训练误差是模型在训练数据上的表现，反映了模型是否能够很好地适应训练数据。如果训练误差很大，可能说明模型不够复杂，<strong>欠</strong>拟合；如果训练误差很小，可能说明模型太复杂，容易<strong>过</strong>拟合。</li><li><strong>测试误差（Test Error）：</strong> 测试误差是模型在未见过的数据上的表现，反映了模型的泛化能力。测试误差应当与训练误差相匹配，若测试误差<strong>远高</strong>于训练误差，通常是<strong>过</strong>拟合。</li></ul><p><strong>总结：</strong></p><ul><li>训练误差和测试误差的差距可以帮助我们判断模型的适应性。</li><li>理想的情况是训练误差和测试误差<strong>都较小</strong>，并且<strong>相对接近</strong>。</li></ul><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>根据任务的不同，机器学习模型的评估指标也不同。以下是常用的一些评估指标：</p><ul><li><strong>准确率（Accuracy）：</strong> <strong>分类</strong>任务中，<strong>正确</strong>分类的样本占总样本的比例。</li><li><strong>精确率（Precision）和召回率（Recall）：</strong> 主要用于处理<strong>不平衡</strong>数据集，精确率衡量的是被模型预<strong>测为正类</strong>的样本中，有多少是<strong>真正的正类</strong>；召回率衡量的是<strong>所有实际</strong>正类<strong>中</strong>，有多少被模型<strong>正确识别</strong>为正类。</li><li><strong>F1 分数：</strong> 精确率与召回率的<strong>调和</strong>平均数，用于综合考虑模型的表现。</li><li><strong>均方误差（MSE）：</strong> 回归任务中，预测值与真实值之间差异的平方的平均值。</li></ul><p><strong>总结：</strong><br>评估指标帮助我们衡量模型的表现，选择最合适的指标可以根据任务的需求来进行。</p><h1 id="深度学习-Deep-Learning-入门——基本概念"><a href="#深度学习-Deep-Learning-入门——基本概念" class="headerlink" title="深度学习(Deep Learning)入门——基本概念"></a>深度学习(Deep Learning)入门——基本概念</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a><strong>引言</strong></h2><p>本文是该系列文章中的第一篇，旨在介绍深度学习基础概念、<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95&zhida_source=entity">优化算法</a>、 调参基本思路、正则化方式等，后续文章将关注深度学习在<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&zhida_source=entity">自然语言处理</a>、语音识别、和计算机视觉领域的应用。</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a><strong>基本概念</strong></h2><p>深度学习是为了解决表示学习难题而被提出的。本节，我们介绍这些深度学习相关的基本概念。</p><p><strong>表示学习（representation learning）</strong> 机器学习旨在自动地学到从数据的<strong>表示</strong>（representation）到数据的<strong>标记</strong>（label）的<strong>映射</strong>。随着机器学习算法的日趋成熟，人们发现，在某些领域（如图像、语音、文本等），如何从数据中<strong>提取合适的表示</strong>成为整个任务的瓶颈所在，而数据表示的好坏直接影响后续学习任务（所谓garbage in，garbage out）。与其依赖人类专家<strong>设计手工特征</strong>（难设计还不见得好用），表示学习希望能从数据中<strong>自动地</strong>学到从数据的原始形式到数据的表示之间的映射。</p><p><strong>深度学习（deep learning，DL）</strong> 表示学习的理想很丰满，但实际中人们发现从数据的原始形式<strong>直接学得</strong>数据表示这件事<strong>很难</strong>。深度学习是<strong>目前最成功的表示学习方法</strong>，因此，目前国际表示学习大会（ICLR）的绝大部分论文都是关于深度学习的。深度学习是把表示学习的任务划分成几个小目标，先从数据的原始形式中先学习<strong>比较低级</strong>的表示，再<strong>从低级</strong>表示<strong>学得比较高级</strong>的表示。这样，每个小目标比较容易达到，综合起来我们就完成表示学习的任务。这类似于算法设计思想中的<strong>分治法</strong>（<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=divide-and-conquer&zhida_source=entity">divide-and-conquer</a>）。</p><p><strong>深度神经网络（deep neural networks，DNN）</strong> 深度学习<strong>目前几乎唯一行之有效</strong>的实现形式。简单的说，深度神经网络就是<strong>很深的</strong>神经网络。我们利用<strong>网络中逐层对特征进行加工</strong>的特性，<strong>逐渐从低级特征提取高级</strong>特征。除了深度神经网络之外，有学者在探索其他深度学习的实现形式，比如深度森林。</p><p>深度神经网络目前的成功取决于三大推动因素。1. <strong>大数据</strong>。当数据量小时，很难从数据中学得合适的表示，而传统算法+<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B&zhida_source=entity">特征工程</a>往往能取得很好的效果；2. <strong>计算能力</strong>。<strong>大的数据</strong>和<strong>大的网络</strong>需要有足够的快的计算能力才能使得模型的应用成为可能。3. <strong>算法创新</strong>。现在很多<strong>算法设计</strong>关注在如何使网络更好地训练、更快地运行、取得更好的性能。</p><p><strong>多层感知机（multi-layer perceptrons，MLP）</strong> <strong>多层</strong>由<strong>全连接层</strong>组成的深度神经网络。<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA&zhida_source=entity">多层感知机</a>的<strong>最后一层</strong>全连接层实质上是一个<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8&zhida_source=entity">线性分类器</a>，而其他部分则是为这个线性分类器<strong>学习一个合适的数据表示</strong>，使倒数第二层的特征<strong>线性可分</strong>。</p><p>**激活函数（activation function）**神经网络的必要组成部分。如果没有激活函数，多次线性运算的堆叠仍然是一个线性运算，即不管用再多层实质只起到了一层神经网络的作用。一个好的激活函数应满足以下性质。1. <strong>不会饱和</strong>。sigmoid和tanh激活函数在两侧尾端会有饱和现象，这会使导数在这些区域接近零，从而阻碍网络的训练。2. <strong>零均值</strong>。ReLU<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=5&q=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&zhida_source=entity">激活函数</a>的输出均值不为零，这会影响网络的训练。3. <strong>容易计算</strong>。</p><p><strong>迁移学习（transfer learning）</strong> 深度学习下的迁移学习旨在<strong>利用源任务数据辅助</strong>目标任务数据下的学习。迁移学习适用于源任务数据比目标任务数据多，并且源任务中学习得到的低层特征可以帮助目标任务的学习的情形。在<strong>计算机视觉</strong>领域，最常用的<strong>源任务数据是ImageNet</strong>。对ImageNet预训练模型的<strong>利用</strong>通常有两种方式。1. <strong>固定特征提取器</strong>。用ImageNett预训练模型提取目标任务数据的高层特征。2. <strong>微调（fine-tuning）</strong>。以ImageNet预训练模型作为目标任务模型的<strong>初始化初始化权值</strong>，之后<strong>在目标任务数据上进行微调</strong>。</p><p><strong>多任务学习（multi-task learning）</strong> 与其针对每个任务训练一个小网络，深度学习下的多任务学习旨在训<strong>练一个大网络以同时完成全部任务</strong>。这些任务中用于提取<strong>低层特征</strong>的层是<strong>共享</strong>的，之后产生分支，各任务拥有各自的若干层用于完成其任务。多任务学习适用于多个任务共享低层特征，并且各个任务的数据很相似的情况。</p><p><strong>端到端学习（end-to-end learning）</strong> 深度学习下的端到端学习旨在通过一个深度神经网络<strong>直接学习从数据的原始形式到数据的标记的映射</strong>。端到端学习并不应该作为我们的一个追求目标，是否要采用端到端学习的一个重要考虑因素是：有没有足够的数据对应端到端的过程，以及我们有没有一些领域知识能够用于整个系统中的一些模块。</p><h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a><strong>优化算法</strong></h2><p>在<strong>网络结构</strong>确定之后，我们需要对网络的<strong>权值</strong>（weights）进行优化。本节，我们介绍优化深度神经网络的基本思想。</p><p><strong>梯度下降（<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=gradient+descent&zhida_source=entity">gradient descent</a>，GD）</strong> 想象你去野足但却迷了路，在漆黑的深夜你一个人被困住山谷中，你<strong>知道谷底是出口</strong>但是天太黑了根本看不清楚路。于是你确定采取一个<strong>贪心</strong>(greedy)算法：先试探在当前位置往<strong>哪个方向走下降最快（即梯度方向）</strong>，再朝着这个方向走<strong>一小步</strong>，重复这个过程直到你到达谷底。这就是梯度下降的基本思想。</p><p><a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95&zhida_source=entity">梯度下降算法</a>的性能大致取决于三个因素。1. <strong>初始位置</strong>。如果你初始位置就离谷底很近，自然很容易走到谷底。2. <strong>山谷地形</strong>。如果山谷是“九曲十八弯”，很有可能你在里面绕半天都绕不出来。3. <strong>步长</strong>。你每步迈多大，当你步子迈太小，很可能你走半天也没走多远，而当你步子迈太大，一不小心就容易撞到旁边的悬崖峭壁，或者错过了谷底。</p><p><strong>误差反向传播（error back-propagation，BP）</strong> 结合微积分中<strong>链式法则</strong>和算法设计中<strong>动态规划思想</strong>用于计算梯度。 直接用纸笔推导出中间某一层的梯度的数学表达式是很困难的，但<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99&zhida_source=entity">链式法则</a>告诉我们，一旦我们知道<strong>后一层的梯度</strong>，再结合后一层<strong>对当前层的导数</strong>，我们就可以得到<strong>当前层的梯度</strong>。<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92&zhida_source=entity">动态规划</a>是一个<strong>高效计算所有梯度</strong>的实现技巧，通过由<strong>高层往低层逐层计算梯度</strong>，避免了对高层梯度的重复计算。</p><p><strong>滑动平均（moving average）</strong> 要前进的方向<strong>不再由当前梯度方向完全</strong>决定，而是最近<strong>几次梯度方向的滑动平均</strong>。利用滑动平均思想的优化算法有带动量（momentum）的SGD、<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=Nesterov%E5%8A%A8%E9%87%8F&zhida_source=entity">Nesterov动量</a>、Adam（ADAptive Momentum estimation）等。</p><p><strong><a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E8%87%AA%E9%80%82%E5%BA%94%E6%AD%A5%E9%95%BF&zhida_source=entity">自适应步长</a></strong> 自适应地确定权值<strong>每一维的步长</strong>。当某一维持续<strong>震荡</strong>时，我们希望这一维的步长小一些；当某一维<strong>一直沿着相同</strong>的方向前进时，我们希望这一维的步长大一些。利用自适应步长思想的优化算法有AdaGrad、RMSProp、Adam等。</p><p><strong>学习率衰减</strong> 当开始训练时，<strong>较大</strong>的学习率可以使你在参数空间有<strong>更大范围的探索</strong>；当优化接近<strong>收敛</strong>时，我们需要<strong>小一些的学习率</strong>使权值更接近<strong>局部最优</strong>点。</p><p><strong>深度神经网络优化的困难</strong> 有学者指出，在很高维的空间中，局部最优是比较少的，而大部分梯度为零的点是<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E9%9E%8D%E7%82%B9&zhida_source=entity">鞍点</a>。平原区域的鞍点会使梯度在<strong>很长一段时间内都接近零</strong>，这会使得拖慢优化过程。</p><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a><strong>初始化</strong></h2><p><strong>权值初始化对网络优化至关重要</strong>。早年深度神经网络无法有效训练的一个重要原因就是早期人们对初始化不太重视。本节，我们介绍几个适用于深度神经网络的初始化方法。</p><p><strong>初始化的基本思想</strong> <strong>方差不变</strong>，即设法对权值进行初始化，使得各层神经元的<strong>方差保持不变</strong>。</p><p><strong>Xavier初始化</strong> 从高斯分布或<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83&zhida_source=entity">均匀分布</a>中对权值进行采样，使得<strong>权值的方差是1&#x2F;n</strong>，其中n是输入神经元的个数。该推导假设<strong>激活函数是线性</strong>的。</p><p><strong>He初始化&#x2F;MSRA初始化</strong> 从高斯分布或均匀分布中对权值进行采样，使得权值的<strong>方差是2&#x2F;n</strong>。该推导假设<strong>激活函数是ReLU</strong>。因为ReLU<strong>会将小于0的神经元置零</strong>，大致上会使一半的神经元置零，所以为了弥补丢失的这部分信息，<strong>方差要乘以2</strong>。</p><p><strong>批量规范化（batch-normalization，BN）</strong> 每层<strong>显式</strong>地对神经元的激活值做<strong>规范化</strong>，使其具有零均值和单位方差。批量规范化使<strong>激活值的分布固定</strong>下来，这样可以使各层<strong>更加独立地进行学习</strong>。批量规范化可以使得网络<strong>对初始化和学习率不太敏感</strong>。此外，批量规范化<strong>有些许正则化</strong>的作用，但不要用其作为正则化手段。</p><h2 id="偏差-方差（bias-variance）"><a href="#偏差-方差（bias-variance）" class="headerlink" title="偏差&#x2F;方差（bias&#x2F;variance）"></a><strong>偏差&#x2F;方差（bias&#x2F;variance）</strong></h2><p>优化完成后，你发现网络的表现不尽如人意，这时诊断网络处于高偏差&#x2F;<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E9%AB%98%E6%96%B9%E5%B7%AE&zhida_source=entity">高方差</a>状态是对你下一步<strong>调参方向的重要指导</strong>。与经典机器学习算法有所不同，因为深度神经网络通常要<strong>处理非常高维</strong>的特征，所以网络可能<strong>同时处于高偏差&#x2F;高方差</strong>的状态，即在<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4&zhida_source=entity">特征空间</a>的一些区域网络处于高偏差，而在另一些区域处于高方差。本节，我们对偏差&#x2F;方差作一简要介绍。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/v2-2d443d459279b52f130fa3096c7e2673_1440w.jpg" alt="img"></p><p><strong>偏差</strong> 偏差度量了网络的<strong>训练集误差</strong>和<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%AF%AF%E5%B7%AE&zhida_source=entity"><strong>贝叶斯误差</strong></a>（即能达到的<strong>最优误差</strong>）的差距。高偏差的网络有很高的训练集误差，说明网络对数据中隐含的一般规律还没有学好。当网络处于高偏差时，通常有以下几种解决方案。<strong>1. 训练更大的网络</strong>。网络<strong>越大</strong>，对数据<strong>潜在</strong>规律的<strong>拟合能力越强</strong>。<strong>2. 更多的训练轮数</strong>。通常训练<strong>时间越久</strong>，对训练集的<strong>拟合能力越强</strong>。<strong>3. 改变网络结构</strong>。不同的网络<strong>结构</strong>对训练集的<strong>拟合能力</strong>有所不同。</p><p><strong>方差</strong> 方差度量了网络的<strong>验证集误差</strong>和<strong>训练集误差</strong>的差距。<strong>高</strong>方差的网络<strong>学习能力太强</strong>，把训练集中<strong>自身独有</strong>的一些特点<strong>也当作一般规律</strong>学得，使网络<strong>不能很好的泛化</strong>（generalize）到<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E9%AA%8C%E8%AF%81%E9%9B%86&zhida_source=entity">验证集</a>。当网络处于高方差时，通常有以下几种解决方案。<strong>1. 更多的数据</strong>。这是对高方差问题<strong>最行之有效</strong>的解决方案。<strong>2. 正则化</strong>。<strong>3. 改变网络结构</strong>。不同的网络结构对方差也会有影响。</p><h2 id="正则化（regularization）"><a href="#正则化（regularization）" class="headerlink" title="正则化（regularization）"></a><strong>正则化（regularization）</strong></h2><p>正则化是<strong>解决高方差</strong>问题的重要方案之一。本节，我们将对常用<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95&zhida_source=entity">正则化方法</a>做一介绍。</p><p><strong><a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=8&q=%E6%AD%A3%E5%88%99%E5%8C%96&zhida_source=entity">正则化</a>的基本思想</strong> 正则化的基本思想是使网络的<strong>有效</strong>大小<strong>变小</strong>。网络变小之后，网络的<strong>拟合能力随之降低</strong>，这会使网络不容易<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E8%BF%87%E6%8B%9F%E5%90%88&zhida_source=entity">过拟合</a>到训练集。</p><p><strong>L2正则化</strong> L2正则化倾向于使网络的<strong>权值接近0</strong>。这会使<strong>前</strong>一层神经元对<strong>后</strong>一层神经元的<strong>影响降低</strong>，使网络<strong>变得简单</strong>，降低网络的有效大小，降低网络的拟合能力。L2正则化实质上是<strong>对权值做线性衰减</strong>，所以<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=4&q=L2%E6%AD%A3%E5%88%99%E5%8C%96&zhida_source=entity">L2正则化</a>也被称为权值衰减（weight decay）。</p><p><strong>随机失活（dropout）</strong> 在训练时，随机失活随机选择一部分神经元，使其置零，不参与本次优化迭代。随机失活减少了每次参与优化迭代的神经元数目，使网络的有效大小变小。随机失活的作用有两点。<strong>1. 降低神经元之间耦合</strong>。因为神经元<strong>会被随机置零</strong>，所以每个神经元不能依赖于其他神经元，这会迫使每个神经元自身要能提取到合适的特征。<strong>2. 网络集成</strong>。随机失活可以看作在训练时<strong>每次迭代定义出一个新的网络</strong>，这些网络共享权值。在测试时的网络是这些网络的集成。</p><p><strong>数据扩充（<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=data+augmentation&zhida_source=entity">data augmentation</a>）</strong> 这实质是获得更多数据的方法。当收集数据很昂贵，或者我们拿到的是第二手数据，数据就这么多时，我们<strong>从现有数据中扩充</strong>生成更多数据，用<strong>生成的“伪造”<strong>数据当作更多的真实数据进行训练。以图像数据做分类任务为例，把图像水平翻转、移动一定位置、旋转一定角度、或做一点色彩变化等，这些操作通常都不会影响这幅图像对应的标记。并且你可以尝试</strong>这些操作的组合</strong>，理论上讲，你可以通过这些组合得到无穷多的训练样本。</p><p><strong>早停（early stopping）</strong> 随着训练的进行，当你发现验证集误差不再变化或者开始上升时，<strong>提前停止</strong>训练。</p><h2 id="调参技巧"><a href="#调参技巧" class="headerlink" title="调参技巧"></a><strong>调参技巧</strong></h2><p>深度神经网络涉及<strong>很多的超参数</strong>，如<strong>学习率大小</strong>、<strong>L2正则化系数</strong>、<strong>动量大小</strong>、<strong>批量大小</strong>、<strong>隐层神经元数目</strong>、<strong>层数</strong>、<strong>学习率衰减率</strong>等。本节，我们介绍调参的基本技巧。</p><p><strong>随机搜索</strong> 由于你<strong>事先并不知道</strong>哪些超参数对你的问题更重要，因此随机搜索通常是比<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2&zhida_source=entity">网格搜索</a>（grid search）更有效的<strong>调参策略</strong>。</p><p><strong>对数空间搜索</strong> 对于<strong>隐层神经元数目</strong>和<strong>层数</strong>，可以直接<strong>从均匀分布采样</strong>进行搜索。而对于<strong>学习率</strong>、L2<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E6%AD%A3%E5%88%99%E5%8C%96%E7%B3%BB%E6%95%B0&zhida_source=entity">正则化系数</a>、和<strong>动量</strong>，在<strong>对数空间搜索</strong>更加有效。例如：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">learning_rate = 10 ** random.uniform(-5, -1)  # From 1e-5 to 1e-1</span><br><span class="line">weight_decay = 10 ** random.uniform(-7, -1)   # From 1e-7 to 1e-1</span><br><span class="line">momentum = 1 - 10 ** random.uniform(-3, -1)   # From 0.9 to 0.999</span><br></pre></td></tr></table></figure><h2 id="实现技巧"><a href="#实现技巧" class="headerlink" title="实现技巧"></a><strong>实现技巧</strong></h2><p><strong><a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E5%9B%BE%E5%BD%A2%E5%A4%84%E7%90%86%E5%8D%95%E5%85%83&zhida_source=entity">图形处理单元</a>（graphics processing units, GPU）</strong> 深度神经网络的<strong>高效实现工具</strong>。简单来说，CPU擅长<strong>串行、复杂</strong>的运算，而GPU擅长<strong>并行、简单</strong>的运算。深度神经网络中的<strong>矩阵运算都十分简单</strong>，但计算<strong>量巨大</strong>。因此，GPU无疑具有非常强大的优势。</p><p><strong><a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E5%90%91%E9%87%8F%E5%8C%96&zhida_source=entity">向量化</a>（vectorization）</strong> 代码<strong>提速</strong>的<strong>基本技巧</strong>。能<strong>少写</strong>一个for循环就少写一个，能<strong>少做</strong>一次矩阵运算就少做一次。实质是尽量将<strong>多次标量运算转化为一次向量</strong>运算；将<strong>多次向量</strong>运算转化为<strong>一次矩阵</strong>运算。因为矩阵运算<strong>可以并行</strong>，这将会比多次单独运算快很多。</p><h1 id="计算机视觉基础"><a href="#计算机视觉基础" class="headerlink" title="计算机视觉基础"></a>计算机视觉基础</h1><h2 id="一、计算机视觉概述"><a href="#一、计算机视觉概述" class="headerlink" title="一、计算机视觉概述"></a>一、计算机视觉概述</h2><p>1、计算机视觉的背景知识<br>对计算机视觉的第一印象：<strong>用计算机代替人类的眼睛，模仿人类视觉去完成各项任务</strong>。</p><p>计算机视觉（Computer Vision） 是一门研究<strong>如何使机器“看</strong>”的科学，也可以看作是研究如何使人工系统<strong>从图像或多维</strong>数据中**“感知”**的科学。</p><p>终极目标：计算机视觉成为机器认知世界的基础，终极目的是使得计算机能够像人一样“看懂世界”。<br>2、计算机视觉与人类视觉的关系</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/b4c77b970b584ed38304b329b367e9c9.png" alt="在这里插入图片描述"></p><h2 id="二、计算机视觉的基本原理"><a href="#二、计算机视觉的基本原理" class="headerlink" title="二、计算机视觉的基本原理"></a>二、计算机视觉的基本原理</h2><p>1、计算机视觉的处理对象</p><h3 id="数字图像"><a href="#数字图像" class="headerlink" title="数字图像"></a>数字图像</h3><p>又称为数码图像或数位图像；<br>是用一个<strong>数字矩阵</strong>来表达客观物体的图像；<br>是由模拟图像数字化得到的；<br>是一个<strong>离散采样点</strong>的集合，每个点具<strong>有其各自的属性</strong>；<br>是<strong>以像素为基本元素</strong>的图像；<br>可以用数字计算机或数字电路存储和处理的图像。<br>数字图像处理包括的内容:</p><p>图像变换；<br>图像增强；<br>图像恢复；<br>图像压缩编码；<br>图像分割；<br>图像分析与描述；<br>图像的识别分类。<br>2、计算机视觉的工作原理<br>图像数字化的两个过程：</p><p><strong>采样</strong>是将空间上连续的图像<strong>变换成离散的点</strong>，采样<strong>频率越高</strong>，还原的图像<strong>越真实</strong>。<br><strong>量化</strong>是将采样出来的像素点<strong>转换成离散的数量值，<strong>一幅数字图像中不同灰度值的个数称为</strong>灰度等级</strong>，级数<strong>越大</strong>，图像<strong>越清晰</strong>。<br>计算机视觉的基础工作原理：<br><strong>①构造多层神经网络 –&gt; ②较低层识别初级的图像特征 –&gt; ③若干底层特征组成更上一层特征 –&gt; ④通过多个层级的组合 –&gt; ⑤最终在顶层做出分类</strong></p><p>3、计算机视觉的关键技术<br><strong>图像分类</strong>：给定一组各自被标记为单一类别的图像，对一组新的测试图像的类别进行预测，并测量预测的准确性结果。</p><p><strong>目标检测</strong>：给定一张图像，让计算机找出其中所有目标的位置，并给出每个目标的具体类别。</p><p><strong>语义分割</strong>：将整个图像分成像素组，然后对像素组进行标记和分类；语义分割是在语义上理解图中每个像素是什么，还须确定每个物体的边界。如一张“人驾驶摩托车行驶在林间小道上”的图片</p><p><strong>实例分割</strong>：在语义分割的基础上进行，将多个重叠物体和不同背景的复杂景象进行分类；同时确定对象的边界、差异和彼此之间的关系。</p><p><strong>视频分类</strong>：分类的对象是由多帧图像构成的、包含语音数据、运动信息等的视频对象需要理解每帧图像包含内容，还需要知道上下文关联信息。</p><p><strong>人体关键点检测</strong>：通过人体关键节点的组合和追踪来识别人的运动和行为对于描述人体姿态，预测人体行为至关重要。</p><p><strong>场景文字识别</strong>：在图像背景复杂、分辨率低下、字体多样、分布随意等情况下，将图像信息转化为文字序列的过程。</p><p><strong>目标跟踪任务</strong>：在特定场景跟踪某一个或多个特定感兴趣对象的过程。</p><p>三、图像分类基础</p><h4 id="1、图像分类的定义"><a href="#1、图像分类的定义" class="headerlink" title="1、图像分类的定义"></a>1、图像分类的定义</h4><p><code>图像分类的定义</code>：图像分类的核心是从给定的分类集合中给图像<strong>分配一个标签</strong>。 </p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/7873a95f36a0446aafe811ef71deaf8b.png" alt="在这里插入图片描述"></p><p>图像分类：<br>1、根据大类、小类加标签<br>2、可以多单个或多个标签<br>3、不同的标签粒度和个数会形成不同的分类任务</p><p>2、图像分类的类别<br>单标签与多标签分类的区别<br>单标签：数据样本<strong>属于一个大类</strong>的；数据进行分类后用可以用一个值代表；单标签内有二分类（两个选项）和多分类（多个选项）；例子：单标签三个样本的二分类整形（0&#x2F;1）输出为：[0,1,0]。</p><p>多标签数据样本可以<strong>划分到几个大的不冲突主题类别</strong>中；在大主题中分别可以进行二分类和多分类问题；例子：多标签(假设为两个标签)三个样本的二分类整形输出为：[[0,1], [0,0],[1,1]]。</p><p>跨物种语义级别的图像分类定义</p><p>在不同物种层次上识别不同类别的对象，如猫狗分类；<br>各个类别之间属于<strong>不同</strong>的物种或大类，往往具有 <strong>较大</strong>的类间方差，而<strong>类内</strong>具有 <strong>较小</strong>的类内方差；<br>多类别图像分类由传统的特征提取方法转到数据驱动的深度学习方向来，取得了较大进展。<br> 子类细粒度图像分类的定义</p><p>子类细粒度分类相较于跨物种图像分类难度更大；<br>是一个大类中的子类的分类，如不同鸟的分类等；<br>在区分出基本类别的基础上，进行更精细的子类划分；<br>由于图像之间具有更加相似的外观和特征，受采集过程中存在干扰影响，导致数据呈现类间差异性大，类内间差异小，分类难度也更高。<img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/9592d4d1d8f444ca8dcf3ac1bd58dcc5.png" alt="在这里插入图片描述"></p><p><strong>多标签图像分类的定义</strong></p><ul><li>给每个样本一系列的目标标签，表示的是样本各属性且不相互排斥的，预测出一个概念集合；</li><li>标签数量较大且复杂；</li><li>标签的标准<strong>很难统一</strong>，且往往类标之间相互依赖并不独立；</li><li>标注的标签并<strong>不能完美覆盖</strong>所有概念面；</li><li>标签往往较短语义少，理解困难。</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/e3c61b5dac6c4e80a2e02d907618f293.png" alt="在这里插入图片描述"></p><p>图像分类会遇到的问题<br>1、一张图片包含的<strong>信息内容太多</strong>，不好分类<br>2、有些类别的<strong>图片太少</strong>，比如罕见害虫</p><p>3、图像分类遇到的挑战<br>虽然图像分类在大赛上的正确率已经接近极限，但在实际工程应用中，面临诸多挑战。如：类别不均衡；数据集小；巨大的类内差异；实际应用环境复杂</p><p>4、图像分类的常用数据集与网络<br>图像分类的常用数据集：CIFAR-10</p><p>介绍</p><p>CIFAR-10 ：一个用于识别普适物体的<strong>小型</strong>图像数据集；<br>包含6万张大小为32 x 32的彩色图像；<br>共有10个类，每类有6000张图；<br>共5万张图组成训练集合，训练集合中每一类均等且有5000张图；<br>共1万张图组成测试集合，测试集合中每一类均等且有1000张图；<br>10个类别：飞机（ airplane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）；<br><strong>类</strong>是<strong>完全互斥</strong>的：在⼀个类别中出现的图⽚不会出现在其它类中。使用的相关神经网络：LeNet-5、AlexNet<br>LeNet-5：是<strong>最早的卷积神经网络</strong>之一; 1998年第一次将LeNet-5应用到图像分类上，在手写数字识别任务中取得了巨大成功; LeNet-5通过连续使用卷积和池化层的组合提取图像特征,总共5层：3层卷积和2层全连接，池化层未计入层数; LeNet-5是卷积神经网络的开篇大作，完成了卷积神经网络从无到有的突破。<br>AlexNet：AlexNet将LeNet的思想发扬光大，把CNN的基本原理应用到了很深很宽的网络中。成功使用ReLU作为CNN的激活函数，并验证其效果优异；训练时使用数据增强和Dropout<strong>随机忽略</strong>一部分神经元，以避免模型过拟合，提升泛化能力；在CNN中使用重叠的最大池化，提升了特征的丰富性；提出了LRN层，增强了模型的泛化能力。</p><p>5、图像分类的典型应用<br>图像分类在图片搜索引擎中的应用</p><p>应用图像分类技术可以开发各种图片搜索引擎；<br>图片搜索引擎能通过用户上传图片，应用图像分类技术，识别出图片的内容并进行分类；<br>搜索互联网上与这张图片相同或相似信息的其他图片资源进行校对和匹配，识别图片的内容并提供相关信息。<br> 图像分类在垃圾分类中的应用-智能环卫</p><p>为了破解传统分类投放模式可能存在的乱扔垃圾等问题，可在传统垃圾分类投放站点部署摄像头进行智能化改造；<br>阿里云提出“智能环卫”产品，提供垃圾分类投放点AI智能检测分析功能；<br>有效针对垃圾桶内的未破袋垃圾包、残余垃圾袋等进行检测和识别，检测效率高，真实环境下检测准确率超过95%。<br>四、目标检测基础<br>目标检测：框出来，用坐标表示</p><p>1、目标检测的定义<br>目标检测的定义：目标检测就是识别图中<strong>有哪些物体</strong>，确定他们的类别并标出各自在图中的位置。目标检测模型读取该图片；寻找识别出图中的物体目标，对其进行定位，框起和标注。</p><p>图像分类与目标检测的区别<br>图像分类：<strong>整幅图像</strong>经过识别后被分类为单一的标签。<br>目标检测：除了识别出图像中的<strong>一个或多个</strong>目标，还需要找出目标在图像中的<strong>具体位置</strong>。</p><p>2、目标检测的评估指标<br>交并比：IoU</p><p>真实边界框：训练集中，<strong>人工标注的</strong>物体边界框；<br>预测边界框：模型预测到的物体边界框；<br>交并比：在分子项中，是真实边界框和预测边界框<strong>重叠的区域</strong>（Intersection）。分母是一个并集（Union），或者更简单地说，是由预测边界框和真实边界框所包括的区域。两者相除就得到了最终的得分<br>精确度（Precision）指目标检测模型判断该图片为正类，该图片<strong>确实是正类的概率</strong>；</p><p>和召回率（Recall）是指的是一个分类器能把<strong>所有的正类都找出来</strong>的能力；</p><p>平均精度值：mAP ：mAP，mean Average Precision, 即<strong>各类别平均</strong>精度均值；mAP是把每个类别的AP都单独拿出来，然后计算所有类别AP的平均值，代表着对检测到的目标平均精度的一个综合评价。每一个类别都可以根据Recall和Precision绘制一条曲线，那么<strong>AP就是该曲线下的面积</strong>，而mAP则是多个类别AP的平均值，这个值介于0到1之间。mAP是目标检测算法里最重要的一个评估指标。</p><p>3、目标检测遇到的挑战<br>目标<strong>数</strong>量问题：在图片输入模型前不清楚图片中有多少个目标，无法知道正确的输出数量。<br>目标<strong>大小</strong>问题：目标的大小不一致,甚至一些目标仅有十几个像素大小，占原始图像中非常小的比例。<br>如何建模：需要同时处理目标定位以及目标物体识别分类这两个问题。</p><p>4、目标检测的常用数据集与网络<br>目标检测的常用数据集：PASCAL VOC</p><p>PASCAL VOC ：一个常用于目标检测的小型图像数据集；<br>包含11530张彩色图像，标定了27450个目标识别区域；<br>从初始4个类发展成最终的20个类；<br>在整个数据集中，平均每张图片有2.4个目标；<br>20个类别：<br>动物：人、鸟、猫、狗、牛、马、羊；<br>运载工具：飞机、自行车、船、巴士、汽车、摩托车、火车；<br>物品：瓶子、椅子、餐桌、盆栽、沙发、电视机。</p><p>使用的相关神经网络：CenterNet</p><p>CenterNet结构优雅简单，直接检测目标的中心点和大小;<br>CenterNet把目标检测任务看作三个部分：寻找物体的中心点；计算物体中心点的偏移量；分析物体的大小;<br>CenterNet检测速度和精度相比于先前的框架都有明显且可观的提高，尤其是与著名的目标检测网络YOLOv3作比较，在相同速度的条件下，CenterNet的精度比YOLOv3提高了大约4个点。<br>5、目标检测的典型应用<br>智慧交通是目标检测的一个重要应用领域，主要包括如下场景：</p><p>检测各种交通异常事件，如车辆占用应急车道、车辆驾驶员的驾驶行为等；<br>第一时间将异常事件上报给交管部门，提高处理效率。<br>目标检测在智慧交通中的应用-智慧眼</p><p>通过目标检测算法，对道路视频图像进行分析；<br>根据分析车流量，调整红绿灯配时策略，提升交通通行能力。<br> 五、图像分割基础<br>“抠图软件”的操作流程？</p><p>1、选中图片中的目标主体<br>2、对主体的边界进行分割<br>3、主体与背景分离，突出显示主体<br>1、图像分割的定义<br>图像分割就是把图像分成若干个特定的、具有独特性质的区域并提出感兴趣目标的技术和过程；</p><p>图像分割包括：语义分割、实例分割和全景分割。</p><p>图像作为分割算法的输入，输出一组区域；</p><p>区域可以表示为一种掩码（灰度或颜色），其中每个部分被分配一个唯一的<strong>颜色或灰度值</strong>来代表它</p><p>2、图像分割的类别<br>语义分割的定义：</p><p>语义分割是在<strong>像素级别上</strong>的分类，属于<strong>同一类的像素</strong>都要被归为一类；<br>语义分割是从像素级别来理解图像的。<br>实例分割的定义：</p><p>实例分割比语义分割更进一步；<br>对于语义分割来说，只要将所有同类别（猫、狗）的像素都归为一类；<br>实例分割还要在具体类别（猫、狗）像素的基础上<strong>区分开不同的实例</strong>（短毛猫、虎斑猫、贵宾犬、柯基犬）。<br>全景分割的定义</p><p>全景分割是<strong>语义和实例分割的相结合</strong>；<br>每个像素都被分配一个类（比如：狗），如果一个类有多个实例，则可知道该像素属于该类的哪个实例（贵宾犬&#x2F;柯基犬）。<br>3、图像分割遇到的挑战<br>分割<strong>边缘不准</strong>：因为相邻临的像素对应感受野内的图像信息太过相似导致。</p><p>样本<strong>质量不一</strong>：样本中的目标物体具有多姿态、多视角问题，会出现物体之间的遮挡和重叠；<br>受场景光照影响，样本质量参差不齐。</p><p>标注成本高：对于数据样本的标注成本非常高，而且标注质量难以保证不含有噪声。</p><p>4、图像分割的常用数据集与网络<br>图像分割的常用数据集：COCO</p><p>COCO：一个常用于图像分割的大型图像数据集；<br>包含33万张彩色图像，标定了50万个目标实例；<br>具有80个目标类、91个物品类以及25万个人物关键点标注；<br>每张图片包含5个描述；<br>每一类的图像多，利于提升识别更多类别位于特定场景的能力；<br>类别包括： person(人) 、bicycle(自行车) 、car(汽车) 、motorbike(摩托车) 、aeroplane(飞机) 、bus(公共汽车) 、train(火车)、truck(卡车) 、boat(船) 、traffic light(信号灯) 、fire hydrant(消防栓) 、stop sign(停车标志) 、parking meter(停车计费器) 、bench(长凳) 、bird(鸟) 、cat(猫) 、dog(狗) 、horse(马) 、sheep(羊) 、cow(牛) 等等。<br>使用的相关神经网络：FCN</p><p><strong>FCN全卷积神经网络</strong>是图像分割的基础网络;<br>全卷积神经网络，顾名思义网络里的所有层<strong>都是卷积层</strong>；<br>卷积神经网络卷到最后特征图尺寸和分辨率越来越小，不适合做图像分割，为解决此问题FCN引入- 上采样的方法，卷积完之后再上采样到大尺寸图；<br>为避免层数不断叠加后原图的信息丢失得比较多，FCN引入一个跳层结构，把前面的层特征引过来- 进行叠加；<br>FCN实现了端到端的网络<br>端到端学习是一种解决问题的思路，与之对应的是多步骤解决问题，也就是将一个问题拆分为多个步骤分步解决，而端到端是由输入端的数据直接得到输出端的结果。</p><p>5、图像分割的典型应用<br>图像分割在抠图软件中的应用</p><p>应用图像分割技术可以开发各种抠图软件；<br>用户在软件平台上传图片，应用图像分割技术，分辨出图片具有独特特征的区域并进行边缘识别分割；<br>返回给用户经过图像分割处理的结果图片。<br>图像分割在智能证件照中的应用</p><p>基于智能视觉生产的人像分割能力，阿里云为用户提供证件照的智能制作与编辑能力；<br>自动从上传的生活照中分割出人像区域，精确到像素级别的分割保证证件照的专业性与准确性，将生活照完美转换成专业证件照。</p><h1 id="NLP-自然语言处理-—-NLP入门指南"><a href="#NLP-自然语言处理-—-NLP入门指南" class="headerlink" title="[NLP] 自然语言处理 — NLP入门指南"></a>[NLP] 自然语言处理 — NLP入门指南</h1><p><strong>NLP的全称是Natuarl Language Processing</strong>，中文意思是自然语言处理，是人工智能领域的一个重要方向</p><p>自然语言处理（NLP）的一个最伟大的方面是跨越多个领域的计算研究，从人工智能到计算语言学的多个计算研究领域都在研究计算机与人类语言之间的相互作用。它主要关注计算机如何准确并快速地处理大量的自然语言语料库。什么是自然语言语料库？它是用现实世界语言表达的语言学习，是从文本和语言与另一种语言的关系中理解一组抽象规则的综合方法。</p><p>人类语言是抽象的信息符号，其中蕴含着丰富的语义信息，人类可以很轻松地理解其中的含义。而<strong>计算机只能处理数值化的信息</strong>，无法直接理解人类语言，所以需要将人类语言进行<strong>数值化转换</strong>。不仅如此，人类间的沟通交流是有上下文信息的，这对于计算机也是巨大的挑战。</p><p>我们首先来看看NLP的任务类型，如下图所示：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/158d23180094de856f74843b80f131a0.png" alt="img"></p><p> 主要划分为了四大类：</p><p>类别到序列<br>序列到类别<br>同步的序列到序列<br>异步的序列到序列<br>其中“类别”可以理解为是<strong>标签或者分类</strong>，而“序列”可以理解为是<strong>一段文本或者一个数组</strong>。简单概况NLP的任务就是从<strong>一种数据类型转换成另一种数据类型</strong>的过程，这与绝大多数的机器学习模型相同或者类似，所以掌握了NLP的<strong>技术栈</strong>就等于掌握了机器学习的技术栈。</p><p>传统方式和深度学习方式 NLP 对比</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1393176ebd38c6650f9d65f2f7685e65.png" alt="img"></p><h2 id="NLP的预处理"><a href="#NLP的预处理" class="headerlink" title="NLP的预处理"></a>NLP的预处理</h2><p>为了能够完成上述的NLP任务，我们需要一些预处理，是NLP任务的基本流程。预处理包括：收集语料库、文本清洗、分词、去掉停用词（可选）、标准化和特征提取等。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/f5471c37eaf151e4bb48c07e1935d5d2.png" alt="img"></p><p>图中红色的部分就是NLP任务的预处理流程，有别于其它机器学习任务的流程</p><p><strong>英文 NLP 语料预处理的 6 个步骤</strong></p><ol><li><a href="https://easyai.tech/ai-definition/tokenization/">分词 – Tokenization</a></li><li><a href="https://easyai.tech/ai-definition/stemming-lemmatisation/">词干提取</a> – <a href="https://easyai.tech/ai-definition/stemming-lemmatisation/">Stemming</a></li><li><a href="https://easyai.tech/ai-definition/stemming-lemmatisation/">词形还原</a> – Lemmatization</li><li><a href="https://easyai.tech/ai-definition/part-of-speech/">词性标注 – Parts of Speech</a></li><li><a href="https://easyai.tech/ai-definition/ner/">命名实体识别 – NER</a></li><li>分块 – Chunking</li></ol><p><strong>中文 NLP 语料预处理的 4 个步骤</strong></p><ol><li><a href="https://easyai.tech/ai-definition/tokenization/">中文分词 – Chinese Word Segmentation</a></li><li><a href="https://easyai.tech/ai-definition/part-of-speech/">词性标注 – Parts of Speech</a></li><li><a href="https://easyai.tech/ai-definition/ner/">命名实体识别 – NER</a></li><li>去除停用词</li></ol><h2 id="第1步：收集您的数据—语料库"><a href="#第1步：收集您的数据—语料库" class="headerlink" title="第1步：收集您的数据—语料库"></a>第1步：收集您的数据—语料库</h2><p>对于NLP任务来说，没有大量高质量的语料，就是巧妇难为无米之炊，是无法工作的。</p><p>而获取语料的途径有很多种，最常见的方式就是<strong>直接下载开源</strong>的语料库，如：维基百科的语料库。</p><p>但这样<strong>开源</strong>的语料库一般都<strong>无法满足业务的个性化</strong>需要，所以就需要<strong>自己动手开发爬虫</strong>去<strong>抓取特定</strong>的内容，这也是一种获取语料库的途径。当然，每家互联网公司根据自身的业务，也都会有大量的语料数据，如：用户评论、电子书、商品描述等等，都是很好的语料库。</p><p>示例数据源</p><p>每个机器学习问题都从数据开始，例如电子邮件，帖子或推文列表。常见的文字信息来源包括：</p><p>产品评论（在亚马逊，Yelp和各种应用商店）<br>用户生成的内容（推文，Facebook帖子，StackOverflow问题）<br>故障排除（客户请求，支持服务单，聊天记录）<br>现在，数据对于互联网公司来说就是<strong>石油</strong>，其中蕴含着巨大的商业价值。所以，小伙伴们在日常工作中一定要养成收集数据的习惯，遇到<strong>好的语料库一定要记得备份</strong>（当然是在合理合法的条件下），它将会对你解决问题提供巨大的帮助。</p><p>第2步：清理数据 — 文本清洗<br>我们遵循的首要规则是：“您的模型将永远与您的数据一样好。”</p><p>数据科学家的关键技能之一是了解<strong>下一步是应该对模型还是数据</strong>进行处理。一个好的经验法则是<strong>首先查看数据然后进行清理</strong>。一个<strong>干净的数据集</strong>将允许模型学习有意义的功能，而不是过度匹配无关的噪音。</p><p>我们通过不同的途径获取到了想要的语料库之后，接下来就需要对其进行清洗。因为很多的语料数据是无法直接使用的，其中包含了大量的无用符号、特殊的文本结构。</p><p>数据类型分为：</p><p><strong>结构化</strong>数据：关系型数据、json等<br><strong>半结构化</strong>数据：XML、HTML等<br><strong>非结构化****数据：Word、PDF、文本、日志等<br>需要将原始的语料数据</strong>转化成易于处理<strong>的格式，一般在处理HTML、XML时，会使用Python的</strong>lxml库**，功能非常丰富且易于使用。对一些日志或者纯文本的数据，我们可以使用<strong>正则表达式</strong>进行处理。</p><p>正则表达式是使用单个字符串来描述、匹配一系列符合某个句法规则的字符串。Python的示例代码如下：</p> <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment"># 定义中文字符的正则表达式</span></span><br><span class="line">re_han_default = re.<span class="built_in">compile</span>(<span class="string">&quot;([\u4E00-\u9FD5]+)&quot;</span>, re.U)</span><br><span class="line">sentence = <span class="string">&quot;我/爱/自/然/语/言/处/理&quot;</span></span><br><span class="line"><span class="comment"># 根据正则表达式进行切分</span></span><br><span class="line">blocks= re_han_default.split(sentence)</span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> blocks:</span><br><span class="line">    <span class="comment"># 校验单个字符是否符合正则表达式</span></span><br><span class="line">    <span class="keyword">if</span> blk <span class="keyword">and</span> re_han_default.<span class="keyword">match</span>(blk):</span><br><span class="line">        <span class="built_in">print</span>(blk)</span><br></pre></td></tr></table></figure><p>除了上述的内容之外，我们还需要注意中文的编码问题，在windows平台下中文的默认编码是<strong>GBK（gb2312）</strong>，而在linux平台下中文的默认编码是<strong>UTF-8</strong>。在执行NLP任务之前，我们需要统一不同来源语料的编码，避免各种莫名其妙的问题。</p><p>如果大家事前无法判断语料的编码，那么我推荐大家可以使用Python的<strong>chardet库来检测编码</strong>，简单易用。既支持命令行：chardetect somefile，也支持代码开发。</p><p>以下是用于清理数据的清单:</p><p><strong>删除所有不相关</strong>的字符，例如任何非字母数字字符<br>令牌化通过将其<strong>分割</strong>成单个的单词文本<br>删除不相关的单词，例如“@”twitter提及或网址<br>将所有字符<strong>转换为小写</strong>，以便将诸如“hello”，“Hello”和“HELLO”之类的单词视为相同<br>考虑将拼写错误或交替拼写的单词<strong>组合成单个表示</strong>（例如“cool”&#x2F;“kewl”&#x2F;“cooool”）<br>考虑<strong>词开还原</strong>（将诸如“am”，“are”和“is”之类的词语简化为诸如“be”之类的常见形式）<br>按照这些步骤并检查其他错误后，我们可以开始使用干净的标记数据来训练模型！</p><p>第3步：分词<br>中英文分词的3个典型区别</p><p> 区别1：分词方式不同，中文更难</p><p>英文有<strong>天然的空格</strong>作为分隔符，但是<strong>中文没有</strong>。所以如何切分是一个难点，再加上中文里一词多意的情况非常多，导致很容易出现歧义。下文中难点部分会详细说明。</p><p>区别2：英文单词有多种形态</p><p>英文单词存在丰富的变形变换。为了应对这些复杂的变换，英文NLP相比中文存在一些独特的处理步骤，我们称为<strong>词形还原（Lemmatization）<strong>和</strong>词干提取(Stemming)</strong>。中文则不需要</p><p>词性还原：does，done，doing，did 需要通过词性还原恢复成 do。</p><p>词干提取：cities，children，teeth 这些词，需要转换为 city，child，tooth”这些基本形态</p><p>区别3：中文分词需要考虑<strong>粒度问题</strong></p><p>例如「中国科学技术大学」就有很<strong>多种分法</strong>：</p><p>中国科学技术大学<br>中国 \ 科学技术 \ 大学<br>中国 \ 科学 \ 技术 \ 大学<br>粒度越大，表达的<strong>意思就越准确</strong>，但是也会导致<strong>召回比较少</strong>。所以中文需要不同的场景和要求选择不同的粒度。这个在英文中是没有的。</p><p>中文分词是一个比较大的课题，相关的知识点和技术栈非常丰富，可以说搞懂了中文分词就等于搞懂了大半个NLP。</p><p>中文分词的3大难点<br> 难点 1：<strong>没有统一的标准</strong></p><p>目前中文分词没有统一的标准，也没有公认的规范。不同的公司和组织各有各的方法和规则。</p><p>难点 2：<strong>歧义词如何切分</strong></p><p>例如「兵乓球拍卖完了」就有2种分词方式表达了2种不同的含义：</p><p>乒乓球 \ 拍卖 \ 完了<br>乒乓 \ 球拍 \ 卖 \ 完了<br>难点 3：<strong>新词的识别</strong></p><p>信息爆炸的时代，三天两头就会冒出来一堆新词，如何快速的识别出这些新词是一大难点。比如当年「蓝瘦香菇」大火，就需要快速识别。</p><p>中文分词经历了20多年的发展，克服了重重困难，取得了巨大的进步，大体可以划分成两个阶段，如下图所示：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/d5e1e3b77629124b9a5aff5cc0c0955f.png" alt="img"></p><p><strong>词典</strong>匹配与规则</p><p>优点：<strong>速度</strong>快、成本低</p><p>缺点：适应性不强，不同领域效果差异大</p><p>基本思想是基于词典匹配，将待分词的中文文本根据一定规则切分和调整，然后跟词典中的词语进行匹配，匹配成功则按照词典的词分词，匹配失败通过调整或者重新选择，如此反复循环即可。代表方法有基于正向最大匹配和基于逆向最大匹配及双向匹配法。</p><p>基于统计与机器学习</p><p>优点：<strong>适应性</strong>较强</p><p>缺点：成本较高，速度较慢</p><p>这类目前常用的是算法是HMM、CRF等算法，比如stanford、Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。</p><p>常见的分词器都是使用<strong>机器学习算法和词典相结合</strong>，一方面能够提高分词准确率，另一方面能够改善领域适应性。</p><p>目前，主流的中文分词技术采用的都是基于词典最大概率路径+未登录词识别（HMM）的方案，其中典型的代表就是<strong>jieba分词</strong>，一个热门的多语言中文分词包。</p><p>中文分词工具</p><p>下面排名根据 GitHub 上的 star 数排名：</p><p>Hanlp<br>Stanford 分词<br>ansj 分词器<br>哈工大 LTP<br>KCWS分词器<br>jieba<br>IK<br>清华大学THULAC<br>ICTCLAS<br>英文分词工具</p><p>Keras<br>Spacy<br>Gensim<br>NLTK<br>第4步：标准化<br>标准化是为了给后续的处理提供一些<strong>必要的基础数据</strong>，包括：去掉停用词、词汇表、训练数据等等。</p><p>当我们完成了分词之后，可以去掉停用词，如：“其中”、“况且”、“什么”等等，但这一步不是必须的，要根据实际业务进行选择，像关键词挖掘就需要去掉停用词，而像训练词向量就不需要。</p><p>词汇表是为语料库建立一个<strong>所有不重复词的列表</strong>，每个词对应一个<strong>索引值</strong>，并索引值不可以改变。词汇表的最大作用就是可以<strong>将词转化成一个向量</strong>，即One-Hot编码。</p><p>假设我们有这样一个词汇表：</p><p>我<br>爱<br>自然<br>语言<br>处理<br>那么，我们就可以得到如下的One-Hot编码：</p><p>我：  [1, 0, 0, 0, 0]<br>爱：  [0, 1, 0, 0, 0]<br>自然：[0, 0, 1, 0, 0]<br>语言：[0, 0, 0, 1, 0]<br>处理：[0, 0, 0, 0, 1]<br>这样我们就可以简单的将词转化成了计算机可以直接处理的数值化数据了。虽然One-Hot编码可以较好的完成部分NLP任务，但它的问题还是不少的。</p><p>当词汇表的维度特别大的时候，就会导致经过One-Hot编码后的词向量非常稀疏，同时One-Hot编码也缺少词的语义信息。由于这些问题，才有了后面大名鼎鼎的Word2vec，以及Word2vec的升级版BERT。</p><p>除了词汇表之外，我们在训练模型时，还需要提供训练数据。模型的学习可以大体分为两类：</p><p><strong>监督</strong>学习，在已知答案的标注数据集上，模型给出的预测结果<strong>尽可能接近真实</strong>答案，适合<strong>预测</strong>任务<br><strong>非监督</strong>学习，学习没有标注的数据，是要<strong>揭示</strong>关于数据隐藏结构的一些规律，适合<strong>描述</strong>任务<br>根据不同的学习任务，我们需要提供不同的标准化数据。一般情况下，标注数据的获取成本非常昂贵，非监督学习虽然不需要花费这样的成本，但在实际问题的解决上，主流的方式还<strong>选择监督学习，因为效果更好</strong>。</p><p>带标注的训练数据大概如下所示（情感分析的训练数据）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">距离 川沙 公路 较近 公交 指示 蔡陆线 麻烦 建议 路线 房间 较为简单__label__1</span><br><span class="line">商务 大床 房 房间 很大 床有 2M 宽 整体 感觉 经济 实惠 不错 !__label__1</span><br><span class="line">半夜 没 暖气 住 ! __label__0</span><br></pre></td></tr></table></figure><p>其中每一行就是一条训练样本，<code>__label__0</code>和<code>__label__1</code>是分类信息，其余的部分就是分词后的文本数据。</p><p>第5步：特征提取<br>为了能够更好的训练模型，我们需要将文本的原始特征<strong>转化成具体</strong>特征，转化的方式主要有两种：统计和Embedding。</p><p>原始特征：需要人类或者机器进行转化，如：文本、图像。</p><p>具体特征：已经被人类进行整理和分析，可以直接使用，如：物体的重要、大小。</p><p>NLP表示方式<br>目前常用的文本表示方式分为：</p><p><strong>离散</strong>式表示（Discrete Representation）；<br><strong>分布</strong>式表示（Distributed Representation）；</p><h2 id="离散式表示（Discrete-Representation）"><a href="#离散式表示（Discrete-Representation）" class="headerlink" title="离散式表示（Discrete Representation）"></a>离散式表示（Discrete Representation）</h2><p>One-Hot<br>One-Hot 编码又称为“独热编码”或“哑编码”，是最传统、最基础的词（或字）特征表示方法。这种编码将词（或字）<strong>表示成一个向量</strong>，该向量的<strong>维度是词典（或字典）的长度</strong>（该词典是通过语料库生成的），该向量中，<strong>当前词的位置的值为1</strong>，其余的位置为0。</p><p>文本使用one-hot 编码步骤：</p><p>根据<strong>语料库创建 词典</strong>（vocabulary），并创建词和索引的 <strong>映射</strong>（stoi，itos)；<br>将句子转换为用<strong>索引表示</strong>；<br>创建OneHot 编码器；<br>使用OneHot 编码器对句子<strong>进行编码</strong>；<br>One-Hot 编码的特点如下：</p><p>词向量长度是词典长度；<br>在向量中，该单词的索引位置的值为  1 ，其余的值都是  0<br>使用One-Hot 进行编码的文本，得到的矩阵是<strong>稀疏矩阵</strong><br>缺点：</p><p>不同词的向量表示互相正交，<strong>无法衡量</strong>不同词之间的<strong>关系</strong>；<br>该编码只能反映某个词<strong>是否在句中出现</strong>，无法衡量不同词的<strong>重要程度</strong>；<br>使用One-Hot 对文本进行编码后得到的是<strong>高维稀疏</strong>矩阵，会<strong>浪费计算和存储资源</strong>；<br>词袋模型（Bag Of Word，BOW）<br>例句：</p><p>Jane wants to go to Shenzhen.<br>Bob wants to go to Shanghai.<br>在词袋模型中不考虑语序和词法的信息，每个单词都是<strong>相互独立</strong>的，将词语放入一个“袋子”里，统计每个单词出现的频率。</p><p>词袋模型编码特点：</p><p>词袋模型是<strong>对文本</strong>（而不是字或词）进行编码；<br>编码后的向量长度是<strong>词典的长度</strong>；<br>该编码<strong>忽略词出现的次序</strong>；<br>在向量中，该单词的<strong>索引位置的值</strong>为单词在文本中出现的<strong>次数</strong>；如果索引位置的单词没有在文本中出现，则该值为  0 ；<br>缺点</p><p>该编码<strong>忽略词的位置信息</strong>，位置信息在文本中是一个很重要信息，词的位置不一样语义会有很大的差别（如 “猫爱吃老鼠” 和 “老鼠爱吃猫” 的编码一样）；<br>该编码方式虽然统计了词在文本中出现的次数，但仅仅通过“出现次数”这个属性<strong>无法区分常用词</strong>（如：“我”、“是”、“的”等）和<strong>关键词</strong>（如：“自然语言处理”、“NLP ”等）在文本中的<strong>重要程度</strong>；<br>TF-IDF（词频-逆文档频率）<br>为了解决词袋模型无法区分常用词（如：“是”、“的”等）和专有名词（如：“自然语言处理”、“NLP ”等）对文本的重要性的问题，TF-IDF 算法应运而生。</p><p>TF-IDF 全称是：term frequency–inverse document frequency 又称 词频-逆文本频率。其中：</p><p>统计的方式主要是计算词的词频（TF）和逆向文件频率（IDF）：</p><p>TF （Term Frequency ）：某个词<strong>在当前文本中</strong>出现的频率，频率高的词语或者是重要的词（如：“自然语言处理”）或者是常用词（如：“我”、“是”、“的”等）；<br>IDF （Inverse Document frequency ）：逆文本频率。文本频率是指：含有某个词的文本<strong>在整个语料库中</strong>所占的比例。逆文本频率是<strong>文本频率的倒数</strong>；<br>那么，每个词都会得到一个TF-IDF值，用来<strong>衡量它的重要程度</strong>，计算公式如下：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4b52b3a5ad29985b686e084588e842fb.png" alt="img"></p><p>优点</p><p>实现简单，算法容易理解且解释性较强；<br>从IDF 的计算方法可以看出常用词（如：“我”、“是”、“的”等）在<strong>语料库中</strong>的很多文章都会出现，故<strong>IDF的值会很小</strong>；而关键词（如：“自然语言处理”、“NLP ”等）只会在<strong>某领域</strong>的文章出现，IDF 的值会<strong>比较大</strong>；故：TF-IDF 在保留文章的重要词的同时可以过滤掉一些常见的、无关紧要的词；<br>缺点</p><p><strong>不能反映词的位置信息</strong>，在对关键词进行提取时，词的位置信息（如：标题、句首、句尾的词应该赋予更高的权重）；<br>IDF 是一种试图抑制噪声的加权，本身倾向于文本中频率比较小的词，这使得IDF 的精度不高；<br>TF-IDF <strong>严重依赖于语料库</strong>（尤其在训练同类语料库时，往往会掩盖一些同类型的关键词；如：在进行TF-IDF 训练时，语料库中的 娱乐 新闻较多，则与 娱乐 相关的关键词的权重就会偏低 ），因此需要选取质量高的语料库进行训练；</p><h2 id="分布式表示（Distributed-Representation"><a href="#分布式表示（Distributed-Representation" class="headerlink" title="分布式表示（Distributed Representation"></a>分布式表示（Distributed Representation</h2><p>理论基础：</p><p>1954年，Harris提出分布式假说（distributional hypothesis）奠定了这种方法的理论基础：A word’s meaning is given by the words that frequently appear close-by（上下文相似的词，其语义也相似）；<br>1957年，Firth对分布式假说做出进一步的阐述和明确：A word is characterized by the company it keeps（词的语义<strong>由其上下文决定</strong>）；<br>n-gram<br>n-gram 是一种 语言模型(Language Model, LM)。语言模型是一种基于概率的判别式模型，该模型的输入是一句话（单词的序列），输出的是这句话的概率，也就是这些单词的<strong>联合概率</strong>（joint probability）。（备注：语言模型就是判断一句话是不是正常人说的。）</p><p>共现矩阵（Co-Occurrence Matrix）<br>首先指定窗口大小，然后统计窗口（和对称窗口）内词语<strong>共同出现</strong>的<strong>次数</strong>作为词的向量（vector）。</p><p>语料库：</p><p>I like deep learning.<br>I like NLP.<br>I enjoy flying.<br>备注： 指定窗口大小为1（即：左右的 window_length&#x3D;1，相当于 bi-gram）统计数据如下：（I, like），（Iike, deep），（deep, learning），（learning, .），（I, like），（like, NLP），（NLP, .），（I, enjoy），（enjoy, flying）， （flying, .）。则语料库的共现矩阵如下表所示：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/8b07f13487955c6279acbecd802fbcf5.png" alt="img"></p><p>从以上的共现矩阵可以看出，单词  like 和  enjoy 都在单词  I 附件出现且统计数目大概相等，则它们在 <strong>语义 和 语法 上的含义大概相同</strong>。</p><p>优点</p><p>考虑了句子中<strong>词的顺序</strong>；<br>缺点</p><p>词表的长度很大，导致词的向量长度也很大；<br>共现矩阵也是稀疏矩阵（可以使用 SVD、PCA 等算法进行降维，但是计算量很大）；<br>Word2Vec<br>word2vec 模型是Google团队在2013年发布的 word representation 方法。该方法一出让 预训练词向量 的使用在NLP 领域遍地开花。</p><p>word2vec模型</p><p>word2vec有两种模型：CBOW 和 SKIP-GRAM；</p><ul><li>CBOW：利用上下文的词预测中心词；</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6a9e694f893c53f1d031e02b03a96dcb.png" alt="img"></p><ul><li>SKIP-GRAM：利用中心词预测上下文的词；</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/a9321a80eaf4aaf89f60ee6c94b3598e.png" alt="img"></p><p><strong>优点</strong></p><ol><li>考虑到词语的<strong>上下文</strong>，学习到了语义和语法的信息；</li><li>得到的词向量<strong>维度小</strong>，节省存储和计算资源；</li><li><strong>通用性</strong>强，可以应用到各种<em>NLP</em> 任务中；</li></ol><p><strong>缺点</strong></p><ol><li>词和向量是一对一的关系，无法解决多义词的问题；</li><li>word2vec是一种<strong>静态</strong>的模型，虽然通用性强，但无法真的特定的任务做动态优化；</li></ol><p>GloVe<br>GloVe 是斯坦福大学Jeffrey、Richard 等提供的一种词向量表示算法，GloVe 的全称是Global Vectors for Word Representation，是一个基于全局词频统计（count-based &amp; overall staticstics）的词表征（word representation）算法。该算法综合了global matrix factorization（全局矩阵分解） 和 local context window（局部上下文窗口） 两种方法的优点。</p><p>效果</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/89c700f1bbd4be29476d9139b05a9679.png" alt="img"></p><p>优点</p><p>考虑到词语的<strong>上下文</strong>、和全局语料库的信息，学习到了语义和语法的信息；<br>得到的词向量<strong>维度小</strong>，节省存储和计算资源；<br>通用性强，可以应用到各种NLP 任务中；<br>缺点</p><p>词和向量是一对一的关系，无法解决多义词的问题；<br>glove也是一种静态的模型，虽然通用性强，但无法真的特定的任务做动态优化；<br>ELMO<br>word2vec 和 glove 算法得到的词向量都是静态词向量（静态词向量会把多义词的语义进行融合，训练结束之后不会根据上下文进行改变），静态词向量无法解决多义词的问题（如：“我今天买了7斤苹果” 和 “我今天买了苹果7” 中的 苹果 就是一个多义词）。而ELMO模型进行训练的词向量可以<strong>解决多义词</strong>的问题。</p><p>ELMO 的全称是“ Embedding from Language Models ”，这个名字不能很好的反映出该模型的特点，提出ELMO 的论文题目可以更准确的表达出该算法的特点“ Deep contextualized word representation ”。</p><p>该算法的精髓是：<strong>用语言模型训练神经网络</strong>，在使用word embedding 时，单词已经具备上下文信息，这个时候神经网络可以根据上下文信息对word embedding 进行调整，这样经过调整之后的word embedding 更能表达在这个上下文中的具体含义，这就解决了静态词向量无法表示多义词的问题。</p><p>网络模型</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/b20fb10062f8b5e18f8e7ea973e17b6d.png" alt="img"></p><p>过程</p><p>上图中的结构使用<strong>字符级卷积神经网络（convolutional neural network, CNN）<strong>来将文本中的词转换成</strong>原始词向量（raw word vector）</strong> ；<br>将原始词向量输入双向语言模型中第一层 ；<br><strong>前向迭代</strong>中包含了该词以及该词之前的一些词汇或语境的信息（即<strong>上文</strong>）；<br><strong>后向迭代</strong>中包含了该词以及该词之后的一些词汇或语境的信息（即<strong>下文</strong>） ；<br>这两种迭代的信息组成了<strong>中间词向量（intermediate word vector）</strong>；<br>中间词向量被输入到模型的<strong>下一层</strong> ；<br>最终向量就是原始词向量和两个中间词向量的<strong>加权和</strong>；</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/0b2bd5a91dfe856b390172a2589311ac.png" alt="img"></p><p>如上图所示：</p><ul><li>使用glove训练的词向量中，与 play 相近的词大多与<strong>体育相关</strong>，这是因为语料中与play相关的语料多时体育领域的有关；</li><li>在使用elmo训练的词向量中，当 play 取 <strong>演出</strong> 的意思时，与其相近的也是 演出 相近的句子</li></ul><h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-e1576624094357.webp" alt="img"></p><p>PyTorch 是一个开源的机器学习库，主要用于进行<strong>计算机视觉（CV）</strong>、<strong>自然语言处理（NLP）</strong>、语音识别等领域的研究和开发。</p><p>PyTorch 以其灵活性和易用性而闻名，特别适合于<strong>深度学习</strong>研究和开发。</p><h2 id="谁适合阅读本教程？"><a href="#谁适合阅读本教程？" class="headerlink" title="谁适合阅读本教程？"></a>谁适合阅读本教程？</h2><p>只要您具备编程的基础知识，您就可以阅读本教程，学习 PyTorch 适合对深度学习和机器学习感兴趣的人，包括数据科学家、工程师、研究人员和学生。</p><h2 id="阅读本教程前，您需要了解的知识："><a href="#阅读本教程前，您需要了解的知识：" class="headerlink" title="阅读本教程前，您需要了解的知识："></a>阅读本教程前，您需要了解的知识：</h2><p>在您开始阅读本教程之前，您必须具备的基础知识包括 Python 编程、基础数学（线性代数、概率论、微积分）、机器学习的基本概念、神经网络知识，以及一定的英语阅读能力来查阅文档和资料。</p><ul><li><strong>编程基础</strong>：熟悉至少一种编程语言，尤其是 <a href="https://www.runoob.com/python3/python3-tutorial.html">Python</a>，因为 PyTorch 主要是用 Python 编写的。</li><li><strong>数学基础</strong>：了解线性代数、概率论和统计学、微积分等基础数学知识，这些是理解和实现机器学习算法的基石。</li><li><strong>机器学习基础</strong>：了解机器学习的基本概念，如监督学习、无监督学习、强化学习、模型评估指标（准确率、召回率、F1分数等）。</li><li><strong>深度学习基础</strong>：熟悉神经网络的基本概念，包括前馈神经网络、卷积神经网络（CNN）、循环神经网络（RNN）、长短期记忆网络（LSTM）等。</li><li><strong>计算机视觉和自然语言处理基础</strong>：如果你打算在这些领域应用 PyTorch，了解相关的背景知识会很有帮助。</li><li><strong>Linux&#x2F;Unix 基础</strong>：虽然不是必需的，但了解 Linux&#x2F;Unix 操作系统的基础知识可以帮助你更有效地使用命令行工具和脚本，特别是在数据预处理和模型训练中。</li><li><strong>英语阅读能力</strong>：由于许多文档、教程和社区讨论都是用英语进行的，具备一定的英语阅读能力将有助于你更好地学习和解决问题。</li></ul><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>下面的是 PyTorch 中一些<strong>基本的张量操作</strong>：如何创建随机张量、进行逐元素运算、访问特定元素以及计算总和和最大值。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 设置数据类型和设备</span></span><br><span class="line">dtype = torch.<span class="built_in">float</span>  <span class="comment"># 张量数据类型为浮点型</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)  <span class="comment"># 本次计算在 CPU 上进行</span></span><br><span class="line"><span class="comment"># 创建并打印两个随机张量 a 和 b</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">3</span>, device=device, dtype=dtype)  <span class="comment"># 创建一个 2x3 的随机张量</span></span><br><span class="line">b = torch.randn(<span class="number">2</span>, <span class="number">3</span>, device=device, dtype=dtype)  <span class="comment"># 创建另一个 2x3 的随机张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 a:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 b:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># 逐元素相乘并输出结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;a 和 b 的逐元素乘积:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a * b)</span><br><span class="line"><span class="comment"># 输出张量 a 所有元素的总和</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 a 所有元素的总和:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># 输出张量 a 中第 2 行第 3 列的元素（注意索引从 0 开始）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 a 第 2 行第 3 列的元素:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 输出张量 a 中的最大值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 a 中的最大值:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">max</span>())</span><br></pre></td></tr></table></figure><p><strong>创建张量：</strong></p><ul><li><code>torch.randn(2, 3)</code> 创建一个 2 行 3 列的张量，<strong>填充随机数</strong>（遵循<strong>正态分布</strong>）。</li><li><code>device=device</code> 和 <code>dtype=dtype</code> 分别指定了<strong>计算设备</strong>（CPU 或 GPU）和<strong>数据类型</strong>（浮点型）。</li></ul><p><strong>张量操作：</strong></p><ul><li><code>a * b</code>：<strong>逐</strong>元素相乘。</li><li><code>a.sum()</code>：计算张量 <code>a</code> 所有元素的和。</li><li><code>a[1, 2]</code>：访问张量 <code>a</code> 第 2 行第 3 列的元素（注意索引从 0 开始）。</li><li><code>a.max()</code>：获取张量 <code>a</code> 中的最大值。</li></ul><p>输出：（<strong>每次</strong>运行时值会有所<strong>不同</strong>）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">张量 a:</span><br><span class="line">tensor([[-0.1460, -0.3490,  0.3705],</span><br><span class="line">        [-1.1141,  0.7661,  1.0823]])</span><br><span class="line">张量 b:</span><br><span class="line">tensor([[ 0.6901, -0.9663,  0.3634],</span><br><span class="line">        [-0.6538, -0.3728, -1.1323]])</span><br><span class="line">a 和 b 的逐元素乘积:</span><br><span class="line">tensor([[-0.1007,  0.3372,  0.1346],</span><br><span class="line">        [ 0.7284, -0.2856, -1.2256]])</span><br><span class="line">张量 a 所有元素的总和:</span><br><span class="line">tensor(0.6097)</span><br><span class="line">张量 a 第 2 行第 3 列的元素:</span><br><span class="line">tensor(1.0823)</span><br><span class="line">张量 a 中的最大值:</span><br><span class="line">tensor(1.0823)</span><br></pre></td></tr></table></figure><h1 id="PyTorch-简介"><a href="#PyTorch-简介" class="headerlink" title="PyTorch 简介"></a>PyTorch 简介</h1><p>PyTorch 是一个开源的 Python 机器学习库，<strong>基于 Torch 库</strong>，**底层由C++**实现，应用于人工智能领域，如计算机视觉和自然语言处理。</p><p>PyTorch 最初由 Meta Platforms 的人工智能研究团队开发，现在属 于Linux 基金会的一部分。</p><p>许多深度学习软件都是基于 PyTorch 构建的，包括特斯拉自动驾驶、Uber 的 Pyro、<strong>Hugging Face 的 Transformers</strong>、 PyTorch Lightning 和 Catalyst。</p><p><strong>PyTorch 主要有两大特征：</strong></p><ul><li>类似于 NumPy 的<strong>张量计算</strong>，能在 GPU 或 MPS 等硬件加速器上加速。</li><li>基于带自动微分系统的<strong>深度神经网络</strong>。</li></ul><p>PyTorch 包括 torch.autograd、torch.nn、torch.optim 等子模块。</p><p>PyTorch 包含多种损失函数，包括 MSE（均方误差 &#x3D; L2 范数）、交叉熵损失和负熵似然损失（对分类器有用）等。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1567769062953.png" alt="img"></p><h2 id="PyTorch-特性"><a href="#PyTorch-特性" class="headerlink" title="PyTorch 特性"></a>PyTorch 特性</h2><ul><li><strong>动态计算图（Dynamic Computation Graphs）</strong>： PyTorch 的计算图是动态的，这意味着它们<strong>在运行时构建</strong>，并且<strong>可以随时改变</strong>。这为实验和调试提供了极大的灵活性，因为开发者可以逐行执行代码，查看中间结果。</li><li><strong>自动微分（Automatic Differentiation）</strong>： PyTorch 的自动微分系统允许开发者轻松地<strong>计算梯度</strong>，这对于训练深度学习模型至关重要。它通过反向传播算法自动计算出<strong>损失函数对模型参数的梯度</strong>。</li><li><strong>张量计算（Tensor Computation）</strong>： PyTorch 提供了类似于 NumPy 的张量操作，这些操作可以在 CPU 和 GPU 上执行，从而<strong>加速</strong>计算过程。张量是 PyTorch 中的<strong>基本数据结构</strong>，用于存储和操作数据。</li><li><strong>丰富的 API</strong>： PyTorch 提供了大量的<strong>预定义层</strong>、<strong>损失函数</strong>和<strong>优化算法</strong>，这些都是构建深度学习模型的常用组件。</li><li><strong>多语言支持</strong>： PyTorch 虽然以 Python 为主要接口，但也提供了 C++ 接口，<strong>允许更底层</strong>的集成和控制。</li></ul><h3 id="动态计算图（Dynamic-Computation-Graph）"><a href="#动态计算图（Dynamic-Computation-Graph）" class="headerlink" title="动态计算图（Dynamic Computation Graph）"></a>动态计算图（Dynamic Computation Graph）</h3><p>PyTorch 最显著的特点之一是其<strong>动态计算图</strong>的机制。</p><p>与 TensorFlow 的静态计算图（graph）不同，PyTorch 在执行时构建计算图，这意味着在每次计算时，图都会根据输入数据的形状自动变化。</p><p><strong>动态计算图的优点：</strong></p><ul><li>更加<strong>灵活</strong>，特别适用于需要条件判断或递归的场景。</li><li>方便调试和修改，能够直接查看中间结果。</li><li>更接近 Python 编程的风格，易于上手。</li></ul><h3 id="张量（Tensor）与自动求导（Autograd）"><a href="#张量（Tensor）与自动求导（Autograd）" class="headerlink" title="张量（Tensor）与自动求导（Autograd）"></a>张量（Tensor）与自动求导（Autograd）</h3><p>PyTorch 中的核心数据结构是 <strong>张量（Tensor）</strong>，它是一个<strong>多维矩阵</strong>，可以在 CPU 或 GPU 上高效地进行计算。张量的操作**支持自动求导（Autograd）**机制，使得在反向传播过程中自动计算梯度，这对于深度学习中的梯度下降优化算法至关重要。</p><p><strong>张量（Tensor）：</strong></p><ul><li>支持在 <strong>CPU 和 GPU 之间</strong>进行切换。</li><li>提供了类似 NumPy 的接口，支持元素级运算。</li><li>支持自动求导，可以方便地进行梯度计算。</li></ul><p><strong>自动求导（Autograd）：</strong></p><ul><li>PyTorch 内置的自动求导引擎，能够<strong>自动追踪</strong>所有张量的操作，并在反向传播时计算梯度。</li><li>通过 <code>requires_grad</code> 属性，<strong>可以指定</strong>张量需要计算梯度。</li><li>支持高效的反向传播，适用于神经网络的训练。</li></ul><h3 id="模型定义与训练"><a href="#模型定义与训练" class="headerlink" title="模型定义与训练"></a>模型定义与训练</h3><p>PyTorch 提供了 <code>torch.nn</code> 模块，允许用户通过继承 <code>nn.Module</code> 类来定义神经网络模型。使用 <code>forward</code> 函数指定前向传播，自动反向传播（通过 <code>autograd</code>）和梯度计算也由 PyTorch 内部处理。</p><p><strong>神经网络模块（torch.nn）：</strong></p><ul><li>提供了<strong>常用的层</strong>（如线性层、卷积层、池化层等）。</li><li>支持定义复杂的神经网络架构（包括多输入、多输出的网络）。</li><li>兼容与优化器（如 <code>torch.optim</code>）一起使用。</li></ul><h3 id="GPU-加速"><a href="#GPU-加速" class="headerlink" title="GPU 加速"></a>GPU 加速</h3><p>PyTorch 完全支持在 GPU 上运行，以加速深度学习模型的训练。通过<strong>简单的 <code>.to(device)</code> 方法</strong>，用户可以将模型和张量转移到 GPU 上进行计算。PyTorch <strong>支持多 GPU</strong> 训练，能够利用 <strong>NVIDIA CUDA</strong> 技术显著提高计算效率。</p><p><strong>GPU 支持：</strong></p><ul><li>自动选择 GPU 或 CPU。</li><li>支持通过 CUDA 加速运算。</li><li>支持多 GPU 并行计算（<code>DataParallel</code> 或 <code>torch.distributed</code>）。</li></ul><h3 id="生态系统与社区支持"><a href="#生态系统与社区支持" class="headerlink" title="生态系统与社区支持"></a>生态系统与社区支持</h3><p>PyTorch 作为一个开源项目，拥有一个庞大的社区和生态系统。它不仅在学术界得到了广泛的应用，也在工业界，特别是在计算机视觉、自然语言处理等领域中得到了广泛部署。PyTorch 还提供了许多与深度学习相关的工具和库，如：</p><ul><li><strong>torchvision</strong>：用于<strong>计算机视觉任务</strong>的<strong>数据集</strong>和<strong>模型</strong>。</li><li><strong>torchtext</strong>：用于<strong>自然语言处理任务</strong>的<strong>数据集</strong>和<strong>预处理工具</strong>。</li><li><strong>torchaudio</strong>：用于音频处理的工具包。</li><li><strong>PyTorch Lightning</strong>：一种<strong>简化</strong> PyTorch 代码的<strong>高层库</strong>，专注于研究和实验的快速迭代。</li></ul><h3 id="与其他框架的对比"><a href="#与其他框架的对比" class="headerlink" title="与其他框架的对比"></a>与其他框架的对比</h3><p>PyTorch 由于其灵活性、易用性和社区支持，已经成为很多深度学习研究者和开发者的首选框架。</p><p><strong>TensorFlow vs PyTorch：</strong></p><ul><li>PyTorch 的动态计算图使得它更加灵活，适合<strong>快速实验和研究</strong>；而 TensorFlow 的静态计算图在生产环境中更具优化空间。</li><li>PyTorch 在调试时更加方便，TensorFlow 则在部署上更加成熟，支持更广泛的硬件和平台。</li><li>近年来，TensorFlow 也引入了动态图（如 TensorFlow 2.x），使得两者在功能上趋于接近。</li><li>其他深度学习框架，如 Keras、Caffe 等也有一定应用，但 PyTorch 由于其灵活性、易用性和社区支持，已经成为很多深度学习研究者和开发者的首选框架。</li></ul><table><thead><tr><th align="left">特性</th><th align="left"><strong>TensorFlow</strong></th><th align="left"><strong>PyTorch</strong></th></tr></thead><tbody><tr><td align="left"><strong>开发公司</strong></td><td align="left">Google</td><td align="left">Facebook (FAIR)</td></tr><tr><td align="left"><strong>计算图类型</strong></td><td align="left">静态计算图（定义后再执行）</td><td align="left">动态计算图（定义即执行）</td></tr><tr><td align="left"><strong>灵活性</strong></td><td align="left">低（计算图在编译时构建，不易修改）</td><td align="left">高（计算图在执行时动态创建，易于修改和调试）</td></tr><tr><td align="left"><strong>调试</strong></td><td align="left">较难（需要使用 <code>tf.debugging</code> 或外部工具调试）</td><td align="left">容易（可以<strong>直接在 Python 中进行调试</strong>）</td></tr><tr><td align="left"><strong>易用性</strong></td><td align="left">低（较复杂，API 较多，学习曲线较陡峭）</td><td align="left">高（API 简洁，语法更加接近 Python，容易上手）</td></tr><tr><td align="left"><strong>部署</strong></td><td align="left">强（支持广泛的硬件，如 TensorFlow Lite、TensorFlow.js）</td><td align="left">较弱（部署工具和平台相对较少，虽然有 TensorFlow 支持）</td></tr><tr><td align="left"><strong>社区支持</strong></td><td align="left">很强（成熟且庞大的社区，广泛的教程和文档）</td><td align="left">很强（社区活跃，特别是在学术界，快速发展的生态）</td></tr><tr><td align="left"><strong>模型训练</strong></td><td align="left">支持分布式训练，支持多种设备（如 CPU、GPU、TPU）</td><td align="left">支持分布式训练，支持多 GPU、CPU 和 TPU</td></tr><tr><td align="left"><strong>API 层级</strong></td><td align="left">高级API：Keras；低级API：TensorFlow Core</td><td align="left">高级API：TorchVision、TorchText 等；低级API：Torch</td></tr><tr><td align="left"><strong>性能</strong></td><td align="left">高（优化方面成熟，适合生产环境）</td><td align="left">高（适合<strong>研究</strong>和<strong>原型</strong>开发，生产性能也在提升）</td></tr><tr><td align="left"><strong>自动求导</strong></td><td align="left">通过 <code>tf.GradientTape</code> 实现动态求导（较复杂）</td><td align="left">通过 <code>autograd</code> 动态求导（更简洁和直观）</td></tr><tr><td align="left"><strong>调优与可扩展性</strong></td><td align="left">强（支持在多平台上运行，如 TensorFlow Serving 等）</td><td align="left">较弱（虽然在学术和实验环境中表现优越，但生产环境支持相对较少）</td></tr><tr><td align="left"><strong>框架灵活性</strong></td><td align="left">较低（TensorFlow 2.x 引入了动态图特性，但仍不完全灵活）</td><td align="left">高（动态图带来更高的灵活性）</td></tr><tr><td align="left"><strong>支持多种语言</strong></td><td align="left">支持多种语言（Python, C++, Java, JavaScript, etc.）</td><td align="left">主要支持 Python（但也有 C++ API）</td></tr><tr><td align="left"><strong>兼容性与迁移</strong></td><td align="left">TensorFlow 2.x 与旧版本兼容性较好</td><td align="left">与 TensorFlow 兼容性差，迁移较难</td></tr></tbody></table><p>PyTorch 是一个强大且灵活的深度学习框架，适合学术研究和工业应用。它的动态计算图、自动求导机制、GPU 加速等特点，使得其成为深度学习研究和实验中不可或缺的工具。</p><h1 id="PyTorch-基础"><a href="#PyTorch-基础" class="headerlink" title="PyTorch 基础"></a>PyTorch 基础</h1><p>PyTorch 是一个开源的深度学习框架，以其灵活性和动态计算图而广受欢迎。</p><p>PyTorch 主要有以下几个基础概念：张量（Tensor）、自动求导（Autograd）、神经网络模块（nn.Module）、优化器（optim）等。</p><ul><li><strong>张量（Tensor）</strong>：PyTorch 的核心数据结构，支持多维数组，并可以在 CPU 或 GPU 上进行加速计算。</li><li><strong>自动求导（Autograd）</strong>：PyTorch 提供了自动求导功能，可以轻松计算模型的梯度，便于进行反向传播和优化。</li><li><strong>神经网络（nn.Module）</strong>：PyTorch 提供了简单且强大的 API 来构建神经网络模型，可以方便地进行前向传播和模型定义。</li><li><strong>优化器（Optimizers）</strong>：使用优化器（如 Adam、SGD 等）来更新模型的参数，使得损失最小化。</li><li><strong>设备（Device）</strong>：可以将模型和张量移动到 GPU 上以加速计算。</li></ul><h2 id="张量（Tensor）"><a href="#张量（Tensor）" class="headerlink" title="张量（Tensor）"></a>张量（Tensor）</h2><p>张量（Tensor）是 PyTorch 中的核心数据结构，用于存储和操作多维数组。</p><p>张量可以视为一个多维数组，支持加速计算的操作。</p><p>在 PyTorch 中，张量的概念类似于 NumPy 中的数组，但是 PyTorch 的张量可以运行在不同的设备上，比如 CPU 和 GPU，这使得它们非常适合于进行大规模并行计算，特别是在深度学习领域。</p><ul><li><strong>维度（Dimensionality）</strong>：张量的维度指的是<strong>数据的多维数组结构</strong>。例如，一个标量（0维张量）是一个单独的数字，一个向量（1维张量）是一个一维数组，一个矩阵（2维张量）是一个二维数组，以此类推。</li><li><strong>形状（Shape）</strong>：张量的形状是指每个维度上的大小。例如，一个形状为<code>(3, 4)</code>的张量意味着它有3行4列。</li><li><strong>数据类型（Dtype）</strong>：张量中的数据类型定义了存储每个元素所需的内存大小和解释方式。PyTorch支持多种数据类型，包括整数型（如<code>torch.int8</code>、<code>torch.int32</code>）、浮点型（如<code>torch.float32</code>、<code>torch.float64</code>）和布尔型（<code>torch.bool</code>）。</li></ul><p><strong>张量创建：</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个 2x3 的全 0 张量</span></span><br><span class="line">a = torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 创建一个 2x3 的全 1 张量</span></span><br><span class="line">b = torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># 创建一个 2x3 的随机数张量</span></span><br><span class="line">c = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="comment"># 从 NumPy 数组创建张量</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">numpy_array = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tensor_from_numpy = torch.from_numpy(numpy_array)</span><br><span class="line"><span class="built_in">print</span>(tensor_from_numpy)</span><br><span class="line"><span class="comment"># 在指定设备（CPU/GPU）上创建张量</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">d = torch.randn(<span class="number">2</span>, <span class="number">3</span>, device=device)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br></pre></td></tr></table></figure><p><strong>常用张量操作：</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 张量相加</span></span><br><span class="line">e = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">f = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(e + f)</span><br><span class="line"><span class="comment"># 逐元素乘法(不同于矩阵乘法)</span></span><br><span class="line"><span class="built_in">print</span>(e * f)</span><br><span class="line"><span class="comment"># 张量的转置</span></span><br><span class="line">g = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(g.t())  <span class="comment"># 或者 g.transpose(0, 1)</span></span><br><span class="line"><span class="comment"># 张量的形状</span></span><br><span class="line"><span class="built_in">print</span>(g.shape)  <span class="comment"># 返回形状</span></span><br></pre></td></tr></table></figure><h3 id="张量与设备"><a href="#张量与设备" class="headerlink" title="张量与设备"></a>张量与设备</h3><p>PyTorch 张量可以存在于不同的设备上，包括CPU和GPU，你可以将张量<strong>移动到 GPU 上</strong>以加速计算：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if torch.cuda.is_available():</span><br><span class="line">    tensor_gpu = tensor_from_list.to(&#x27;cuda&#x27;)  # 将张量移动到GPU</span><br></pre></td></tr></table></figure><h3 id="梯度和自动微分"><a href="#梯度和自动微分" class="headerlink" title="梯度和自动微分"></a>梯度和自动微分</h3><p>PyTorch的张量<strong>支持自动微分</strong>，这是深度学习中的关键特性。当你创建一个<strong>需要梯度</strong>的张量时，PyTorch可以<strong>自动计算其梯度</strong>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个需要梯度的张量</span></span><br><span class="line">tensor_requires_grad = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 进行一些操作</span></span><br><span class="line">tensor_result = tensor_requires_grad * <span class="number">2</span></span><br><span class="line"><span class="comment"># 计算梯度</span></span><br><span class="line">tensor_result.backward()</span><br><span class="line"><span class="built_in">print</span>(tensor_requires_grad.grad)  <span class="comment"># 输出梯度</span></span><br></pre></td></tr></table></figure><h3 id="内存和性能"><a href="#内存和性能" class="headerlink" title="内存和性能"></a>内存和性能</h3><p>PyTorch 张量还提供了一些内存管理功能，比如.clone()、.detach() 和 .to() 方法，它们可以帮助你优化内存使用和提高性能。</p><hr><h2 id="自动求导（Autograd）"><a href="#自动求导（Autograd）" class="headerlink" title="自动求导（Autograd）"></a>自动求导（Autograd）</h2><p>自动求导（Automatic Differentiation，简称Autograd）是深度学习框架中的一个核心特性，它允许计算机自动计算数学函数的导数。</p><p>在深度学习中，自动求导主要用于两个方面：<strong>一是在训练神经网络时计算梯度</strong>，<strong>二是进行反向传播算法的实现</strong>。</p><p>自动求导基于链式法则（Chain Rule），这是一个用于计算复杂函数导数的数学法则。链式法则表明，复合函数的导数是其各个组成部分导数的乘积。在深度学习中，模型<strong>通常是由许多层组成的复杂函数</strong>，自动求导能够高效地计算这些层的梯度。</p><p><strong>动态图与静态图：</strong></p><ul><li><strong>动态图（Dynamic Graph）</strong>：在动态图中，计算图在运行时动态构建。每次执行操作时，计算图都会更新，这使得调试和修改模型变得更加容易。PyTorch使用的是<strong>动态图</strong>。</li><li><strong>静态图（Static Graph）</strong>：在静态图中，计算图在开始执行之前构建完成，并且不会改变。TensorFlow最初使用的是静态图，但后来也支持动态图。</li></ul><p>PyTorch 提供了自动求导功能，通过 autograd 模块来自动计算梯度。</p><p>torch.Tensor 对象有一个 <strong>requires_grad</strong> 属性，用于指示<strong>是否需要计算该张量的梯度</strong>。</p><p>当你创建一个 requires_grad&#x3D;True 的张量时，PyTorch 会自动跟踪所有对它的操作，以便在之后计算梯度。</p><p>创建需要梯度的张量:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个需要计算梯度的张量</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># 执行某些操作</span></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure><h3 id="反向传播（Backpropagation）"><a href="#反向传播（Backpropagation）" class="headerlink" title="反向传播（Backpropagation）"></a>反向传播（Backpropagation）</h3><p>一旦定义了计算图，可以通过 <strong>.backward()</strong> 方法来计算梯度。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">out.backward()</span><br><span class="line"><span class="comment"># 查看 x 的梯度</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><p>在神经网络训练中，<strong>自动求导</strong>主要用于实现<strong>反向传播</strong>算法。</p><p>反向传播是一种通过<strong>计算损失函数关于网络参数的梯度</strong>来训练神经网络的方法。在<strong>每次迭代</strong>中，网络的<strong>前向</strong>传播会<strong>计算输出和损失</strong>，然后反向传播会<strong>计算损失关于每个参数的梯度</strong>，并<strong>使用这些梯度来更新</strong>参数。</p><h3 id="停止梯度计算"><a href="#停止梯度计算" class="headerlink" title="停止梯度计算"></a>停止梯度计算</h3><p>如果你不希望某些张量的梯度被计算（例如，当你<strong>不需要反向传播</strong>时），可以使用 <strong>torch.no_grad()</strong> 或设置 <strong>requires_grad&#x3D;False</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 torch.no_grad() 禁用梯度计算</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y = x * <span class="number">2</span></span><br></pre></td></tr></table></figure><h2 id="神经网络（nn-Module）"><a href="#神经网络（nn-Module）" class="headerlink" title="神经网络（nn.Module）"></a>神经网络（nn.Module）</h2><p>神经网络是一种<strong>模仿人脑神经元连接的计算模型</strong>，由多层节点（神经元）组成，用于学习数据之间的复杂模式和关系。</p><p>神经网络通过调整<strong>神经元之间的连接权重</strong>来优化预测结果，这一过程涉及前向传播、损失计算、反向传播和参数更新。</p><p>神经网络的类型包括<strong>前馈神经网络</strong>、<strong>卷积神经网络（CNN）</strong>、<strong>循环神经网络（RNN）<strong>和</strong>长短期记忆网络（LSTM）</strong>，它们在图像识别、语音处理、自然语言处理等多个领域都有广泛应用。</p><p>PyTorch 提供了一个非常方便的接口来<strong>构建神经网络模</strong>型，即 <strong>torch.nn.Module</strong>。</p><p>我们可以继承 nn.Module 类并定义自己的网络层。</p><p>创建一个简单的神经网络：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 定义一个简单的全连接神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 输入层到隐藏层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 隐藏层到输出层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># ReLU 激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建网络实例</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"><span class="comment"># 打印模型结构</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><p><strong>训练过程：</strong></p><ol><li><strong>前向传播（Forward Propagation）</strong>： 在前向传播阶段，<strong>输入</strong>数据通过网络层传递，每层<strong>应用权重和激活函数</strong>，<strong>直到产生输出</strong>。</li><li><strong>计算损失（Calculate Loss）</strong>： 根据网络的<strong>输出</strong>和<strong>真实</strong>标签，计算<strong>损失函数的值</strong>。</li><li><strong>反向传播（Backpropagation）</strong>： 反向传播利用<strong>自动求导</strong>技术计算损失函数关于每个参数的<strong>梯度</strong>。</li><li><strong>参数更新（Parameter Update）</strong>： 使用<strong>优化器</strong>根据梯度更新网络的权重和偏置。</li><li><strong>迭代（Iteration）</strong>： 重复上述过程，直到模型在训练数据上的性能达到满意的水平。</li></ol><h3 id="前向传播与损失计算"><a href="#前向传播与损失计算" class="headerlink" title="前向传播与损失计算"></a>前向传播与损失计算</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机输入</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = model(x)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="comment"># 定义损失函数（例如均方误差 MSE）</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"><span class="comment"># 假设目标值为 1</span></span><br><span class="line">target = torch.randn(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure><h3 id="优化器（Optimizers）"><a href="#优化器（Optimizers）" class="headerlink" title="优化器（Optimizers）"></a>优化器（Optimizers）</h3><p>优化器在训练过程中更新神经网络的参数，以减少损失函数的值。</p><p>PyTorch 提供了多种优化器，例如 <strong>SGD、Adam</strong> 等。</p><p>使用优化器进行参数更新：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义优化器（使用 Adam 优化器）</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 训练步骤</span></span><br><span class="line">optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">optimizer.step()  <span class="comment"># 更新参数</span></span><br></pre></td></tr></table></figure><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>训练模型是机器学习和深度学习中的核心过程，旨在通过大量数据学习模型参数，以便模型能够对新的、未见过的数据做出准确的预测。</p><p>训练模型通常包括以下几个步骤：</p><ol><li><strong>数据准备</strong>：<ul><li>收集和处理数据，包括清洗、标准化和归一化。</li><li>将数据分为训练集、验证集和测试集。</li></ul></li><li><strong>定义模型</strong>：<ul><li>选择<strong>模型架构</strong>，例如决策树、神经网络等。</li><li><strong>初始化模型参数</strong>（权重和偏置）。</li></ul></li><li><strong>选择损失函数</strong>：<ul><li><strong>根据任务类型</strong>（如分类、回归）选择合适的损失函数。</li></ul></li><li><strong>选择优化器</strong>：<ul><li><strong>选择一个优化算法</strong>，如SGD、Adam等，来更新模型参数。</li></ul></li><li><strong>前向传播</strong>：<ul><li>在每次迭代中，将输入数据<strong>通过模型传递</strong>，计算预测输出。</li></ul></li><li><strong>计算损失</strong>：<ul><li>使用<strong>损失函数评估</strong>预测输出与真实标签之间的差异。</li></ul></li><li><strong>反向传播</strong>：<ul><li>利用自动求导计算损失相对于模型参数的梯度。</li></ul></li><li><strong>参数更新</strong>：<ul><li>根据计算出的梯度和优化器的策略更新模型参数。</li></ul></li><li><strong>迭代优化</strong>：<ul><li><strong>重复步骤5-8</strong>，直到模型在验证集上的性能不再提升或达到预定的迭代次数。</li></ul></li><li><strong>评估和测试</strong>：<ul><li>使用测试集评估模型的最终性能，确保模型没有过拟合。</li></ul></li><li><strong>模型调优</strong>：<ul><li>根据模型在测试集上的表现进行调参，如改变学习率、增加正则化等。</li></ul></li><li><strong>部署模型</strong>：<ul><li>将训练好的模型部署到生产环境中，用于实际的预测任务。</li></ul></li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 1. 定义一个简单的神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 输入层到隐藏层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 隐藏层到输出层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># ReLU 激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 2. 创建模型实例</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"><span class="comment"># 3. 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.MSELoss()  <span class="comment"># 均方误差损失函数</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)  <span class="comment"># Adam 优化器</span></span><br><span class="line"><span class="comment"># 4. 假设我们有训练数据 X 和 Y</span></span><br><span class="line">X = torch.randn(<span class="number">10</span>, <span class="number">2</span>)  <span class="comment"># 10 个样本，2 个特征</span></span><br><span class="line">Y = torch.randn(<span class="number">10</span>, <span class="number">1</span>)  <span class="comment"># 10 个目标值</span></span><br><span class="line"><span class="comment"># 5. 训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 训练 100 轮</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清空之前的梯度</span></span><br><span class="line">    output = model(X)  <span class="comment"># 前向传播</span></span><br><span class="line">    loss = criterion(output, Y)  <span class="comment"># 计算损失</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="comment"># 每 10 轮输出一次损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/100], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>在每 10 轮，程序会输出当前的损失值，帮助我们跟踪模型的<strong>训练进度</strong>。随着训练的进行，损失值应该会逐渐降低，表示模型在不断学习并优化其参数。</p><p>训练模型是一个迭代的过程，需要不断地调整和优化，直到达到满意的性能。这个过程涉及到大量的实验和调优，目的是使模型在新的、未见过的数据上也能有良好的泛化能力。</p><hr><h2 id="设备（Device）"><a href="#设备（Device）" class="headerlink" title="设备（Device）"></a>设备（Device）</h2><p>PyTorch 允许你将模型和数据移动到 GPU 上进行加速。</p><p>使用 <strong>torch.device</strong> 来指定计算设备。</p><p>将模型和数据移至 GPU:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># 将模型移动到设备</span></span><br><span class="line">model.to(device)</span><br><span class="line"><span class="comment"># 将数据移动到设备</span></span><br><span class="line">X = X.to(device)</span><br><span class="line">Y = Y.to(device)</span><br></pre></td></tr></table></figure><h1 id="PyTorch-张量（Tensor）"><a href="#PyTorch-张量（Tensor）" class="headerlink" title="PyTorch 张量（Tensor）"></a>PyTorch 张量（Tensor）</h1><p>张量是一个多维数组，可以是标量、向量、矩阵或更高维度的数据结构。</p><p>在 PyTorch 中，张量（Tensor）是<strong>数据的核心表示形式</strong>，类似于 NumPy 的多维数组，但具有更强大的功能，例如<strong>支持 GPU 加速</strong>和自动梯度计算。</p><p>张量支持多种数据类型（整型、浮点型、布尔型等）。</p><p>张量可以存储在 CPU 或 GPU 中，GPU 张量可显著加速计算。</p><p>下图展示了不同维度的张量（Tensor）在 PyTorch 中的表示方法：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1__D5ZvufDS38WkhK9rK32hQ.jpg" alt="img"></p><p><strong>说明：</strong></p><ul><li><strong>1D Tensor &#x2F; Vector（一维张量&#x2F;向量）:</strong> 最基本的张量形式，可以看作是一个数组，图中的例子是一个包含 10 个元素的向量。</li><li><strong>2D Tensor &#x2F; Matrix（二维张量&#x2F;矩阵）:</strong> 二维数组，通常用于表示矩阵，图中的例子是一个 4x5 的矩阵，包含了 20 个元素。</li><li><strong>3D Tensor &#x2F; Cube（三维张量&#x2F;立方体）:</strong> 三维数组，可以看作是由多个矩阵堆叠而成的立方体，图中的例子展示了一个 3x4x5 的立方体，其中每个 5x5 的矩阵代表立方体的一个”层”。</li><li><strong>4D Tensor &#x2F; Vector of Cubes（四维张量&#x2F;立方体向量）:</strong> 四维数组，可以看作是由多个立方体组成的向量，图中的例子没有具体数值，但可以理解为一个<strong>包含多个 3D 张量的集合</strong>。</li><li><strong>5D Tensor &#x2F; Matrix of Cubes（五维张量&#x2F;立方体矩阵）:</strong> 五维数组，可以看作是由多个4D张量组成的矩阵，图中的例子同样没有具体数值，但可以理解为一个包含多个 4D 张量的集合。</li></ul><h2 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h2><p>张量创建的方式有：</p><table><thead><tr><th align="left"><strong>方法</strong></th><th align="left"><strong>说明</strong></th><th align="left"><strong>示例代码</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.tensor(data)</code></td><td align="left"><strong>从 Python 列表或 NumPy 数组</strong>创建张量。</td><td align="left"><code>x = torch.tensor([[1, 2], [3, 4]])</code></td></tr><tr><td align="left"><code>torch.zeros(size)</code></td><td align="left">创建一个<strong>全为零</strong>的张量。</td><td align="left"><code>x = torch.zeros((2, 3))</code></td></tr><tr><td align="left"><code>torch.ones(size)</code></td><td align="left">创建一个<strong>全为 1</strong> 的张量。</td><td align="left"><code>x = torch.ones((2, 3))</code></td></tr><tr><td align="left"><code>torch.empty(size)</code></td><td align="left">创建一个<strong>未初始化</strong>的张量。</td><td align="left"><code>x = torch.empty((2, 3))</code></td></tr><tr><td align="left"><code>torch.rand(size)</code></td><td align="left">创建一个<strong>服从均匀分布的随机张量</strong>，值在 <code>[0, 1)</code>。</td><td align="left"><code>x = torch.rand((2, 3))</code></td></tr><tr><td align="left"><code>torch.randn(size)</code></td><td align="left">创建一个<strong>服从正态分布的随机张量</strong>，均值为 0，标准差为 1。</td><td align="left"><code>x = torch.randn((2, 3))</code></td></tr><tr><td align="left"><code>torch.arange(start, end, step)</code></td><td align="left">创建一个<strong>一维序列</strong>张量，类似于 Python 的 <code>range</code>。</td><td align="left"><code>x = torch.arange(0, 10, 2)</code></td></tr><tr><td align="left"><code>torch.linspace(start, end, steps)</code></td><td align="left">创建一个在指定范围内<strong>等间隔</strong>的序列张量。</td><td align="left"><code>x = torch.linspace(0, 1, 5)</code></td></tr><tr><td align="left"><code>torch.eye(size)</code></td><td align="left">创建一个<strong>单位矩阵</strong>（对角线为 1，其他为 0）。</td><td align="left"><code>x = torch.eye(3)</code></td></tr><tr><td align="left"><code>torch.from_numpy(ndarray)</code></td><td align="left"><strong>将 NumPy 数组转换</strong>为张量。</td><td align="left"><code>x = torch.from_numpy(np.array([1, 2, 3]))</code></td></tr></tbody></table><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tensor = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure><p>如果你有一个 NumPy 数组，可以使用 torch.from_numpy() 将其转换为张量：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np_array = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor = torch.from_numpy(np_array)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3])</span><br></pre></td></tr></table></figure><p>创建 2D 张量（矩阵）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tensor_2d = torch.tensor([</span><br><span class="line">    [-<span class="number">9</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">0</span>, <span class="number">12</span>, <span class="number">8</span>, <span class="number">6</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">23</span>, -<span class="number">6</span>, <span class="number">45</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">22</span>, <span class="number">3</span>, -<span class="number">1</span>, <span class="number">72</span>, <span class="number">6</span>]</span><br><span class="line">])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;2D Tensor (Matrix):\n&quot;</span>, tensor_2d)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor_2d.shape)  <span class="comment"># 形状</span></span><br></pre></td></tr></table></figure><p>其他维度的创建：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 3D 张量（立方体）</span></span><br><span class="line">tensor_3d = torch.stack([tensor_2d, tensor_2d + <span class="number">10</span>, tensor_2d - <span class="number">5</span>])  <span class="comment"># 堆叠 3 个 2D 张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;3D Tensor (Cube):\n&quot;</span>, tensor_3d)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor_3d.shape)  <span class="comment"># 形状</span></span><br><span class="line"><span class="comment"># 创建 4D 张量（向量的立方体）</span></span><br><span class="line">tensor_4d = torch.stack([tensor_3d, tensor_3d + <span class="number">100</span>])  <span class="comment"># 堆叠 2 个 3D 张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;4D Tensor (Vector of Cubes):\n&quot;</span>, tensor_4d)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor_4d.shape)  <span class="comment"># 形状</span></span><br><span class="line"><span class="comment"># 创建 5D 张量（矩阵的立方体）</span></span><br><span class="line">tensor_5d = torch.stack([tensor_4d, tensor_4d + <span class="number">1000</span>])  <span class="comment"># 堆叠 2 个 4D 张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;5D Tensor (Matrix of Cubes):\n&quot;</span>, tensor_5d)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor_5d.shape)  <span class="comment"># 形状</span></span><br></pre></td></tr></table></figure><h2 id="张量的属性"><a href="#张量的属性" class="headerlink" title="张量的属性"></a>张量的属性</h2><p>张量的属性如下表：</p><table><thead><tr><th align="left"><strong>属性</strong></th><th align="left"><strong>说明</strong></th><th align="left"><strong>示例</strong></th></tr></thead><tbody><tr><td align="left"><code>.shape</code></td><td align="left">获取张量的形状</td><td align="left"><code>tensor.shape</code></td></tr><tr><td align="left"><code>.size()</code></td><td align="left">获取张量的形状</td><td align="left"><code>tensor.size()</code></td></tr><tr><td align="left"><code>.dtype</code></td><td align="left">获取张量的<strong>数据类型</strong></td><td align="left"><code>tensor.dtype</code></td></tr><tr><td align="left"><code>.device</code></td><td align="left">查看张量<strong>所在的设备</strong> (CPU&#x2F;GPU)</td><td align="left"><code>tensor.device</code></td></tr><tr><td align="left"><code>.dim()</code></td><td align="left">获取张量的<strong>维度数</strong></td><td align="left"><code>tensor.dim()</code></td></tr><tr><td align="left"><code>.requires_grad</code></td><td align="left">是否启用<strong>梯度</strong>计算</td><td align="left"><code>tensor.requires_grad</code></td></tr><tr><td align="left"><code>.numel()</code></td><td align="left">获取张量中的<strong>元素总数</strong></td><td align="left"><code>tensor.numel()</code></td></tr><tr><td align="left"><code>.is_cuda</code></td><td align="left">检查张量<strong>是否在 GPU 上</strong></td><td align="left"><code>tensor.is_cuda</code></td></tr><tr><td align="left"><code>.T</code></td><td align="left">获取张量的<strong>转置</strong>（适用于 <strong>2D 张量</strong>）</td><td align="left"><code>tensor.T</code></td></tr><tr><td align="left"><code>.item()</code></td><td align="left">获取<strong>单元素张量的值</strong></td><td align="left"><code>tensor.item()</code></td></tr><tr><td align="left"><code>.is_contiguous()</code></td><td align="left">检查张量<strong>是否连续</strong>存储</td><td align="left"><code>tensor.is_contiguous()</code></td></tr></tbody></table><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个 2D 张量</span></span><br><span class="line">tensor = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 张量的属性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor:\n&quot;</span>, tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor.shape)  <span class="comment"># 获取形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Size:&quot;</span>, tensor.size())  <span class="comment"># 获取形状（另一种方法）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Data Type:&quot;</span>, tensor.dtype)  <span class="comment"># 数据类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Device:&quot;</span>, tensor.device)  <span class="comment"># 设备</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Dimensions:&quot;</span>, tensor.dim())  <span class="comment"># 维度数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Total Elements:&quot;</span>, tensor.numel())  <span class="comment"># 元素总数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Requires Grad:&quot;</span>, tensor.requires_grad)  <span class="comment"># 是否启用梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Is CUDA:&quot;</span>, tensor.is_cuda)  <span class="comment"># 是否在 GPU 上</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Is Contiguous:&quot;</span>, tensor.is_contiguous())  <span class="comment"># 是否连续存储</span></span><br><span class="line"><span class="comment"># 获取单元素值</span></span><br><span class="line">single_value = torch.tensor(<span class="number">42</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Single Element Value:&quot;</span>, single_value.item())</span><br><span class="line"><span class="comment"># 转置张量</span></span><br><span class="line">tensor_T = tensor.T</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Transposed Tensor:\n&quot;</span>, tensor_T)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">         [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">Shape: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Data <span class="type">Type</span>: torch.float32</span><br><span class="line">Device: cpu</span><br><span class="line">Dimensions: <span class="number">2</span></span><br><span class="line">Total Elements: <span class="number">6</span></span><br><span class="line">Requires Grad: <span class="literal">False</span></span><br><span class="line">Is CUDA: <span class="literal">False</span></span><br><span class="line">Is Contiguous: <span class="literal">True</span></span><br><span class="line">Single Element Value: <span class="number">42</span></span><br><span class="line">Transposed Tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">4.</span>],</span><br><span class="line">         [<span class="number">2.</span>, <span class="number">5.</span>],</span><br><span class="line">         [<span class="number">3.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure><h2 id="张量的操作"><a href="#张量的操作" class="headerlink" title="张量的操作"></a>张量的操作</h2><p>张量操作方法说明如下。</p><h4 id="基础操作："><a href="#基础操作：" class="headerlink" title="基础操作："></a>基础操作：</h4><table><thead><tr><th align="left"><strong>操作</strong></th><th align="left"><strong>说明</strong></th><th align="left"><strong>示例代码</strong></th></tr></thead><tbody><tr><td align="left"><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code></td><td align="left">元素级加法、减法、乘法、除法。</td><td align="left"><code>z = x + y</code></td></tr><tr><td align="left"><code>torch.matmul(x, y)</code></td><td align="left">矩阵乘法。</td><td align="left"><code>z = torch.matmul(x, y)</code></td></tr><tr><td align="left"><code>torch.dot(x, y)</code></td><td align="left">向量点积（仅适用于 1D 张量）。</td><td align="left"><code>z = torch.dot(x, y)</code></td></tr><tr><td align="left"><code>torch.sum(x)</code></td><td align="left">求和。</td><td align="left"><code>z = torch.sum(x)</code></td></tr><tr><td align="left"><code>torch.mean(x)</code></td><td align="left">求均值。</td><td align="left"><code>z = torch.mean(x)</code></td></tr><tr><td align="left"><code>torch.max(x)</code></td><td align="left">求最大值。</td><td align="left"><code>z = torch.max(x)</code></td></tr><tr><td align="left"><code>torch.min(x)</code></td><td align="left">求最小值。</td><td align="left"><code>z = torch.min(x)</code></td></tr><tr><td align="left"><code>torch.argmax(x, dim)</code></td><td align="left">返回<strong>最大值的索引</strong>（指定维度）。</td><td align="left"><code>z = torch.argmax(x, dim=1)</code></td></tr><tr><td align="left"><code>torch.softmax(x, dim)</code></td><td align="left">计算 softmax（指定维度）。</td><td align="left"><code>z = torch.softmax(x, dim=1)</code></td></tr></tbody></table><h4 id="形状操作"><a href="#形状操作" class="headerlink" title="形状操作"></a><strong>形状操作</strong></h4><table><thead><tr><th align="left"><strong>操作</strong></th><th align="left"><strong>说明</strong></th><th align="left"><strong>示例代码</strong></th></tr></thead><tbody><tr><td align="left"><code>x.view(shape)</code></td><td align="left">改变张量的<strong>形状</strong>（<strong>不改变数据</strong>）。</td><td align="left"><code>z = x.view(3, 4)</code></td></tr><tr><td align="left"><code>x.reshape(shape)</code></td><td align="left">类似于 <code>view</code>，但更灵活。</td><td align="left"><code>z = x.reshape(3, 4)</code></td></tr><tr><td align="left"><code>x.t()</code></td><td align="left"><strong>转置</strong>矩阵。</td><td align="left"><code>z = x.t()</code></td></tr><tr><td align="left"><code>x.unsqueeze(dim)</code></td><td align="left">在<strong>指定维度</strong>添加一个维度。</td><td align="left"><code>z = x.unsqueeze(0)</code></td></tr><tr><td align="left"><code>x.squeeze(dim)</code></td><td align="left">去掉指定维度为 1 的维度。</td><td align="left"><code>z = x.squeeze(0)</code></td></tr><tr><td align="left"><code>torch.cat((x, y), dim)</code></td><td align="left">按指定维度连接多个张量。</td><td align="left"><code>z = torch.cat((x, y), dim=1)</code></td></tr></tbody></table><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个 2D 张量</span></span><br><span class="line">tensor = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始张量:\n&quot;</span>, tensor)</span><br><span class="line"><span class="comment"># 1. **索引和切片操作**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【索引和切片】&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;获取第一行:&quot;</span>, tensor[<span class="number">0</span>])  <span class="comment"># 获取第一行</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;获取第一行第一列的元素:&quot;</span>, tensor[<span class="number">0</span>, <span class="number">0</span>])  <span class="comment"># 获取特定元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;获取第二列的所有元素:&quot;</span>, tensor[:, <span class="number">1</span>])  <span class="comment"># 获取第二列所有元素</span></span><br><span class="line"><span class="comment"># 2. **形状变换操作**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【形状变换】&quot;</span>)</span><br><span class="line">reshaped = tensor.view(<span class="number">3</span>, <span class="number">2</span>)  <span class="comment"># 改变张量形状为 3x2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;改变形状后的张量:\n&quot;</span>, reshaped)</span><br><span class="line">flattened = tensor.flatten()  <span class="comment"># 将张量展平成一维</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;展平后的张量:\n&quot;</span>, flattened)</span><br><span class="line"><span class="comment"># 3. **数学运算操作**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【数学运算】&quot;</span>)</span><br><span class="line">tensor_add = tensor + <span class="number">10</span>  <span class="comment"># 张量加法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量加 10:\n&quot;</span>, tensor_add)</span><br><span class="line">tensor_mul = tensor * <span class="number">2</span>  <span class="comment"># 张量乘法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量乘 2:\n&quot;</span>, tensor_mul)</span><br><span class="line">tensor_sum = tensor.<span class="built_in">sum</span>()  <span class="comment"># 计算所有元素的和</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量元素的和:&quot;</span>, tensor_sum.item())</span><br><span class="line"><span class="comment"># 4. **与其他张量的操作**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【与其他张量操作】&quot;</span>)</span><br><span class="line">tensor2 = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;另一个张量:\n&quot;</span>, tensor2)</span><br><span class="line">tensor_dot = torch.matmul(tensor, tensor2.T)  <span class="comment"># 张量矩阵乘法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵乘法结果:\n&quot;</span>, tensor_dot)</span><br><span class="line"><span class="comment"># 5. **条件判断和筛选**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【条件判断和筛选】&quot;</span>)</span><br><span class="line">mask = tensor &gt; <span class="number">3</span>  <span class="comment"># 创建一个布尔掩码</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;大于 3 的元素的布尔掩码:\n&quot;</span>, mask)</span><br><span class="line">filtered_tensor = tensor[tensor &gt; <span class="number">3</span>]  <span class="comment"># 筛选出符合条件的元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;大于 3 的元素:\n&quot;</span>, filtered_tensor)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">原始张量:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">         [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">【索引和切片】</span><br><span class="line">获取第一行: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">获取第一行第一列的元素: tensor(<span class="number">1.</span>)</span><br><span class="line">获取第二列的所有元素: tensor([<span class="number">2.</span>, <span class="number">5.</span>])</span><br><span class="line">【形状变换】</span><br><span class="line">改变形状后的张量:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">         [<span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">         [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">展平后的张量:</span><br><span class="line"> tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line">【数学运算】</span><br><span class="line">张量加 <span class="number">10</span>:</span><br><span class="line"> tensor([[<span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>],</span><br><span class="line">         [<span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>]])</span><br><span class="line">张量乘 <span class="number">2</span>:</span><br><span class="line"> tensor([[ <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>],</span><br><span class="line">         [ <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">12.</span>]])</span><br><span class="line">张量元素的和: <span class="number">21.0</span></span><br><span class="line">【与其他张量操作】</span><br><span class="line">另一个张量:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">矩阵乘法结果:</span><br><span class="line"> tensor([[ <span class="number">6.</span>,  <span class="number">6.</span>],</span><br><span class="line">         [<span class="number">15.</span>, <span class="number">15.</span>]])</span><br><span class="line">【条件判断和筛选】</span><br><span class="line">大于 <span class="number">3</span> 的元素的布尔掩码:</span><br><span class="line"> tensor([[<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">         [ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>]])</span><br><span class="line">大于 <span class="number">3</span> 的元素:</span><br><span class="line"> tensor([<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br></pre></td></tr></table></figure><h2 id="张量的-GPU-加速"><a href="#张量的-GPU-加速" class="headerlink" title="张量的 GPU 加速"></a>张量的 GPU 加速</h2><p>将张量转移到 GPU：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line">x = torch.tensor([1.0, 2.0, 3.0], device=device)</span><br></pre></td></tr></table></figure><p>检查 GPU 是否可用：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available()  # 返回 True 或 False</span><br></pre></td></tr></table></figure><hr><h2 id="张量与-NumPy-的互操作"><a href="#张量与-NumPy-的互操作" class="headerlink" title="张量与 NumPy 的互操作"></a>张量与 NumPy 的互操作</h2><p>张量与 NumPy 的互操作如下表所示：</p><table><thead><tr><th align="left"><strong>操作</strong></th><th align="left"><strong>说明</strong></th><th align="left"><strong>示例代码</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.from_numpy(ndarray)</code></td><td align="left">将 NumPy 数组转换为张量。</td><td align="left"><code>x = torch.from_numpy(np_array)</code></td></tr><tr><td align="left"><code>x.numpy()</code></td><td align="left">将张量<strong>转换为 NumPy 数组</strong>（仅限 <strong>CPU 张量</strong>）。</td><td align="left"><code>np_array = x.numpy()</code></td></tr></tbody></table><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 1. NumPy 数组转换为 PyTorch 张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1. NumPy 转为 PyTorch 张量&quot;</span>)</span><br><span class="line">numpy_array = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy 数组:\n&quot;</span>, numpy_array)</span><br><span class="line"><span class="comment"># 使用 torch.from_numpy() 将 NumPy 数组转换为张量</span></span><br><span class="line">tensor_from_numpy = torch.from_numpy(numpy_array)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;转换后的 PyTorch 张量:\n&quot;</span>, tensor_from_numpy)</span><br><span class="line"><span class="comment"># 修改 NumPy 数组，观察张量的变化（共享内存）</span></span><br><span class="line">numpy_array[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;修改后的 NumPy 数组:\n&quot;</span>, numpy_array)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;PyTorch 张量也会同步变化:\n&quot;</span>, tensor_from_numpy)</span><br><span class="line"><span class="comment"># 2. PyTorch 张量转换为 NumPy 数组</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n2. PyTorch 张量转为 NumPy 数组&quot;</span>)</span><br><span class="line">tensor = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;PyTorch 张量:\n&quot;</span>, tensor)</span><br><span class="line"><span class="comment"># 使用 tensor.numpy() 将张量转换为 NumPy 数组</span></span><br><span class="line">numpy_from_tensor = tensor.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;转换后的 NumPy 数组:\n&quot;</span>, numpy_from_tensor)</span><br><span class="line"><span class="comment"># 修改张量，观察 NumPy 数组的变化（共享内存）</span></span><br><span class="line">tensor[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">77</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;修改后的 PyTorch 张量:\n&quot;</span>, tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy 数组也会同步变化:\n&quot;</span>, numpy_from_tensor)</span><br><span class="line"><span class="comment"># 3. 注意：不共享内存的情况（需要复制数据）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n3. 使用 clone() 保证独立数据&quot;</span>)</span><br><span class="line">tensor_independent = torch.tensor([[<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>], [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]], dtype=torch.float32)</span><br><span class="line">numpy_independent = tensor_independent.clone().numpy()  <span class="comment"># 使用 clone 复制数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始张量:\n&quot;</span>, tensor_independent)</span><br><span class="line">tensor_independent[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># 修改张量数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;修改后的张量:\n&quot;</span>, tensor_independent)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy 数组（不会同步变化）:\n&quot;</span>, numpy_independent)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> NumPy 转为 PyTorch 张量</span><br><span class="line">NumPy 数组:</span><br><span class="line"> [[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line">转换后的 PyTorch 张量:</span><br><span class="line"> tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">         [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">修改后的 NumPy 数组:</span><br><span class="line"> [[<span class="number">100</span>   <span class="number">2</span>   <span class="number">3</span>]</span><br><span class="line"> [  <span class="number">4</span>   <span class="number">5</span>   <span class="number">6</span>]]</span><br><span class="line">PyTorch 张量也会同步变化:</span><br><span class="line"> tensor([[<span class="number">100</span>,   <span class="number">2</span>,   <span class="number">3</span>],</span><br><span class="line">         [  <span class="number">4</span>,   <span class="number">5</span>,   <span class="number">6</span>]])</span><br><span class="line"><span class="number">2.</span> PyTorch 张量转为 NumPy 数组</span><br><span class="line">PyTorch 张量:</span><br><span class="line"> tensor([[ <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">         [<span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>]])</span><br><span class="line">转换后的 NumPy 数组:</span><br><span class="line"> [[ <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span>]</span><br><span class="line"> [<span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span>]]</span><br><span class="line">修改后的 PyTorch 张量:</span><br><span class="line"> tensor([[<span class="number">77.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">         [<span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>]])</span><br><span class="line">NumPy 数组也会同步变化:</span><br><span class="line"> [[<span class="number">77.</span>  <span class="number">8.</span>  <span class="number">9.</span>]</span><br><span class="line"> [<span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span>]]</span><br><span class="line"><span class="number">3.</span> 使用 clone() 保证独立数据</span><br><span class="line">原始张量:</span><br><span class="line"> tensor([[<span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>]])</span><br><span class="line">修改后的张量:</span><br><span class="line"> tensor([[ <span class="number">0.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>]])</span><br><span class="line">NumPy 数组（不会同步变化）:</span><br><span class="line"> [[<span class="number">13.</span> <span class="number">14.</span> <span class="number">15.</span>]</span><br><span class="line"> [<span class="number">16.</span> <span class="number">17.</span> <span class="number">18.</span>]]</span><br></pre></td></tr></table></figure><h1 id="PyTorch-神经网络基础"><a href="#PyTorch-神经网络基础" class="headerlink" title="PyTorch 神经网络基础"></a>PyTorch 神经网络基础</h1><p>神经网络是一种模仿人脑处理信息方式的计算模型，它由许多相互连接的节点（神经元）组成，这些节点按层次排列。</p><p>神经网络的强大之处在于其能够<strong>自动</strong>从大量数据中学习复杂的模式和特征，<strong>无需人工设计特征提取器</strong>。</p><p>随着深度学习的发展，神经网络已经成为解决许多复杂问题的关键技术。</p><h3 id="神经元（Neuron）"><a href="#神经元（Neuron）" class="headerlink" title="神经元（Neuron）"></a>神经元（Neuron）</h3><p>神经元是神经网络的基本单元，它接收输入信号，通过<strong>加权求和后与偏置（bias）相加</strong>，然后<strong>通过激活函数处理以产生输出</strong>。</p><p>神经元的<strong>权重</strong>和<strong>偏置</strong>是网络学习过程中<strong>需要调整的参数</strong>。</p><p><strong>输入和输出:</strong></p><ul><li><strong>输入（Input）</strong>：输入是网络的起始点，可以是特征数据，如图像的像素值或文本的词向量。</li><li><strong>输出（Output）</strong>：输出是网络的终点，表示模型的预测结果，如分类任务中的类别标签。</li></ul><p>神经元接收<strong>多个输入</strong>（例如x1, x2, …, xn），如果输入的<strong>加权和大于激活阈值</strong>（activation potential），则<strong>产生二进制输出</strong>。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_upfpVueoUuKPkyX3PR3KBg.png" alt="img">神经元的输出可以看作是<strong>输入的加权和加上偏置（bias)</strong>，神经元的数学表示：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/f0b929045ae6eef23514bd7024be62f0.png" alt="img"></p><p>这里，<strong>wj</strong> 是权重，<strong>xj</strong> 是输入，而 <strong>Bias</strong> 是偏置项。</p><h3 id="层（Layer）"><a href="#层（Layer）" class="headerlink" title="层（Layer）"></a>层（Layer）</h3><p>输入层和输出层之间的层被称为隐藏层，层与层之间的连接密度和类型构成了网络的配置。</p><p>神经网络由多个层组成，包括：</p><ul><li><strong>输入层（Input Layer）</strong>：接收原始输入数据。</li><li><strong>隐藏层（Hidden Layer）</strong>：对输入数据进行处理，可以有多个隐藏层。</li><li><strong>输出层（Output Layer）</strong>：产生最终的输出结果。</li></ul><p>典型的神经网络架构:</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_3fA77_mLNiJTSgZFhYnU0Q3K5DV4.webp" alt="img"></p><h3 id="前馈神经网络（Feedforward-Neural-Network，FNN）"><a href="#前馈神经网络（Feedforward-Neural-Network，FNN）" class="headerlink" title="前馈神经网络（Feedforward Neural Network，FNN）"></a>前馈神经网络（Feedforward Neural Network，FNN）</h3><p>前馈神经网络（Feedforward Neural Network，FNN）是神经网络家族中的基本单元。</p><p>前馈神经网络特点是数据从输入层开始，<strong>经过一个或多个隐藏</strong>层，最后到达输出层，全过程没有循环或反馈。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/neural-net.png" alt="img"></p><p><strong>前馈神经网络的基本结构：</strong></p><ul><li><strong>输入层：</strong> 数据进入网络的入口点。输入层的每个节点代表一个输入特征。</li><li><strong>隐藏层：<strong>一个或多个层，用于捕获数据的</strong>非线性</strong>特征。每个隐藏层由多个神经元组成，每个神经元通过激活函数增加非线性能力。</li><li>**输出层：**输出网络的预测结果。<strong>节点数和问题类型相关</strong>，例如分类问题的输出节点数等于类别数。</li><li>**连接权重与偏置：**每个神经元的输入通过权重进行加权求和，并加上偏置值，然后通过激活函数传递。</li></ul><h3 id="循环神经网络（Recurrent-Neural-Network-RNN）"><a href="#循环神经网络（Recurrent-Neural-Network-RNN）" class="headerlink" title="循环神经网络（Recurrent Neural Network, RNN）"></a>循环神经网络（Recurrent Neural Network, RNN）</h3><p>循环神经网络（Recurrent Neural Network, RNN）络是一类<strong>专门处理序列数据</strong>的神经网络，能够捕获输入数据中<strong>时间或顺序信息的依赖</strong>关系。</p><p>RNN 的特别之处在于它具有”<strong>记忆</strong>能力”，可以在网络的隐藏状态中保存之前时间步的信息。</p><p>循环神经网络用于<strong>处理随时间变化的数据模式</strong>。</p><p>在 RNN 中，<strong>相同的层被用来接收</strong>输入参数，并在<strong>指定的神经网络中显示输出</strong>参数。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/0_xs3Dya3qQBx6IU7C.png" alt="img"></p><p>PyTorch 提供了强大的工具来构建和训练神经网络。</p><p>神经网络在 PyTorch 中是通过 <strong>torch.nn</strong> 模块来实现的。</p><p><strong>torch.nn</strong> 模块提供了各种网络层（如全连接层、卷积层等）、损失函数和优化器，让神经网络的构建和训练变得更加方便。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_3DUs-90altOgaBcVJ9LTGg.png" alt="img"></p><p>在 PyTorch 中，构建神经网络通常需要继承 nn.Module 类。</p><p>nn.Module 是所有神经网络模块的基类，你需要定义以下两个部分：</p><ul><li><strong><code>__init__()</code></strong>：定义网络层。</li><li><strong><code>forward()</code></strong>：定义数据的前向传播过程。</li></ul><p>简单的全连接神经网络（Fully Connected Network）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># 定义一个简单的神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个输入层到隐藏层的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 输入 2 个特征，输出 2 个特征</span></span><br><span class="line">        <span class="comment"># 定义一个隐藏层到输出层的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 输入 2 个特征，输出 1 个预测值    </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 前向传播过程</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># 使用 ReLU 激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)  <span class="comment"># 输出层</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"><span class="comment"># 打印模型</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SimpleNN(</span><br><span class="line">  (fc1): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc2): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>PyTorch 提供了许多常见的神经网络层，以下是几个常见的：</p><ul><li><strong><code>nn.Linear(in_features, out_features)</code></strong>：<strong>全连接层</strong>，输入 <code>in_features</code> 个特征，输出 <code>out_features</code> 个特征。</li><li><strong><code>nn.Conv2d(in_channels, out_channels, kernel_size)</code></strong>：<strong>2D 卷积层</strong>，用于图像处理。</li><li><strong><code>nn.MaxPool2d(kernel_size)</code></strong>：<strong>2D 最大池化</strong>层，用于降维。</li><li><strong><code>nn.ReLU()</code></strong>：ReLU <strong>激活函数</strong>，常用于隐藏层。</li><li><strong><code>nn.Softmax(dim)</code></strong>：Softmax 激活函数，<strong>通常用于输出层</strong>，适用于<strong>多类分类</strong>问题。</li></ul><h3 id="激活函数（Activation-Function）"><a href="#激活函数（Activation-Function）" class="headerlink" title="激活函数（Activation Function）"></a>激活函数（Activation Function）</h3><p>激活函数决定了神经元是否应该被激活。它们是<strong>非线性函数</strong>，使得神经网络能够学习和执行更复杂的任务。常见的激活函数包括：</p><ul><li><strong>Sigmoid</strong>：用于二分类问题，输出值在 0 和 1 之间。</li><li><strong>Tanh</strong>：输出值在 -1 和 1 之间，常用于输出层之前。</li><li><strong>ReLU</strong>（Rectified Linear Unit）：目前最流行的激活函数之一，定义为 <code>f(x) = max(0, x)</code>，有助于<strong>解决梯度消失问题</strong>。</li><li><strong>Softmax</strong>：常用于多分类问题的输出层，将输出转换为<strong>概率分布</strong>。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># ReLU 激活</span></span><br><span class="line">output = F.relu(input_tensor)</span><br><span class="line"><span class="comment"># Sigmoid 激活</span></span><br><span class="line">output = torch.sigmoid(input_tensor)</span><br><span class="line"><span class="comment"># Tanh 激活</span></span><br><span class="line">output = torch.tanh(input_tensor)</span><br></pre></td></tr></table></figure><h3 id="损失函数（Loss-Function）"><a href="#损失函数（Loss-Function）" class="headerlink" title="损失函数（Loss Function）"></a>损失函数（Loss Function）</h3><p>损失函数用于衡量模型的预测值与真实值之间的差异。</p><p>常见的损失函数包括：</p><ul><li><strong>均方误差（MSELoss）</strong>：<strong>回归</strong>问题常用，计算输出与目标值的<strong>平方差</strong>。</li><li><strong>交叉熵损失（CrossEntropyLoss）</strong>：<strong>分类</strong>问题常用，计算输出和真实标签之间的<strong>交叉熵</strong>。</li><li><strong>BCEWithLogitsLoss</strong>：<strong>二分类</strong>问题，结合了 Sigmoid 激活和二元交叉熵损失。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 均方误差损失</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"><span class="comment"># 交叉熵损失</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 二分类交叉熵损失</span></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br></pre></td></tr></table></figure><h3 id="优化器（Optimizer）"><a href="#优化器（Optimizer）" class="headerlink" title="优化器（Optimizer）"></a>优化器（Optimizer）</h3><p>优化器负责在训练过程中更新网络的权重和偏置。</p><p>常见的优化器包括：</p><ul><li><strong>SGD</strong>（随机梯度下降）</li><li><strong>Adam</strong>（自适应矩估计）</li><li><strong>RMSprop</strong>（均方根传播）</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 使用 SGD 优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 使用 Adam 优化器</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><h3 id="训练过程（Training-Process）"><a href="#训练过程（Training-Process）" class="headerlink" title="训练过程（Training Process）"></a>训练过程（Training Process）</h3><p>训练神经网络涉及以下步骤：</p><ol><li><strong>准备数据</strong>：通过 <code>DataLoader</code> 加载数据。</li><li><strong>定义损失函数和优化器</strong>。</li><li><strong>前向传播</strong>：计算模型的输出。</li><li><strong>计算损失</strong>：与目标进行比较，得到损失值。</li><li><strong>反向传播</strong>：通过 <code>loss.backward()</code> 计算梯度。</li><li><strong>更新参数</strong>：通过 <code>optimizer.step()</code> 更新模型的参数。</li><li><strong>重复上述步骤</strong>，直到达到预定的训练轮数。</li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设已经定义好了模型、损失函数和优化器</span></span><br><span class="line"><span class="comment"># 训练数据示例</span></span><br><span class="line">X = torch.randn(<span class="number">10</span>, <span class="number">2</span>)  <span class="comment"># 10 个样本，每个样本有 2 个特征</span></span><br><span class="line">Y = torch.randn(<span class="number">10</span>, <span class="number">1</span>)  <span class="comment"># 10 个目标标签</span></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 训练 100 轮</span></span><br><span class="line">    model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清除梯度</span></span><br><span class="line">    output = model(X)  <span class="comment"># 前向传播</span></span><br><span class="line">    loss = criterion(output, Y)  <span class="comment"># 计算损失</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新权重</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:  <span class="comment"># 每 10 轮输出一次损失</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/100], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="测试与评估"><a href="#测试与评估" class="headerlink" title="测试与评估"></a>测试与评估</h3><p>训练完成后，需要对模型进行测试和评估。</p><p>常见的步骤包括：</p><ul><li><strong>计算测试集的损失</strong>：测试模型在未见过的数据上的表现。</li><li><strong>计算准确率（Accuracy）</strong>：对于分类问题，计算正确预测的比例。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设你有测试集 X_test 和 Y_test</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 在评估过程中禁用梯度计算</span></span><br><span class="line">    output = model(X_test)</span><br><span class="line">    loss = criterion(output, Y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Test Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="神经网络类型"><a href="#神经网络类型" class="headerlink" title="神经网络类型"></a>神经网络类型</h3><ol><li><strong>前馈神经网络（Feedforward Neural Networks）</strong>：数据<strong>单向流动</strong>，从输入层到输出层，无反馈连接。</li><li><strong>卷积神经网络（Convolutional Neural Networks, CNNs）</strong>：适用于<strong>图像处理</strong>，使用卷积层<strong>提取空间特征</strong>。</li><li><strong>循环神经网络（Recurrent Neural Networks, RNNs）</strong>：适用于<strong>序列数据</strong>，如<strong>时间序列分析</strong>和<strong>自然语言处理</strong>，允许信息反馈循环。</li><li><strong>长短期记忆网络（Long Short-Term Memory, LSTM）</strong>：一种特殊的RNN，能够<strong>学习长期依赖关系</strong>。</li></ol><h1 id="PyTorch-第一个神经网络"><a href="#PyTorch-第一个神经网络" class="headerlink" title="PyTorch 第一个神经网络"></a>PyTorch 第一个神经网络</h1><p>本章节我们将介绍如何用 PyTorch 实现一个<strong>简单的前馈神经网络</strong>，完成一个<strong>二分类</strong>任务。</p><p>以下实例展示了如何使用 PyTorch 实现一个简单的神经网络进行二分类任务训练。</p><p>网络结构包括输入层、隐藏层和输出层，使用了 ReLU 激活函数和 Sigmoid 激活函数。</p><p>采用了均方误差损失函数和随机梯度下降优化器。</p><p>训练过程是通过前向传播、计算损失、反向传播和参数更新来逐步调整模型参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入PyTorch库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># 定义输入层大小、隐藏层大小、输出层大小和批量大小</span></span><br><span class="line">n_in, n_h, n_out, batch_size = <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># 创建虚拟输入数据和目标数据</span></span><br><span class="line">x = torch.randn(batch_size, n_in)  <span class="comment"># 随机生成输入数据</span></span><br><span class="line">y = torch.tensor([[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], </span><br><span class="line">                 [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>]])  <span class="comment"># 目标输出数据</span></span><br><span class="line"><span class="comment"># 创建顺序模型，包含线性层、ReLU激活函数和Sigmoid激活函数</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">   nn.Linear(n_in, n_h),  <span class="comment"># 输入层到隐藏层的线性变换</span></span><br><span class="line">   nn.ReLU(),            <span class="comment"># 隐藏层的ReLU激活函数</span></span><br><span class="line">   nn.Linear(n_h, n_out),  <span class="comment"># 隐藏层到输出层的线性变换</span></span><br><span class="line">   nn.Sigmoid()           <span class="comment"># 输出层的Sigmoid激活函数</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 定义均方误差损失函数和随机梯度下降优化器</span></span><br><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 学习率为0.01</span></span><br><span class="line"><span class="comment"># 执行梯度下降算法进行模型训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):  <span class="comment"># 迭代50次</span></span><br><span class="line">   y_pred = model(x)  <span class="comment"># 前向传播，计算预测值</span></span><br><span class="line">   loss = criterion(y_pred, y)  <span class="comment"># 计算损失</span></span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span>, epoch, <span class="string">&#x27;loss: &#x27;</span>, loss.item())  <span class="comment"># 打印损失值</span></span><br><span class="line">   optimizer.zero_grad()  <span class="comment"># 清零梯度</span></span><br><span class="line">   loss.backward()  <span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">   optimizer.step()  <span class="comment"># 更新模型参数</span></span><br></pre></td></tr></table></figure><p><strong>定义网络参数：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n_in, n_h, n_out, batch_size = 10, 5, 1, 10</span><br></pre></td></tr></table></figure><ul><li><code>n_in</code>：输入层大小为 10，即<strong>每个数据点有 10 个特征</strong>。</li><li><code>n_h</code>：隐藏层大小为 5，即<strong>隐藏层包含 5 个神经元</strong>。</li><li><code>n_out</code>：输出层大小为 1，即<strong>输出一个标量</strong>，表示<strong>二分类结果</strong>（0 或 1）。</li><li><code>batch_size</code>：每个批次包含 <strong>10 个样本</strong>。</li></ul><p>生成输入数据和目标数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(batch_size, n_in)  # 随机生成输入数据</span><br><span class="line">y = torch.tensor([[1.0], [0.0], [0.0], </span><br><span class="line">                 [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])  # 目标输出数据</span><br></pre></td></tr></table></figure><ul><li><code>x</code>：随机生成一个<strong>形状为 <code>(10, 10)</code> 的输入数据矩阵</strong>，表示 10 个样本，每个样本有 10 个特征。</li><li><code>y</code>：目标输出数据（标签），表示每个输入样本的类别标签（0 或 1），是一个 10×1 的张量。</li></ul><p><strong>定义神经网络模型：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">   nn.Linear(n_in, n_h),  # 输入层到隐藏层的线性变换</span><br><span class="line">   nn.ReLU(),            # 隐藏层的ReLU激活函数</span><br><span class="line">   nn.Linear(n_h, n_out),  # 隐藏层到输出层的线性变换</span><br><span class="line">   nn.Sigmoid()           # 输出层的Sigmoid激活函数</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><code>nn.Sequential</code> 用于<strong>按顺序定义</strong>网络层。</p><ul><li><code>nn.Linear(n_in, n_h)</code>：定义<strong>输入层到隐藏层的线性变换</strong>，<strong>输入特征</strong>是 10 个，<strong>隐藏层</strong>有 5 个神经元。</li><li><code>nn.ReLU()</code>：在隐藏层后<strong>添加 ReLU 激活函数</strong>，<strong>增加非线性</strong>。</li><li><code>nn.Linear(n_h, n_out)</code>：定义隐藏层到输出层的线性变换，<strong>输出为 1 个神经元</strong>。</li><li><code>nn.Sigmoid()</code>：输出层使用 <strong>Sigmoid 激活函数</strong>，将<strong>结果映射到 0 到 1 之间</strong>，用于二分类任务。</li></ul><p><strong>定义损失函数和优化器：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.MSELoss()  # 使用均方误差损失函数</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # 使用随机梯度下降优化器，学习率为 0.01</span><br></pre></td></tr></table></figure><p><strong>训练循环：</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):  <span class="comment"># 训练50轮</span></span><br><span class="line">   y_pred = model(x)  <span class="comment"># 前向传播，计算预测值</span></span><br><span class="line">   loss = criterion(y_pred, y)  <span class="comment"># 计算损失</span></span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span>, epoch, <span class="string">&#x27;loss: &#x27;</span>, loss.item())  <span class="comment"># 打印损失值</span></span><br><span class="line">   optimizer.zero_grad()  <span class="comment"># 清零梯度</span></span><br><span class="line">   loss.backward()  <span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">   optimizer.step()  <span class="comment"># 更新模型参数</span></span><br></pre></td></tr></table></figure><ul><li><code>for epoch in range(50)</code>：进行 <strong>50 次训练迭代</strong>。</li><li><code>y_pred = model(x)</code>：进行<strong>前向</strong>传播，使用<strong>当前模型参数计算输入数据 <code>x</code> 的预测值</strong>。</li><li><code>loss = criterion(y_pred, y)</code>：计算<strong>预测值和目标值 <code>y</code></strong> 之间的<strong>损失</strong>。</li><li><code>optimizer.zero_grad()</code>：<strong>清除上一轮</strong>训练时的<strong>梯度值</strong>。</li><li><code>loss.backward()</code>：<strong>反向</strong>传播，计算<strong>损失函数相对于模型参数的梯度</strong>。</li><li><code>optimizer.step()</code><strong>：根据计算出的梯度更新</strong>模型参数。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 定义输入层大小、隐藏层大小、输出层大小和批量大小</span></span><br><span class="line">n_in, n_h, n_out, batch_size = <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># 创建虚拟输入数据和目标数据</span></span><br><span class="line">x = torch.randn(batch_size, n_in)  <span class="comment"># 随机生成输入数据</span></span><br><span class="line">y = torch.tensor([[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], </span><br><span class="line">                  [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>]])  <span class="comment"># 目标输出数据</span></span><br><span class="line"><span class="comment"># 创建顺序模型，包含线性层、ReLU激活函数和Sigmoid激活函数</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Linear(n_in, n_h),  <span class="comment"># 输入层到隐藏层的线性变换</span></span><br><span class="line">    nn.ReLU(),            <span class="comment"># 隐藏层的ReLU激活函数</span></span><br><span class="line">    nn.Linear(n_h, n_out),  <span class="comment"># 隐藏层到输出层的线性变换</span></span><br><span class="line">    nn.Sigmoid()           <span class="comment"># 输出层的Sigmoid激活函数</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 定义均方误差损失函数和随机梯度下降优化器</span></span><br><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 学习率为0.01</span></span><br><span class="line"><span class="comment"># 用于存储每轮的损失值</span></span><br><span class="line">losses = []</span><br><span class="line"><span class="comment"># 执行梯度下降算法进行模型训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):  <span class="comment"># 迭代50次</span></span><br><span class="line">    y_pred = model(x)  <span class="comment"># 前向传播，计算预测值</span></span><br><span class="line">    loss = criterion(y_pred, y)  <span class="comment"># 计算损失</span></span><br><span class="line">    losses.append(loss.item())  <span class="comment"># 记录损失值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/50], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)  <span class="comment"># 打印损失值</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清零梯度</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新模型参数</span></span><br><span class="line"><span class="comment"># 可视化损失变化曲线</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">51</span>), losses, label=<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss Over Epochs&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 可视化预测结果与实际目标值对比</span></span><br><span class="line">y_pred_final = model(x).detach().numpy()  <span class="comment"># 最终预测值</span></span><br><span class="line">y_actual = y.numpy()  <span class="comment"># 实际值</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, batch_size + <span class="number">1</span>), y_actual, <span class="string">&#x27;o-&#x27;</span>, label=<span class="string">&#x27;Actual&#x27;</span>, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, batch_size + <span class="number">1</span>), y_pred_final, <span class="string">&#x27;x--&#x27;</span>, label=<span class="string">&#x27;Predicted&#x27;</span>, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Sample Index&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Value&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Actual vs Predicted Values&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/first_n_runoob_1.png" alt="img"></p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/a7d9d307c8172f34fc906368ab8cf0fe.png" alt="a7d9d307c8172f34fc906368ab8cf0fe"></p><h2 id="另外一个实例"><a href="#另外一个实例" class="headerlink" title="另外一个实例"></a>另外一个实例</h2><p>我们假设有一个二维数据集，目标是根据点的位置将它们<strong>分类到两个类别中</strong>（例如，红色和蓝色点）。</p><p>以下实例展示了如何使用神经网络完成<strong>简单的二分类</strong>任务，为更复杂的任务奠定了基础，通过 PyTorch 的<strong>模块化接口</strong>，神经网络的构建、训练和可视化都非常直观。</p><h3 id="1、数据准备"><a href="#1、数据准备" class="headerlink" title="1、数据准备"></a>1、数据准备</h3><p>首先，我们生成一些简单的二维数据：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 生成一些随机数据</span></span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">data = torch.randn(n_samples, <span class="number">2</span>)  <span class="comment"># 生成 100 个二维数据点</span></span><br><span class="line">labels = (data[:, <span class="number">0</span>]**<span class="number">2</span> + data[:, <span class="number">1</span>]**<span class="number">2</span> &lt; <span class="number">1</span>).<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)  <span class="comment"># 点在圆内为1，圆外为0</span></span><br><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line">plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], c=labels.squeeze(), cmap=<span class="string">&#x27;coolwarm&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Generated Data&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 2&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>数据说明：</strong></p><ul><li><code>data</code> 是输入的二维点，每个点有两个特征。</li><li><code>labels</code> 是目标分类，点在圆形区域内为 1，否则为 0。</li></ul><p>显示如下：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/fnn_runoob_1.png" alt="img"></p><h3 id="2、定义神经网络"><a href="#2、定义神经网络" class="headerlink" title="2、定义神经网络"></a>2、定义神经网络</h3><p>用 PyTorch 创建一个简单的前馈神经网络。</p><p>前馈神经网络使用了一层隐藏层，通过简单的线性变换和激活函数捕获数据的非线性模式。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义神经网络的层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">4</span>)  <span class="comment"># 输入层有 2 个特征，隐藏层有 4 个神经元</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">4</span>, <span class="number">1</span>)  <span class="comment"># 隐藏层输出到 1 个神经元（用于二分类）</span></span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = nn.Sigmoid()  <span class="comment"># 二分类激活函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># 使用 ReLU 激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.sigmoid(<span class="variable language_">self</span>.fc2(x))  <span class="comment"># 输出层使用 Sigmoid 激活函数</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = SimpleNN()</span><br></pre></td></tr></table></figure><h3 id="3、定义损失函数和优化器"><a href="#3、定义损失函数和优化器" class="headerlink" title="3、定义损失函数和优化器"></a>3、定义损失函数和优化器</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义二分类的损失函数和优化器</span></span><br><span class="line">criterion = nn.BCELoss()  <span class="comment"># 二元交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)  <span class="comment"># 使用随机梯度下降优化器</span></span><br></pre></td></tr></table></figure><h3 id="4、训练模型"><a href="#4、训练模型" class="headerlink" title="4、训练模型"></a>4、训练模型</h3><p>用数据训练模型，让它<strong>学会分类</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    outputs = model(data)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment"># 每 10 轮打印一次损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="5、测试模型并可视化结果"><a href="#5、测试模型并可视化结果" class="headerlink" title="5、测试模型并可视化结果"></a>5、测试模型并可视化结果</h3><p>我们测试模型，并在<strong>图像上绘制决策边界</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化决策边界</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_decision_boundary</span>(<span class="params">model, data</span>):</span><br><span class="line">    x_min, x_max = data[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span>, data[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = data[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span>, data[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    xx, yy = torch.meshgrid(torch.arange(x_min, x_max, <span class="number">0.1</span>), torch.arange(y_min, y_max, <span class="number">0.1</span>), indexing=<span class="string">&#x27;ij&#x27;</span>)</span><br><span class="line">    grid = torch.cat([xx.reshape(-<span class="number">1</span>, <span class="number">1</span>), yy.reshape(-<span class="number">1</span>, <span class="number">1</span>)], dim=<span class="number">1</span>)</span><br><span class="line">    predictions = model(grid).detach().numpy().reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, predictions, levels=[<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1</span>], cmap=<span class="string">&#x27;coolwarm&#x27;</span>, alpha=<span class="number">0.7</span>)</span><br><span class="line">    plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], c=labels.squeeze(), cmap=<span class="string">&#x27;coolwarm&#x27;</span>, edgecolors=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Decision Boundary&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(model, data)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epoch [10/100], Loss: 0.5247</span><br><span class="line">Epoch [20/100], Loss: 0.3142</span><br><span class="line">...</span><br><span class="line">Epoch [100/100], Loss: 0.0957</span><br></pre></td></tr></table></figure><p>图中显示了原始数据点（红色和蓝色），以及<strong>模型学习到的分类边界</strong>。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/fnn_runoob_3.png" alt="img"></p><h1 id="PyTorch-数据处理与加载"><a href="#PyTorch-数据处理与加载" class="headerlink" title="PyTorch 数据处理与加载"></a>PyTorch 数据处理与加载</h1><p>在 PyTorch 中，处理和加载数据是深度学习训练过程中的关键步骤。</p><p>为了高效地处理数据，PyTorch 提供了强大的工具，包括 <strong>torch.utils.data.Dataset</strong> 和 <strong>torch.utils.data.DataLoader</strong>，帮助我们<strong>管理数据集</strong>、<strong>批量加载</strong>和<strong>数据增强</strong>等任务。</p><p>PyTorch 数据处理与加载的介绍：</p><ul><li><strong>自定义 Dataset</strong>：通过<strong>继承 <code>torch.utils.data.Dataset</code> 来加载自己的</strong>数据集。</li><li><strong>DataLoader</strong>：<code>DataLoader</code> <strong>按批次加载</strong>数据，支持多线程加载并进行数据<strong>打乱</strong>。</li><li><strong>数据预处理与增强</strong>：使用 <code>torchvision.transforms</code> 进行常见的图像<strong>预处理和增强</strong>操作，提高模型的<strong>泛化能力</strong>。</li><li><strong>加载标准数据集</strong>：<code>torchvision.datasets</code> 提供了许多<strong>常见的数据集</strong>，简化了数据加载过程。</li><li><strong>多个数据源</strong>：通过组合多个 <code>Dataset</code> 实例来处理来自不同来源的数据。</li></ul><h2 id="自定义-Dataset"><a href="#自定义-Dataset" class="headerlink" title="自定义 Dataset"></a>自定义 Dataset</h2><p><strong>torch.utils.data.Dataset</strong> 是一个抽象类，允许你从自己的数据源中创建数据集。</p><p>我们需要继承该类并实现以下两个方法：</p><ul><li><code>__len__(self)</code>：返回数据集中的样本数量。</li><li><code>__getitem__(self, idx)</code>：通过索引返回一个样本。</li></ul><p>假设我们有一个简单的 CSV 文件或一些列表数据，我们可以通过继承 Dataset 类来创建自己的数据集。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="comment"># 自定义数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X_data, Y_data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化数据集，X_data 和 Y_data 是两个列表或数组</span></span><br><span class="line"><span class="string">        X_data: 输入特征</span></span><br><span class="line"><span class="string">        Y_data: 目标标签</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.X_data = X_data</span><br><span class="line">        <span class="variable language_">self</span>.Y_data = Y_data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回数据集的大小&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.X_data)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回指定索引的数据&quot;&quot;&quot;</span></span><br><span class="line">        x = torch.tensor(<span class="variable language_">self</span>.X_data[idx], dtype=torch.float32)  <span class="comment"># 转换为 Tensor</span></span><br><span class="line">        y = torch.tensor(<span class="variable language_">self</span>.Y_data[idx], dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">X_data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]  <span class="comment"># 输入特征</span></span><br><span class="line">Y_data = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># 目标标签</span></span><br><span class="line"><span class="comment"># 创建数据集实例</span></span><br><span class="line">dataset = MyDataset(X_data, Y_data)</span><br></pre></td></tr></table></figure><h2 id="使用-DataLoader-加载数据"><a href="#使用-DataLoader-加载数据" class="headerlink" title="使用 DataLoader 加载数据"></a>使用 DataLoader 加载数据</h2><p>DataLoader 是 PyTorch 提供的一个重要工具，用于从 Dataset 中**按批次（batch）**加载数据。</p><p>DataLoader 允许我们批量读取数据并进行多线程加载，从而提高训练效率。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment"># 创建 DataLoader 实例，batch_size 设置每次加载的样本数量</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 打印加载的数据</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Batch <span class="subst">&#123;batch_idx + <span class="number">1</span>&#125;</span>:&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Inputs: <span class="subst">&#123;inputs&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Labels: <span class="subst">&#123;labels&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li><strong><code>batch_size</code></strong>: 每次加载的<strong>样本数量</strong>。</li><li><strong><code>shuffle</code></strong>: <strong>是否</strong>对数据进行<strong>洗牌</strong>，通常训练时需要将数据打乱。</li><li><strong><code>drop_last</code></strong>: 如果数据集中的样本数<strong>不能被 <code>batch_size</code> 整除</strong>，设置为 <code>True</code> 时，<strong>丢弃最后一个</strong>不完整的 batch。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Batch <span class="number">1</span>:</span><br><span class="line">Inputs: tensor([[<span class="number">3.</span>, <span class="number">4.</span>], [<span class="number">1.</span>, <span class="number">2.</span>]])</span><br><span class="line">Labels: tensor([<span class="number">0.</span>, <span class="number">1.</span>])</span><br><span class="line">Batch <span class="number">2</span>:</span><br><span class="line">Inputs: tensor([[<span class="number">7.</span>, <span class="number">8.</span>], [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">Labels: tensor([<span class="number">0.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure><p>每次循环中，DataLoader 会返回一个批次的数据，包括<strong>输入特征（inputs）<strong>和</strong>目标标签（labels）</strong>。</p><h2 id="预处理与数据增强"><a href="#预处理与数据增强" class="headerlink" title="预处理与数据增强"></a>预处理与数据增强</h2><p>数据预处理和增强对于提高模型的性能至关重要。</p><p>PyTorch 提供了 torchvision.transforms 模块来进行常见的图像预处理和增强操作，如旋转、裁剪、归一化等。</p><p>常见的图像预处理操作:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="comment"># 定义数据预处理的流水线</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),  <span class="comment"># 将图像调整为 128x128</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为张量</span></span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># 加载图像</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;image.jpg&#x27;</span>)</span><br><span class="line"><span class="comment"># 应用预处理</span></span><br><span class="line">image_tensor = transform(image)</span><br><span class="line"><span class="built_in">print</span>(image_tensor.shape)  <span class="comment"># 输出张量的形状</span></span><br></pre></td></tr></table></figure><ul><li><strong><code>transforms.Compose()</code></strong>：将多个变换操作组合在一起。</li><li><strong><code>transforms.Resize()</code></strong>：调整图像大小。</li><li><strong><code>transforms.ToTensor()</code></strong>：将图像转换为 PyTorch 张量，值会被归一化到 <code>[0, 1]</code> 范围。</li><li><strong><code>transforms.Normalize()</code></strong>：标准化图像数据，通常使用预训练模型时需要进行标准化处理。</li></ul><h3 id="图像数据增强"><a href="#图像数据增强" class="headerlink" title="图像数据增强"></a>图像数据增强</h3><p>数据增强技术通过对训练数据进行随机变换，增加数据的多样性，帮助模型更好地泛化。例如，随机翻转、旋转、裁剪等。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(),  <span class="comment"># 随机水平翻转</span></span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),  <span class="comment"># 随机旋转 30 度</span></span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">128</span>),  <span class="comment"># 随机裁剪并调整为 128x128</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>这些数据增强方法可以通过 transforms.Compose() 组合使用，保证每个图像在训练时具有不同的变换。</p><h2 id="加载图像数据集"><a href="#加载图像数据集" class="headerlink" title="加载图像数据集"></a>加载图像数据集</h2><p>对于图像数据集，torchvision.datasets 提供了许多<strong>常见数据集</strong>（如 CIFAR-10、ImageNet、MNIST 等）以及用于加载图像数据的工具。</p><p>加载 MNIST 数据集:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="comment"># 定义预处理操作</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 对灰度图像进行标准化</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># 下载并加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"><span class="comment"># 创建 DataLoader</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 迭代训练数据</span></span><br><span class="line"><span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="built_in">print</span>(inputs.shape)  <span class="comment"># 每个批次的输入数据形状</span></span><br><span class="line">    <span class="built_in">print</span>(labels.shape)  <span class="comment"># 每个批次的标签形状</span></span><br></pre></td></tr></table></figure><ul><li><code>datasets.MNIST()</code> 会自动<strong>下载</strong> MNIST 数据集<strong>并加载</strong>。</li><li><code>transform</code> 参数允许我们对数据进行<strong>预处理</strong>。</li><li><code>train=True</code> 和 <code>train=False</code> 分别表示<strong>训练</strong>集和<strong>测试</strong>集。</li></ul><h2 id="用多个数据源（Multi-source-Dataset）"><a href="#用多个数据源（Multi-source-Dataset）" class="headerlink" title="用多个数据源（Multi-source Dataset）"></a>用多个数据源（Multi-source Dataset）</h2><p>如果你的数据集由多个文件、多个来源（例如多个图像文件夹）组成，可以通过<strong>继承 Dataset 类自定义加载多个</strong>数据源。</p><p>PyTorch 提供了 <strong>ConcatDataset 和 ChainDataset 等类</strong>来连接多个数据集。</p><p>例如，假设我们有多个图像文件夹的数据，可以将它们合并为一个数据集：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> ConcatDataset</span><br><span class="line"><span class="comment"># 假设 dataset1 和 dataset2 是两个 Dataset 对象</span></span><br><span class="line">combined_dataset = ConcatDataset([dataset1, dataset2])</span><br><span class="line">combined_loader = DataLoader(combined_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h1 id="PyTorch-线性回归"><a href="#PyTorch-线性回归" class="headerlink" title="PyTorch 线性回归"></a>PyTorch 线性回归</h1><p>线性回归是最基本的机器学习算法之一，用于<strong>预测一个连续值</strong>。</p><p>线性回归是一种<strong>简单且常见的回归分析方法</strong>，目的是通过<strong>拟合一个线性函数来预测输出</strong>。</p><p>对于一个简单的线性回归问题，模型可以表示为：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/lg-1.png" alt="img"></p><ul><li>y 是预测值（目标值）。</li><li>x1，x2，xn 是输入特征。</li><li>w1，w2，wn是待学习的权重（模型参数）。</li><li>b 是偏置项。</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Linear_regression.svg.png" alt="img"></p><p>在 PyTorch 中，线性回归模型可以通过<strong>继承 nn.Module 类</strong>来实现。我们将通过一个简单的示例来详细说明如何使用 PyTorch 实现线性回归模型。</p><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>我们首先准备一些假数据，用于训练我们的线性回归模型。这里，我们可以生成一个简单的线性关系的数据集，其中每个样本有两个特征 x1，x2。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 随机种子，确保每次运行结果一致</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line">X = torch.randn(<span class="number">100</span>, <span class="number">2</span>)  <span class="comment"># 100 个样本，每个样本 2 个特征</span></span><br><span class="line">true_w = torch.tensor([<span class="number">2.0</span>, <span class="number">3.0</span>])  <span class="comment"># 假设真实权重</span></span><br><span class="line">true_b = <span class="number">4.0</span>  <span class="comment"># 偏置项</span></span><br><span class="line">Y = X @ true_w + true_b + torch.randn(<span class="number">100</span>) * <span class="number">0.1</span>  <span class="comment"># 加入一些噪声</span></span><br><span class="line"><span class="comment"># 打印部分数据</span></span><br><span class="line"><span class="built_in">print</span>(X[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(Y[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.9269,  1.4873],</span><br><span class="line">        [ 0.9007, -2.1055],</span><br><span class="line">        [ 0.6784, -1.2345],</span><br><span class="line">        [-0.0431, -1.6047],</span><br><span class="line">        [-0.7521,  1.6487]])</span><br><span class="line">tensor([12.4460, -0.4663,  1.7666, -0.9357,  7.4781])</span><br></pre></td></tr></table></figure><p>这段代码创建了一个带有噪声的线性数据集，输入 X 为 100x2 的矩阵，每个样本有<strong>两个特征</strong>，输出 Y 由真实的权重和偏置生成，并<strong>加上了一些随机噪声</strong>。</p><h3 id="定义线性回归模型"><a href="#定义线性回归模型" class="headerlink" title="定义线性回归模型"></a>定义线性回归模型</h3><p>我们可以通过继承 <code>nn.Module</code> 来定义一个简单的线性回归模型。在 PyTorch 中，线性回归的核心是 <code>nn.Linear()</code> 层，它会自动处理权重和偏置的初始化。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearRegressionModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个线性层，输入为2个特征，输出为1个预测值</span></span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 输入维度2，输出维度1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x)  <span class="comment"># 前向传播，返回预测结果</span></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = LinearRegressionModel()</span><br></pre></td></tr></table></figure><p>这里的 <code>nn.Linear(2, 1)</code> 表示一个线性层，它有 2 个输入特征和 1 个输出。<code>forward</code> 方法定义了如何通过这个层进行前向传播。</p><h2 id="定义损失函数与优化器"><a href="#定义损失函数与优化器" class="headerlink" title="定义损失函数与优化器"></a>定义损失函数与优化器</h2><p>线性回归的常见损失函数是 <strong>均方误差损失（MSELoss）</strong>，用于衡量预测值与真实值之间的差异。PyTorch 中提供了现成的 MSELoss 函数。</p><p>我们将使用 <strong>SGD（随机梯度下降）</strong> 或 <strong>Adam</strong> 优化器来最小化损失函数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数（均方误差）</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"><span class="comment"># 优化器（使用 SGD 或 Adam）</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 学习率设置为0.01</span></span><br></pre></td></tr></table></figure><ul><li><strong><code>MSELoss</code></strong>：计算预测值与真实值的均方误差。</li><li><strong><code>SGD</code></strong>：使用随机梯度下降法更新参数。</li></ul><h3 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h3><p>在训练过程中，我们将执行以下步骤：</p><ol><li>使用输入数据 X 进行前向传播，得到预测值。</li><li>计算损失（预测值与实际值之间的差异）。</li><li>使用反向传播计算梯度。</li><li>更新模型参数（权重和偏置）。</li></ol><p>我们将<strong>训练模型 1000 轮</strong>，并在每 100 轮打印一次损失。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">num_epochs = <span class="number">1000</span>  <span class="comment"># 训练 1000 轮</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    predictions = model(X)  <span class="comment"># 模型输出预测值</span></span><br><span class="line">    loss = criterion(predictions.squeeze(), Y)  <span class="comment"># 计算损失（注意预测值需要压缩为1D）</span></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清空之前的梯度</span></span><br><span class="line">    loss.backward()  <span class="comment"># 计算梯度</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新模型参数</span></span><br><span class="line">    <span class="comment"># 打印损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/1000], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li><strong><code>predictions.squeeze()</code></strong>：我们在这里将模型的输出从 2D 张量压缩为 1D，因为目标值 <code>Y</code> 是一个一维数组。</li><li><strong><code>optimizer.zero_grad()</code></strong>：每次反向传播前需要清空之前的梯度。</li><li><strong><code>loss.backward()</code></strong>：计算梯度。</li><li><strong><code>optimizer.step()</code></strong>：更新权重和偏置。</li></ul><h3 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h3><p>训练完成后，我们可以通过查看<strong>模型的权重和偏置来评估模型的效果</strong>。我们还可以在新的数据上进行预测并<strong>与实际值进行比较</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看训练后的权重和偏置</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Predicted weight: <span class="subst">&#123;model.linear.weight.data.numpy()&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Predicted bias: <span class="subst">&#123;model.linear.bias.data.numpy()&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 在新数据上做预测</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 评估时不需要计算梯度</span></span><br><span class="line">    predictions = model(X)</span><br><span class="line"><span class="comment"># 可视化预测与实际值</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], Y, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;True values&#x27;</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], predictions, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Predictions&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li><strong><code>model.linear.weight.data</code></strong> 和 <strong><code>model.linear.bias.data</code></strong>：这些属性存储了模型的权重和偏置。</li><li><strong><code>torch.no_grad()</code></strong>：在评估模式下，不需要计算梯度，节省内存。</li></ul><h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p>在训练过程中，随着损失逐渐减小，我们希望最终的模型能够拟合我们生成的数据。通过查看训练后的权重和偏置，我们可以比较其与真实值（<code>true_w</code> 和 <code>true_b</code>）的差异。理论上，模型的输出权重应该接近 <code>true_w</code> 和 <code>true_b</code>。</p><p>在可视化的散点图中，蓝色点表示真实值，红色点表示模型的预测值。我们希望看到红色点与蓝色点尽可能接近，表明模型成功学习了数据的线性关系。</p><h1 id="PyTorch-卷积神经网络"><a href="#PyTorch-卷积神经网络" class="headerlink" title="PyTorch 卷积神经网络"></a>PyTorch 卷积神经网络</h1><p>PyTorch <strong>卷积神经网络 (Convolutional Neural Networks, CNN)</strong> 是一类<strong>专门用于处理具有网格状拓扑结构数据（如图像）<strong>的</strong>深度学习</strong>模型。</p><p>CNN 是<strong>计算机视觉任务（如图像分类、目标检测和分割）的核心技术。</strong></p><p>下面这张图展示了一个典型的卷积神经网络（CNN）的结构和工作流程，用于图像识别任务。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1686919918947.jpeg" alt="img"></p><p>在图中，CNN 的输出层给出了三个类别的概率：Donald（0.2）、Goofy（0.1）和Tweety（0.7），这表明网络认为输入图像最有可能是 Tweety。</p><p>以下是各个部分的简要说明：</p><ul><li><strong>输入图像（Input Image）</strong>：网络接收的原始图像数据。</li><li><strong>卷积（Convolution）</strong>：使用<strong>卷积核（Kernel）在输入图像上滑动，提取特征</strong>，<strong>生成特征图（Feature Maps）</strong>。</li><li><strong>池化（Pooling）</strong>：通常在卷积层之后，通过<strong>最大池化或平均池化减少特征图的尺寸</strong>，同时<strong>保留重要特征，生成池化特征图（Pooled Feature Maps）</strong>。</li><li><strong>特征提取（Feature Extraction）</strong>：通过<strong>多个</strong>卷积和池化层的组合，<strong>逐步提取</strong>图像的<strong>高级</strong>特征。</li><li><strong>展平层（Flatten Layer）</strong>：将多维的特征图<strong>转换为一维向量</strong>，以便输入到全连接层。</li><li><strong>全连接层（Fully Connected Layer）</strong>：类似于传统的神经网络层，用于将提取的特征映射到输出类别。</li><li><strong>分类（Classification）</strong>：网络的输出层，根据全连接层的输出进行分类。</li><li><strong>概率分布（Probabilistic Distribution）</strong>：输出层给出每个类别的概率，表示<strong>输入图像属于各个类别的可能性。</strong></li></ul><h3 id="卷积神经网络的基本结构"><a href="#卷积神经网络的基本结构" class="headerlink" title="卷积神经网络的基本结构"></a>卷积神经网络的基本结构</h3><p><strong>1、输入层（Input Layer）</strong></p><p>接收原始图像数据，图像通常被表示为一个<strong>三维数组</strong>，其中两个维度代表图像的宽度和高度，第三个维度代表<strong>颜色通道</strong>（例如，RGB图像有三个通道）。</p><p><strong>2、卷积层（Convolutional Layer）</strong></p><p>用卷积核<strong>提取局部特征</strong>，如边缘、纹理等。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-cnn-1.png" alt="img"></p><ul><li>x：输入图像。</li><li>k：卷积核（权重矩阵）。</li><li>b：偏置。</li></ul><p>**应用一组可学习的滤波器（或卷积核）**在输入图像上进行卷积操作，以提取局部特征。</p><p>每个滤波器在输入图像上滑动，生成一个特征图（Feature Map），表示滤波器在不同位置的激活。</p><p>卷积层可以有多个滤波器，每个滤波器生成一个特征图，所有特征图组成一个特征图集合。</p><h3 id="3、激活函数（Activation-Function）"><a href="#3、激活函数（Activation-Function）" class="headerlink" title="3、激活函数（Activation Function）"></a>3、激活函数（Activation Function）</h3><p>通常在<strong>卷积层之后应用非线性激活函数，如 ReLU</strong>（Rectified Linear Unit），以引入非线性特性，使网络能够<strong>学习更复杂</strong>的模式。</p><p>ReLU 函数定义为 ：<strong>f(x)&#x3D;max(0,x)</strong>，即如果输入<strong>小于 0 则输出 0，否则输出输入值</strong>。</p><p><strong>4、池化层（Pooling Layer）</strong></p><ul><li>用于<strong>降低特征图的空间维度</strong>，减少计算量和参数数量，同时保留最重要的特征信息。</li><li>最常见的池化操作是最大池化（Max Pooling）和平均池化（Average Pooling）。</li><li>最大池化选择区域内的最大值，而平均池化计算区域内的平均值。</li></ul><p><strong>5、归一化层（Normalization Layer，可选）</strong></p><ul><li>例如，局部响应归一化（Local Response Normalization, LRN）或批归一化（Batch Normalization）。</li><li>这些层有助于<strong>加速训练过程，提高模型的稳定性</strong>。</li></ul><p><strong>6、全连接层（Fully Connected Layer）</strong></p><ul><li>在 <strong>CNN 的末端</strong>，将前面层提取的特征图展平（Flatten）成一维向量，然后输入到全连接层。</li><li>全连接层的每个神经元都与前一层的所有神经元相连，<strong>用于综合特征并进行最终的分类或回归</strong>。</li></ul><p><strong>7、输出层（Output Layer）</strong></p><p>根据任务的不同，输出层可以有不同的形式。</p><p>对于分类任务，通常使用 <strong>Softmax 函数将输出转换为概率分布</strong>，表示输入属于各个类别的概率。</p><p><strong>8、损失函数（Loss Function）</strong></p><p>用于衡量模型预测<strong>与真实标签之间的差异</strong>。</p><p>常见的损失函数包括<strong>交叉熵损失（Cross-Entropy Loss）<strong>用于</strong>多分类</strong>任务，<strong>均方误差（Mean Squared Error, MSE）<strong>用于</strong>回归</strong>任务。</p><p><strong>9、优化器（Optimizer）</strong></p><p>用于<strong>根据损失函数的梯度更新</strong>网络的权重。常见的优化器包括随机梯度下降（SGD）、Adam、RMSprop等。</p><p><strong>10、正则化（Regularization，可选）</strong></p><p>包括 Dropout、L1&#x2F;L2 正则化等技术，用于<strong>防止模型过拟合</strong>。</p><p><strong>这些层可以堆叠形成更深的网络结构，以提高模型的学习能力。</strong></p><p>CNN 的深度和复杂性可以根据任务的需求进行调整。</p><h2 id="PyTorch-实现一个-CNN-实例"><a href="#PyTorch-实现一个-CNN-实例" class="headerlink" title="PyTorch 实现一个 CNN 实例"></a>PyTorch 实现一个 CNN 实例</h2><p>以下示例展示如何用 PyTorch 构建一个简单的 CNN 模型，用于 MNIST 数据集的数字分类。</p><p>主要步骤：</p><ul><li><strong>数据加载与预处理</strong>：使用 <code>torchvision</code> 加载和预处理 MNIST 数据。</li><li><strong>模型构建</strong>：定义卷积层、池化层和全连接层。</li><li><strong>训练</strong>：通过损失函数和优化器进行模型训练。</li><li><strong>评估</strong>：测试集上计算模型的准确率。</li><li><strong>可视化</strong>：展示部分测试样本及其预测结果。</li></ul><h3 id="1、导入必要库"><a href="#1、导入必要库" class="headerlink" title="1、导入必要库"></a>1、导入必要库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.optim as optim</span><br></pre></td></tr></table></figure><h3 id="2、数据加载"><a href="#2、数据加载" class="headerlink" title="2、数据加载"></a>2、数据加载</h3><p>使用 torchvision 提供的 MNIST 数据集，<strong>加载和预处理</strong>数据。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="3、定义-CNN-模型"><a href="#3、定义-CNN-模型" class="headerlink" title="3、定义 CNN 模型"></a>3、定义 CNN 模型</h3><p>使用 <strong>nn.Module</strong> 构建一个 CNN。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层：输入1通道，输出32通道，卷积核大小3x3</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义卷积层：输入32通道，输出64通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)  <span class="comment"># 输入大小 = 特征图大小 * 通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)  <span class="comment"># 10 个类别</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))  <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))  <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>) <span class="comment"># 展平操作</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))    <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)            <span class="comment"># 全连接层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleCNN()</span><br></pre></td></tr></table></figure><h3 id="4、定义损失函数与优化器"><a href="#4、定义损失函数与优化器" class="headerlink" title="4、定义损失函数与优化器"></a>4、定义损失函数与优化器</h3><p>使用交叉熵损失和随机梯度下降优化器。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()  # 多分类交叉熵损失</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # 学习率和动量</span><br></pre></td></tr></table></figure><h3 id="5、训练模型"><a href="#5、训练模型" class="headerlink" title="5、训练模型"></a>5、训练模型</h3><p>训练模型 5 个 epoch，每个 epoch 后输出训练损失。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">model.train()  <span class="comment"># 设为训练模式</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="6、测试模型"><a href="#6、测试模型" class="headerlink" title="6、测试模型"></a>6、测试模型</h3><p>在测试集上评估模型的准确率。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 评估时不需要计算梯度</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)  <span class="comment"># 预测类别</span></span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 数据加载与预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 定义 CNN 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># 输入1通道，输出32通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># 输入32通道，输出64通道</span></span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)  <span class="comment"># 展平后输入到全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)  <span class="comment"># 10 个类别</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))  <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))  <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>) <span class="comment"># 展平</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))    <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)            <span class="comment"># 最后一层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleCNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 定义损失函数与优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 多分类交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 模型训练</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(images)  <span class="comment"># 前向传播</span></span><br><span class="line">        loss = criterion(outputs, labels)  <span class="comment"># 计算损失</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">        loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 模型测试</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 关闭梯度计算</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="运行结果说明"><a href="#运行结果说明" class="headerlink" title="运行结果说明"></a>运行结果说明</h3><p><strong>1. 输出的训练损失</strong></p><p>代码中每个 epoch 会输出一次平均损失，例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Epoch [1/5], Loss: 0.2325</span><br><span class="line">Epoch [2/5], Loss: 0.0526</span><br><span class="line">Epoch [3/5], Loss: 0.0366</span><br><span class="line">Epoch [4/5], Loss: 0.0273</span><br><span class="line">Epoch [5/5], Loss: 0.0221</span><br></pre></td></tr></table></figure><p>**解释：**损失逐渐下降表明模型在逐步收敛。</p><p><strong>2. 测试集的准确率</strong></p><p>代码在测试集上输出<strong>最终的分类准确率</strong>，例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Test Accuracy: 98.96%</span><br></pre></td></tr></table></figure><p>**解释：**模型对 MNIST 测试集的分类准确率为 98.96%，对于简单的 CNN 模型来说是一个不错的结果。</p><h3 id="7、可视化结果"><a href="#7、可视化结果" class="headerlink" title="7、可视化结果"></a>7、可视化结果</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 数据加载与预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 定义 CNN 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># 输入1通道，输出32通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># 输入32通道，输出64通道</span></span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)  <span class="comment"># 展平后输入到全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)  <span class="comment"># 10 个类别</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))  <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))  <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>) <span class="comment"># 展平</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))    <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)            <span class="comment"># 最后一层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleCNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 定义损失函数与优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 多分类交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 模型训练</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(images)  <span class="comment"># 前向传播</span></span><br><span class="line">        loss = criterion(outputs, labels)  <span class="comment"># 计算损失</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">        loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 模型测试</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 关闭梯度计算</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 可视化测试结果</span></span><br><span class="line">dataiter = <span class="built_in">iter</span>(test_loader)</span><br><span class="line">images, labels = <span class="built_in">next</span>(dataiter)</span><br><span class="line">outputs = model(images)</span><br><span class="line">_, predictions = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">6</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    axes[i].imshow(images[i][<span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    axes[i].set_title(<span class="string">f&quot;Label: <span class="subst">&#123;labels[i]&#125;</span>\nPred: <span class="subst">&#123;predictions[i]&#125;</span>&quot;</span>)</span><br><span class="line">    axes[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="PyTorch-循环神经网络（RNN）"><a href="#PyTorch-循环神经网络（RNN）" class="headerlink" title="PyTorch 循环神经网络（RNN）"></a>PyTorch 循环神经网络（RNN）</h1><p><strong>循环神经网络（Recurrent Neural Networks, RNN）是一类神经网络架构</strong>，专门用于<strong>处理序列数据</strong>，能够<strong>捕捉时间序列或有序数据的动态信息</strong>，能够处理序列数据，如文本、时间序列或音频。</p><p>RNN 在自然语言处理（NLP）、语音识别、时间序列预测等任务中有着广泛的应用。</p><p>RNN 的关键特性是其能够保持隐状态（hidden state），使得网络能够记住先前时间步的信息，这对于处理序列数据至关重要。</p><h3 id="RNN-的基本结构"><a href="#RNN-的基本结构" class="headerlink" title="RNN 的基本结构"></a>RNN 的基本结构</h3><p>在传统的前馈神经网络（Feedforward Neural Network）中，数据是<strong>从输入层流向输出层</strong>的，而在 RNN 中，数据不仅沿着网络层级流动，还会在每个时间步骤上传播到当前的隐层状态，从而将之前的信息传递到下一个时间步骤。</p><p><strong>隐状态（Hidden State）：</strong> RNN <strong>通过隐状态来记住序列中的信息</strong>。</p><p>隐状态是通过上一时间步的隐状态和当前输入共同计算得到的。</p><p>公式：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-rnn-1.png" alt="img"></p><ul><li>ht：当前时刻的隐状态。</li><li>ht-1：<strong>前一时刻</strong>的隐状态。</li><li>Xt：<strong>当前</strong>时刻的<strong>输入</strong>。</li><li>Whh、Wxh：<strong>权重矩阵</strong>。</li><li>b：偏置项。</li><li>f：<strong>激活函数（如 Tanh 或 ReLU）</strong>。</li></ul><p><strong>输出（Output）：</strong> RNN 的输出<strong>不仅依赖当前的输入，还依赖于隐状态的历史信息</strong>。</p><p>公式：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-rnn-2.png" alt="img"></p><ul><li>yt：当前时刻的隐状态。</li><li>Why：当前时刻的隐状态。</li></ul><h3 id="RNN-如何处理序列数据"><a href="#RNN-如何处理序列数据" class="headerlink" title="RNN 如何处理序列数据"></a>RNN 如何处理序列数据</h3><p>循环神经网络（RNN）在处理序列数据时的展开（unfold）视图如下：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_dznTsiaHCvRc70fxWWEcgw.png" alt="img"></p><p>RNN 是一种处理序列数据的神经网络，它通过<strong>循环连接来处理序列中的每个元素</strong>，并在每个时间步传递信息，以下是图中各部分的说明：</p><ul><li><strong>输入序列（Xt, Xt-1, Xt+1, …）</strong>：图中的粉色圆圈代表输入序列中的各个元素，如Xt表示当前时间步的输入，Xt-1表示前一个时间步的输入，以此类推。</li><li><strong>隐藏状态（ht, ht-1, ht+1, …）</strong>：绿色矩形代表<strong>RNN的隐藏状态</strong>，它在每个时间步存储有关序列的信息。ht是当前时间步的隐藏状态，ht-1是前一个时间步的隐藏状态。</li><li><strong>权重矩阵（U, W, V）</strong>：<ul><li><code>U</code>：<strong>输入到隐藏状态的权重矩阵</strong>，用于将输入<code>Xt</code><strong>转换为隐藏状态的一部分</strong>。</li><li><code>W</code>：<strong>隐藏状态到隐藏状态</strong>的权重矩阵，用于将<strong>前一时间步的隐藏状态<code>ht-1</code>转换为当前</strong>时间步隐藏状态的一部分。</li><li><code>V</code>：<strong>隐藏状态到输出的权重矩阵</strong>，用于将隐藏状态<code>ht</code>转换为输出<code>Yt</code>。</li></ul></li><li><strong>输出序列（Yt, Yt-1, Yt+1, …）</strong>：蓝色圆圈代表RNN在每个时间步的输出，如Yt是当前时间步的输出。</li><li><strong>循环连接</strong>：RNN的特点是<strong>隐藏状态的循环连接</strong>，这允许网络在<strong>处理当前时间步的输入时考虑到之前时间步的信息</strong>。</li><li><strong>展开（Unfold）</strong>：图中展示了RNN在序列上的展开过程，这有助于理解RNN如何<strong>在时间上处理序列数据</strong>。在实际的RNN实现中，这些步骤是并行处理的，但在概念上，我们可以将其展开来理解信息是如何流动的。</li><li><strong>信息流动</strong>：信息从输入序列通过权重矩阵U传递到隐藏状态，然后通过权重矩阵W在时间步之间传递，最后通过权重矩阵V从隐藏状态传递到输出序列。</li></ul><hr><h2 id="PyTorch-中的-RNN-基础"><a href="#PyTorch-中的-RNN-基础" class="headerlink" title="PyTorch 中的 RNN 基础"></a>PyTorch 中的 RNN 基础</h2><p>在 PyTorch 中，RNN 可以用于构建复杂的序列模型。</p><p>PyTorch 提供了几种 RNN 模块，包括：</p><ul><li><code>torch.nn.RNN</code>：<strong>基本的RNN单元。</strong></li><li><code>torch.nn.LSTM</code>：**长短期记忆单元，**能够学习长期依赖关系。</li><li><code>torch.nn.GRU</code>：门控循环单元，是<strong>LSTM的简化版本</strong>，但通常更容易训练。</li></ul><p>使用 RNN 类时，您需要指定输入的维度、隐藏层的维度以及其他一些超参数。</p><h3 id="PyTorch-实现一个简单的-RNN-实例"><a href="#PyTorch-实现一个简单的-RNN-实例" class="headerlink" title="PyTorch 实现一个简单的 RNN 实例"></a>PyTorch 实现一个简单的 RNN 实例</h3><p>以下是一个简单的 PyTorch 实现例子，使用 RNN 模型来处理序列数据并进行分类。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集：字符序列预测（Hello -&gt; Elloh）</span></span><br><span class="line">char_set = <span class="built_in">list</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">char_to_idx = &#123;c: i <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(char_set)&#125;</span><br><span class="line">idx_to_char = &#123;i: c <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(char_set)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备</span></span><br><span class="line">input_str = <span class="string">&quot;hello&quot;</span></span><br><span class="line">target_str = <span class="string">&quot;elloh&quot;</span></span><br><span class="line">input_data = [char_to_idx[c] <span class="keyword">for</span> c <span class="keyword">in</span> input_str]</span><br><span class="line">target_data = [char_to_idx[c] <span class="keyword">for</span> c <span class="keyword">in</span> target_str]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为独热编码</span></span><br><span class="line">input_one_hot = np.eye(<span class="built_in">len</span>(char_set))[input_data]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为 PyTorch Tensor</span></span><br><span class="line">inputs = torch.tensor(input_one_hot, dtype=torch.float32)</span><br><span class="line">targets = torch.tensor(target_data, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型超参数</span></span><br><span class="line">input_size = <span class="built_in">len</span>(char_set)</span><br><span class="line">hidden_size = <span class="number">8</span></span><br><span class="line">output_size = <span class="built_in">len</span>(char_set)</span><br><span class="line">num_epochs = <span class="number">200</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 RNN 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.RNN(input_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, hidden</span>):</span><br><span class="line">        out, hidden = <span class="variable language_">self</span>.rnn(x, hidden)</span><br><span class="line">        out = <span class="variable language_">self</span>.fc(out)  <span class="comment"># 应用全连接层</span></span><br><span class="line">        <span class="keyword">return</span> out, hidden</span><br><span class="line"></span><br><span class="line">model = RNNModel(input_size, hidden_size, output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练 RNN</span></span><br><span class="line">losses = []</span><br><span class="line">hidden = <span class="literal">None</span>  <span class="comment"># 初始隐藏状态为 None</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    outputs, hidden = model(inputs.unsqueeze(<span class="number">0</span>), hidden)</span><br><span class="line">    hidden = hidden.detach()  <span class="comment"># 防止梯度爆炸</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = criterion(outputs.view(-<span class="number">1</span>, output_size), targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试 RNN</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    test_hidden = <span class="literal">None</span></span><br><span class="line">    test_output, _ = model(inputs.unsqueeze(<span class="number">0</span>), test_hidden)</span><br><span class="line">    predicted = torch.argmax(test_output, dim=<span class="number">2</span>).squeeze().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Input sequence: &quot;</span>, <span class="string">&#x27;&#x27;</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> input_data]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Predicted sequence: &quot;</span>, <span class="string">&#x27;&#x27;</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> predicted]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化损失</span></span><br><span class="line">plt.plot(losses, label=<span class="string">&quot;Training Loss&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Epoch&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;RNN Training Loss Over Epochs&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>代码解析：</strong></p><ol><li><strong>数据准备</strong>：<ul><li>使用字符序列 <code>hello</code>，并将其转化为独热编码。</li><li>目标序列为 <code>elloh</code>，即向右旋转一个字符。</li></ul></li><li><strong>模型构建</strong>：<ul><li>使用 <code>torch.nn.RNN</code> 创建循环神经网络。</li><li>加入全连接层 <code>torch.nn.Linear</code> 用于映射隐藏状态到输出。</li></ul></li><li><strong>训练部分</strong>：<ul><li>每一轮都计算损失并反向传播。</li><li>隐藏状态通过 <code>hidden.detach()</code> <strong>防止梯度爆炸</strong>。</li></ul></li><li><strong>测试部分</strong>：<ul><li>模型输出字符的预测结果。</li></ul></li><li><strong>可视化</strong>：<ul><li>用 Matplotlib 绘制训练损失的变化趋势。</li></ul></li></ol><p>假设你的模型训练良好，输出可能如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Epoch [20/200], Loss: 0.0013</span><br><span class="line">Epoch [40/200], Loss: 0.0003</span><br><span class="line">Epoch [60/200], Loss: 0.0002</span><br><span class="line">Epoch [80/200], Loss: 0.0001</span><br><span class="line">Epoch [100/200], Loss: 0.0001</span><br><span class="line">Epoch [120/200], Loss: 0.0001</span><br><span class="line">Epoch [140/200], Loss: 0.0001</span><br><span class="line">Epoch [160/200], Loss: 0.0001</span><br><span class="line">Epoch [180/200], Loss: 0.0001</span><br><span class="line">Epoch [200/200], Loss: 0.0001</span><br><span class="line">Input sequence:  hello</span><br></pre></td></tr></table></figure><p>从结果来看，图像显示损失逐渐减少，表明模型训练有效。</p><h1 id="PyTorch-数据集"><a href="#PyTorch-数据集" class="headerlink" title="PyTorch 数据集"></a>PyTorch 数据集</h1><p>在深度学习任务中，数据加载和处理是至关重要的一环。</p><p>PyTorch 提供了<strong>强大的数据加载和处理工具</strong>，主要包括：</p><ul><li><strong><code>torch.utils.data.Dataset</code></strong>：数据集的<strong>抽象类</strong>，需要<strong>自定义并实现 <code>__len__</code>（数据集大小）和 <code>__getitem__</code>（按索引获取样本）</strong>。</li><li><strong><code>torch.utils.data.TensorDataset</code></strong>：<strong>基于张量</strong>的数据集，适合<strong>处理数据-标签对</strong>，直接支持批处理和迭代。</li><li><strong><code>torch.utils.data.DataLoader</code></strong>：<strong>封装 Dataset 的迭代器</strong>，提供<strong>批处理、数据打乱、多线程加载</strong>等功能，便于数据输入模型训练。</li><li><strong><code>torchvision.datasets.ImageFolder</code></strong>：<strong>从文件夹加载图像数据</strong>，每个子文件夹代表一个类别，适用于图像分类任务。</li></ul><h3 id="PyTorch-内置数据集"><a href="#PyTorch-内置数据集" class="headerlink" title="PyTorch 内置数据集"></a>PyTorch 内置数据集</h3><p>PyTorch 通过 torchvision.datasets 模块提供了许多常用的数据集，例如：</p><ul><li><strong>MNIST</strong>：<strong>手写数字图像数据集</strong>，用于图像分类任务。</li><li><strong>CIFAR</strong>：包含 10 个类别、60000 张 32x32 的<strong>彩色图像数据集</strong>，用于图像分类任务。</li><li><strong>COCO</strong>：<strong>通用物体检测、分割、关键点检测数据集</strong>，包含超过 330k 个图像和 2.5M 个目标实例的大规模数据集。</li><li><strong>ImageNet</strong>：包含超过 1400 万张图像，用于<strong>图像分类和物体检测等任务</strong>。</li><li><strong>STL-10</strong>：包含 100k 张 96x96 的彩色图像数据集，<strong>用于图像分类任务</strong>。</li><li><strong>Cityscapes</strong>：包含 5000 张精细注释的城市街道场景图像，<strong>用于语义分割任务</strong>。</li><li><strong>SQUAD</strong>：<strong>用于机器阅读理解任务的数据集</strong>。</li></ul><p>以上数据集可以通过 <strong>torchvision.datasets 模块中的函数</strong>进行加载，也可以通过自定义的方式加载其他数据集。</p><h3 id="torchvision-和-torchtext"><a href="#torchvision-和-torchtext" class="headerlink" title="torchvision 和 torchtext"></a>torchvision 和 torchtext</h3><ul><li><strong>torchvision</strong>： 一个<strong>图形库，提供了图片数据处理相关的 API 和数据集接口</strong>，包括数据集加载函数和常用的图像变换。</li><li><strong>torchtext</strong>： <strong>自然语言处理工具包，提供了文本数据处理和建模的工具</strong>，包括数据预处理和数据加载的方式。</li></ul><hr><h2 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h2><p>Dataset 是 PyTorch 中用于数据集抽象的类。</p><p>自定义数据集需要继承 torch.utils.data.Dataset 并重写以下两个方法：</p><ul><li><code>__len__</code>：返回数据集的大小。</li><li><code>__getitem__</code>：按索引获取一个数据样本及其标签。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, labels</span>):</span><br><span class="line">        <span class="comment"># 数据初始化</span></span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 按索引返回数据和标签</span></span><br><span class="line">        sample = <span class="variable language_">self</span>.data[idx]</span><br><span class="line">        label = <span class="variable language_">self</span>.labels[idx]</span><br><span class="line">        <span class="keyword">return</span> sample, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">data = torch.randn(<span class="number">100</span>, <span class="number">5</span>)  <span class="comment"># 100 个样本，每个样本有 5 个特征</span></span><br><span class="line">labels = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">100</span>,))  <span class="comment"># 100 个标签，取值为 0 或 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化数据集</span></span><br><span class="line">dataset = MyDataset(data, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集大小:&quot;</span>, <span class="built_in">len</span>(dataset))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第 0 个样本:&quot;</span>, dataset[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据集大小: <span class="number">100</span></span><br><span class="line">第 <span class="number">0</span> 个样本: (tensor([-<span class="number">0.2006</span>,  <span class="number">0.7304</span>, -<span class="number">1.3911</span>, -<span class="number">0.4408</span>,  <span class="number">1.1447</span>]), tensor(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><h2 id="torch-utils-data-DataLoader"><a href="#torch-utils-data-DataLoader" class="headerlink" title="torch.utils.data.DataLoader"></a>torch.utils.data.DataLoader</h2><p>DataLoader 是 PyTorch 提供的<strong>数据加载器</strong>，用于批量加载数据集。</p><p>提供了以下功能：</p><ul><li><strong>批量加载</strong>：通过设置 <code>batch_size</code>。</li><li><strong>数据打乱</strong>：通过设置 <code>shuffle=True</code>。</li><li><strong>多线程加速</strong>：通过设置 <code>num_workers</code>。</li><li><strong>迭代访问</strong>：方便地按批次访问数据。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, labels</span>):</span><br><span class="line">        <span class="comment"># 数据初始化</span></span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 按索引返回数据和标签</span></span><br><span class="line">        sample = <span class="variable language_">self</span>.data[idx]</span><br><span class="line">        label = <span class="variable language_">self</span>.labels[idx]</span><br><span class="line">        <span class="keyword">return</span> sample, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">data = torch.randn(<span class="number">100</span>, <span class="number">5</span>)  <span class="comment"># 100 个样本，每个样本有 5 个特征</span></span><br><span class="line">labels = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">100</span>,))  <span class="comment"># 100 个标签，取值为 0 或 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化数据集</span></span><br><span class="line">dataset = MyDataset(data, labels)</span><br><span class="line"><span class="comment"># 实例化 DataLoader</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历 DataLoader</span></span><br><span class="line"><span class="keyword">for</span> batch_idx, (batch_data, batch_labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;批次 <span class="subst">&#123;batch_idx + <span class="number">1</span>&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;数据:&quot;</span>, batch_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;标签:&quot;</span>, batch_labels)</span><br><span class="line">    <span class="keyword">if</span> batch_idx == <span class="number">2</span>:  <span class="comment"># 仅显示前 3 个批次</span></span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">批次 <span class="number">1</span></span><br><span class="line">数据: tensor([[ <span class="number">0.4689</span>,  <span class="number">0.6666</span>, -<span class="number">1.0234</span>,  <span class="number">0.8948</span>,  <span class="number">0.4503</span>],</span><br><span class="line">        [ <span class="number">0.0273</span>, -<span class="number">0.4684</span>, -<span class="number">0.7762</span>,  <span class="number">0.7963</span>,  <span class="number">0.2168</span>],</span><br><span class="line">        [ <span class="number">1.0677</span>, -<span class="number">0.3502</span>, -<span class="number">0.9594</span>, -<span class="number">1.1318</span>, -<span class="number">0.2196</span>],</span><br><span class="line">        [-<span class="number">1.4989</span>,  <span class="number">0.0267</span>,  <span class="number">1.0405</span>, -<span class="number">0.7284</span>,  <span class="number">0.2335</span>],</span><br><span class="line">        [-<span class="number">0.5887</span>, -<span class="number">0.4934</span>,  <span class="number">1.6283</span>,  <span class="number">1.4638</span>,  <span class="number">0.0157</span>],</span><br><span class="line">        [-<span class="number">1.1047</span>, -<span class="number">0.6550</span>, -<span class="number">0.0381</span>,  <span class="number">0.3617</span>, -<span class="number">1.2792</span>],</span><br><span class="line">        [ <span class="number">0.3592</span>, -<span class="number">0.8264</span>,  <span class="number">0.0231</span>, -<span class="number">1.5508</span>,  <span class="number">0.6833</span>],</span><br><span class="line">        [-<span class="number">0.6835</span>,  <span class="number">0.6979</span>,  <span class="number">0.9048</span>, -<span class="number">0.4756</span>,  <span class="number">0.3003</span>],</span><br><span class="line">        [ <span class="number">1.1562</span>, -<span class="number">0.4516</span>, -<span class="number">1.2415</span>,  <span class="number">0.2859</span>,  <span class="number">0.5837</span>],</span><br><span class="line">        [ <span class="number">0.7937</span>,  <span class="number">1.5316</span>, -<span class="number">0.6139</span>,  <span class="number">0.7999</span>,  <span class="number">0.5506</span>]])</span><br><span class="line">标签: tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">批次 <span class="number">2</span></span><br><span class="line">数据: tensor([[-<span class="number">0.0388</span>, -<span class="number">0.3658</span>,  <span class="number">0.8993</span>, -<span class="number">1.5027</span>,  <span class="number">1.0738</span>],</span><br><span class="line">        [-<span class="number">0.6182</span>,  <span class="number">1.0684</span>, -<span class="number">2.3049</span>,  <span class="number">0.8338</span>,  <span class="number">0.1363</span>],</span><br><span class="line">        [-<span class="number">0.5289</span>,  <span class="number">0.1661</span>, -<span class="number">0.0349</span>,  <span class="number">0.2112</span>,  <span class="number">1.4745</span>],</span><br><span class="line">        [-<span class="number">0.3304</span>, -<span class="number">1.2114</span>, -<span class="number">0.2982</span>, -<span class="number">0.3006</span>,  <span class="number">0.5252</span>],</span><br><span class="line">        [-<span class="number">1.4394</span>, -<span class="number">0.3732</span>,  <span class="number">1.0281</span>,  <span class="number">0.5754</span>,  <span class="number">1.0081</span>],</span><br><span class="line">        [ <span class="number">0.8714</span>, -<span class="number">0.1945</span>, -<span class="number">0.2451</span>, -<span class="number">0.2879</span>, -<span class="number">2.0520</span>],</span><br><span class="line">        [ <span class="number">0.0235</span>,  <span class="number">0.4360</span>,  <span class="number">0.1233</span>,  <span class="number">0.0504</span>,  <span class="number">0.5908</span>],</span><br><span class="line">        [ <span class="number">0.5927</span>,  <span class="number">0.1785</span>, -<span class="number">0.9052</span>, -<span class="number">0.9012</span>,  <span class="number">0.8914</span>],</span><br><span class="line">        [ <span class="number">0.4693</span>,  <span class="number">0.5533</span>, -<span class="number">0.1903</span>,  <span class="number">0.0267</span>,  <span class="number">0.4077</span>],</span><br><span class="line">        [-<span class="number">1.1683</span>,  <span class="number">1.6699</span>, -<span class="number">0.4846</span>, -<span class="number">0.7404</span>,  <span class="number">0.3370</span>]])</span><br><span class="line">标签: tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">批次 <span class="number">3</span></span><br><span class="line">数据: tensor([[ <span class="number">0.2103</span>, -<span class="number">0.7839</span>,  <span class="number">1.4899</span>,  <span class="number">2.2749</span>, -<span class="number">0.7548</span>],</span><br><span class="line">        [-<span class="number">1.2836</span>,  <span class="number">1.0025</span>, -<span class="number">1.1162</span>, -<span class="number">0.4261</span>,  <span class="number">1.0690</span>],</span><br><span class="line">        [-<span class="number">0.7969</span>,  <span class="number">1.0418</span>, -<span class="number">0.7405</span>,  <span class="number">0.8766</span>,  <span class="number">0.2347</span>],</span><br><span class="line">        [-<span class="number">1.1071</span>,  <span class="number">1.8560</span>, -<span class="number">1.2979</span>, -<span class="number">0.8364</span>, -<span class="number">0.2925</span>],</span><br><span class="line">        [-<span class="number">1.0488</span>,  <span class="number">0.4802</span>, -<span class="number">0.6453</span>,  <span class="number">0.2009</span>,  <span class="number">0.5693</span>],</span><br><span class="line">        [ <span class="number">0.8883</span>,  <span class="number">0.4619</span>, -<span class="number">0.2087</span>,  <span class="number">0.2189</span>, -<span class="number">0.3708</span>],</span><br><span class="line">        [-<span class="number">1.4578</span>,  <span class="number">0.3629</span>,  <span class="number">1.8282</span>,  <span class="number">0.5353</span>, -<span class="number">1.1783</span>],</span><br><span class="line">        [-<span class="number">1.2813</span>,  <span class="number">0.5129</span>, -<span class="number">0.4598</span>, -<span class="number">0.2131</span>, -<span class="number">1.2804</span>],</span><br><span class="line">        [ <span class="number">1.7831</span>,  <span class="number">1.1730</span>, -<span class="number">0.2305</span>, -<span class="number">0.6550</span>,  <span class="number">0.1197</span>],</span><br><span class="line">        [-<span class="number">0.9384</span>, -<span class="number">0.0483</span>,  <span class="number">1.9626</span>,  <span class="number">0.3342</span>,  <span class="number">0.1700</span>]])</span><br><span class="line">标签: tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><h2 id="使用内置数据集"><a href="#使用内置数据集" class="headerlink" title="使用内置数据集"></a>使用内置数据集</h2><p>PyTorch 提供了多个常用数据集，存放在 <strong>torchvision 中</strong>，特别适合图像任务。</p><p>加载 MNIST 数据集:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转换为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练数据集</span></span><br><span class="line">train_dataset = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 DataLoader 加载数据</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看一个批次的数据</span></span><br><span class="line">data_iter = <span class="built_in">iter</span>(train_loader)</span><br><span class="line">images, labels = <span class="built_in">next</span>(data_iter)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;批次图像大小: <span class="subst">&#123;images.shape&#125;</span>&quot;</span>)  <span class="comment"># 输出形状为 [batch_size, 1, 28, 28]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;批次标签: <span class="subst">&#123;labels&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">批次图像大小: torch.Size([32, 1, 28, 28])</span><br><span class="line">批次标签: tensor([0, 4, 9, 8, 1, 3, 8, 1, 7, 2, 1, 1, 1, 2, 6, 3, 9, 7, 6, 9, 4, 9, 7, 1,</span><br><span class="line">        3, 7, 3, 0, 7, 7, 6, 7])</span><br></pre></td></tr></table></figure><hr><h2 id="Dataset-与-DataLoader-的自定义应用"><a href="#Dataset-与-DataLoader-的自定义应用" class="headerlink" title="Dataset 与 DataLoader 的自定义应用"></a>Dataset 与 DataLoader 的自定义应用</h2><p>以下是一个将 <strong>CSV 文件 作为数据源</strong>，并通过<strong>自定义 Dataset 和 DataLoader 读取数据</strong>。</p><p><strong>CSV 文件内容如下（下载<a href="https://static.jyshare.com/download/runoob_pytorch_data.csv">runoob_pytorch_data.csv</a>）：</strong></p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-dataset-21.png" alt="img"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义 CSV 数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CSVDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path</span>):</span><br><span class="line">        <span class="comment"># 读取 CSV 文件</span></span><br><span class="line">        <span class="variable language_">self</span>.data = pd.read_csv(file_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 使用 .iloc 明确基于位置索引</span></span><br><span class="line">        row = <span class="variable language_">self</span>.data.iloc[idx]</span><br><span class="line">        <span class="comment"># 将特征和标签分开</span></span><br><span class="line">        features = torch.tensor(row.iloc[:-<span class="number">1</span>].to_numpy(), dtype=torch.float32)  <span class="comment"># 特征</span></span><br><span class="line">        label = torch.tensor(row.iloc[-<span class="number">1</span>], dtype=torch.float32)  <span class="comment"># 标签</span></span><br><span class="line">        <span class="keyword">return</span> features, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化数据集和 DataLoader</span></span><br><span class="line">dataset = CSVDataset(<span class="string">&quot;runoob_pytorch_data.csv&quot;</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历 DataLoader</span></span><br><span class="line"><span class="keyword">for</span> features, label <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;特征:&quot;</span>, features)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;标签:&quot;</span>, label)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">特征: tensor([[ 1.2000,  2.1000, -3.0000],</span><br><span class="line">        [ 1.0000,  1.1000, -2.0000],</span><br><span class="line">        [ 0.5000, -1.2000,  3.3000],</span><br><span class="line">        [-0.3000,  0.8000,  1.2000]])</span><br><span class="line">标签: tensor([1., 0., 1., 0.])</span><br><span class="line">tianqixin@Mac-mini runoob-test % python3 test.py</span><br><span class="line">特征: tensor([[ 1.5000,  2.2000, -1.1000],</span><br><span class="line">        [ 2.1000, -3.3000,  0.0000],</span><br><span class="line">        [-2.3000,  0.4000,  0.7000],</span><br><span class="line">        [-0.3000,  0.8000,  1.2000]])</span><br><span class="line">标签: tensor([0., 1., 0., 0.])</span><br></pre></td></tr></table></figure><h1 id="PyTorch-数据转换"><a href="#PyTorch-数据转换" class="headerlink" title="PyTorch 数据转换"></a>PyTorch 数据转换</h1><p>在 PyTorch 中，数据转换（Data Transformation） 是一种在加载数据时<strong>对数据进行处理的机制</strong>，将原始数据<strong>转换成适合模型训练的格式</strong>，主要通过 <strong>torchvision.transforms</strong> 提供的工具完成。</p><p>数据转换不仅可以实现基本的<strong>数据预处理（如归一化、大小调整等）</strong>，还能帮助进行<strong>数据增强（如随机裁剪、翻转等）</strong>，提高模型的<strong>泛化能力</strong>。</p><h3 id="为什么需要数据转换？"><a href="#为什么需要数据转换？" class="headerlink" title="为什么需要数据转换？"></a>为什么需要数据转换？</h3><p><strong>数据预处理</strong>：</p><ul><li><strong>调整数据格式、大小和范围</strong>，使其<strong>适合模型输入</strong>。</li><li>例如，图像需要调整为固定大小、张量格式并归一化到 [0,1]。</li></ul><p><strong>数据增强</strong>：</p><ul><li>在训练时对数据进行<strong>变换</strong>，以<strong>增加多样性</strong>。</li><li>例如，通过随机旋转、翻转和裁剪增加数据样本的变种，<strong>避免过拟合</strong>。</li></ul><p><strong>灵活性</strong>：</p><ul><li>通过定义一系列转换操作，可以动态地对数据进行处理，简化数据加载的复杂度。</li></ul><p>在 PyTorch 中，torchvision.transforms 模块提供了多种用于图像处理的变换操作。</p><h3 id="基础变换操作"><a href="#基础变换操作" class="headerlink" title="基础变换操作"></a>基础变换操作</h3><table><thead><tr><th align="left">变换函数名称</th><th align="left">描述</th><th align="left">实例</th></tr></thead><tbody><tr><td align="left"><strong>transforms.ToTensor()</strong></td><td align="left">将<strong>PIL图像或NumPy数组转换为PyTorch张量</strong>，并自动<strong>将像素值归一化到 [0, 1]</strong>。</td><td align="left"><code>transform = transforms.ToTensor()</code></td></tr><tr><td align="left"><strong>transforms.Normalize(mean, std)</strong></td><td align="left">对图像进行<strong>标准化</strong>，使数据<strong>符合零均值和单位方差</strong>。</td><td align="left"><code>transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</code></td></tr><tr><td align="left"><strong>transforms.Resize(size)</strong></td><td align="left">调整图像尺寸，确保输入到网络的图像<strong>大小一致</strong>。</td><td align="left"><code>transform = transforms.Resize((256, 256))</code></td></tr><tr><td align="left"><strong>transforms.CenterCrop(size)</strong></td><td align="left"><strong>从图像中心裁剪指定大小的区域。</strong></td><td align="left"><code>transform = transforms.CenterCrop(224)</code></td></tr></tbody></table><p><strong>1、ToTensor</strong></p><p>将 PIL 图像或 NumPy 数组转换为 PyTorch 张量。</p><p><strong>同时将像素值从 [0, 255] 归一化为 [0, 1]。</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from torchvision import transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.ToTensor()</span><br></pre></td></tr></table></figure><p><strong>2、Normalize</strong></p><p>对数据进行<strong>标准化</strong>，使其符合特定的均值和标准差。</p><p><strong>通常用于图像数据</strong>，将其像素值归一化为零均值和单位方差。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Normalize(mean=[0.5], std=[0.5])  # 归一化到 [-1, 1]</span><br></pre></td></tr></table></figure><p><strong>3、Resize</strong></p><p>调整图像的大小。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Resize((128, 128))  # 将图像调整为 128x128</span><br></pre></td></tr></table></figure><p><strong>4、CenterCrop</strong></p><p>从图像中心裁剪指定大小的区域。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.CenterCrop(128)  # 裁剪 128x128 的区域</span><br></pre></td></tr></table></figure><h3 id="数据增强操作"><a href="#数据增强操作" class="headerlink" title="数据增强操作"></a>数据增强操作</h3><table><thead><tr><th align="left">变换函数名称</th><th align="left">描述</th><th align="left">实例</th></tr></thead><tbody><tr><td align="left"><strong>transforms.RandomHorizontalFlip(p)</strong></td><td align="left"><strong>随机水平翻转</strong>图像。</td><td align="left"><code>transform = transforms.RandomHorizontalFlip(p=0.5)</code></td></tr><tr><td align="left"><strong>transforms.RandomRotation(degrees)</strong></td><td align="left"><strong>随机旋转</strong>图像。</td><td align="left"><code>transform = transforms.RandomRotation(degrees=45)</code></td></tr><tr><td align="left"><strong>transforms.ColorJitter(brightness, contrast, saturation, hue)</strong></td><td align="left">调整图像的<strong>亮度、对比度、饱和度和色调</strong>。</td><td align="left"><code>transform = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)</code></td></tr><tr><td align="left"><strong>transforms.RandomCrop(size)</strong></td><td align="left"><strong>随机裁剪指定大小</strong>的区域。</td><td align="left"><code>transform = transforms.RandomCrop(224)</code></td></tr><tr><td align="left"><strong>transforms.RandomResizedCrop(size)</strong></td><td align="left"><strong>随机裁剪图像并调整到指定大小。</strong></td><td align="left"><code>transform = transforms.RandomResizedCrop(224)</code></td></tr></tbody></table><p><strong>1、RandomCrop</strong></p><p>从图像中随机裁剪指定大小。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.RandomCrop(128)</span><br></pre></td></tr></table></figure><p><strong>2、RandomHorizontalFlip</strong></p><p>以一定概率水平翻转图像。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.RandomHorizontalFlip(p=0.5)  # 50% 概率翻转</span><br></pre></td></tr></table></figure><p><strong>3、RandomRotation</strong></p><p>随机旋转一定角度。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.RandomRotation(degrees=30)  # 随机旋转 -30 到 +30 度</span><br></pre></td></tr></table></figure><p><strong>4、ColorJitter</strong></p><p>随机改变图像的亮度、对比度、饱和度或色调。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.ColorJitter(brightness=0.5, contrast=0.5)</span><br></pre></td></tr></table></figure><h3 id="组合变换"><a href="#组合变换" class="headerlink" title="组合变换"></a>组合变换</h3><table><thead><tr><th align="left">变换函数名称</th><th align="left">描述</th><th align="left">实例</th></tr></thead><tbody><tr><td align="left">transforms.Compose()</td><td align="left">将多个变换组合在一起，按照顺序依次应用。</td><td align="left"><code>transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), transforms.Resize((256, 256))])</code></td></tr></tbody></table><h3 id="自定义转换"><a href="#自定义转换" class="headerlink" title="自定义转换"></a>自定义转换</h3><p>如果 transforms 提供的功能无法满足需求，可以通过自定义类或函数实现。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomTransform</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 这里可以自定义任何变换逻辑</span></span><br><span class="line">        <span class="keyword">return</span> x * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">transform = CustomTransform()</span><br></pre></td></tr></table></figure><h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h2><h3 id="对图像数据集应用转换"><a href="#对图像数据集应用转换" class="headerlink" title="对图像数据集应用转换"></a>对图像数据集应用转换</h3><p>加载 MNIST 数据集，并应用转换。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment"># 定义转换</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.5</span>], std=[<span class="number">0.5</span>])</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 使用 DataLoader</span></span><br><span class="line">train_loader = DataLoader(dataset=train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 查看转换后的数据</span></span><br><span class="line"><span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;图像张量大小:&quot;</span>, images.size())  <span class="comment"># [batch_size, 1, 128, 128]</span></span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">图像张量大小: torch.Size([32, 1, 128, 128])</span><br></pre></td></tr></table></figure><h3 id="可视化转换效果"><a href="#可视化转换效果" class="headerlink" title="可视化转换效果"></a>可视化转换效果</h3><p>以下代码展示了原始数据和经过转换后的数据对比。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="comment"># 原始和增强后的图像可视化</span></span><br><span class="line">transform_augment = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform_augment)</span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">dataset</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        image, label = dataset[i]</span><br><span class="line">        axs[i].imshow(image.squeeze(<span class="number">0</span>), cmap=<span class="string">&#x27;gray&#x27;</span>)  <span class="comment"># 将 (1, H, W) 转为 (H, W)</span></span><br><span class="line">        axs[i].set_title(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br><span class="line">        axs[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">show_images(dataset)</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-transforms-1.png" alt="img"></p><h1 id="Pytorch-torch-参考手册"><a href="#Pytorch-torch-参考手册" class="headerlink" title="Pytorch torch 参考手册"></a>Pytorch torch 参考手册</h1><p>PyTorch 软件包包含了用于多维张量的数据结构，并定义了在这些张量上执行的数学运算。此外，它还提供了许多实用工具，用于高效地序列化张量和任意类型的数据，以及其他有用的工具。</p><p>它还有一个 CUDA 版本，可以让你在计算能力 &gt;&#x3D; 3.0 的 NVIDIA GPU 上运行张量计算。</p><h2 id="PyTorch-torch-API-手册"><a href="#PyTorch-torch-API-手册" class="headerlink" title="PyTorch torch API 手册"></a>PyTorch torch API 手册</h2><table><thead><tr><th align="left">类别</th><th align="left">API</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">Tensors</td><td align="left"><code>is_tensor(obj)</code></td><td align="left">检查 <code>obj</code> 是否为 PyTorch 张量。</td></tr><tr><td align="left"></td><td align="left"><code>is_storage(obj)</code></td><td align="left">检查 <code>obj</code> 是否为 PyTorch 存储对象。</td></tr><tr><td align="left"></td><td align="left"><code>is_complex(input)</code></td><td align="left">检查 <code>input</code> 数据类型是否为复数数据类型。</td></tr><tr><td align="left"></td><td align="left"><code>is_conj(input)</code></td><td align="left">检查 <code>input</code> 是否为共轭张量。</td></tr><tr><td align="left"></td><td align="left"><code>is_floating_point(input)</code></td><td align="left">检查 <code>input</code> 数据类型是否为浮点数据类型。</td></tr><tr><td align="left"></td><td align="left"><code>is_nonzero(input)</code></td><td align="left">检查 <code>input</code> 是否为非零单一元素张量。</td></tr><tr><td align="left"></td><td align="left"><code>set_default_dtype(d)</code></td><td align="left">设置默认浮点数据类型为 <code>d</code>。</td></tr><tr><td align="left"></td><td align="left"><code>get_default_dtype()</code></td><td align="left">获取当前默认浮点 <code>torch.dtype</code>。</td></tr><tr><td align="left"></td><td align="left"><code>set_default_device(device)</code></td><td align="left">设置默认 <code>torch.Tensor</code> 分配的设备为 <code>device</code>。</td></tr><tr><td align="left"></td><td align="left"><code>get_default_device()</code></td><td align="left">获取默认 <code>torch.Tensor</code> 分配的设备。</td></tr><tr><td align="left"></td><td align="left"><code>numel(input)</code></td><td align="left">返回 <code>input</code> 张量中的元素总数。</td></tr><tr><td align="left">Creation Ops</td><td align="left"><code>tensor(data)</code></td><td align="left">通过复制 <code>data</code> 构造无自动梯度历史的张量。</td></tr><tr><td align="left"></td><td align="left"><code>sparse_coo_tensor(indices, values)</code></td><td align="left">在指定的 <code>indices</code> 处构造稀疏张量，具有指定的值。</td></tr><tr><td align="left"></td><td align="left"><code>as_tensor(data)</code></td><td align="left">将 <code>data</code> 转换为张量，共享数据并尽可能保留自动梯度历史。</td></tr><tr><td align="left"></td><td align="left"><code>zeros(size)</code></td><td align="left">返回一个用标量值 0 填充的张量，形状由 <code>size</code> 定义。</td></tr><tr><td align="left"></td><td align="left"><code>ones(size)</code></td><td align="left">返回一个用标量值 1 填充的张量，形状由 <code>size</code> 定义。</td></tr><tr><td align="left"></td><td align="left"><code>arange(start, end, step)</code></td><td align="left">返回一个 1-D 张量，包含从 <code>start</code> 到 <code>end</code> 的值，步长为 <code>step</code>。</td></tr><tr><td align="left"></td><td align="left"><code>rand(size)</code></td><td align="left">返回一个从 [0, 1) 区间均匀分布的随机数填充的张量。</td></tr><tr><td align="left"></td><td align="left"><code>randn(size)</code></td><td align="left">返回一个从标准正态分布填充的张量。</td></tr><tr><td align="left">Math operations</td><td align="left"><code>add(input, other, alpha)</code></td><td align="left">将 <code>other</code>（由 <code>alpha</code> 缩放）加到 <code>input</code> 上。</td></tr><tr><td align="left"></td><td align="left"><code>mul(input, other)</code></td><td align="left">将 <code>input</code> 与 <code>other</code> 相乘。</td></tr><tr><td align="left"></td><td align="left"><code>matmul(input, other)</code></td><td align="left">执行 <code>input</code> 和 <code>other</code> 的矩阵乘法。</td></tr><tr><td align="left"></td><td align="left"><code>mean(input, dim)</code></td><td align="left">计算 <code>input</code> 在维度 <code>dim</code> 上的均值。</td></tr><tr><td align="left"></td><td align="left"><code>sum(input, dim)</code></td><td align="left">计算 <code>input</code> 在维度 <code>dim</code> 上的和。</td></tr><tr><td align="left"></td><td align="left"><code>max(input, dim)</code></td><td align="left">返回 <code>input</code> 在维度 <code>dim</code> 上的最大值。</td></tr><tr><td align="left"></td><td align="left"><code>min(input, dim)</code></td><td align="left">返回 <code>input</code> 在维度 <code>dim</code> 上的最小值。</td></tr></tbody></table><h3 id="Tensor-创建"><a href="#Tensor-创建" class="headerlink" title="Tensor 创建"></a><strong>Tensor 创建</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.tensor(data, dtype, device, requires_grad)</code></td><td align="left">从数据创建张量。</td></tr><tr><td align="left"><code>torch.as_tensor(data, dtype, device)</code></td><td align="left">将数据转换为张量（共享内存）。</td></tr><tr><td align="left"><code>torch.from_numpy(ndarray)</code></td><td align="left">从 NumPy 数组创建张量（共享内存）。</td></tr><tr><td align="left"><code>torch.zeros(*size, dtype, device, requires_grad)</code></td><td align="left">创建全零张量。</td></tr><tr><td align="left"><code>torch.ones(*size, dtype, device, requires_grad)</code></td><td align="left">创建全一张量。</td></tr><tr><td align="left"><code>torch.empty(*size, dtype, device, requires_grad)</code></td><td align="left">创建未初始化的张量。</td></tr><tr><td align="left"><code>torch.arange(start, end, step, dtype, device, requires_grad)</code></td><td align="left">创建等差序列张量。</td></tr><tr><td align="left"><code>torch.linspace(start, end, steps, dtype, device, requires_grad)</code></td><td align="left">创建等间隔序列张量。</td></tr><tr><td align="left"><code>torch.logspace(start, end, steps, base, dtype, device, requires_grad)</code></td><td align="left">创建对数间隔序列张量。</td></tr><tr><td align="left"><code>torch.eye(n, m, dtype, device, requires_grad)</code></td><td align="left">创建单位矩阵。</td></tr><tr><td align="left"><code>torch.full(size, fill_value, dtype, device, requires_grad)</code></td><td align="left">创建填充指定值的张量。</td></tr><tr><td align="left"><code>torch.rand(*size, dtype, device, requires_grad)</code></td><td align="left">创建均匀分布随机张量（范围 [0, 1)）。</td></tr><tr><td align="left"><code>torch.randn(*size, dtype, device, requires_grad)</code></td><td align="left">创建标准正态分布随机张量。</td></tr><tr><td align="left"><code>torch.randint(low, high, size, dtype, device, requires_grad)</code></td><td align="left">创建整数随机张量。</td></tr><tr><td align="left"><code>torch.randperm(n, dtype, device, requires_grad)</code></td><td align="left">创建 0 到 n-1 的随机排列。</td></tr></tbody></table><hr><h3 id="Tensor-操作"><a href="#Tensor-操作" class="headerlink" title="Tensor 操作"></a><strong>Tensor 操作</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.cat(tensors, dim)</code></td><td align="left">沿指定维度连接张量。</td></tr><tr><td align="left"><code>torch.stack(tensors, dim)</code></td><td align="left">沿新维度堆叠张量。</td></tr><tr><td align="left"><code>torch.split(tensor, split_size, dim)</code></td><td align="left">将张量沿指定维度分割。</td></tr><tr><td align="left"><code>torch.chunk(tensor, chunks, dim)</code></td><td align="left">将张量沿指定维度分块。</td></tr><tr><td align="left"><code>torch.reshape(input, shape)</code></td><td align="left">改变张量的形状。</td></tr><tr><td align="left"><code>torch.transpose(input, dim0, dim1)</code></td><td align="left">交换张量的两个维度。</td></tr><tr><td align="left"><code>torch.squeeze(input, dim)</code></td><td align="left">移除大小为 1 的维度。</td></tr><tr><td align="left"><code>torch.unsqueeze(input, dim)</code></td><td align="left">在指定位置插入大小为 1 的维度。</td></tr><tr><td align="left"><code>torch.expand(input, size)</code></td><td align="left">扩展张量的尺寸。</td></tr><tr><td align="left"><code>torch.narrow(input, dim, start, length)</code></td><td align="left">返回张量的切片。</td></tr><tr><td align="left"><code>torch.permute(input, dims)</code></td><td align="left">重新排列张量的维度。</td></tr><tr><td align="left"><code>torch.masked_select(input, mask)</code></td><td align="left">根据布尔掩码选择元素。</td></tr><tr><td align="left"><code>torch.index_select(input, dim, index)</code></td><td align="left">沿指定维度选择索引对应的元素。</td></tr><tr><td align="left"><code>torch.gather(input, dim, index)</code></td><td align="left">沿指定维度收集指定索引的元素。</td></tr><tr><td align="left"><code>torch.scatter(input, dim, index, src)</code></td><td align="left">将 <code>src</code> 的值散布到 <code>input</code> 的指定位置。</td></tr><tr><td align="left"><code>torch.nonzero(input)</code></td><td align="left">返回非零元素的索引。</td></tr></tbody></table><hr><h3 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a><strong>数学运算</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.add(input, other)</code></td><td align="left">逐元素加法。</td></tr><tr><td align="left"><code>torch.sub(input, other)</code></td><td align="left">逐元素减法。</td></tr><tr><td align="left"><code>torch.mul(input, other)</code></td><td align="left">逐元素乘法。</td></tr><tr><td align="left"><code>torch.div(input, other)</code></td><td align="left">逐元素除法。</td></tr><tr><td align="left"><code>torch.matmul(input, other)</code></td><td align="left">矩阵乘法。</td></tr><tr><td align="left"><code>torch.pow(input, exponent)</code></td><td align="left">逐元素幂运算。</td></tr><tr><td align="left"><code>torch.sqrt(input)</code></td><td align="left">逐元素平方根。</td></tr><tr><td align="left"><code>torch.exp(input)</code></td><td align="left">逐元素指数函数。</td></tr><tr><td align="left"><code>torch.log(input)</code></td><td align="left">逐元素自然对数。</td></tr><tr><td align="left"><code>torch.sum(input, dim)</code></td><td align="left">沿指定维度求和。</td></tr><tr><td align="left"><code>torch.mean(input, dim)</code></td><td align="left">沿指定维度求均值。</td></tr><tr><td align="left"><code>torch.max(input, dim)</code></td><td align="left">沿指定维度求最大值。</td></tr><tr><td align="left"><code>torch.min(input, dim)</code></td><td align="left">沿指定维度求最小值。</td></tr><tr><td align="left"><code>torch.abs(input)</code></td><td align="left">逐元素绝对值。</td></tr><tr><td align="left"><code>torch.clamp(input, min, max)</code></td><td align="left">将张量值限制在指定范围内。</td></tr><tr><td align="left"><code>torch.round(input)</code></td><td align="left">逐元素四舍五入。</td></tr><tr><td align="left"><code>torch.floor(input)</code></td><td align="left">逐元素向下取整。</td></tr><tr><td align="left"><code>torch.ceil(input)</code></td><td align="left">逐元素向上取整。</td></tr></tbody></table><hr><h3 id="随机数生成"><a href="#随机数生成" class="headerlink" title="随机数生成"></a><strong>随机数生成</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.manual_seed(seed)</code></td><td align="left">设置随机种子。</td></tr><tr><td align="left"><code>torch.initial_seed()</code></td><td align="left">返回当前随机种子。</td></tr><tr><td align="left"><code>torch.rand(*size)</code></td><td align="left">创建均匀分布随机张量（范围 [0, 1)）。</td></tr><tr><td align="left"><code>torch.randn(*size)</code></td><td align="left">创建标准正态分布随机张量。</td></tr><tr><td align="left"><code>torch.randint(low, high, size)</code></td><td align="left">创建整数随机张量。</td></tr><tr><td align="left"><code>torch.randperm(n)</code></td><td align="left">返回 0 到 n-1 的随机排列。</td></tr></tbody></table><hr><h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a><strong>线性代数</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.dot(input, other)</code></td><td align="left">计算两个向量的点积。</td></tr><tr><td align="left"><code>torch.mm(input, mat2)</code></td><td align="left">矩阵乘法。</td></tr><tr><td align="left"><code>torch.bmm(input, mat2)</code></td><td align="left">批量矩阵乘法。</td></tr><tr><td align="left"><code>torch.eig(input)</code></td><td align="left">计算矩阵的特征值和特征向量。</td></tr><tr><td align="left"><code>torch.svd(input)</code></td><td align="left">计算矩阵的奇异值分解。</td></tr><tr><td align="left"><code>torch.inverse(input)</code></td><td align="left">计算矩阵的逆。</td></tr><tr><td align="left"><code>torch.det(input)</code></td><td align="left">计算矩阵的行列式。</td></tr><tr><td align="left"><code>torch.trace(input)</code></td><td align="left">计算矩阵的迹。</td></tr></tbody></table><hr><h3 id="设备管理"><a href="#设备管理" class="headerlink" title="设备管理"></a><strong>设备管理</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.cuda.is_available()</code></td><td align="left">检查 CUDA 是否可用。</td></tr><tr><td align="left"><code>torch.device(device)</code></td><td align="left">创建一个设备对象（如 <code>&#39;cpu&#39;</code> 或 <code>&#39;cuda:0&#39;</code>）。</td></tr><tr><td align="left"><code>torch.to(device)</code></td><td align="left">将张量移动到指定设备。</td></tr></tbody></table><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建张量</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y = torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 数学运算</span></span><br><span class="line">z = torch.add(x, <span class="number">1</span>)  <span class="comment"># 逐元素加 1</span></span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="comment"># 索引和切片</span></span><br><span class="line">mask = x &gt; <span class="number">1</span></span><br><span class="line">selected = torch.masked_select(x, mask)</span><br><span class="line"><span class="built_in">print</span>(selected)</span><br><span class="line"><span class="comment"># 设备管理</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    <span class="built_in">print</span>(x.device)</span><br></pre></td></tr></table></figure><h1 id="PyTorch-torch-nn-参考手册"><a href="#PyTorch-torch-nn-参考手册" class="headerlink" title="PyTorch torch.nn 参考手册"></a>PyTorch torch.nn 参考手册</h1><p>PyTorch 的 <code>torch.nn</code> 模块是<strong>构建和训练神经网络的核心模块</strong>，它提供了丰富的类和函数来定义和操作神经网络。</p><p>以下是 <code>torch.nn</code> 模块的一些关键组成部分及其功能：</p><p><strong>1、nn.Module 类</strong>：</p><ul><li><code>nn.Module</code> 是<strong>所有自定义神经网络模型的基类</strong>。用户通常会从这个类<strong>派生自己的模型类</strong>，并在其中<strong>定义网络层结构以及前向传播函数（forward pass）</strong>。</li></ul><p><strong>2、预定义层（Modules）</strong>：</p><ul><li>包括各种类型的层组件，例如<strong>卷积层（<code>nn.Conv1d</code>, <code>nn.Conv2d</code>, <code>nn.Conv3d</code>）</strong>、<strong>全连接层（<code>nn.Linear</code>）</strong>、**激活函数（<code>nn.ReLU</code>, <code>nn.Sigmoid</code>, <code>nn.Tanh</code>）**等。</li></ul><p><strong>3、容器类</strong>：</p><ul><li><code>nn.Sequential</code>：允许将多个层按顺序组合起来，<strong>形成简单的线性堆叠网络</strong>。</li><li><code>nn.ModuleList</code> 和 <code>nn.ModuleDict</code>：可以动态地存储和访问子模块，支持可变长度或命名的模块集合。</li></ul><p><strong>4、损失函数（Loss Functions）</strong>：</p><ul><li><code>torch.nn</code> 包含了一系列用于衡量模型预测与真实标签之间差异的<strong>损失函数</strong>，例如<strong>均方误差损失（<code>nn.MSELoss</code>）</strong>、**交叉熵损失（<code>nn.CrossEntropyLoss</code>）**等。</li></ul><p><strong>5、实用函数接口（Functional Interface）</strong>：</p><ul><li><strong><code>nn.functional</code>（通常简写为 <code>F</code>）</strong>，包含了许多可以<strong>直接作用于张量上</strong>的函数，它们实现了与层对象相同的功能，但不具有参数保存和更新的能力。例如，可以<strong>使用 <code>F.relu()</code> 直接进行 ReLU 操作</strong>，或者 <strong><code>F.conv2d()</code> 进行卷积操作</strong>。</li></ul><p><strong>6、初始化方法</strong>：</p><ul><li><code>torch.nn.init</code> 提供了一些常用的<strong>权重初始化策略</strong>，比如 <strong>Xavier 初始化 (<code>nn.init.xavier_uniform_()</code></strong>) 和 <strong>Kaiming 初始化 (<code>nn.init.kaiming_uniform_()</code>)</strong>，这些对于成功训练神经网络至关重要。</li></ul><hr><h2 id="PyTorch-torch-nn-模块参考手册"><a href="#PyTorch-torch-nn-模块参考手册" class="headerlink" title="PyTorch torch.nn 模块参考手册"></a>PyTorch torch.nn 模块参考手册</h2><h3 id="神经网络容器"><a href="#神经网络容器" class="headerlink" title="神经网络容器"></a><strong>神经网络容器</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.Module</code></td><td align="left">所有神经网络模块的基类。</td></tr><tr><td align="left"><code>torch.nn.Sequential(*args)</code></td><td align="left">按顺序组合多个模块。</td></tr><tr><td align="left"><code>torch.nn.ModuleList(modules)</code></td><td align="left">将子模块存储在列表中。</td></tr><tr><td align="left"><code>torch.nn.ModuleDict(modules)</code></td><td align="left">将子模块存储在字典中。</td></tr><tr><td align="left"><code>torch.nn.ParameterList(parameters)</code></td><td align="left">将参数存储在列表中。</td></tr><tr><td align="left"><code>torch.nn.ParameterDict(parameters)</code></td><td align="left">将参数存储在字典中。</td></tr></tbody></table><hr><h3 id="线性层"><a href="#线性层" class="headerlink" title="线性层"></a><strong>线性层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.Linear(in_features, out_features)</code></td><td align="left">全连接层。</td></tr><tr><td align="left"><code>torch.nn.Bilinear(in1_features, in2_features, out_features)</code></td><td align="left">双线性层。</td></tr></tbody></table><hr><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a><strong>卷积层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.Conv1d(in_channels, out_channels, kernel_size)</code></td><td align="left">一维卷积层。</td></tr><tr><td align="left"><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size)</code></td><td align="left">二维卷积层。</td></tr><tr><td align="left"><code>torch.nn.Conv3d(in_channels, out_channels, kernel_size)</code></td><td align="left">三维卷积层。</td></tr><tr><td align="left"><code>torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size)</code></td><td align="left">一维转置卷积层。</td></tr><tr><td align="left"><code>torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size)</code></td><td align="left">二维转置卷积层。</td></tr><tr><td align="left"><code>torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size)</code></td><td align="left">三维转置卷积层。</td></tr></tbody></table><hr><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a><strong>池化层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.MaxPool1d(kernel_size)</code></td><td align="left">一维最大池化层。</td></tr><tr><td align="left"><code>torch.nn.MaxPool2d(kernel_size)</code></td><td align="left">二维最大池化层。</td></tr><tr><td align="left"><code>torch.nn.MaxPool3d(kernel_size)</code></td><td align="left">三维最大池化层。</td></tr><tr><td align="left"><code>torch.nn.AvgPool1d(kernel_size)</code></td><td align="left">一维平均池化层。</td></tr><tr><td align="left"><code>torch.nn.AvgPool2d(kernel_size)</code></td><td align="left">二维平均池化层。</td></tr><tr><td align="left"><code>torch.nn.AvgPool3d(kernel_size)</code></td><td align="left">三维平均池化层。</td></tr><tr><td align="left"><code>torch.nn.AdaptiveMaxPool1d(output_size)</code></td><td align="left">一维自适应最大池化层。</td></tr><tr><td align="left"><code>torch.nn.AdaptiveAvgPool1d(output_size)</code></td><td align="left">一维自适应平均池化层。</td></tr><tr><td align="left"><code>torch.nn.AdaptiveMaxPool2d(output_size)</code></td><td align="left">二维自适应最大池化层。</td></tr><tr><td align="left"><code>torch.nn.AdaptiveAvgPool2d(output_size)</code></td><td align="left">二维自适应平均池化层。</td></tr><tr><td align="left"><code>torch.nn.AdaptiveMaxPool3d(output_size)</code></td><td align="left">三维自适应最大池化层。</td></tr><tr><td align="left"><code>torch.nn.AdaptiveAvgPool3d(output_size)</code></td><td align="left">三维自适应平均池化层。</td></tr></tbody></table><hr><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a><strong>激活函数</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.ReLU()</code></td><td align="left">ReLU 激活函数。</td></tr><tr><td align="left"><code>torch.nn.Sigmoid()</code></td><td align="left">Sigmoid 激活函数。</td></tr><tr><td align="left"><code>torch.nn.Tanh()</code></td><td align="left">Tanh 激活函数。</td></tr><tr><td align="left"><code>torch.nn.Softmax(dim)</code></td><td align="left">Softmax 激活函数。</td></tr><tr><td align="left"><code>torch.nn.LogSoftmax(dim)</code></td><td align="left">LogSoftmax 激活函数。</td></tr><tr><td align="left"><code>torch.nn.LeakyReLU(negative_slope)</code></td><td align="left">LeakyReLU 激活函数。</td></tr><tr><td align="left"><code>torch.nn.ELU(alpha)</code></td><td align="left">ELU 激活函数。</td></tr><tr><td align="left"><code>torch.nn.SELU()</code></td><td align="left">SELU 激活函数。</td></tr><tr><td align="left"><code>torch.nn.GELU()</code></td><td align="left">GELU 激活函数。</td></tr></tbody></table><hr><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a><strong>损失函数</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.MSELoss()</code></td><td align="left">均方误差损失。</td></tr><tr><td align="left"><code>torch.nn.L1Loss()</code></td><td align="left">L1 损失。</td></tr><tr><td align="left"><code>torch.nn.CrossEntropyLoss()</code></td><td align="left">交叉熵损失。</td></tr><tr><td align="left"><code>torch.nn.NLLLoss()</code></td><td align="left">负对数似然损失。</td></tr><tr><td align="left"><code>torch.nn.BCELoss()</code></td><td align="left">二分类交叉熵损失。</td></tr><tr><td align="left"><code>torch.nn.BCEWithLogitsLoss()</code></td><td align="left">带 Sigmoid 的二分类交叉熵损失。</td></tr><tr><td align="left"><code>torch.nn.KLDivLoss()</code></td><td align="left">KL 散度损失。</td></tr><tr><td align="left"><code>torch.nn.HingeEmbeddingLoss()</code></td><td align="left">铰链嵌入损失。</td></tr><tr><td align="left"><code>torch.nn.MultiMarginLoss()</code></td><td align="left">多分类间隔损失。</td></tr><tr><td align="left"><code>torch.nn.SmoothL1Loss()</code></td><td align="left">平滑 L1 损失。</td></tr></tbody></table><hr><h3 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a><strong>归一化层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.BatchNorm1d(num_features)</code></td><td align="left">一维批归一化层。</td></tr><tr><td align="left"><code>torch.nn.BatchNorm2d(num_features)</code></td><td align="left">二维批归一化层。</td></tr><tr><td align="left"><code>torch.nn.BatchNorm3d(num_features)</code></td><td align="left">三维批归一化层。</td></tr><tr><td align="left"><code>torch.nn.LayerNorm(normalized_shape)</code></td><td align="left">层归一化。</td></tr><tr><td align="left"><code>torch.nn.InstanceNorm1d(num_features)</code></td><td align="left">一维实例归一化层。</td></tr><tr><td align="left"><code>torch.nn.InstanceNorm2d(num_features)</code></td><td align="left">二维实例归一化层。</td></tr><tr><td align="left"><code>torch.nn.InstanceNorm3d(num_features)</code></td><td align="left">三维实例归一化层。</td></tr><tr><td align="left"><code>torch.nn.GroupNorm(num_groups, num_channels)</code></td><td align="left">组归一化。</td></tr></tbody></table><hr><h3 id="循环神经网络层"><a href="#循环神经网络层" class="headerlink" title="循环神经网络层"></a><strong>循环神经网络层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.RNN(input_size, hidden_size)</code></td><td align="left">简单 RNN 层。</td></tr><tr><td align="left"><code>torch.nn.LSTM(input_size, hidden_size)</code></td><td align="left">LSTM 层。</td></tr><tr><td align="left"><code>torch.nn.GRU(input_size, hidden_size)</code></td><td align="left">GRU 层。</td></tr><tr><td align="left"><code>torch.nn.RNNCell(input_size, hidden_size)</code></td><td align="left">简单 RNN 单元。</td></tr><tr><td align="left"><code>torch.nn.LSTMCell(input_size, hidden_size)</code></td><td align="left">LSTM 单元。</td></tr><tr><td align="left"><code>torch.nn.GRUCell(input_size, hidden_size)</code></td><td align="left">GRU 单元。</td></tr></tbody></table><hr><h3 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a><strong>嵌入层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.Embedding(num_embeddings, embedding_dim)</code></td><td align="left">嵌入层。</td></tr></tbody></table><hr><h3 id="Dropout-层"><a href="#Dropout-层" class="headerlink" title="Dropout 层"></a><strong>Dropout 层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.Dropout(p)</code></td><td align="left">Dropout 层。</td></tr><tr><td align="left"><code>torch.nn.Dropout2d(p)</code></td><td align="left">2D Dropout 层。</td></tr><tr><td align="left"><code>torch.nn.Dropout3d(p)</code></td><td align="left">3D Dropout 层。</td></tr></tbody></table><hr><h3 id="实用函数"><a href="#实用函数" class="headerlink" title="实用函数"></a><strong>实用函数</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.functional.relu(input)</code></td><td align="left">应用 ReLU 激活函数。</td></tr><tr><td align="left"><code>torch.nn.functional.sigmoid(input)</code></td><td align="left">应用 Sigmoid 激活函数。</td></tr><tr><td align="left"><code>torch.nn.functional.softmax(input, dim)</code></td><td align="left">应用 Softmax 激活函数。</td></tr><tr><td align="left"><code>torch.nn.functional.cross_entropy(input, target)</code></td><td align="left">计算交叉熵损失。</td></tr><tr><td align="left"><code>torch.nn.functional.mse_loss(input, target)</code></td><td align="left">计算均方误差损失。</td></tr></tbody></table><hr><h3 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># 定义一个简单的神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">20</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建模型和输入</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">output = model(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h1 id="Transformer-模型"><a href="#Transformer-模型" class="headerlink" title="Transformer 模型"></a>Transformer 模型</h1><p>Transformer 模型是一种<strong>基于注意力机制的深度学习模型</strong>，最初由 Vaswani 等人在 2017 年的论文《Attention is All You Need》中提出。</p><p>Transformer <strong>彻底改变了自然语言处理（NLP）领域，并逐渐扩展到计算机视觉（CV）等领域</strong>。</p><p>Transformer 的核心思想是<strong>完全摒弃传统的循环神经网络（RNN）结构，仅依赖注意力机制来处理序列数据</strong>，从而实现<strong>更高的并行性</strong>和<strong>更快的训练速度</strong>。</p><p>以下是 Transformer 架构图，左边为编码器，右边为解码器。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Transformer_full_architecture.png" alt="img"></p><p>Transformer 模型由 编码器（Encoder） 和 解码器（Decoder） 两部分组成，每部分都由多层堆叠的相同模块构成。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/runoob-transformer-1.png" alt="img">编码器（Encoder）</p><p>编码器<strong>由 N 层相同的模块堆叠</strong>而成，每层包含两个子层：</p><ul><li><strong>多头自注意力机制（Multi-Head Self-Attention）：<strong>计算输入序列中每个词与其他词的</strong>相关性</strong>。</li><li><strong>前馈神经网络（Feed-Forward Neural Network）：<strong>对每个词进行独立的</strong>非线性变换</strong>。</li></ul><p>每个子层后面都接有 <strong>残差连接（Residual Connection）</strong> 和 <strong>层归一化（Layer Normalization）</strong>。</p><h3 id="解码器（Decoder）"><a href="#解码器（Decoder）" class="headerlink" title="解码器（Decoder）"></a>解码器（Decoder）</h3><p>解码器也由 N 层相同的模块堆叠而成，每层包含三个子层：</p><ul><li><strong>掩码多头自注意力机制（Masked Multi-Head Self-Attention）：<strong>计算输出序列中每个词与前面词的</strong>相关性（使用掩码防止未来信息泄露）</strong>。</li><li><strong>编码器-解码器注意力机制（Encoder-Decoder Attention）：<strong>计算输出序列</strong>与输入序列的相关性</strong>。</li><li><strong>前馈神经网络（Feed-Forward Neural Network）：<strong>对每个词进行独立的</strong>非线性变换</strong>。</li></ul><p>同样，每个子层后面都接有残差连接和层归一化。</p><p>在 Transformer 模型出现之前，NLP 领域的主流模型是基于 RNN 的架构，如长短期记忆网络（LSTM）和门控循环单元（GRU）。这些模型通过顺序处理输入数据来捕捉序列中的依赖关系，但存在以下问题：</p><ol><li><strong>梯度消失问题</strong>：<strong>长距离依赖关系难以捕捉</strong>。</li><li><strong>顺序计算的局限性</strong>：无法充分利用现代硬件的并行计算能力，训练效率低下。</li></ol><p>Transformer 通过引入自注意力机制解决了这些问题，允许模型同时处理整个输入序列，并动态地为序列中的每个位置分配不同的权重。</p><hr><h2 id="Transformer-的核心思想"><a href="#Transformer-的核心思想" class="headerlink" title="Transformer 的核心思想"></a>Transformer 的核心思想</h2><h3 id="1-自注意力机制（Self-Attention）"><a href="#1-自注意力机制（Self-Attention）" class="headerlink" title="1. 自注意力机制（Self-Attention）"></a>1. 自注意力机制（Self-Attention）</h3><p>自注意力机制是 Transformer 的核心组件。</p><p>自注意力机制允许模型在处理序列时，<strong>动态地为每个位置分配不同的权重</strong>，从而捕捉序列中任意两个位置之间的依赖关系。</p><ul><li><strong>输入表示</strong>：输入序列中的每个词（或标记）<strong>通过词嵌入（Embedding）转换为向量表示</strong>。</li><li><strong>注意力权重计算</strong>：通过<strong>计算查询（Query）、键（Key）和值（Value）之间的点积</strong>，得到每个词<strong>与其他词的相关性权重</strong>。</li><li><strong>加权求和</strong>：使用注意力权重对值（Value）进行<strong>加权求和</strong>，得到<strong>每个词的上下文表示</strong>。</li></ul><p>公式如下：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/192f2ceca2b507875d3c7eef6f0367e5.png" alt="192f2ceca2b507875d3c7eef6f0367e5"></p><p>其中：</p><ul><li>Q 是查询矩阵，K 是键矩阵，V是值矩阵。</li><li>dk是向量的维度，用于<strong>缩放点积，防止梯度爆炸</strong>。</li></ul><h3 id="多头注意力（Multi-Head-Attention）"><a href="#多头注意力（Multi-Head-Attention）" class="headerlink" title="多头注意力（Multi-Head Attention）"></a>多头注意力（Multi-Head Attention）</h3><p>为了捕捉更丰富的特征，Transformer 使用<strong>多头</strong>注意力机制。它将<strong>输入分成多个子空间</strong>，每个子空间独立计算注意力，<strong>最后将结果拼接</strong>起来。</p><ul><li><strong>多头注意力的优势</strong>：允许模型关注序列中不同的部分，例如语法结构、语义关系等。</li><li><strong>并行计算</strong>：多个注意力头可以并行计算，提高效率。</li></ul><h3 id="位置编码（Positional-Encoding）"><a href="#位置编码（Positional-Encoding）" class="headerlink" title="位置编码（Positional Encoding）"></a>位置编码（Positional Encoding）</h3><p>由于 Transformer <strong>没有显式的序列信息（如 RNN 中的时间步）</strong>，<strong>位置编码</strong>被用来<strong>为输入序列中的每个词添加位置信息</strong>。通常<strong>使用正弦和余弦函数生成位置编码</strong>：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/b92014de131411175caef93b06e4006f.png" alt="b92014de131411175caef93b06e4006f"></p><h2 id="Transformer-的应用"><a href="#Transformer-的应用" class="headerlink" title="Transformer 的应用"></a>Transformer 的应用</h2><ol><li><strong>自然语言处理（NLP）</strong>：<ul><li>机器翻译（如 Google Translate）</li><li>文本生成（如 GPT 系列模型）</li><li>文本分类、问答系统等。</li></ul></li><li><strong>计算机视觉（CV）</strong>：<ul><li>图像分类（如 Vision Transformer）</li><li>目标检测、图像生成等。</li></ul></li><li><strong>多模态任务</strong>：<ul><li>结合文本和图像的任务（如 CLIP、DALL-E）。</li></ul></li></ol><hr><h2 id="PyTorch-实现-Transformer"><a href="#PyTorch-实现-Transformer" class="headerlink" title="PyTorch 实现 Transformer"></a>PyTorch 实现 Transformer</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, model_dim, num_heads, num_layers, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(input_dim, model_dim)</span><br><span class="line">        <span class="variable language_">self</span>.positional_encoding = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1000</span>, model_dim))  <span class="comment"># 假设序列长度最大为1000</span></span><br><span class="line">        <span class="variable language_">self</span>.transformer = nn.Transformer(d_model=model_dim, nhead=num_heads, num_encoder_layers=num_layers)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(model_dim, output_dim)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        src_seq_length, tgt_seq_length = src.size(<span class="number">1</span>), tgt.size(<span class="number">1</span>)</span><br><span class="line">        src = <span class="variable language_">self</span>.embedding(src) + <span class="variable language_">self</span>.positional_encoding[:, :src_seq_length, :]</span><br><span class="line">        tgt = <span class="variable language_">self</span>.embedding(tgt) + <span class="variable language_">self</span>.positional_encoding[:, :tgt_seq_length, :]</span><br><span class="line">        transformer_output = <span class="variable language_">self</span>.transformer(src, tgt)</span><br><span class="line">        output = <span class="variable language_">self</span>.fc(transformer_output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">input_dim = <span class="number">10000</span>  <span class="comment"># 词汇表大小</span></span><br><span class="line">model_dim = <span class="number">512</span>    <span class="comment"># 模型维度</span></span><br><span class="line">num_heads = <span class="number">8</span>      <span class="comment"># 多头注意力头数</span></span><br><span class="line">num_layers = <span class="number">6</span>     <span class="comment"># 编码器和解码器层数</span></span><br><span class="line">output_dim = <span class="number">10000</span> <span class="comment"># 输出维度（通常与词汇表大小相同）</span></span><br><span class="line"><span class="comment"># 初始化模型、损失函数和优化器</span></span><br><span class="line">model = TransformerModel(input_dim, model_dim, num_heads, num_layers, output_dim)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 假设输入数据</span></span><br><span class="line">src = torch.randint(<span class="number">0</span>, input_dim, (<span class="number">10</span>, <span class="number">32</span>))  <span class="comment"># (序列长度, 批量大小)</span></span><br><span class="line">tgt = torch.randint(<span class="number">0</span>, input_dim, (<span class="number">20</span>, <span class="number">32</span>))  <span class="comment"># (序列长度, 批量大小)</span></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = model(src, tgt)</span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = criterion(output.view(-<span class="number">1</span>, output_dim), tgt.view(-<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 反向传播和优化</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Loss:&quot;</span>, loss.item())</span><br></pre></td></tr></table></figure><h1 id="PyTorch-构建-Transformer-模型"><a href="#PyTorch-构建-Transformer-模型" class="headerlink" title="PyTorch 构建 Transformer 模型"></a>PyTorch 构建 Transformer 模型</h1><p>Transformer 是现代机器学习中最强大的模型之一。</p><p>Transformer 模型是一种基于自注意力机制（Self-Attention） 的深度学习架构，它彻底改变了自然语言处理（NLP）领域，并成为现代深度学习模型（如 BERT、GPT 等）的基础。</p><p>Transformer 是现代 NLP 领域的核心架构，凭借其强大的长距离依赖建模能力和高效的并行计算优势，在语言翻译和文本摘要等任务中超越了传统的 长短期记忆 (LSTM) 网络。</p><h2 id="使用-PyTorch-构建-Transformer-模型"><a href="#使用-PyTorch-构建-Transformer-模型" class="headerlink" title="使用 PyTorch 构建 Transformer 模型"></a>使用 PyTorch 构建 Transformer 模型</h2><p><strong>构建 Transformer 模型的步骤如下：</strong></p><h3 id="1、导入必要的库和模块"><a href="#1、导入必要的库和模块" class="headerlink" title="1、导入必要的库和模块"></a>1、导入必要的库和模块</h3><p>导入 PyTorch 核心库、神经网络模块、优化器模块、数据处理工具，以及数学和对象复制模块，为定义模型架构、管理数据和训练过程提供支持。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> copy</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><code>torch</code>：PyTorch 的<strong>核心库</strong>，<strong>用于张量操作和自动求导</strong>。</li><li><code>torch.nn</code>：PyTorch 的<strong>神经网络模块</strong>，包含各<strong>种层和损失函数</strong>。</li><li><code>torch.optim</code>：<strong>优化算法模块</strong>，如 Adam、SGD 等。</li><li><code>math</code>：数学函数库，用于计算平方根等。</li><li><code>copy</code>：用于<strong>深度复制对象</strong>。</li></ul><h3 id="定义基本构建块：多头注意力、位置前馈网络、位置编码"><a href="#定义基本构建块：多头注意力、位置前馈网络、位置编码" class="headerlink" title="定义基本构建块：多头注意力、位置前馈网络、位置编码"></a>定义基本构建块：多头注意力、位置前馈网络、位置编码</h3><p><strong>多头注意力</strong>通过多个”注意力头”计算序列中每对位置之间的关系，能够捕捉输入序列的不同特征和模式。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Figure_1_Multi_Head_Attention_source_image_created_by_author_653bad32f1.avif" alt="img"></p><p><strong>MultiHeadAttention 类封装了 Transformer 模型中常用的多头注意力机制</strong>，负责<strong>将输入拆分成多个注意力头</strong>，对每个注意力头施加注意力，然后将结果组合起来，这样模型就可以在不同尺度上捕捉输入数据中的各种关系，提高模型的表达能力。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % num_heads == <span class="number">0</span>, <span class="string">&quot;d_model必须能被num_heads整除&quot;</span>        </span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model    <span class="comment"># 模型维度（如512）</span></span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads <span class="comment"># 注意力头数（如8）</span></span><br><span class="line">        <span class="variable language_">self</span>.d_k = d_model // num_heads <span class="comment"># 每个头的维度（如64）        </span></span><br><span class="line">        <span class="comment"># 定义线性变换层（无需偏置）</span></span><br><span class="line">        <span class="variable language_">self</span>.W_q = nn.Linear(d_model, d_model) <span class="comment"># 查询变换</span></span><br><span class="line">        <span class="variable language_">self</span>.W_k = nn.Linear(d_model, d_model) <span class="comment"># 键变换</span></span><br><span class="line">        <span class="variable language_">self</span>.W_v = nn.Linear(d_model, d_model) <span class="comment"># 值变换</span></span><br><span class="line">        <span class="variable language_">self</span>.W_o = nn.Linear(d_model, d_model) <span class="comment"># 输出变换       </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算缩放点积注意力</span></span><br><span class="line"><span class="string">        输入形状：</span></span><br><span class="line"><span class="string">            Q: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">            K, V: 同Q</span></span><br><span class="line"><span class="string">        输出形状： (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 计算注意力分数（Q和K的点积）</span></span><br><span class="line">        attn_scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(<span class="variable language_">self</span>.d_k)       </span><br><span class="line">        <span class="comment"># 应用掩码（如填充掩码或未来信息掩码）</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn_scores = attn_scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)     </span><br><span class="line">        <span class="comment"># 计算注意力权重（softmax归一化）</span></span><br><span class="line">        attn_probs = torch.softmax(attn_scores, dim=-<span class="number">1</span>)      </span><br><span class="line">        <span class="comment"># 对值向量加权求和</span></span><br><span class="line">        output = torch.matmul(attn_probs, V)</span><br><span class="line">        <span class="keyword">return</span> output       </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">split_heads</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将输入张量分割为多个头</span></span><br><span class="line"><span class="string">        输入形状: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        输出形状: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, seq_length, d_model = x.size()</span><br><span class="line">        <span class="keyword">return</span> x.view(batch_size, seq_length, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">combine_heads</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将多个头的输出合并回原始形状</span></span><br><span class="line"><span class="string">        输入形状: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">        输出形状: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, _, seq_length, d_k = x.size()</span><br><span class="line">        <span class="keyword">return</span> x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, seq_length, <span class="variable language_">self</span>.d_model)       </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播</span></span><br><span class="line"><span class="string">        输入形状: Q/K/V: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        输出形状: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 线性变换并分割多头</span></span><br><span class="line">        Q = <span class="variable language_">self</span>.split_heads(<span class="variable language_">self</span>.W_q(Q)) <span class="comment"># (batch, heads, seq_len, d_k)</span></span><br><span class="line">        K = <span class="variable language_">self</span>.split_heads(<span class="variable language_">self</span>.W_k(K))</span><br><span class="line">        V = <span class="variable language_">self</span>.split_heads(<span class="variable language_">self</span>.W_v(V))        </span><br><span class="line">        <span class="comment"># 计算注意力</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.scaled_dot_product_attention(Q, K, V, mask)        </span><br><span class="line">        <span class="comment"># 合并多头并输出变换</span></span><br><span class="line">        output = <span class="variable language_">self</span>.W_o(<span class="variable language_">self</span>.combine_heads(attn_output))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><strong>多头注意力机制</strong>：将输入分割成多个头，每个头独立计算注意力，最后将结果合并。</li><li><strong>缩放点积注意力</strong>：计算查询和键的点积，缩放后使用 softmax 计算注意力权重，最后对值进行加权求和。</li><li><strong>掩码</strong>：用于屏蔽无效位置（如填充部分）。</li></ul><h3 id="位置前馈网络（Position-wise-Feed-Forward-Network）"><a href="#位置前馈网络（Position-wise-Feed-Forward-Network）" class="headerlink" title="位置前馈网络（Position-wise Feed-Forward Network）"></a>位置前馈网络（Position-wise Feed-Forward Network）</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFeedForward, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(d_model, d_ff)  <span class="comment"># 第一层全连接</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(d_ff, d_model)  <span class="comment"># 第二层全连接</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()  <span class="comment"># 激活函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 前馈网络的计算</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc2(<span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.fc1(x)))</span><br></pre></td></tr></table></figure><p>**前馈网络：**由两个全连接层和一个 ReLU 激活函数组成，用于进一步处理注意力机制的输出。</p><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>位置编码用于注入输入序列中<strong>每个 token 的位置信息</strong>。</p><p>使用<strong>不同频率的正弦和余弦函数来生成位置编码</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_seq_length</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        pe = torch.zeros(max_seq_length, d_model)  <span class="comment"># 初始化位置编码矩阵</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_seq_length, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)  <span class="comment"># 偶数位置使用正弦函数</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)  <span class="comment"># 奇数位置使用余弦函数</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe.unsqueeze(<span class="number">0</span>))  <span class="comment"># 注册为缓冲区</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 将位置编码添加到输入中</span></span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.pe[:, :x.size(<span class="number">1</span>)]</span><br></pre></td></tr></table></figure><h3 id="构建编码器块（Encoder-Layer）"><a href="#构建编码器块（Encoder-Layer）" class="headerlink" title="构建编码器块（Encoder Layer）"></a>构建编码器块（Encoder Layer）</h3><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Figure_2_The_Encoder_part_of_the_transformer_network_Source_image_from_the_original_paper_b0e3ac40fa.avif" alt="img"></p><p><strong>编码器层：<strong>包含</strong>一个自注意力机制和一个前馈网络</strong>，每个子层后接残差连接和层归一化。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads, d_ff, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = MultiHeadAttention(d_model, num_heads)  <span class="comment"># 自注意力机制</span></span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = PositionWiseFeedForward(d_model, d_ff)  <span class="comment"># 前馈网络</span></span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(d_model)  <span class="comment"># 层归一化</span></span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># Dropout       </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="comment"># 自注意力机制</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.self_attn(x, x, x, mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x + <span class="variable language_">self</span>.dropout(attn_output))  <span class="comment"># 残差连接和层归一化     </span></span><br><span class="line">        <span class="comment"># 前馈网络</span></span><br><span class="line">        ff_output = <span class="variable language_">self</span>.feed_forward(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x + <span class="variable language_">self</span>.dropout(ff_output))  <span class="comment"># 残差连接和层归一化</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="构建解码器模块"><a href="#构建解码器模块" class="headerlink" title="构建解码器模块"></a>构建解码器模块</h3><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Figure_3_The_Decoder_part_of_the_Transformer_network_Souce_Image_from_the_original_paper_b90d9e7f66.avif" alt="img"></p><p><strong>解码器层：<strong>包含</strong>一个自注意力机制、一个交叉注意力机制和一个前馈网络</strong>，每个子层后接残差连接和层归一化。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads, d_ff, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = MultiHeadAttention(d_model, num_heads)  <span class="comment"># 自注意力机制</span></span><br><span class="line">        <span class="variable language_">self</span>.cross_attn = MultiHeadAttention(d_model, num_heads)  <span class="comment"># 交叉注意力机制</span></span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = PositionWiseFeedForward(d_model, d_ff)  <span class="comment"># 前馈网络</span></span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(d_model)  <span class="comment"># 层归一化</span></span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.norm3 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># Dropout</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, enc_output, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="comment"># 自注意力机制</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.self_attn(x, x, x, tgt_mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x + <span class="variable language_">self</span>.dropout(attn_output))  <span class="comment"># 残差连接和层归一化     </span></span><br><span class="line">        <span class="comment"># 交叉注意力机制</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.cross_attn(x, enc_output, enc_output, src_mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x + <span class="variable language_">self</span>.dropout(attn_output))  <span class="comment"># 残差连接和层归一化  </span></span><br><span class="line">        <span class="comment"># 前馈网络</span></span><br><span class="line">        ff_output = <span class="variable language_">self</span>.feed_forward(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm3(x + <span class="variable language_">self</span>.dropout(ff_output))  <span class="comment"># 残差连接和层归一化</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="构建完整的-Transformer-模型"><a href="#构建完整的-Transformer-模型" class="headerlink" title="构建完整的 Transformer 模型"></a>构建完整的 Transformer 模型</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder_embedding = nn.Embedding(src_vocab_size, d_model)  <span class="comment"># 编码器词嵌入</span></span><br><span class="line">        <span class="variable language_">self</span>.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)  <span class="comment"># 解码器词嵌入</span></span><br><span class="line">        <span class="variable language_">self</span>.positional_encoding = PositionalEncoding(d_model, max_seq_length)  <span class="comment"># 位置编码</span></span><br><span class="line">        <span class="comment"># 编码器和解码器层</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)])</span><br><span class="line">        <span class="variable language_">self</span>.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)])</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(d_model, tgt_vocab_size)  <span class="comment"># 最终的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># Dropout</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_mask</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        <span class="comment"># 源掩码：屏蔽填充符（假设填充符索引为0）</span></span><br><span class="line">        <span class="comment"># 形状：(batch_size, 1, 1, seq_length)</span></span><br><span class="line">        src_mask = (src != <span class="number">0</span>).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 目标掩码：屏蔽填充符和未来信息</span></span><br><span class="line">        <span class="comment"># 形状：(batch_size, 1, seq_length, 1)</span></span><br><span class="line">        tgt_mask = (tgt != <span class="number">0</span>).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">3</span>)</span><br><span class="line">        seq_length = tgt.size(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 生成上三角矩阵掩码，防止解码时看到未来信息</span></span><br><span class="line">        nopeak_mask = (<span class="number">1</span> - torch.triu(torch.ones(<span class="number">1</span>, seq_length, seq_length), diagonal=<span class="number">1</span>)).<span class="built_in">bool</span>()</span><br><span class="line">        tgt_mask = tgt_mask &amp; nopeak_mask  <span class="comment"># 合并填充掩码和未来信息掩码</span></span><br><span class="line">        <span class="keyword">return</span> src_mask, tgt_mask</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        <span class="comment"># 生成掩码</span></span><br><span class="line">        src_mask, tgt_mask = <span class="variable language_">self</span>.generate_mask(src, tgt)</span><br><span class="line">        <span class="comment"># 编码器部分</span></span><br><span class="line">        src_embedded = <span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.positional_encoding(<span class="variable language_">self</span>.encoder_embedding(src)))</span><br><span class="line">        enc_output = src_embedded</span><br><span class="line">        <span class="keyword">for</span> enc_layer <span class="keyword">in</span> <span class="variable language_">self</span>.encoder_layers:</span><br><span class="line">            enc_output = enc_layer(enc_output, src_mask)</span><br><span class="line">        <span class="comment"># 解码器部分</span></span><br><span class="line">        tgt_embedded = <span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.positional_encoding(<span class="variable language_">self</span>.decoder_embedding(tgt)))</span><br><span class="line">        dec_output = tgt_embedded</span><br><span class="line">        <span class="keyword">for</span> dec_layer <span class="keyword">in</span> <span class="variable language_">self</span>.decoder_layers:</span><br><span class="line">            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)        </span><br><span class="line">        <span class="comment"># 最终输出</span></span><br><span class="line">        output = <span class="variable language_">self</span>.fc(dec_output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><strong>Transformer 模型</strong>：包含编码器和解码器部分，每个部分由多个层堆叠而成。</li><li><strong>掩码生成</strong>：用于屏蔽无效位置和未来信息。</li><li><strong>前向传播</strong>：依次通过编码器和解码器，最后通过全连接层输出。</li></ul><p>模型初始化参数说明：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Transformer(nn.Module):</span><br><span class="line">    def __init__(</span><br><span class="line">        self, </span><br><span class="line">        src_vocab_size,  # 源语言词汇表大小（如英文单词数）</span><br><span class="line">        tgt_vocab_size,  # 目标语言词汇表大小（如中文单词数）</span><br><span class="line">        d_model=512,     # 模型维度（每个词向量的长度）</span><br><span class="line">        num_heads=8,     # 多头注意力的头数</span><br><span class="line">        num_layers=6,    # 编码器/解码器的堆叠层数</span><br><span class="line">        d_ff=2048,       # 前馈网络隐藏层维度</span><br><span class="line">        max_seq_length=100, # 最大序列长度（用于位置编码）</span><br><span class="line">        dropout=0.1      # Dropout概率</span><br><span class="line">    ):</span><br></pre></td></tr></table></figure><h3 id="训练-PyTorch-Transformer-模型"><a href="#训练-PyTorch-Transformer-模型" class="headerlink" title="训练 PyTorch Transformer 模型"></a>训练 PyTorch Transformer 模型</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">src_vocab_size = <span class="number">5000</span>  <span class="comment"># 源词汇表大小</span></span><br><span class="line">tgt_vocab_size = <span class="number">5000</span>  <span class="comment"># 目标词汇表大小</span></span><br><span class="line">d_model = <span class="number">512</span>  <span class="comment"># 模型维度</span></span><br><span class="line">num_heads = <span class="number">8</span>  <span class="comment"># 注意力头数量</span></span><br><span class="line">num_layers = <span class="number">6</span>  <span class="comment"># 编码器和解码器层数</span></span><br><span class="line">d_ff = <span class="number">2048</span>  <span class="comment"># 前馈网络内层维度</span></span><br><span class="line">max_seq_length = <span class="number">100</span>  <span class="comment"># 最大序列长度</span></span><br><span class="line">dropout = <span class="number">0.1</span>  <span class="comment"># Dropout 概率</span></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)</span><br><span class="line"><span class="comment"># 生成随机数据</span></span><br><span class="line">src_data = torch.randint(<span class="number">1</span>, src_vocab_size, (<span class="number">64</span>, max_seq_length))  <span class="comment"># 源序列</span></span><br><span class="line">tgt_data = torch.randint(<span class="number">1</span>, tgt_vocab_size, (<span class="number">64</span>, max_seq_length))  <span class="comment"># 目标序列</span></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>)  <span class="comment"># 忽略填充部分的损失</span></span><br><span class="line">optimizer = optim.Adam(transformer.parameters(), lr=<span class="number">0.0001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>)</span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line">transformer.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清空梯度，防止累积</span></span><br><span class="line">    <span class="comment"># 输入目标序列时去掉最后一个词（用于预测下一个词）</span></span><br><span class="line">    output = transformer(src_data, tgt_data[:, :-<span class="number">1</span>])      </span><br><span class="line">    <span class="comment"># 计算损失时，目标序列从第二个词开始（即预测下一个词）</span></span><br><span class="line">    <span class="comment"># output形状: (batch_size, seq_length-1, tgt_vocab_size)</span></span><br><span class="line">    <span class="comment"># 目标形状: (batch_size, seq_length-1)</span></span><br><span class="line">    loss = criterion(</span><br><span class="line">        output.contiguous().view(-<span class="number">1</span>, tgt_vocab_size), </span><br><span class="line">        tgt_data[:, <span class="number">1</span>:].contiguous().view(-<span class="number">1</span>)</span><br><span class="line">    )    </span><br><span class="line">    loss.backward()        <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()       <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">transformer.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 生成验证数据</span></span><br><span class="line">val_src_data = torch.randint(<span class="number">1</span>, src_vocab_size, (<span class="number">64</span>, max_seq_length))</span><br><span class="line">val_tgt_data = torch.randint(<span class="number">1</span>, tgt_vocab_size, (<span class="number">64</span>, max_seq_length))</span><br><span class="line"><span class="comment"># 假设输入为一批英文和对应的中文翻译（已转换为索引）</span></span><br><span class="line"><span class="comment"># 示例数据：</span></span><br><span class="line"><span class="comment"># src_data: [[3, 14, 25, ..., 0, 0], ...]  # 英文句子（0为填充符）</span></span><br><span class="line"><span class="comment"># tgt_data: [[5, 20, 36, ..., 0, 0], ...]  # 中文翻译（0为填充符）</span></span><br><span class="line"><span class="comment"># 注意：实际应用中需对文本进行分词、编码、填充等预处理</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    val_output = transformer(val_src_data, val_tgt_data[:, :-<span class="number">1</span>])</span><br><span class="line">    val_loss = criterion(val_output.contiguous().view(-<span class="number">1</span>, tgt_vocab_size), val_tgt_data[:, <span class="number">1</span>:].contiguous().view(-<span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Validation Loss: <span class="subst">&#123;val_loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="Python-入门机器学习"><a href="#Python-入门机器学习" class="headerlink" title="Python 入门机器学习"></a>Python 入门机器学习</h1><p>在使用 Python 进行机器学习时，整个过程一般遵循以下步骤：</p><ol><li><strong>导入必要的库</strong> - 例如，NumPy、Pandas 和 Scikit-learn。</li><li><strong>加载和准备数据</strong> - 数据是机器学习的核心。你需要加载数据并进行必要的预处理（例如数据清洗、缺失值填补等）。</li><li><strong>选择模型和算法</strong> - 根据任务选择适合的机器学习算法（如线性回归、决策树等）。</li><li><strong>训练模型</strong> - 使用训练集数据来训练模型。</li><li><strong>评估模型</strong> - 使用测试集评估模型的准确性，并根据评估结果优化模型。</li><li><strong>调整模型和超参数</strong> - 根据评估结果调整模型的超参数，进一步优化模型性能。</li></ol><h2 id="一个简单的机器学习例子：使用-Scikit-learn-做分类"><a href="#一个简单的机器学习例子：使用-Scikit-learn-做分类" class="headerlink" title="一个简单的机器学习例子：使用 Scikit-learn 做分类"></a>一个简单的机器学习例子：使用 Scikit-learn 做分类</h2><p><strong>Scikit-learn（简称 Sklearn）是一个开源的机器学习库</strong>，建立在 NumPy、SciPy 和 matplotlib 这些科学计算库之上，提供了简单高效的数据挖掘和数据分析工具。</p><p>Scikit-learn 包含了许多常见的机器学习算法，包括：</p><ul><li>线性回归、岭回归、Lasso回归</li><li>支持向量机（SVM）</li><li>决策树、随机森林、梯度提升树</li><li>聚类算法（如K-Means、层次聚类、DBSCAN）</li><li>降维技术（如PCA、t-SNE）</li><li>神经网络</li></ul><p>接下来我们通过一个简单的分类任务——使用鸢尾花数据集（Iris Dataset）来演示机器学习的流程，鸢尾花数据集是一个经典的数据集，包含 150 个样本，描述了三种不同类型的鸢尾花的花瓣和萼片的长度和宽度。</p><h3 id="步骤-1：导入库"><a href="#步骤-1：导入库" class="headerlink" title="步骤 1：导入库"></a>步骤 1：导入库</h3><p>导入需要的 Python 库：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br></pre></td></tr></table></figure><h3 id="步骤-2：加载数据"><a href="#步骤-2：加载数据" class="headerlink" title="步骤 2：加载数据"></a>步骤 2：加载数据</h3><p>加载鸢尾花数据集：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转化为 pandas DataFrame</span></span><br><span class="line">X = pd.DataFrame(iris.data, columns=iris.feature_names)  <span class="comment"># 特征数据</span></span><br><span class="line">y = pd.Series(iris.target)  <span class="comment"># 标签数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示前五行数据</span></span><br><span class="line"><span class="built_in">print</span>(X.head())</span><br></pre></td></tr></table></figure><p>打印输出数据如下所示：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)</span><br><span class="line"><span class="number">0</span>                <span class="number">5.1</span>               <span class="number">3.5</span>                <span class="number">1.4</span>               <span class="number">0.2</span></span><br><span class="line"><span class="number">1</span>                <span class="number">4.9</span>               <span class="number">3.0</span>                <span class="number">1.4</span>               <span class="number">0.2</span></span><br><span class="line"><span class="number">2</span>                <span class="number">4.7</span>               <span class="number">3.2</span>                <span class="number">1.3</span>               <span class="number">0.2</span></span><br><span class="line"><span class="number">3</span>                <span class="number">4.6</span>               <span class="number">3.1</span>                <span class="number">1.5</span>               <span class="number">0.2</span></span><br><span class="line"><span class="number">4</span>                <span class="number">5.0</span>               <span class="number">3.6</span>                <span class="number">1.4</span>               <span class="number">0.2</span></span><br></pre></td></tr></table></figure><h3 id="步骤-3：数据集划分"><a href="#步骤-3：数据集划分" class="headerlink" title="步骤 3：数据集划分"></a>步骤 3：数据集划分</h3><p>将数据集划分为训练集和测试集，<strong>通常使用 70% 训练集和 30% 测试集</strong>的比例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分训练集和测试集（80% 训练集，20% 测试集）</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><h3 id="步骤-4：特征缩放（标准化）"><a href="#步骤-4：特征缩放（标准化）" class="headerlink" title="步骤 4：特征缩放（标准化）"></a>步骤 4：特征缩放（标准化）</h3><p>许多机器学习算法都依赖于特征的尺度，特别是像 K 最近邻算法。为了确保每个特征的均值为 0，标准差为 1，我们使用标准化来处理数据：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标准化特征</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_test = scaler.transform(X_test)</span><br></pre></td></tr></table></figure><h3 id="步骤-5：选择模型并训练"><a href="#步骤-5：选择模型并训练" class="headerlink" title="步骤 5：选择模型并训练"></a>步骤 5：选择模型并训练</h3><p>在这个例子中，我们选择 K-Nearest Neighbors（KNN） 算法来进行分类：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 KNN 分类器</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><h3 id="步骤-6：评估模型"><a href="#步骤-6：评估模型" class="headerlink" title="步骤 6：评估模型"></a>步骤 6：评估模型</h3><p>训练完成后，我们使用测试集评估模型的准确性：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测测试集</span></span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;模型准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>完成以上代码，输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">模型准确率: 1.00</span><br></pre></td></tr></table></figure><h3 id="步骤-7：可视化结果（可选）"><a href="#步骤-7：可视化结果（可选）" class="headerlink" title="步骤 7：可视化结果（可选）"></a>步骤 7：可视化结果（可选）</h3><p>你可以通过可视化来进一步了解模型的表现，尤其是在多维数据集的情况下。例如，你可以用二维图来显示 KNN 分类的结果（不过在这里需要对数据进行降维，<strong>简化为二维</strong>）。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转化为 pandas DataFrame</span></span><br><span class="line">X = pd.DataFrame(iris.data, columns=iris.feature_names)  <span class="comment"># 特征数据</span></span><br><span class="line">y = pd.Series(iris.target)  <span class="comment"># 标签数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集（80% 训练集，20% 测试集）</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化特征</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_test = scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 KNN 分类器</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测测试集</span></span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化 - 这里只是一个简单示例，具体可根据实际情况选择绘图方式</span></span><br><span class="line">plt.scatter(X_test[:, <span class="number">0</span>], X_test[:, <span class="number">1</span>], c=y_pred, cmap=<span class="string">&#x27;viridis&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;KNN Classification Results&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 2&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/knn-python-ml-1.png" alt="img"></p><h1 id="机器学习算法"><a href="#机器学习算法" class="headerlink" title="机器学习算法"></a>机器学习算法</h1><p>机器学习算法可以分为监督学习、无监督学习、强化学习等类别。</p><p><strong>监督学习算法</strong>：</p><ul><li><strong>线性回归</strong>（Linear Regression）：用于<strong>回归任务</strong>，预测连续的数值。</li><li><strong>逻辑回归</strong>（Logistic Regression）：用于<strong>二分类任务</strong>，预测类别。</li><li><strong>支持向量机</strong>（SVM）：用于<strong>分类任务</strong>，构建超平面进行分类。</li><li><strong>决策树</strong>（Decision Tree）：<strong>基于树状结构</strong>进行决策的分类或回归方法。</li></ul><p><strong>无监督学习算法</strong>：</p><ul><li><strong>K-means 聚类</strong>：<strong>通过聚类中心</strong>将数据分组。</li><li><strong>主成分分析（PCA）</strong>：用于<strong>降维</strong>，提取数据的主成分。</li></ul><p>每种算法都有其适用的场景，在实际应用中，可以根据数据的特征（如是否有标签、数据的维度等）来选择最合适的机器学习算法。</p><hr><h2 id="监督学习算法"><a href="#监督学习算法" class="headerlink" title="监督学习算法"></a>监督学习算法</h2><h3 id="线性回归（Linear-Regression）"><a href="#线性回归（Linear-Regression）" class="headerlink" title="线性回归（Linear Regression）"></a>线性回归（Linear Regression）</h3><p>线性回归是一种用于回归问题的算法，它通过学习输入特征与目标值之间的线性关系，来预测一个连续的输出。</p><p>**应用场景：**预测房价、股票价格等。</p><p>线性回归的目标是找到一个最佳的线性方程：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/lg-1-1742965831922-27.png" alt="img"></p><ul><li>y 是预测值（目标值）。</li><li>x1，x2，xn 是输入特征。</li><li>w1，w2，wn是待学习的权重（模型参数）。</li><li>b 是偏置项。</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Linear_regression.svg-1742965850608-30.png" alt="img"></p><p>接下来我们使用 sklearn 进行简单的房价预测：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个简单的房价数据集</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;面积&#x27;</span>: [<span class="number">50</span>, <span class="number">60</span>, <span class="number">80</span>, <span class="number">100</span>, <span class="number">120</span>],</span><br><span class="line">    <span class="string">&#x27;房价&#x27;</span>: [<span class="number">150</span>, <span class="number">180</span>, <span class="number">240</span>, <span class="number">300</span>, <span class="number">350</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征和标签</span></span><br><span class="line">X = df[[<span class="string">&#x27;面积&#x27;</span>]]</span><br><span class="line">y = df[<span class="string">&#x27;房价&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;预测的房价: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">预测的房价: [180.8411215]</span><br></pre></td></tr></table></figure><h3 id="逻辑回归（Logistic-Regression）"><a href="#逻辑回归（Logistic-Regression）" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h3><p>逻辑回归是一种用于分类问题的算法，<strong>尽管名字中包含”回归”，它是用来处理二分类问题的</strong>。</p><p>逻辑回归通过学习输入特征与类别之间的关系，来预测一个类别标签。</p><p>**应用场景：**垃圾邮件分类、疾病诊断（是否患病）。</p><p>逻辑回归的输出是一个概率值，表示样本属于某一类别的概率。</p><p>通常使用 Sigmoid 函数：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/881f2480-9f80-448e-a83b-8abfb784d065.png" alt="img"></p><p>使用逻辑回归进行二分类任务:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只取前两类做二分类任务</span></span><br><span class="line">X = X[y != <span class="number">2</span>]</span><br><span class="line">y = y[y != <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练逻辑回归模型</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;分类准确率: <span class="subst">&#123;accuracy_score(y_test, y_pred):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">分类准确率: 1.00</span><br></pre></td></tr></table></figure><h3 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h3><p>支持向量机是一种常用的分类算法，它通过构造超平面来最大化类别之间的间隔（Margin），使得分类的误差最小。</p><p>**应用场景：**文本分类、人脸识别等。</p><p>使用 SVM 进行鸢尾花分类任务：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练 SVM 模型</span></span><br><span class="line">model = SVC(kernel=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;SVM 分类准确率: <span class="subst">&#123;accuracy_score(y_test, y_pred):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="决策树（Decision-Tree）"><a href="#决策树（Decision-Tree）" class="headerlink" title="决策树（Decision Tree）"></a>决策树（Decision Tree）</h3><p>决策树是一种基于树结构进行决策的<strong>分类和回归方法</strong>。它通过一系列的”判断条件”来决定一个样本属于哪个类别。</p><p>**应用场景：**客户分类、信用评分等。</p><p>使用决策树进行分类任务：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练决策树模型</span></span><br><span class="line">model = DecisionTreeClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;决策树分类准确率: <span class="subst">&#123;accuracy_score(y_test, y_pred):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">决策树分类准确率: 1.00</span><br></pre></td></tr></table></figure><hr><h2 id="无监督学习算法"><a href="#无监督学习算法" class="headerlink" title="无监督学习算法"></a>无监督学习算法</h2><h3 id="K-means-聚类（K-means-Clustering）"><a href="#K-means-聚类（K-means-Clustering）" class="headerlink" title="K-means 聚类（K-means Clustering）"></a>K-means 聚类（K-means Clustering）</h3><p>K-means 是一种<strong>基于中心点的聚类算法</strong>，通过不断<strong>调整簇的中心点</strong>，使每个簇中的数据点尽可能靠近簇中心。</p><p>**应用场景：**<strong><strong>客户分群、市场分析、图像压缩。</strong></strong></p><p>使用 K-means 进行客户分群:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">from sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 生成一个简单的二维数据集</span><br><span class="line">X, _ = <span class="built_in">make_blobs</span>(n_samples=<span class="number">300</span>, centers=<span class="number">4</span>, cluster_std=<span class="number">0.60</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"># 训练 K-means 模型</span><br><span class="line">model = <span class="built_in">KMeans</span>(n_clusters=<span class="number">4</span>)</span><br><span class="line">model.<span class="built_in">fit</span>(X)</span><br><span class="line"></span><br><span class="line"># 预测聚类结果</span><br><span class="line">y_kmeans = model.<span class="built_in">predict</span>(X)</span><br><span class="line"></span><br><span class="line"># 可视化聚类结果</span><br><span class="line">plt.<span class="built_in">scatter</span>(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_kmeans, s=<span class="number">50</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.<span class="built_in">show</span>()</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ml-algorithms-1.png" alt="img"></p><h3 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h3><p>PCA 是一种<strong>降维技术</strong>，它通过线性变换将数据转换到新的坐标系中，使得大部分的方差集中在前几个主成分上。</p><p>**应用场景：**图像降维、特征选择、数据可视化。</p><p>使用 PCA 降维并可视化高维数据:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 降维到 2 维</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">X_pca = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line">plt.scatter(X_pca[:, <span class="number">0</span>], X_pca[:, <span class="number">1</span>], c=y, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;PCA of Iris Dataset&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ml-algorithms-2.png" alt="img"></p><h1 id="线性回归-Linear-Regression"><a href="#线性回归-Linear-Regression" class="headerlink" title="线性回归 (Linear Regression)"></a>线性回归 (Linear Regression)</h1><p>线性回归（Linear Regression）是机器学习中最基础且广泛应用的算法之一。</p><p>线性回归 (Linear Regression) 是一种用于<strong>预测连续值的最基本</strong>的机器学习算法，它<strong>假设</strong>目标变量 <strong>y</strong> 和特征变量 <strong>x</strong> 之间<strong>存在线性</strong>关系，并<strong>试图找到一条最佳拟合直线来描述</strong>这种关系。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = w * x + b</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li><code>y</code> 是预测值</li><li><code>x</code> 是特征变量</li><li><code>w</code> 是权重 (斜率)</li><li><code>b</code> 是偏置 (截距)</li></ul><p>线性回归的目标是<strong>找到最佳的 <code>w</code> 和 <code>b</code>，使得预测值 <code>y</code> 与真实值之间的误差最小</strong>。常用的<strong>误差函数是均方误差 (MSE)</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MSE = 1/n * Σ(y_i - y_pred_i)^2</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>y_i 是<strong>实际</strong>值。</li><li>y_pred_i 是<strong>预测</strong>值。</li><li>n 是数据点的数量。</li></ul><p>我们的目标是通过<strong>调整 w 和 b ，使得 MSE 最小化。</strong></p><h2 id="如何求解线性回归？"><a href="#如何求解线性回归？" class="headerlink" title="如何求解线性回归？"></a>如何求解线性回归？</h2><h3 id="1、最小二乘法"><a href="#1、最小二乘法" class="headerlink" title="1、最小二乘法"></a>1、最小二乘法</h3><p>最小二乘法是一种常用的求解线性回归的方法，它通过求解以下方程来找到最佳的 ( w ) 和 ( b )。</p><p>最小二乘法的目标是<strong>最小化残差平方和（RSS）</strong>，其公式为：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/647bf95e98352a445cab7d547a88ede5.png" alt="647bf95e98352a445cab7d547a88ede5"></p><p>其中：</p><ul><li><code>yi</code> 是实际值。</li><li><code>y^i</code> 是预测值，由线性回归模型 <code>y^i=wxi+by^i=wxi+b</code> 计算得到。</li></ul><p>通过<strong>最小化 RSS</strong>，可以得到以下正规方程：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c4cfc420f1878f532910b15d481b66a3.png" alt="c4cfc420f1878f532910b15d481b66a3"></p><h3 id="矩阵形式"><a href="#矩阵形式" class="headerlink" title="矩阵形式"></a>矩阵形式</h3><p>将正规方程写成<strong>矩阵形式</strong>：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4b024cba97bb200af12c9907fadae8f0.png" alt="4b024cba97bb200af12c9907fadae8f0"></p><h3 id="求解方法"><a href="#求解方法" class="headerlink" title="求解方法"></a>求解方法</h3><p>通过<strong>求解上述矩阵方程</strong>，可以得到最佳的 w 和 <code>b</code>：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/9a03245250f7f00499476096b0df39b2.png" alt="9a03245250f7f00499476096b0df39b2"></p><h3 id="2、梯度下降法"><a href="#2、梯度下降法" class="headerlink" title="2、梯度下降法"></a>2、梯度下降法</h3><p>梯度下降法的目标是<strong>最小化损失函数 <code>J(w,b)</code></strong>。对于线性回归问题，通常使用<strong>均方误差（MSE）作为损失函数</strong>：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/836d14994f1d38d667b8f5556328c600.png" alt="836d14994f1d38d667b8f5556328c600"></p><p>其中：</p><ul><li><code>m</code> 是<strong>样本</strong>数量。</li><li><code>yi</code> 是实际值。</li><li><code>y^i</code> 是预测值，由线性回归模型 <code>y^i=wxi+by^i=wxi+b</code> 计算得到。</li></ul><p>梯度是<strong>损失函数对参数的偏导数</strong>，表示损失函数<strong>在参数空间中的变化方向</strong>。对于线性回归，梯度计算如下：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4fc791e09f80dc2129464eb7a3791883.png" alt="4fc791e09f80dc2129464eb7a3791883"></p><h3 id="参数更新规则"><a href="#参数更新规则" class="headerlink" title="参数更新规则"></a>参数更新规则</h3><p>梯度下降法通过以下规则更新参数 <code>w</code> 和 <code>b</code>：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/83363dfa9d7249a76cf8fb13d3a73467.png" alt="83363dfa9d7249a76cf8fb13d3a73467"></p><p>其中：</p><ul><li><code>α</code> 是学习率（learning rate），控制每次更新的步长。</li></ul><h4 id="梯度下降法的步骤"><a href="#梯度下降法的步骤" class="headerlink" title="梯度下降法的步骤"></a>梯度下降法的步骤</h4><ol><li><strong>初始化参数</strong>：初始化 <code>w</code> 和 <code>b</code> 的值（通常设为 0 或随机值）。</li><li><strong>计算损失函数</strong>：计算<strong>当前参数下</strong>的损失函数值 <code>J(w,b)</code>。</li><li><strong>计算梯度</strong>：计算损失函数对 <code>w</code> 和 <code>b</code> 的<strong>偏导数</strong>。</li><li><strong>更新参数</strong>：<strong>根据梯度更新</strong> <code>w</code> 和 <code>b</code>。</li><li><strong>重复迭代</strong>：重复步骤 2 到 4，<strong>直到损失函数收敛或达到最大迭代次数</strong>。</li></ol><h2 id="使用-Python-实现线性回归"><a href="#使用-Python-实现线性回归" class="headerlink" title="使用 Python 实现线性回归"></a>使用 Python 实现线性回归</h2><p>下面我们通过一个简单的例子来演示如何使用 Python 实现线性回归。</p><h3 id="1、导入必要的库"><a href="#1、导入必要的库" class="headerlink" title="1、导入必要的库"></a>1、导入必要的库</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br></pre></td></tr></table></figure><h3 id="2、生成模拟数据"><a href="#2、生成模拟数据" class="headerlink" title="2、生成模拟数据"></a>2、生成模拟数据</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一些随机数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">x = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * x + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Generated Data From Runoob&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>显示如下所示：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ml-linear-regression-1.png" alt="img"></p><h3 id="3、使用-Scikit-learn-进行线性回归"><a href="#3、使用-Scikit-learn-进行线性回归" class="headerlink" title="3、使用 Scikit-learn 进行线性回归"></a>3、使用 Scikit-learn 进行线性回归</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一些随机数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">x = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * x + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合模型</span></span><br><span class="line">model.fit(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出模型的参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;斜率 (w): <span class="subst">&#123;model.coef_[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;截距 (b): <span class="subst">&#123;model.intercept_[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化拟合结果</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(x, y_pred, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Linear Regression Fit&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">斜率 (w): 2.968467510701019</span><br><span class="line">截距 (b): 4.222151077447231</span><br></pre></td></tr></table></figure><p>显示如下所示：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ml-linear-regression-2.png" alt="img"></p><p>我们可以使用 <code>score()</code> 方法来评估模型性能，返回 R^2 值。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一些随机数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">x = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * x + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合模型</span></span><br><span class="line">model.fit(x, y)</span><br><span class="line"><span class="comment"># 计算模型得分</span></span><br><span class="line">score = model.score(x, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型得分:&quot;</span>, score)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">模型得分: 0.7469629925504755</span><br></pre></td></tr></table></figure><h3 id="4、手动实现梯度下降法"><a href="#4、手动实现梯度下降法" class="headerlink" title="4、手动实现梯度下降法"></a>4、手动实现梯度下降法</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一些随机数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">x = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * x + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = <span class="number">0</span></span><br><span class="line">b = <span class="number">0</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">n_iterations = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_iterations):</span><br><span class="line">    y_pred = w * x + b</span><br><span class="line">    dw = -(<span class="number">2</span>/<span class="built_in">len</span>(x)) * np.<span class="built_in">sum</span>(x * (y - y_pred))</span><br><span class="line">    db = -(<span class="number">2</span>/<span class="built_in">len</span>(x)) * np.<span class="built_in">sum</span>(y - y_pred)</span><br><span class="line">    w = w - learning_rate * dw</span><br><span class="line">    b = b - learning_rate * db</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出最终参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;手动实现的斜率 (w): <span class="subst">&#123;w&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;手动实现的截距 (b): <span class="subst">&#123;b&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化手动实现的拟合结果</span></span><br><span class="line">y_pred_manual = w * x + b</span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(x, y_pred_manual, color=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Manual Gradient Descent Fit&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ml-linear-regression-3.png" alt="img"></p><h1 id="逻辑回归（Logistic-Regression）-1"><a href="#逻辑回归（Logistic-Regression）-1" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h1><p>逻辑回归（Logistic Regression）是一种<strong>广泛应用于分类问题</strong>的统计学习方法，尽管名字中带有”回归”，但它实际上是一种<strong>用于二分类或多分类</strong>问题的算法。</p><p>逻辑回归<strong>通过使用逻辑函数（也称为 Sigmoid 函数）将线性回归的输出映射到 0 和 1 之间</strong>，从而预测某个事件发生的概率。</p><p>逻辑回归广泛应用于各种分类问题，例如：</p><ul><li>垃圾邮件检测（是垃圾邮件&#x2F;不是垃圾邮件）</li><li>疾病预测（患病&#x2F;不患病）</li><li>客户流失预测（流失&#x2F;不流失）</li></ul><h3 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h3><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/abbcee7c-eba8-491d-bf0d-a6654b32afcf.png" alt="img"></p><h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>逻辑回归的损失函数是对数损失函数（Log Loss），其形式如下：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/07115c1d-24e2-4a3f-a26e-da2457a4e220.png" alt="img"></p><h3 id="梯度下降法求解"><a href="#梯度下降法求解" class="headerlink" title="梯度下降法求解"></a>梯度下降法求解</h3><p>和线性回归一样，逻辑回归通常也使用梯度下降法来优化损失函数，求解参数 w 和 b。逻辑回归的梯度更新规则如下：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4c1f2dd9-e137-4c77-9af1-bf1aade65dfd.png" alt="img"></p><p>通过<strong>不断更新 w 和 b，<strong>直到</strong>损失函数收敛</strong>。</p><h2 id="使用-Python-实现逻辑回归"><a href="#使用-Python-实现逻辑回归" class="headerlink" title="使用 Python 实现逻辑回归"></a>使用 Python 实现逻辑回归</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]  <span class="comment"># 只使用前两个特征</span></span><br><span class="line">y = (iris.target != <span class="number">0</span>) * <span class="number">1</span>  <span class="comment"># 将目标转化为二分类问题</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建逻辑回归模型</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测测试集</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 混淆矩阵</span></span><br><span class="line">conf_matrix = confusion_matrix(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;混淆矩阵:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(conf_matrix)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分类报告</span></span><br><span class="line">class_report = classification_report(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;分类报告:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(class_report)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">模型准确率: 1.00</span><br><span class="line">混淆矩阵:</span><br><span class="line">[[19  0]</span><br><span class="line"> [ 0 26]]</span><br><span class="line">分类报告:</span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       1.00      1.00      1.00        19</span><br><span class="line">           1       1.00      1.00      1.00        26</span><br><span class="line"></span><br><span class="line">    accuracy                           1.00        45</span><br><span class="line">   macro avg       1.00      1.00      1.00        45</span><br><span class="line">weighted avg       1.00      1.00      1.00        45</span><br></pre></td></tr></table></figure><h3 id="可视化决策边界"><a href="#可视化决策边界" class="headerlink" title="可视化决策边界"></a>可视化决策边界</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]  <span class="comment"># 只使用前两个特征</span></span><br><span class="line">y = (iris.target != <span class="number">0</span>) * <span class="number">1</span>  <span class="comment"># 将目标转化为二分类问题</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建逻辑回归模型</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测测试集</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化决策边界</span></span><br><span class="line">x_min, x_max = X[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span>, X[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">y_min, y_max = X[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span>, X[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.01</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line">Z = model.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">Z = Z.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">plt.contourf(xx, yy, Z, alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, edgecolors=<span class="string">&#x27;k&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Sepal length&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Sepal width&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Logistic Regression Decision Boundary&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ml-logistic-regression-1.png" alt="img"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>逻辑回归通过<strong>使用Sigmoid函数将线性回归的输出转换为概率值</strong>，用于解决二分类问题。</li><li>逻辑回归的训练过程通过最小化对数损失函数来优化模型参数。</li><li>梯度下降法是常用的优化方法，用来更新模型参数 w 和 b。</li><li>Python中的<code>scikit-learn</code>库提供了简单易用的接口来实现逻辑回归，并且能够轻松地进行模型训练、评估和可视化。</li></ul><h1 id="决策树（Decision-Tree）-1"><a href="#决策树（Decision-Tree）-1" class="headerlink" title="决策树（Decision Tree）"></a>决策树（Decision Tree）</h1><p>决策树（Decision Tree）是一种常用的机器学习算法，广泛应用于<strong>分类和回归</strong>问题。</p><p>决策树通过<strong>树状结构来表示决策过程</strong>，每个内部节点代表一个特征或属性的测试，每个分支代表测试的结果，每个叶节点代表一个类别或值。</p><h3 id="决策树的基本概念"><a href="#决策树的基本概念" class="headerlink" title="决策树的基本概念"></a>决策树的基本概念</h3><ul><li><strong>节点（Node）</strong>：树中的每个点称为节点。根节点是树的起点，<strong>内部节点是决策点</strong>，叶节点是<strong>最终的决策</strong>结果。</li><li><strong>分支（Branch）</strong>：从一个节点到另一个节点的路径称为分支。</li><li><strong>分裂（Split）</strong>：根据某个特征<strong>将数据集分成多个子集</strong>的过程。</li><li><strong>纯度（Purity）</strong>：衡量一个子集中样本的<strong>类别是否一致</strong>。纯度越高，说明子集中的样本越相似。</li></ul><h3 id="决策树的工作原理"><a href="#决策树的工作原理" class="headerlink" title="决策树的工作原理"></a>决策树的工作原理</h3><p>决策树通过递归地<strong>将数据集分割成更小的子集</strong>来构建树结构。具体步骤如下：</p><ol><li><strong>选择最佳特征</strong>：根据某种标准（如信息增益、基尼指数等）选择最佳特征进行分割。</li><li><strong>分割数据集</strong>：根据选定的特征将数据集分成多个子集。</li><li><strong>递归构建子树</strong>：对每个子集重复上述过程，<strong>直到满足停止条件</strong>（如所有样本属于同一类别、达到最大深度等）。</li><li><strong>生成叶节点</strong>：当满足停止条件时，<strong>生成叶节点并赋予类别或值</strong>。</li></ol><h3 id="决策树的构建标准"><a href="#决策树的构建标准" class="headerlink" title="决策树的构建标准"></a>决策树的构建标准</h3><p>在构建决策树时，我们需要选择最佳特征进行分割，常用的标准有：</p><p><strong>1. 信息增益（Information Gain）</strong></p><p>用于分类问题，衡量选择某一特征后数据集的纯度提升。计算公式为：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/b891fa79-647b-4dfa-b0a2-4a763062a998-1747123160218-17.png" alt="img"></p><p>其中 <strong>Entropy</strong> 是数据集的熵，用来衡量数据的不确定性。</p><p><strong>2. 基尼指数（Gini Index）</strong></p><p>也是用于分类问题的分裂标准，计算公式为：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2115f0bc-cc83-422a-8a7d-a3ae2753b990-1747123160218-19.png" alt="img"></p><p>其中 pi 是类别 i 的样本占比。基尼指数越小，表示数据集越纯净。</p><p><strong>3. 均方误差（MSE）</strong></p><p>用于回归问题，衡量预测值和真实值的差异。</p><p>MSE 越小，表示回归树的预测效果越好。</p><hr><h2 id="决策树的优缺点"><a href="#决策树的优缺点" class="headerlink" title="决策树的优缺点"></a>决策树的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li><strong>易于理解和解释</strong>：决策树的<strong>结构直观</strong>，易于理解和解释。</li><li><strong>处理多种数据类型</strong>：可以处理<strong>数值型和类别型</strong>数据。</li><li><strong>不需要数据标准化</strong>：决策树不需要对数据进行标准化或归一化处理。</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li><strong>容易过拟合</strong>：决策树<strong>容易过拟合</strong>，特别是在数据集较小或树深度较大时。</li><li><strong>对噪声敏感</strong>：决策树对噪声数据较为敏感，可能导致模型性能下降。</li><li><strong>不稳定</strong>：数据的小变化可能导致生成完全不同的树。</li></ul><h2 id="使用Python实现决策树"><a href="#使用Python实现决策树" class="headerlink" title="使用Python实现决策树"></a>使用Python实现决策树</h2><p>使用<code>DecisionTreeClassifier</code>来训练决策树模型。决策树的可视化图：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集分为训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建决策树分类器</span></span><br><span class="line">clf = DecisionTreeClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集进行预测</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导出决策树为dot文件</span></span><br><span class="line">dot_data = export_graphviz(clf, out_file=<span class="literal">None</span>, </span><br><span class="line">                           feature_names=iris.feature_names,  </span><br><span class="line">                           class_names=iris.target_names,  </span><br><span class="line">                           filled=<span class="literal">True</span>, rounded=<span class="literal">True</span>,  </span><br><span class="line">                           special_characters=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用graphviz渲染决策树</span></span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line">graph.render(<span class="string">&quot;iris_decision_tree&quot;</span>)  <span class="comment"># 保存为PDF文件</span></span><br><span class="line">graph.view()  <span class="comment"># 在浏览器中查看</span></span><br></pre></td></tr></table></figure><h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h1><p>支持向量机（Support Vector Machine，简称 SVM）是一种<strong>监督学习算法</strong>，主要用于<strong>分类和回归</strong>问题。</p><p>SVM 的核心思想是找到一个<strong>最优的超平面</strong>，将<strong>不同类别的数据分开</strong>。这个超平面不仅要能够<strong>正确分类数据</strong>，还要使得两个类别之间的<strong>间隔（margin）最大化</strong>。</p><p><strong>超平面</strong>：</p><ul><li>在二维空间中，超平面是一个直线。</li><li>在三维空间中，超平面是一个平面。</li><li>在更高维空间中，超平面是一个分割空间的超平面。</li></ul><p><strong>支持向量</strong>：</p><ul><li>支持向量是离超平面最近的样本点。这些支持向量对于定义超平面至关重要。</li><li>支持向量机通过最大化支持向量到超平面的距离（即最大化间隔）来选择最佳的超平面。</li></ul><p><strong>最大间隔</strong>：</p><ul><li>SVM的目标是<strong>最大化分类间隔</strong>，使得分类边界尽可能远离两类数据点。这可以有效地减少模型的泛化误差。</li></ul><p><strong>核技巧（Kernel Trick）</strong>：</p><ul><li>对于<strong>非线性可分</strong>的数据，SVM使用核函数将数据映射到<strong>更高维的空间</strong>，在这个空间中，数据可能是线性可分的。</li><li>常用的核函数有：线性核、多项式核、径向基函数（RBF）核等。</li></ul><h3 id="SVM-分类流程"><a href="#SVM-分类流程" class="headerlink" title="SVM 分类流程"></a>SVM 分类流程</h3><ol><li><strong>选择一个超平面</strong>：找到一个能够最大化分类边界的超平面。</li><li><strong>训练支持向量</strong>：通过支持向量机算法，选择离超平面最近的样本点作为支持向量。</li><li><strong>通过最大化间隔来找到最优超平面</strong>：选择一个最优超平面，使得间隔最大化。</li><li><strong>使用核函数处理非线性问题</strong>：通过核函数将数据映射到高维空间来解决非线性可分问题。</li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm, datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]  <span class="comment"># 只使用前两个特征</span></span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集划分为训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建SVM分类器</span></span><br><span class="line">clf = svm.SVC(kernel=<span class="string">&#x27;linear&#x27;</span>)  <span class="comment"># 使用线性核函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上进行预测</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制决策边界</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_decision_boundary</span>(<span class="params">X, y, model</span>):</span><br><span class="line">    h = <span class="number">.02</span>  <span class="comment"># 网格步长</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span>, X[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span>, X[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line">    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, Z, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, edgecolors=<span class="string">&#x27;k&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Sepal length&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Sepal width&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;SVM Decision Boundary&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(X_train, y_train, clf)</span><br></pre></td></tr></table></figure><p>执行以上代码，输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">模型准确率: 0.80</span><br></pre></td></tr></table></figure><p>图片显示为：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/svm-1.png" alt="img"></p><h1 id="K-近邻算法"><a href="#K-近邻算法" class="headerlink" title="K 近邻算法"></a>K 近邻算法</h1><p>K 近邻算法（K-Nearest Neighbors，简称 KNN）是一种简单且常用的<strong>分类和回归</strong>算法。</p><p>K 近邻算法属于<strong>监督学习</strong>的一种，核心思想是通过计算<strong>待分类样本与训练集中各个样本</strong>的<strong>距离</strong>，<strong>找到距离最近的 K 个样本</strong>，然后根据这 K 个样本的类别或值来预测待分类样本的类别或值。</p><h3 id="KNN-的基本原理"><a href="#KNN-的基本原理" class="headerlink" title="KNN 的基本原理"></a>KNN 的基本原理</h3><p>KNN 算法的基本原理可以概括为以下几个步骤：</p><ol><li><strong>计算距离</strong>：计算待分类样本与训练集中每个样本的距离。常用的距离度量方法有欧氏距离、曼哈顿距离等。</li><li><strong>选择 K 个最近邻</strong>：根据计算出的距离，选择距离最近的 K 个样本。</li><li><strong>投票或平均</strong>：对于分类问题，K 个最近邻中出现次数最多的类别即为待分类样本的类别；对于回归问题，K 个最近邻的值的平均值即为待分类样本的值。</li></ol><h3 id="KNN-的特点"><a href="#KNN-的特点" class="headerlink" title="KNN 的特点"></a>KNN 的特点</h3><ul><li><strong>简单易理解</strong>：KNN 算法的原理非常简单，容易理解和实现。</li><li><strong>无需训练</strong>：KNN 是一种”懒惰学习”算法，不需要显式的训练过程，所有的计算都在预测时进行。</li><li><strong>对数据分布无假设</strong>：KNN 不对数据的分布做任何假设，适用于各种类型的数据。</li><li><strong>计算复杂度高</strong>：由于 KNN 需要在预测时计算所有样本的距离，当数据集较大时，计算复杂度会很高。</li></ul><h3 id="KNN-算法的优缺点"><a href="#KNN-算法的优缺点" class="headerlink" title="KNN 算法的优缺点"></a>KNN 算法的优缺点</h3><p><strong>优点</strong></p><ul><li><strong>简单易用</strong>：KNN 算法的原理简单，易于理解和实现。</li><li><strong>无需训练</strong>：KNN 不需要显式的训练过程，所有的计算都在预测时进行。</li><li><strong>适用于多分类问题</strong>：KNN 可以轻松处理多分类问题。</li></ul><p><strong>缺点</strong></p><ul><li><strong>计算复杂度高</strong>：KNN 需要在预测时计算所有样本的距离，当数据集较大时，计算复杂度会很高。</li><li><strong>对噪声敏感</strong>：KNN 对噪声数据较为敏感，噪声数据可能会影响预测结果。</li><li><strong>需要选择合适的 K 值</strong>：K 值的选择对模型的性能有很大影响，选择合适的 K 值是一个挑战。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载Iris数据集</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]  <span class="comment"># 只取前两个特征，便于可视化</span></span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集拆分为训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建KNN模型，设置K值为3</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上进行预测</span></span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;KNN模型的准确率: <span class="subst">&#123;accuracy:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制决策边界和数据点</span></span><br><span class="line">h = <span class="number">.02</span>  <span class="comment"># 网格步长</span></span><br><span class="line">x_min, x_max = X[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span>, X[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">y_min, y_max = X[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span>, X[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个二维网格，表示不同的样本空间</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                     np.arange(y_min, y_max, h))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用KNN模型预测网格中的每个点的类别</span></span><br><span class="line">Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">Z = Z.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制决策边界</span></span><br><span class="line">plt.contourf(xx, yy, Z, alpha=<span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制训练数据点</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, edgecolors=<span class="string">&#x27;k&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, s=<span class="number">50</span>)</span><br><span class="line">plt.title(<span class="string">&quot;KNN Demo&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 2&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/knn-1.png" alt="img"></p><h3 id="调整-K-值"><a href="#调整-K-值" class="headerlink" title="调整 K 值"></a>调整 K 值</h3><p>K 值的选择对模型的性能有重要影响。</p><p>通常我们通过<strong>交叉验证或可视化方法</strong>选择最佳的 K 值。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 尝试不同的K值并绘制准确率变化</span></span><br><span class="line">k_range = <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">21</span>)</span><br><span class="line">accuracies = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_range:</span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">    knn.fit(X_train, y_train)</span><br><span class="line">    y_pred = knn.predict(X_test)</span><br><span class="line">    accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">    accuracies.append(accuracy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制K值与准确率的关系</span></span><br><span class="line">plt.plot(k_range, accuracies, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;K值与准确率的关系&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;K值&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;准确率&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="使用-KNN-进行回归任务"><a href="#使用-KNN-进行回归任务" class="headerlink" title="使用 KNN 进行回归任务"></a>使用 KNN 进行回归任务</h3><p>KNN 同样可以用于回归任务（KNN Regression）。</p><p>在回归任务中，KNN 根据 K 个最近邻的目标值进行平均来预测输出。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">10</span></span><br><span class="line">y = np.sin(X).ravel() + <span class="number">0.1</span> * np.random.randn(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分为训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建KNN回归模型</span></span><br><span class="line">knn_reg = KNeighborsRegressor(n_neighbors=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">knn_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上进行预测</span></span><br><span class="line">y_pred = knn_reg.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化回归结果</span></span><br><span class="line">plt.scatter(X_test, y_test, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;True Values&#x27;</span>)</span><br><span class="line">plt.scatter(X_test, y_pred, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;Predicted Values&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;KNN Regression&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Target&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>红色为真实值，蓝色为预测值：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/knn-3.png" alt="img"></p><h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h1><p>在机器学习领域，集成学习（Ensemble Learning）是一种<strong>通过结合多个模型的预测结果</strong>来提高整体性能的技术。</p><p>集成学习的核心思想是”三个臭皮匠，顶个诸葛亮”，即通过多个弱学习器的组合，可以构建一个强学习器。</p><p>集成学习的主要目标是通过组合多个模型来提高预测的准确性和鲁棒性。</p><p>常见的集成学习方法包括：</p><ol><li><strong>Bagging</strong>：通过<strong>自助采样法（Bootstrap Sampling）<strong>生成多个训练集，然后分别训练多个模型，最后</strong>通过投票或平均</strong>的方式得到最终结果。</li><li><strong>Boosting</strong>：通过<strong>迭代</strong>的方式训练多个模型，每个模型都试图纠正前一个模型的错误，最终通过<strong>加权投票</strong>的方式得到结果。</li><li><strong>Stacking</strong>：通过训练多个不同的模型，然后将这些模型的输出作为<strong>新的特征</strong>，<strong>再训练一个元模型</strong>（Meta-Model）来进行最终的预测。</li></ol><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ensemble-learning.png" alt="img"></p><h3 id="1-Bagging（Bootstrap-Aggregating）"><a href="#1-Bagging（Bootstrap-Aggregating）" class="headerlink" title="1. Bagging（Bootstrap Aggregating）"></a>1. <strong>Bagging（Bootstrap Aggregating）</strong></h3><p>Bagging 的目标是通过<strong>减少模型的方差</strong>来提高性能，<strong>适用于高方差、易过拟合</strong>的模型。它通过以下步骤实现：</p><ul><li><strong>数据集重采样</strong>：对训练数据集进行<strong>多次有放回的随机采样</strong>（bootstrap），每次采样得到一个子数据集。</li><li><strong>训练多个模型</strong>：在每个子数据集上<strong>训练一个基学习器</strong>（通常是相同类型的模型）。</li><li><strong>结果合并</strong>：将<strong>多个基学习器的结果进行合并</strong>，通常是通过<strong>投票（分类问题）或平均（回归问题）</strong>。</li></ul><p><strong>典型算法</strong>：</p><ul><li><strong>随机森林（Random Forest）</strong>：<strong>随机森林是 Bagging 的经典实现</strong>，它通过<strong>构建多个决策树</strong>，每棵树在训练时随机选择特征，从而减少过拟合的风险。</li></ul><p><strong>优势</strong>：</p><ul><li>可以有效减少方差，提高模型稳定性。</li><li>适用于高方差的模型，如决策树。</li></ul><p><strong>缺点</strong>：</p><ul><li>训练过程时间较长，因为需要训练多个模型。</li><li>结果难以解释，因为没有单一的模型。</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ensemble-learning-bagging.png" alt="img"></p><hr><h3 id="2-Boosting"><a href="#2-Boosting" class="headerlink" title="2. Boosting"></a>2. <strong>Boosting</strong></h3><p>Boosting 的目标是通过<strong>减少模型的偏差</strong>来提高性能，<strong>适用于弱学习器</strong>。Boosting 的核心思想是逐步调整每个模型的权重，强调那些被前一轮模型错误分类的样本。Boosting 通过以下步骤实现：</p><ul><li><strong>序列化训练</strong>：模型是<strong>一个接一个</strong>地训练的，每一轮训练都会根据前一轮的错误进行调整。</li><li><strong>加权投票</strong>：最终的预测是所有弱学习器预测的<strong>加权和</strong>，其中错误分类的样本会被赋予更高的权重。</li><li><strong>合并模型</strong>：每个模型的权重是根据其在训练过程中的表现来确定的。</li></ul><p><strong>典型算法</strong>：</p><ul><li><strong>AdaBoost（Adaptive Boosting）</strong>：AdaBoost 通过改变样本的权重，使得每个后续分类器<strong>更加关注前一轮错误分类</strong>的样本。</li><li><strong>梯度提升树（Gradient Boosting Trees, GBT）</strong>：GBT 通过<strong>迭代优化目标函数</strong>，逐步减少偏差。</li><li><strong>XGBoost（Extreme Gradient Boosting）</strong>：XGBoost 是一种<strong>高效的梯度提升算法</strong>，广泛应用于数据科学竞赛中，具有较强的性能和优化。</li><li><strong>LightGBM（Light Gradient Boosting Machine）</strong>：LightGBM 是一种<strong>基于梯度提升树</strong>的框架，相较于 XGBoost，具有更快的训练速度和更低的内存使用。</li></ul><p><strong>优势</strong>：</p><ul><li>适用于偏差较大的模型，能有效提高预测准确性。</li><li>强大的性能，在许多实际应用中表现优异。</li></ul><p><strong>缺点</strong>：</p><ul><li>对噪声数据比较敏感，容易导致过拟合。</li><li>训练过程较慢，特别是在数据量较大的情况下。</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ensemble-learning-boosting.png" alt="img"></p><hr><h3 id="3-Stacking（Stacked-Generalization）"><a href="#3-Stacking（Stacked-Generalization）" class="headerlink" title="3. Stacking（Stacked Generalization）"></a>3. <strong>Stacking（Stacked Generalization）</strong></h3><p>Stacking 是一种通过训练不同种类的模型并组合它们的预测来提高整体预测准确度的方法。其核心思想是：</p><ul><li><strong>第一层（基学习器）</strong>：训练<strong>多个不同类型的基学习器</strong>（例如，决策树、SVM、KNN 等）来对数据进行预测。</li><li><strong>第二层（元学习器）</strong>：将第一层学习器的<strong>预测结果作为输入</strong>，训练一个元学习器（通常是逻辑回归、线性回归等），来做最终的预测。</li></ul><p><strong>优势</strong>：</p><ul><li>可以使用不同类型的基学习器，捕捉数据中不同的模式。</li><li>理论上可以结合多种模型的优势，达到更强的预测能力。</li></ul><p><strong>缺点</strong>：</p><ul><li>训练过程复杂，需要对多个模型进行训练，且模型之间的结合方式也需要精心设计。</li><li>比其他集成方法如 Bagging 和 Boosting 更复杂，且容易过拟合。</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ensemble-learning-stacking.png" alt="img"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建随机森林分类器</span></span><br><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">rf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = rf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;随机森林的准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>代码解释:</strong></p><ul><li><strong>加载数据集</strong>：我们使用<code>load_iris()</code>函数加载经典的鸢尾花数据集。</li><li><strong>划分训练集和测试集</strong>：使用<code>train_test_split()</code>函数将数据集划分为训练集和测试集。</li><li><strong>创建随机森林分类器</strong>：使用<code>RandomForestClassifier</code>类创建一个<strong>随机森林分类器</strong>，<code>n_estimators=100</code>表示<strong>使用100棵决策树</strong>。</li><li><strong>训练模型</strong>：使用<code>fit()</code>方法训练模型。</li><li><strong>预测</strong>：使用<code>predict()</code>方法对测试集进行预测。</li><li><strong>计算准确率</strong>：使用<code>accuracy_score()</code>函数计算模型的准确率。</li></ul><h3 id="Boosting：AdaBoost"><a href="#Boosting：AdaBoost" class="headerlink" title="Boosting：AdaBoost"></a>Boosting：AdaBoost</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用默认的弱学习器（决策树），并指定使用 SAMME 算法</span></span><br><span class="line">ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=<span class="number">1</span>), </span><br><span class="line">                         n_estimators=<span class="number">50</span>, </span><br><span class="line">                         random_state=<span class="number">42</span>, </span><br><span class="line">                         algorithm=<span class="string">&#x27;SAMME&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">ada.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = ada.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;AdaBoost的准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AdaBoost的准确率: 1.00</span><br></pre></td></tr></table></figure><p><strong>代码解释：</strong></p><ol><li><strong>加载数据集</strong>：使用<code>load_iris()</code>函数加载鸢尾花数据集，包含特征数据<code>X</code>和标签数据<code>y</code>。</li><li><strong>划分训练集和测试集</strong>：使用<code>train_test_split()</code>函数将数据集拆分为训练集和测试集，其中测试集占30%，训练集占70%。</li><li><strong>创建决策树分类器</strong>：使用<code>DecisionTreeClassifier(max_depth=1)</code>创建一个深度为1的决策树分类器，作为AdaBoost的基础学习器。</li><li><strong>创建AdaBoost分类器</strong>：使用<code>AdaBoostClassifier()</code>类创建AdaBoost分类器，<code>n_estimators=50</code>表示使用50个弱学习器，<code>algorithm=&#39;SAMME&#39;</code>指定使用SAMME算法。</li><li><strong>训练模型</strong>：使用<code>fit()</code>方法在训练数据上训练AdaBoost模型。</li><li><strong>预测</strong>：使用<code>predict()</code>方法对测试集进行预测，生成预测标签<code>y_pred</code>。</li><li><strong>计算准确率</strong>：使用<code>accuracy_score()</code>函数计算并输出模型的预测准确率。</li></ol><h3 id="Stacking：模型堆叠"><a href="#Stacking：模型堆叠" class="headerlink" title="Stacking：模型堆叠"></a>Stacking：模型堆叠</h3><p>**算法原理：**Stacking 的核心思想是通过训练多个不同的模型，然后将这些模型的输出作为新的特征，再训练一个元模型（Meta-Model）来进行最终的预测。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> StackingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义基学习器</span></span><br><span class="line">estimators = [</span><br><span class="line">    (<span class="string">&#x27;dt&#x27;</span>, DecisionTreeClassifier(max_depth=<span class="number">1</span>)),</span><br><span class="line">    (<span class="string">&#x27;svc&#x27;</span>, SVC(kernel=<span class="string">&#x27;linear&#x27;</span>, probability=<span class="literal">True</span>))</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Stacking分类器</span></span><br><span class="line">stacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">stacking.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = stacking.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Stacking的准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tacking的准确率: 1.00</span><br></pre></td></tr></table></figure><p><strong>代码解释：</strong></p><ol><li><strong>加载数据集</strong>：同样使用<code>load_iris()</code>函数加载鸢尾花数据集。</li><li><strong>划分训练集和测试集</strong>：使用<code>train_test_split()</code>函数将数据集划分为训练集和测试集。</li><li><strong>定义基学习器</strong>：使用<code>DecisionTreeClassifier</code>和<code>SVC</code>作为基学习器。</li><li><strong>创建Stacking分类器</strong>：使用<code>StackingClassifier</code>类创建一个Stacking分类器，<code>final_estimator=LogisticRegression()</code>表示使用逻辑回归作为元模型。</li><li><strong>训练模型</strong>：使用<code>fit()</code>方法训练模型。</li><li><strong>预测</strong>：使用<code>predict()</code>方法对测试集进行预测。</li><li><strong>计算准确率</strong>：使用<code>accuracy_score()</code>函数计算模型的准确率。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>进行一个一个前端的雪</title>
      <link href="/2025/01/23/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E5%89%8D%E7%AB%AF%E7%9A%84%E9%9B%AA/"/>
      <url>/2025/01/23/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E5%89%8D%E7%AB%AF%E7%9A%84%E9%9B%AA/</url>
      
        <content type="html"><![CDATA[<h2 id="Web-开发概况"><a href="#Web-开发概况" class="headerlink" title="Web 开发概况"></a>Web 开发概况</h2><p>Web 开发是指创建和维护<strong>网站</strong>、<strong>客户端程序</strong>、<strong>服务器</strong>与<strong>其他 Web 应用程序</strong>的过程。它包括使用不同的编程语言和技术来编写、测试和部署 Web 应用程序，以满足特定的业务需求和用户需求</p><p>通过 Web 开发技术，开发者能够设计实现诸多满足不同需求场景的<strong>应用程序</strong>，包括但不限于：网站开发、Android&#x2F;IOS&#x2F;Harmony OS NEXT 移动端应用程序、微信小程序、桌面应用、群聊机器人、游戏、浏览器插件、3D 建模、高性能服务器、分布式应用、虚拟现实应用、区块链、物联网设备……</p><h2 id="前端和后端"><a href="#前端和后端" class="headerlink" title="前端和后端"></a>前端和后端</h2><p>在软件架构和程序设计领域，前端是软件系统中<strong>直接</strong>和用户交互的部分，而后端控制着<strong>软件的输出</strong>。将软件分为前端和后端是一种将<strong>软件不同功能</strong>的部分相互分离的抽象</p><p>在 Web 开发中，前端在绝大多数情况下指能够被<strong>用户直接访问与交互的模块</strong>，如<strong>网页、手机 App、桌面应用、小程序等</strong>。后端包括<strong>程序运行的后台服务器</strong>、<strong>存储数据的数据库</strong>以及<strong>其他数据中间件</strong>。大部分软件都<strong>概念性地</strong>分成了前端和后端，在大多数情况下，软件的后端经常是隐藏着而不被用户看到</p><p><strong>狭义</strong>的前端通常是指网站或应用程序中与用户直接交互的部分。它是一种用于<strong>构建用户界面的技术和工具的集合</strong>，这些界面可以在 <strong>Web 浏览器</strong>中运行</p><p>后端开发主要负责编写<strong>运行在服务端上的</strong>代码，通常来说，这部分的工作需要和<strong>数据库</strong>与 <strong>Web API</strong> 打交道，比如读写数据、读写文件、实现业务逻辑等。有些时候，业务逻辑存储在<strong>客户端</strong>，这时后台就是用来<strong>以 Web 服务的形式</strong>提供数据库中的数据</p><p>开发者<strong>可以同时掌握</strong>前端和后端的技术，但大多数 Web 开发者都还是有<strong>一定的专精</strong>方向，甚至只在某一方面深入研究。尽管前后端是有天然的区别，但并没有规定它们各自的具体任务。有时前端只是完成数据的显示，而其他主要工作都在后端完成。但也有时，后端只是提供数据，而所有的计算和具体功能都在前端完成。前后端工作的分配，通常都是<strong>由项目的设计和架构来决定</strong>的</p><h2 id="浏览器"><a href="#浏览器" class="headerlink" title="浏览器"></a>浏览器</h2><p>浏览器是用来<strong>检索、展示以及传递</strong> <strong>Web 信息资源的应用程序</strong>。Web 信息资源由<strong>统一资源标识符 (Uniform Resource Identifier，URI)</strong> 所标记，它可以是一张网页、一张图片、一段视频或者<strong>任何在 Web 上所呈现的内容</strong>。使用者可以<strong>借助超链接</strong>，<strong>通过浏览器浏览</strong>互相关联的信息</p><p>浏览器<strong>内核</strong> (Rendering Engine)，是指浏览器最核心的部分，负责<strong>对网页语法的解释（如标准通用标记语言下的一个应用 HTML、CSS、JavaScript）并渲染网页</strong>。通常所谓的浏览器内核也就是浏览器所采用的<strong>渲染引擎</strong>，渲染引擎决定了浏览器<strong>如何显示网页的内容</strong>以及<strong>页面的格式信息</strong>。不同的浏览器内核对网页编写语法的解释也有不同</p><h2 id="C-S-与-B-S-架构"><a href="#C-S-与-B-S-架构" class="headerlink" title="C&#x2F;S 与 B&#x2F;S 架构"></a><strong>C&#x2F;S</strong> <strong>与</strong> <strong>B&#x2F;S</strong> 架构</h2><p>C&#x2F;S 架构是一种典型的<strong>两层架构</strong>，其全称是 <strong>Client&#x2F;Server</strong>，即<strong>客户</strong>端<strong>服务器</strong>端架构，其客户端包含一个或多个在用户的电脑上运行的<strong>程序</strong>，而服务器端有两种，一种是<strong>数据库</strong>服务器端，客户端<strong>通过数据库连接访问服务器端的数据</strong>；另一种是 <strong>Socket</strong> 服务器端，<strong>服务器端的程序</strong>通过 Socket 与<strong>客户端的程序</strong>通信</p><p>B&#x2F;S 架构的全称为 <strong>Browser&#x2F;Server</strong>，即<strong>浏览器</strong>&#x2F;<strong>服务器</strong>结构。Browser 指的是 <strong>Web 浏览器</strong>，极少数事务逻辑在前端实现，但<strong>主要</strong>事务逻辑在<strong>服务器端</strong>实现。B&#x2F;S 架构的系统无须特别安装，<strong>只要有 Web 浏览器</strong>即可</p><h2 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h2><p>超文本标记语言（Hyper Text Markup Language，简称：HTML）是一种<strong>用于创建网页</strong>的标准标记语言。HTML 是一种基础技术，常与 CSS、JavaScript 一起被众多网站用于<strong>设计网页</strong>、<strong>网页应用程序</strong>以及<strong>移动应用程序</strong>的<strong>用户界面</strong>。网页<strong>浏览器</strong>可以读取 HTML 文件，并将其<strong>渲染成可视化网页</strong>。HTML 描述了<strong>一个网站的结构语义随着线索的呈现</strong>，使之成为一种<strong>标记</strong>语言而非编程语言</p><h2 id="CSS"><a href="#CSS" class="headerlink" title="CSS"></a>CSS</h2><p><strong>层叠样式表（Cascading Style Sheets）<strong>是一种用来</strong>为结构化文档（如 HTML 文档或 XML 应用）添加样式（字体、间距和颜色等）<strong>的计算机语言。CSS3 现在已被大部分现代浏览器支持，而下一版的 CSS4 仍在开发中。CSS 不仅可以</strong>静态地修饰</strong>网页，还可以<strong>配合各种脚本语言动态地</strong>对网页各元素进行<strong>格式</strong>化。CSS 能够对网页中元素位置的<strong>排版进行像素级精确控制</strong>，支持几乎所有的字体字号样式，拥有对网页对象和模型样式编辑的能力</p><h2 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h2><p>JavaScript 是一种高级的、<strong>解释型</strong>的<strong>编程</strong>语言</p><p>JavaScript 是一门基于原型、头等函数的语言，是一门多范式的语言，它支持面向对象程序设计，指令式编程，以及函数式编程。它由 ECMA（欧洲电脑制造商协会）通过 ECMAScript 实现语言的标准化</p><p>ECMAScript 6.0（简称 ES6）是 JavaScript 语言的下一代<strong>标准</strong>，于 2015 年 6 月正式发布。它的目标，是使得 JavaScript 语言可以用来编写复杂的大型应用程序，成为企业级开发语言</p><p>ES6 既是一个历史名词，也是一个泛指，含义是 5.1 版以后的 JavaScript 的下一代标准，涵盖了 ES2015、ES2016、ES2017 等等，而 ES2015 则是正式名称，特指该年发布的正式版本的语言标准</p><h2 id="计算机网络基础知识"><a href="#计算机网络基础知识" class="headerlink" title="计算机网络基础知识"></a>计算机网络基础知识</h2><h4 id="HTTP-协议"><a href="#HTTP-协议" class="headerlink" title="HTTP 协议"></a>HTTP 协议</h4><p><strong>HTTP 是 <em>Hyper Text Transfer Protocol</em>（超文本传输协议）<strong>的缩写。HTTP 协议用于</strong>从 WWW 服务器传输超文本到本地浏览器</strong>的传送协议。它可以使浏览器更加高效，使<strong>网络传输减少</strong>。它不仅保证计算机正确快速地传输超文本文档，还确定传输文档中的哪一部分，以及哪部分内容<strong>首先</strong>显示 (如文本先于图形) 等。HTTP 是一个<strong>应用层协议</strong>，由<strong>请求</strong>和<strong>响应</strong>构成，是一个标准的<strong>客户端服务器模型</strong>。</p><h4 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h4><p>在互联网上，<strong>每一</strong>信息资源都有<strong>统一的且在网上唯一的地址</strong>，该地址就叫 URL（Uniform Resource Locator, 统一资源定位符）。</p><p>URL 由三部分组成：资源<strong>类型</strong>、存放资源的<strong>主机域名</strong>、资源<strong>文件名</strong>。</p><p>也可认为由 4 部分组成：<strong>协议</strong>、<strong>主机</strong>、<strong>端口</strong>、<strong>路径</strong>。</p><p>URL 的一般格式为：<em>protocol :&#x2F;&#x2F; hostname [:port] &#x2F; path &#x2F; [:parameters] [?query] #fragment</em></p><h4 id="IP-地址和-DNS"><a href="#IP-地址和-DNS" class="headerlink" title="IP 地址和 DNS"></a>IP 地址和 DNS</h4><p><strong>IP 地址</strong>（类似 192.168.1.1 内网网关）是<strong>互联网协议地址</strong>，它给因特网上的每<strong>台计算机和其它设备</strong>都规定了一个唯一的地址。由于有这种唯一的地址，才保证了用户在连网的计算机上操作时，能够高效而且方便地从千千万万台计算机中选出自己所需的对象来</p><p>但是 IP 地址毕竟是一串<strong>毫无规律</strong>的数字，并不方便人类的记忆和书写。因此在 IP 地址的基础上又发展出一种<strong>符号化</strong>的地址方案，来代替数字型的 IP 地址，每一个符号化的地址都与特定的 IP 地址对应。这个与网络上的数字型 IP 地址相对应的字符型地址，就是<strong>域名</strong></p><p>类似 <a href="https://www.google.com/">http://www.google.com</a> 这样的字符串就是“域名”，当访问 <a href="http://www.google.com/">www.google.com</a> 时，首先由 DNS（Domain Name System, DNS）域名系统<strong>解析为 IP 地址，随后再访问 IP</strong></p><h4 id="HTTP-请求"><a href="#HTTP-请求" class="headerlink" title="HTTP 请求"></a>HTTP 请求</h4><p>HTTP 请求是指从<strong>客户端到服务器端</strong>的请求消息，请求报文由请求行 (Request line)、请求头 (Header)，空行、请求正文 4 部分组成</p><h4 id="HTTP-响应"><a href="#HTTP-响应" class="headerlink" title="HTTP 响应"></a>HTTP 响应</h4><p>在接收和解释请求消息后，服务器会<strong>返回一个 HTTP 响应消息</strong>。HTTP 响应报文也由四个部分组成，分别是：状态行、消息报头、空行和响应正文</p><h4 id="HTTP-方法"><a href="#HTTP-方法" class="headerlink" title="HTTP 方法"></a>HTTP 方法</h4><p>根据 HTTP 标准，HTTP 请求可以使用多种<strong>请求方法</strong>。HTTP 方法描述了对给定资源的期望动作，每一种请求方法都抽象出了一种不同给定语义。</p><p>HTTP1.0 定义了三种请求方法：GET、POST 和 HEAD 方法。</p><p>HTTP1.1 新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT 方法。</p><p>在实际开发中 <strong>GET、POST、PUT、DELETE</strong> 四类 HTTP 方法的使用率最高，能够用一套统一的语法规范对资源的 CRUD (增删改查) 逻辑进行抽象</p><p>GET 方法请求一个指定资源的<strong>表示形式</strong>，使用 GET 的请求应该<strong>只被用于获取数据</strong></p><p>POST 方法用于<strong>将实体提交到指定的资源</strong>，通常导致在服务器上的<strong>状态变化</strong>或副作用</p><p>PUT 方法用<strong>请求有效载荷替换</strong>目标资源的所有当前表示</p><p>DELETE 方法<strong>删除指定的资源</strong></p><h4 id="HTTP-状态码"><a href="#HTTP-状态码" class="headerlink" title="HTTP 状态码"></a>HTTP 状态码</h4><p>当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一<strong>个包含 HTTP 状态码的信息头</strong>（server header）<strong>用以响应</strong>浏览器的请求</p><p>状态码类型：</p><table><thead><tr><th>状态码</th><th>类别</th><th>原因</th></tr></thead><tbody><tr><td>1xx</td><td><strong>信息性</strong>状态码</td><td>接收的请求<strong>正在处理</strong></td></tr><tr><td>2xx</td><td><strong>成功</strong>状态码</td><td>请求正常处理<strong>完毕</strong></td></tr><tr><td>3xx</td><td><strong>重定向</strong>状态码</td><td>需要<strong>进行附加操作</strong>以完成请求</td></tr><tr><td>4xx</td><td><strong>客户端错误</strong>状态码</td><td>服务器无法处理请求</td></tr><tr><td>5xx</td><td><strong>服务端错误</strong>状态码</td><td>服务器处理请求出错</td></tr></tbody></table><p>常见的状态码：</p><p><strong>100 Continue</strong></p><p>客户端应继续其请求</p><p><strong>200 OK</strong></p><p>请求成功，一般用于 <strong>GET 与 POST</strong> 请求</p><p><strong>201 Created</strong></p><p>已创建，<strong>成功请求并创建</strong>了新的资源</p><p><strong>401 Unauthorized</strong></p><p>请求要求用户的<strong>身份认证</strong></p><p><strong>403 Forbidden</strong></p><p>服务器理解请求客户端的请求，但是<strong>拒绝执行</strong>此请求</p><p><strong>404 Not Found</strong></p><p>服务器<strong>无法</strong>根据客户端的请求<strong>找到</strong>资源（网页）。</p><p><strong>500 Internal Server Error</strong></p><p>服务器<strong>内部错误</strong>，无法完成请求</p><h4 id="RESTful-API"><a href="#RESTful-API" class="headerlink" title="RESTful API"></a><strong>RESTful</strong> <strong>API</strong></h4><p><strong>REST 全称是 Representational State Transfer</strong>，中文意思是<strong>表述性状态转移</strong>。</p><p>RESTful 架构应该遵循<strong>统一接口原则</strong>，统一接口包含了一组<strong>受限的预定义的操作</strong>，不论什么样的资源，都是<strong>通过使用相同的接口</strong>进行资源的访问。</p><p>接口应该使用<strong>标准的 HTTP 方法</strong>如 GET，PUT 和 POST，并遵循这些方法的语义。</p><p>REST 所谓的表述指的是<strong>对资源的表述</strong>。要让一个资源可以被<strong>识别</strong>，需要有个<strong>唯一标识</strong>，在 Web 中这个唯一标识就是 <strong>URI</strong></p><h2 id="版本控制工具"><a href="#版本控制工具" class="headerlink" title="版本控制工具"></a>版本控制工具</h2><h4 id="Git-版本控制工具"><a href="#Git-版本控制工具" class="headerlink" title="Git 版本控制工具"></a>Git 版本控制工具</h4><p>Git 是一个开源的<strong>分布式版本控制系统</strong>，可以有效、高速地处理从很小到非常大的项目版本管理。也是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件</p><h4 id="GitHub-代码托管仓库"><a href="#GitHub-代码托管仓库" class="headerlink" title="GitHub 代码托管仓库"></a>GitHub 代码托管仓库</h4><p>GitHub 是一个面向开源及私有软件项目的托管平台，因为<strong>只支持 Git</strong> 作为<strong>唯一的版本库格式</strong>进行托管</p><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h4 id="库"><a href="#库" class="headerlink" title="库"></a>库</h4><p>库是一系列预先定义好的数据结构和函数或类的集合，程序员可以通过调用这些代码实现功能。简单来说就是库为我们提供了很多封装好的函数，看起来比较零散，但使用起来更灵活</p><p>使用库可以简化开发流程，提高开发效率。例如，jQuery 提供了简化 DOM 操作的语法，减少了编写繁琐代码的需要。React 通过虚拟 DOM 和声明式 UI ，便于<strong>快速构建用户界面</strong></p><p>如果需要<strong>在网页中使用 JavaScript 库</strong>，可以去网上<strong>下载库文件</strong>，<strong>放在网页的同一目录</strong>下，再到**<code>script</code>标签中引入**。或者不下载通过<strong>通过链接在<code>&lt;script&gt;</code>标签中引用该库</strong>即可：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=<span class="string">&quot;https://cdn.staticfile.org/jquery/3.4.0/jquery.min.js&quot;</span>&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>或者在代码中通过 <code>require</code> 或者 <code>import</code> 中引入库。在现代的前端开发中，通常推荐使用 <code>import</code> 来进行模块导入，特别是在使用现代 JavaScript 特性的项目中。这主要与现代 JavaScript 的发展趋势和语言特性有关</p><p><code>import</code> 是 ES6 新引入的关键字，支持<strong>按需导入</strong>，而不需要导入整个模块。同时<code>import</code> 的语法也比 <code>require</code> 更直观清晰，更符合现代变成风格</p><p>随着 JavaScript 生态的发展，越来越多的库和工具采用了 ES6 模块系统，使用 <code>import</code> 能够更好地与这些现代化的工具和库进行集成。</p><h4 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h4><p>框架是提供<strong>如何构建应用程序的意见的库</strong>，是<strong>一整套的工具</strong>，所有东西已经准备齐全了，可以按照它的规定就可以很简单的完成一些事情，但我们<strong>不能去改变它，只能按照要求</strong>使用，并且其他人拿到这套工具也是一样的，如 Vue、Angular 等等。</p><p>注意是<strong>一套而不是单个</strong>，比如 React 就是一个库，它本身只是一个前端渲染的库，纯粹地写 UI 组件，没有什么异步处理机制、模块化等，但是当它结合 Redux 和 React-router 的时候，就是一个框架了。</p><p>框架和库的联系紧密，都是为了提高我们的开发效率而存在，库的使用上会<strong>简单</strong>一些，更加<strong>灵活</strong>，但<strong>功能不全</strong>。而框架的功能很<strong>全面</strong>，但需要我们<strong>按规定</strong>去使用。也就是说库是一种工具，我提供了，你可以不用，即使你用了，也没影响你自己的代码结构，控制权在使用者手中。框架则是面向一个领域，提供了<strong>一套解决方案</strong></p><h4 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h4><p>库是一组已经实现的<strong>代码集合</strong>，提供了特定功能的函数和方法，开发者可以根据需要选择性地使用。库不控制应用程序的整体架构，而是为开发者提供了可调用的工具，以便在应用程序中实现特定功能</p><p>框架是一种提供了<strong>一整套解决方案的软件结构</strong>，它规定了整个应用程序的架构，定义了组织代码的方式，并提供了一系列工具和库，以便开发者可以在框架的基础上构建应用。框架通常有一个完整的生命周期，控制着应用程序的流程，开发者需要按照框架的规则来编写代码。</p><h2 id="Node-js"><a href="#Node-js" class="headerlink" title="Node.js"></a>Node.js</h2><h4 id="什么是-Node-js"><a href="#什么是-Node-js" class="headerlink" title="什么是 Node.js"></a>什么是 Node.js</h4><p>JavaScript 是一个<strong>脚本语言</strong>，最初用来处理网页中的一些动态功能和一些用户行为。它一般运行于浏览器</p><p>但是这门语言后续不断更新，越来越多的人开始使用 JavaScript 。为了把它迁移到了服务端，但服务端上又不能跑浏览器，那我们就需要一种新的运行环境。就这样，这个基于 Chrome V8 引擎的 JavaScript 运行时 Node.js 诞生了</p><h2 id="模块化编程"><a href="#模块化编程" class="headerlink" title="模块化编程"></a>模块化编程</h2><p>在计算机编程中，模块是指一个<strong>相对独立的程序文件或代码库</strong>，通常包含一组相关的函数、变量、类或其他可重用的代码构件，每个模块在内部执行某个功能。并向外部公开一定的接口以供其他模块使用。在编程语言中，通常有一些标准库或第三方库，这些库都是由多个模块组成的，可以在程序中被引用和使用。模块化主要是为了帮助程序员组织和管理大型代码库，可以将大型的程序有逻辑地拆分成一个个相对较小的部分，实现代码复用，让程序设计更加灵活，使其更易于维护和扩展。这是优点之一。并且还可以避免变量名和函数名命名冲突的问题以及解决不同模块之间的依赖关系。</p><p>比如，我要写一个 Wordle 小游戏，普通代码编写就把所有代码像画布渲染，键盘的输入，逻辑判断等都写到一个 HTML 文件里，如果使用模块化概念，我们可以简单分块，分成主文件，键盘输入，逻辑判断以及读取 json 等多个模块，然后在各个文件里实现相应的逻辑，这样假如你发现 json 的读取有问题，你就可以直接去找读 json 那个文件有没有问题，这样会让代码的后续维护更简单，目的更明确。</p><p><code>import</code> 和 <code>export</code> 是 ES6 引入的模块系统的关键字，用于在 JavaScript 中进行模块化编程。模块化使得代码更结构化、可维护，并允许开发者将代码分割为<strong>小的可重用部分</strong></p><h4 id="export-的使用："><a href="#export-的使用：" class="headerlink" title="export 的使用："></a><code>export</code> 的使用：</h4><p><code>export</code> 用于将变量、函数、类或其他声明<strong>导出为模块的公共接口</strong>，以便其他模块可以使用。有三种常见的 <code>export</code> 的方式</p><h5 id="命名导出"><a href="#命名导出" class="headerlink" title="命名导出"></a>命名导出</h5><p>可以通过 <code>export</code> 关键字<strong>单独导出</strong>多个成员</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// module.js</span></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">const</span> myVariable = <span class="number">42</span>;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">function</span> <span class="title function_">myFunction</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="默认导出"><a href="#默认导出" class="headerlink" title="默认导出"></a>默认导出</h5><p>通过 <code>export default</code> 关键字导出一个<strong>默认</strong>成员，每个模块只能有<strong>一个</strong>默认导出</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// module.js</span></span><br><span class="line"><span class="keyword">const</span> myVariable = <span class="number">42</span>;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> myVariable;</span><br></pre></td></tr></table></figure><h4 id="import-的使用："><a href="#import-的使用：" class="headerlink" title="import 的使用："></a><code>import</code> 的使用：</h4><p><code>import</code> 用于在一个模块中引入其他模块导出的成员，以便在当前模块中使用。有三种常见的 <code>import</code> 的方式：</p><h5 id="命名导入"><a href="#命名导入" class="headerlink" title="命名导入"></a>命名导入</h5><p>导入其他模块中的命名导出</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// main.js</span></span><br><span class="line"><span class="keyword">import</span> &#123; myVariable, myFunction &#125; <span class="keyword">from</span> <span class="string">&quot;./module&quot;</span>;</span><br></pre></td></tr></table></figure><h5 id="默认导入"><a href="#默认导入" class="headerlink" title="默认导入"></a>默认导入</h5><p>导入其他模块中的默认导出</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// main.js</span></span><br><span class="line"><span class="keyword">import</span> myVariable <span class="keyword">from</span> <span class="string">&quot;./module&quot;</span>;</span><br></pre></td></tr></table></figure><h5 id="导入所有"><a href="#导入所有" class="headerlink" title="导入所有"></a>导入所有</h5><p>导入其他模块的所有导出，形成一个<strong>命名空间对象</strong></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// main.js</span></span><br><span class="line"><span class="keyword">import</span> * <span class="keyword">as</span> myModule <span class="keyword">from</span> <span class="string">&quot;./module&quot;</span>;</span><br></pre></td></tr></table></figure><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// module.js</span></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">const</span> myVariable = <span class="number">42</span>;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">function</span> <span class="title function_">myFunction</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">const</span> internalVariable = <span class="string">&quot;internal&quot;</span>;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> internalVariable;</span><br><span class="line"><span class="comment">// main.js</span></span><br><span class="line"><span class="keyword">import</span> &#123; myVariable, myFunction &#125; <span class="keyword">from</span> <span class="string">&quot;./module&quot;</span>;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(myVariable); <span class="comment">// 42</span></span><br><span class="line"><span class="title function_">myFunction</span>();</span><br><span class="line"><span class="keyword">import</span> internalVariable <span class="keyword">from</span> <span class="string">&quot;./module&quot;</span>;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(internalVariable); <span class="comment">// &#x27;internal&#x27;</span></span><br><span class="line"><span class="keyword">import</span> * <span class="keyword">as</span> myModule <span class="keyword">from</span> <span class="string">&quot;./module&quot;</span>;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(myModule.<span class="property">myVariable</span>); <span class="comment">// 42</span></span><br></pre></td></tr></table></figure><h2 id="npm"><a href="#npm" class="headerlink" title="npm"></a>npm</h2><p>JavaScript 包是一种封装了代码、资源的组织形式，能够方便共享、安装和管理代码。这些包可以包含 JavaScript 库、框架、工具或应用程序等。而 <strong><code>npm</code> 就是管理这些包的工具</strong>（当然除了 <code>npm</code> 也有其他工具，比如 <code>yarn</code>、<code>yum</code>等），专门用于在服务器端和命令行工具中管理 JavaScript 包</p><p>为什么我们需要包管理工具呢？我们一次性把包都下载到电脑里，像 C 语言的头文件一样，需要用什么拿什么不就好了吗？首先，JavaScript 的包<strong>多达 90 万个</strong>，将所有这些包完全下载到本地会占用大量存储空间。这对于开发者的计算机来说可能是不切实际的，特别是在多个项目中共享相同的依赖项时。其次，软件包和库经<strong>常会更新</strong>，手动下载所有包可能导致更新不及时，使得项目失去了最新的功能和安全性修复。最后，有的项目需要使用某个包特定的版本，使用其他版本会导致项目无法运行或出现其他 bug，而<strong>包管理工具允许开发者指定项目所使用的依赖项的特定版本</strong>，以确保项目的稳定性和一致性。手动下载所有包可能会导致版本冲突和不同环境之间的不一致。因此我们需要使用包管理工具</p><p><code>npm</code> 是<strong>随同 Node.js 安装的包管理工具</strong>，安装好 node 之后就会默认安装好 <code>npm</code> 了</p><p>我们可以在命令行中输入 <code>npm -v</code> 判断是否安装了 <code>npm</code></p><h5 id="npm-的常见命令"><a href="#npm-的常见命令" class="headerlink" title="npm 的常见命令"></a><code>npm</code> 的常见命令</h5><p><code>npm install &lt;Module Name&gt;</code> 使用 <code>npm</code> 命令<strong>本地安装</strong>模块</p><p><code>npm install -g &lt;Module Name&gt;</code> <strong>全局</strong>安装</p><p>两个的区别就是本地安装将安装包<strong>放在当前文件夹的 <code>node_modules</code></strong> （如果没有则会自动生成）文件夹下，<strong>通过 <code>import</code></strong> 来引入本地安装的包；全局安装包则通常放<strong>在 <code>node</code> 的安装目录下</strong>，可以<strong>直接在命令行里</strong>使用</p><p><code>npm uninstall &lt;Name&gt;</code> 卸载模块</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g npm@&lt;版本号&gt;//更新 npm</span><br></pre></td></tr></table></figure><p><code>npm publish</code> 将自己的代码发布到 <strong><code>npm</code> 上的全球开源库</strong>中</p><h5 id="package-json"><a href="#package-json" class="headerlink" title="package.json"></a><code>package.json</code></h5><p><code>package.json</code> 是 Node.js 项目中的一个重要文件，它用于<strong>存储项目的配置信息</strong>。包含了项目的元数据（metadata），如项目名称、版本、作者、依赖库等信息。通过描述项目上下文、所需依赖和开发脚本，使项目具备可重复性和可移植性</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;learn_react&quot;</span>, <span class="comment">// 项目的名称</span></span><br><span class="line">    <span class="string">&quot;version&quot;</span>: <span class="string">&quot;0.1.0&quot;</span>, <span class="comment">// 项目的版本号</span></span><br><span class="line">    <span class="string">&quot;private&quot;</span>: <span class="literal">true</span>, <span class="comment">// 用于指示是否将该项目发布到公共的包注册表的标志</span></span><br><span class="line">    <span class="string">&quot;dependencies&quot;</span>: &#123;</span><br><span class="line">        <span class="comment">// 项目运行时所依赖的第三方包</span></span><br><span class="line">        <span class="string">&quot;@testing-library/jest-dom&quot;</span>: <span class="string">&quot;^5.14.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;@testing-library/react&quot;</span>: <span class="string">&quot;^13.0.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;@testing-library/user-event&quot;</span>: <span class="string">&quot;^13.2.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;@types/jest&quot;</span>: <span class="string">&quot;^27.0.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;@types/node&quot;</span>: <span class="string">&quot;^16.7.13&quot;</span>,</span><br><span class="line">        <span class="string">&quot;@types/react&quot;</span>: <span class="string">&quot;^18.0.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;@types/react-dom&quot;</span>: <span class="string">&quot;^18.0.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;react&quot;</span>: <span class="string">&quot;^18.2.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;react-dom&quot;</span>: <span class="string">&quot;^18.2.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;react-scripts&quot;</span>: <span class="string">&quot;5.0.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;typescript&quot;</span>: <span class="string">&quot;^4.4.2&quot;</span>,</span><br><span class="line">        <span class="string">&quot;web-vitals&quot;</span>: <span class="string">&quot;^2.1.0&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;scripts&quot;</span>: &#123;</span><br><span class="line">        <span class="comment">// 定义一组自定义的命令脚本</span></span><br><span class="line">        <span class="string">&quot;start&quot;</span>: <span class="string">&quot;react-scripts start&quot;</span>,</span><br><span class="line">        <span class="string">&quot;build&quot;</span>: <span class="string">&quot;react-scripts build&quot;</span>,</span><br><span class="line">        <span class="string">&quot;test&quot;</span>: <span class="string">&quot;react-scripts test&quot;</span>,</span><br><span class="line">        <span class="string">&quot;eject&quot;</span>: <span class="string">&quot;react-scripts eject&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;browserslist&quot;</span>: &#123;</span><br><span class="line">        <span class="comment">// 用于指定项目所支持的目标浏览器范围的配置文件，通常用于前端开发</span></span><br><span class="line">        <span class="string">&quot;production&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;&gt;0.2%&quot;</span>, <span class="comment">// 支持全球使用率超过0.2%的浏览器</span></span><br><span class="line">            <span class="string">&quot;not dead&quot;</span>, <span class="comment">// 排除已经被官方宣布为不再更新的浏览器</span></span><br><span class="line">            <span class="string">&quot;not op_mini all&quot;</span> <span class="comment">// 用于排除 Opera Mini 浏览器，Opera Mini 具有一些独特的行为或限制，需要在项目中进行特殊处理</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="string">&quot;development&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;last 1 chrome version&quot;</span>, <span class="comment">// 支持每个浏览器的最后一个版本</span></span><br><span class="line">            <span class="string">&quot;last 1 firefox version&quot;</span>,</span><br><span class="line">            <span class="string">&quot;last 1 safari version&quot;</span></span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果项目有 <code>package.json</code> 文件，则通过命令 <code>npm install</code> <strong>可以根据 <code>&quot;dependencies&quot;</code></strong> 自动在 <code>node_modules</code> 文件夹中安装项目所需的所有包</p><p>注：上述 <code>package.json</code> 的注释是粘贴到 md 后再加的，目的是讲解<strong>键值对的意义</strong>，而 json 文件中是<strong>不允许添加注释的</strong>：</p><h2 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h2><p>打包是指<strong>将多个模块（ JavaScript、CSS、图片等）打包成为一个文件</strong>，这有助于代码管理、发布和使用。在前端开发中，通常需要使用打包工具将代码打包成<strong>浏览器可识别</strong>的格式，并优化加载速度和性能。</p><p>为什么前端需要打包？以前的前端开发存在三个大问题：没有模块化、第三方包的引入繁琐困难、代码以明文形式展示出来</p><p>我们利用打包工具就可以实现：支持模块化、自动打包第三方包、代码混淆，使得其他人无法阅读</p><p>下面介绍两个常使用的与打包有关的工具.</p><h4 id="Babel"><a href="#Babel" class="headerlink" title="Babel"></a>Babel</h4><p>Babel 是一个 JavaScript 编译器，它能够将 ECMAScript 2015+ 的新特性转换为向后兼容的 JavaScript 代码，例如将 ES6 的箭头函数转换为普通函数、将模板字符串转换为常规字符串等等，使得我们可以在现代浏览器中使用最新的 JavaScript 特性，从而<strong>解决浏览器兼容性问题</strong></p><p>执行 <code>npm install -g babel-cli</code> 安装 Babel</p><p>在项目根目录创建 <code>.babelrc</code> 文件，这是 Babel 的配置文件，并编写：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;presets&quot;</span>: [<span class="string">&quot;es2015&quot;</span>],</span><br><span class="line">    <span class="string">&quot;plugins&quot;</span>: []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行 <code>npm install babel-preset-es2015</code> 安装转码器，就是从源码转到老版本的代码中间的语法映射表</p><p>在根目录创建 <code>src</code> 文件夹，新建 <code>index.js</code> 并编写如下代码</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ./src/index.js</span></span><br><span class="line"><span class="keyword">let</span> [a, b, c] = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>];</span><br><span class="line">[a, b, c] = [b, c, a + <span class="number">1</span>];</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(a, b, c);</span><br></pre></td></tr></table></figure><p>这里用到了 ES6 的新特性<strong>解构赋值</strong>，执行 <code>babel src -d dist</code> Babel 就能够将它转换为旧的 ES2015 代码：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ./dist/index.js</span></span><br><span class="line"><span class="meta">&quot;use strict&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> a = <span class="number">1</span>,</span><br><span class="line">    b = <span class="number">2</span>,</span><br><span class="line">    c = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">var</span> _ref = [b, c, a + <span class="number">1</span>];</span><br><span class="line">a = _ref[<span class="number">0</span>];</span><br><span class="line">b = _ref[<span class="number">1</span>];</span><br><span class="line">c = _ref[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(a, b, c);</span><br></pre></td></tr></table></figure><h4 id="Webpack"><a href="#Webpack" class="headerlink" title="Webpack"></a>Webpack</h4><p>Webpack 是一个模块打包工具，它可以将<strong>多个模块打包</strong>成<strong>一个或多个 JavaScript 文件</strong>，而这些 JavaScript 文件可以被浏览器正确加载执行。Webpack 可以处理各种类型的资源文件，如 JS、CSS、图片等，并提供了各种插件和 loader 用于对不同类型的资源进行处理和优化，同时还支持热更新功能，方便开发人员进行调试和开发</p><p>Webpack 会<strong>隐藏源码的细节</strong>，把多个 JavaScript 合并成一个 JavaScript，提高浏览器的<strong>访问速度</strong>，使源码<strong>更加安全</strong></p><p>执行 <code>npm install -g webpack webpack-cli</code> 安装 Webpack</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//导入path模块,nodejs的内置模块</span></span><br><span class="line"><span class="keyword">const</span> path = <span class="built_in">require</span>(<span class="string">&quot;path&quot;</span>);</span><br><span class="line"><span class="comment">//定义JS打包的规则</span></span><br><span class="line"><span class="variable language_">module</span>.<span class="property">exports</span> = &#123;</span><br><span class="line">    <span class="comment">//指定构建的模式</span></span><br><span class="line">    <span class="attr">mode</span>: <span class="string">&quot;development&quot;</span>,</span><br><span class="line">    <span class="comment">//入口函数从哪里开始进行编译打包</span></span><br><span class="line">    <span class="attr">entry</span>: <span class="string">&quot;./src/main.js&quot;</span>,</span><br><span class="line">    <span class="comment">//编译成功以后要把内容输出到那里去</span></span><br><span class="line">    <span class="attr">output</span>: &#123;</span><br><span class="line">        <span class="comment">//定义输出的指定的目录__dirname 当前项目根目录，将生成一个dist文件夹</span></span><br><span class="line">        <span class="attr">path</span>: path.<span class="title function_">resolve</span>(__dirname, <span class="string">&quot;./dist&quot;</span>),</span><br><span class="line">        <span class="comment">//合并的js文件存储在dist/bundle.js文件中</span></span><br><span class="line">        <span class="attr">filename</span>: <span class="string">&quot;res.js&quot;</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>终端执行 <code>webpack</code> 即可在 <strong><code>dist</code> 文件夹中</strong>看到生成的 <code>res.js</code>，这就是合并后的 JavaScript 代码</p><p>通常在前端项目中，我们会<strong>将 Babel 和 Webpack 结合</strong>使用，使用 Babel 将最新版本的语法<strong>转换成向后兼容的代码</strong>，再由 Webpack 将这些代码<strong>打包并优化</strong>，最终生成浏览器可以解析的文件。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>计算机教育中缺失的一课笔记</title>
      <link href="/2025/01/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%95%99%E8%82%B2%E4%B8%AD%E7%BC%BA%E5%A4%B1%E7%9A%84%E4%B8%80%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
      <url>/2025/01/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%95%99%E8%82%B2%E4%B8%AD%E7%BC%BA%E5%A4%B1%E7%9A%84%E4%B8%80%E8%AF%BE%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>我们中的大多数人实际上只利用了这些工具中的<strong>很少</strong>一部分，我们常常只是死记硬背一些如咒语般的命令， 或是当我们卡住的时候，盲目地从网上复制粘贴一些命令。</p><h1 id="主题-1-The-Shell"><a href="#主题-1-The-Shell" class="headerlink" title="主题 1: The Shell"></a>主题 1: The Shell</h1><h2 id="shell-是什么？"><a href="#shell-是什么？" class="headerlink" title="shell 是什么？"></a>shell 是什么？</h2><p>如今的计算机有着多种多样的<strong>交互接口</strong>让我们可以进行指令的的输入，从炫酷的图像用户界面**（GUI）<strong>，<strong>语音输入</strong>甚至是 <strong>AR&#x2F;VR</strong> 都已经无处不在。 这些交互接口可以覆盖 80% 的使用场景，但是它们也从根本上限制了您的操作方式——你不能点击一个</strong>不存在的<strong>按钮或者是用语音输入一个</strong>还没有被录入的<strong>指令。 为了充分利用计算机的能力，我们不得不回到</strong>最根本<strong>的方式，使用</strong>文字接口：Shell**</p><p>几乎所有您能够接触到的平台都支持某种形式的 shell，有些甚至还提供了<strong>多种 shell</strong> 供您选择。虽然它们之间有些细节上的差异，但是其<strong>核心功能</strong>都是一样的：它允许你<strong>执行程序</strong>，<strong>输入并获取某种半结构化的输出</strong>。</p><p>本节课我们会使用 <strong>Bourne Again SHell, 简称 “bash”</strong> 。 这是被最广泛使用的一种 shell，它的语法和其他的 shell 都是类似的。打开 shell <em>提示符</em>（您输入指令的地方），您首先需要打开 <em>终端</em> 。您的设备通常都已经内置了终端，或者您也可以安装一个，非常简单。</p><h2 id="使用-shell-往光标前插东西"><a href="#使用-shell-往光标前插东西" class="headerlink" title="使用 shell(往光标前插东西)"></a>使用 shell(往光标前插东西)</h2><p>当您打开终端时，您会看到一个提示符，它看起来一般是这个样子的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">missing:~$ </span><br></pre></td></tr></table></figure><p>这是 shell 最主要的文本接口。它告诉你，你的主机名是 <code>missing</code> 并且您当前的工作目录（”current working directory”）或者说您当前所在的位置是 <code>~</code> (表示 “home”)。 <code>$</code> 符号表示您现在的身份不是 root 用户（稍后会介绍）。在这个提示符中，您可以输入 <em>命令</em> ，命令最终会被 shell 解析。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">missing:~$ <span class="built_in">date</span></span><br><span class="line">Fri 10 Jan 2020 11:49:31 AM EST</span><br><span class="line">missing:~$ </span><br></pre></td></tr></table></figure><p>我们可以在执行命令的同时向程序传递 <em>参数</em> ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">missing:~$ echo hello</span><br><span class="line">hello</span><br></pre></td></tr></table></figure><p>上例中，我们让 shell 执行 <code>echo</code> ，同时指定参数 <code>hello</code>。<code>echo</code> 程序将该参数<strong>打印</strong>出来。 shell <strong>基于空格分割命令</strong>并进行解析，然后<strong>执行第一个单词</strong>代表的程序，并将<strong>后续的单词作为程序可以访问的参数</strong>。如果您希望传递的参数中<strong>包含空格</strong>（例如一个名为 My Photos 的文件夹），您要么用使用<strong>单引号，双引号将其包裹</strong>起来，要么使用<strong>转义符号 <code>\</code></strong> 进行处理（<code>My\ Photos</code>）。</p><p>但是，shell 是如何知道去哪里寻找 <code>date</code> 或 <code>echo</code> 的呢？其实，类似于 Python 或 Ruby，shell 是一个<strong>编程环境</strong>，所以它具备变量、条件、循环和函数（下一课进行讲解）。当你在 shell 中执行命令时，您实际上是在执行一段 shell 可以解释执行的<strong>简短代码</strong>。如果你要求 shell 执行某个指令，但是该指令<strong>并不是 shell 所了解的</strong>编程关键字，那么它会去<strong>咨询 <em>环境变量</em> <code>$PATH</code></strong>，它会列<strong>出当 shell 接到某条指令时，进行程序搜索的路径</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">missing:~$ echo $PATH</span><br><span class="line">/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</span><br><span class="line">missing:~$ which echo</span><br><span class="line">/bin/echo</span><br><span class="line">missing:~$ /bin/echo $PATH</span><br><span class="line">/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</span><br></pre></td></tr></table></figure><p>当我们执行 <code>echo</code> 命令时，shell 了解到需要执行 <code>echo</code> 这个程序，随后它便会在 <code>$PATH</code> 中搜索由 <code>:</code> 所分割的一系列目录，<strong>基于名字搜索该程序</strong>。当找到该程序时便执行（假定该文件是 <em><strong>可执行程序</strong></em>，后续课程将详细讲解）。确定某个程序<strong>名代表的是哪个具体</strong>的程序，可以使用 <code>which</code> 程序。我们也可以<strong>绕过 <code>$PATH</code></strong>，通过<strong>直接指定</strong>需要执行的程序的路径来执行该程序</p><h2 id="在-shell-中导航"><a href="#在-shell-中导航" class="headerlink" title="在 shell 中导航"></a>在 shell 中导航</h2><p>shell 中的路径是一组被分割的目录，在 Linux 和 macOS 上使用 <strong><code>/</code> 分割</strong>，而在 Windows 上是 <code>\</code>。<strong>路径 <code>/</code></strong> 代表的是系统的<strong>根</strong>目录，所有的文件夹都包括在这个路径之下，在 Windows 上每个<strong>盘</strong>都有一个根目录（例如： <code>C:\</code>）。 我们假设您在学习本课程时使用的是 Linux 文件系统。如果某个路径以 <strong><code>/</code> 开头</strong>，那么它是一个 <em><strong>绝对</strong>路径</em>，其他的都是 <em>相对路径</em> 。相对路径是指<strong>相对于当前工作目录</strong>的路径，当前工作目录可以使用 <strong><code>pwd</code> 命令来获取</strong>。此外，切换目录需要使用 <code>cd</code> 命令(linux中单独用直接切回<strong>最初</strong>)。在路径中，<code>.</code> <strong>表示的是当前</strong>目录，而 <strong><code>..</code> 表示上级</strong>目录(用于切换)：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">missing:~$ pwd</span><br><span class="line">/home/missing</span><br><span class="line">missing:~$ cd /home</span><br><span class="line">missing:/home$ pwd</span><br><span class="line">/home</span><br><span class="line">missing:/home$ cd ..</span><br><span class="line">missing:/$ pwd</span><br><span class="line">/</span><br><span class="line">missing:/$ cd ./home</span><br><span class="line">missing:/home$ pwd</span><br><span class="line">/home</span><br><span class="line">missing:/home$ cd missing</span><br><span class="line">missing:~$ pwd</span><br><span class="line">/home/missing</span><br><span class="line">missing:~$ ../../bin/echo hello</span><br><span class="line">hello</span><br></pre></td></tr></table></figure><p>注意，shell 会<strong>实时显示</strong>当前的路径信息。您可以通过配置 shell 提示符来显示各种有用的信息，这一内容我们会在后面的课程中进行讨论。</p><p>一般来说，当我们运行一个程序时，如果我们<strong>没有指定路径</strong>，则该程序会<strong>默认在当前目录下</strong>执行。例如，我们常常会搜索文件，并在需要时创建文件。</p><p>为了查看指定目录下包含哪些文件，我们使用 <code>ls</code> 命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">missing:~$ ls//未指定路径看不了</span><br><span class="line">missing:~$ cd ..//切至根目录/</span><br><span class="line">missing:/home$ ls</span><br><span class="line">missing</span><br><span class="line">missing:/home$ cd ..</span><br><span class="line">missing:/$ ls</span><br><span class="line">bin</span><br><span class="line">boot</span><br><span class="line">dev</span><br><span class="line">etc</span><br><span class="line">home</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>除非我们利用<strong>第一个参数指定目录</strong>，否则 <code>ls</code> 会打印当前目录下的文件。大多数的命令接受<strong>标记</strong>和<strong>选项（带有值的标记）</strong>，它们以 <strong><code>-</code> 开头</strong>，并可以改变程序的行为。通常，在执行程序时使用 <code>-h</code> 或 <code>--help</code> (不是单独用)标记可以<strong>打印帮助信息</strong>，以便了解有哪些可用的标记或选项。例如，<code>ls --help</code> 的输出如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  -l                         use a long listing format</span><br><span class="line">missing:~$ <span class="built_in">ls</span> -l /home</span><br><span class="line">drwxr-xr-x 1 missing  <span class="built_in">users</span>  4096 Jun 15  2019 missing</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>算法日记</title>
      <link href="/2025/01/22/%E7%AE%97%E6%B3%95%E6%97%A5%E8%AE%B0/"/>
      <url>/2025/01/22/%E7%AE%97%E6%B3%95%E6%97%A5%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> lld long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> lf double</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> endl <span class="string">&#x27;\n&#x27;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> llu unsigned long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ci const int</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> clld const long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> cllu const unsigned long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> mo (1e9+7)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pi (acos(-1))</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> gc() getchar()</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> repp(i,a,b) for(lld i=(a);i&lt;=(b);i++)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> repm(i,a,b) for(lld i=(a);i&gt;=(b);i--)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ret return</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ct continue</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> <span class="keyword">elif</span> <span class="keyword">else</span> <span class="keyword">if</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pii pair<span class="string">&lt;int,int&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> mp make_pair</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> srepp(i,a) for(auto i=a.begin();i!=a.end();i++)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> srepm(i,a) for(auto i=--a.end();i!=--a.begin();i--)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> vi vector <span class="string">&lt;int&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> vii vector <span class="string">&lt;pair&lt;int,int&gt;</span>&gt;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> vlld vector <span class="string">&lt;long long&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> st struct</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> M 403</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> P</span></span><br></pre></td></tr></table></figure><h2 id="2025-1-25"><a href="#2025-1-25" class="headerlink" title="2025.1.25"></a>2025.1.25</h2><h4 id="马的遍历"><a href="#马的遍历" class="headerlink" title="马的遍历"></a>马的遍历</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> n,m,x,y,a[M][M];</span><br><span class="line">vector&lt;pii&gt;d;</span><br><span class="line">queue&lt;pii&gt;q;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">vd</span><span class="params">(pii s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(s.first&lt;=<span class="number">0</span>||s.second&lt;=<span class="number">0</span>||s.first&gt;=n<span class="number">+1</span>||s.second&gt;=m<span class="number">+1</span>)</span><br><span class="line">    ret <span class="number">1</span>;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bfs</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    <span class="built_in">repp</span>(j,<span class="number">1</span>,m)</span><br><span class="line">    a[i][j]=<span class="number">-1</span>;</span><br><span class="line">    a[x][y]=<span class="number">0</span>;</span><br><span class="line">    q.<span class="built_in">push</span>(<span class="built_in">mp</span>(x,y));</span><br><span class="line">    <span class="keyword">while</span>(q.<span class="built_in">size</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        pii now=q.<span class="built_in">front</span>();q.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">0</span>,<span class="number">7</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            pii nx=<span class="built_in">mp</span>(now.first-d[i].first,now.second-d[i].second);</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">vd</span>(nx))ct;</span><br><span class="line">            <span class="keyword">if</span>(a[nx.first][nx.second]==<span class="number">-1</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                a[nx.first][nx.second]=a[now.first][now.second]<span class="number">+1</span>;</span><br><span class="line">                q.<span class="built_in">push</span>(nx);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="number">0</span>);cin.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin&gt;&gt;n&gt;&gt;m&gt;&gt;x&gt;&gt;y;</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">-2</span>,<span class="number">1</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">-2</span>,<span class="number">-1</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">2</span>,<span class="number">1</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">2</span>,<span class="number">-1</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">1</span>,<span class="number">2</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">1</span>,<span class="number">-2</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">-1</span>,<span class="number">2</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">-1</span>,<span class="number">-2</span>));</span><br><span class="line">    <span class="built_in">bfs</span>();</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">repp</span>(j,<span class="number">1</span>,m)</span><br><span class="line">        &#123;</span><br><span class="line">            cout&lt;&lt;a[i][j]&lt;&lt;<span class="string">&quot;    &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="luogu-P2895"><a href="#luogu-P2895" class="headerlink" title="luogu P2895"></a>luogu P2895</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">bool</span> s[<span class="number">304</span>][<span class="number">304</span>] ,biao,vis[<span class="number">304</span>][<span class="number">304</span>],ditu[<span class="number">304</span>][<span class="number">304</span>];<span class="type">int</span> maxt,mint,t;</span><br><span class="line">queue&lt;pii&gt;q;</span><br><span class="line"><span class="type">int</span> dx[<span class="number">4</span>]=&#123;<span class="number">-1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>&#125;;</span><br><span class="line"><span class="type">int</span> dy[<span class="number">4</span>]=&#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">-1</span>,<span class="number">1</span>&#125;;</span><br><span class="line">unordered_map&lt;<span class="type">int</span>,vii&gt;tu;<span class="comment">//是unordered而不是multi</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">vd</span><span class="params">(<span class="type">int</span> x,<span class="type">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(ditu[x][y]||x&lt;=<span class="number">0</span>||y&lt;=<span class="number">0</span>)ret <span class="number">0</span>;</span><br><span class="line">    ret <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bfs</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   q.<span class="built_in">push</span>(<span class="built_in">mp</span>(<span class="number">1</span>,<span class="number">1</span>));</span><br><span class="line">   <span class="keyword">while</span>(q.<span class="built_in">size</span>())</span><br><span class="line">   &#123;</span><br><span class="line">    <span class="type">int</span> cishu=q.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span>(tu.<span class="built_in">find</span>(t)!=tu.<span class="built_in">end</span>() )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> it : tu[t])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> xx=it.first;<span class="type">int</span> yy=it.second;</span><br><span class="line">            ditu[xx][yy]=ditu[xx<span class="number">-1</span>][yy]=ditu[xx][yy<span class="number">-1</span>]=ditu[xx<span class="number">+1</span>][yy]=ditu[xx][yy<span class="number">+1</span>]=<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,cishu)</span><br><span class="line">    &#123;</span><br><span class="line">        pii it=q.<span class="built_in">front</span>();q.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="type">int</span> xx=it.first;<span class="type">int</span> yy=it.second;</span><br><span class="line">        <span class="keyword">if</span>(vis[xx][yy])&#123;</span><br><span class="line">            mint=t;ret;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(ditu[xx][yy])ct;</span><br><span class="line">        ditu[xx][yy]=<span class="number">1</span>;</span><br><span class="line">            <span class="built_in">repp</span>(i,<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(<span class="built_in">vd</span>(xx-dx[i],yy-dy[i]))</span><br><span class="line">                q.<span class="built_in">push</span>(<span class="built_in">mp</span>(xx-dx[i],yy-dy[i]));</span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    t++;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="type">int</span> m;mint = <span class="number">1002</span>;</span><br><span class="line">    cin&gt;&gt;m;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> x,y,t;</span><br><span class="line">        cin&gt;&gt;x&gt;&gt;y&gt;&gt;t;</span><br><span class="line">        x++;y++;</span><br><span class="line">        tu[t].<span class="built_in">push_back</span>(<span class="built_in">mp</span>(x,y));</span><br><span class="line">        s[x][y]=s[x<span class="number">-1</span>][y]=s[x<span class="number">+1</span>][y]=s[x][y<span class="number">-1</span>]=s[x][y<span class="number">+1</span>]=<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,<span class="number">302</span>)</span><br><span class="line">    <span class="built_in">repp</span>(j,<span class="number">1</span>,<span class="number">302</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(s[i][j]==<span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            biao=<span class="number">1</span>;</span><br><span class="line">            vis[i][j]=<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(!biao)</span><br><span class="line">    &#123;</span><br><span class="line">        cout&lt;&lt;<span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">bfs</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(mint==<span class="number">1002</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        cout&lt;&lt;<span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        cout&lt;&lt;mint;</span><br><span class="line">    &#125;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2025-2-4"><a href="#2025-2-4" class="headerlink" title="2025.2.4"></a>2025.2.4</h2><h4 id="P1955-程序自动分析"><a href="#P1955-程序自动分析" class="headerlink" title="P1955 程序自动分析"></a>P1955 程序自动分析</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> f[<span class="number">200003</span>],cnt;<span class="type">bool</span> biao;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(x==f[x])ret x;</span><br><span class="line">    <span class="built_in">ret</span> (f[x]=<span class="built_in">get</span>(f[x]));</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">merge</span><span class="params">(<span class="type">int</span> x,<span class="type">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     (f[<span class="built_in">get</span>(x)]=<span class="built_in">get</span>(y)) ;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="type">int</span> t;cin&gt;&gt;t;</span><br><span class="line">    <span class="keyword">while</span>(t--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n,e,x,y;</span><br><span class="line">        cin&gt;&gt;n ;</span><br><span class="line">        biao=cnt=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">memset</span>(f,<span class="number">0</span>,<span class="built_in">sizeof</span>(f));</span><br><span class="line">    unordered_map&lt;<span class="type">int</span>,<span class="type">int</span>&gt;p;</span><br><span class="line">    vii ck ;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,<span class="number">2</span>*n<span class="number">+1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        f[i]=i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        cin&gt;&gt;x&gt;&gt;y&gt;&gt;e;</span><br><span class="line">        <span class="keyword">if</span>(p.<span class="built_in">find</span>(x)!=p.<span class="built_in">end</span>())x=p[x];</span><br><span class="line">        <span class="keyword">else</span>    &#123;p[x]=++cnt;x=cnt;&#125;</span><br><span class="line">        <span class="keyword">if</span>(p.<span class="built_in">find</span>(y)!=p.<span class="built_in">end</span>())y=p[y];</span><br><span class="line">        <span class="keyword">else</span>    &#123;p[y]=++cnt;y=cnt;&#125;</span><br><span class="line">        <span class="keyword">if</span>(e)</span><br><span class="line">        <span class="built_in">merge</span>(x,y);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        ck.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(x,y));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> it:ck)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">get</span>(it.first)==<span class="built_in">get</span>(it.second))&#123;</span><br><span class="line">            biao=<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(biao)cout&lt;&lt;<span class="string">&quot;NO&quot;</span>;</span><br><span class="line">    <span class="keyword">else</span>    cout&lt;&lt;<span class="string">&quot;YES&quot;</span>;</span><br><span class="line">    cout&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="P1090-合并果子"><a href="#P1090-合并果子" class="headerlink" title="P1090 合并果子"></a>P1090 合并果子</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">lld heap[M],Size,n;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Up</span><span class="params">(lld x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(x&gt;<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(heap[x]&lt;heap[x/<span class="number">2</span>])</span><br><span class="line">        &#123;<span class="built_in">swap</span>(heap[x],heap[x/<span class="number">2</span>]);</span><br><span class="line">        x/=<span class="number">2</span>;&#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Down</span><span class="params">(lld x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    lld s=x*<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">while</span>(s&lt;=Size)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(s&lt;Size&amp;&amp;heap[s]&gt;heap[s<span class="number">+1</span>])s++;</span><br><span class="line">        <span class="keyword">if</span>(heap[s]&lt;heap[x])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">swap</span>(heap[s],heap[x]);</span><br><span class="line">            x=s;s=x*<span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Insert</span><span class="params">(lld x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    heap[++Size]=x;</span><br><span class="line">    <span class="built_in">Up</span>(Size);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">lld <span class="title">Top</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ret heap[<span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Pop</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    heap[<span class="number">1</span>]=heap[Size--];</span><br><span class="line">    <span class="built_in">Down</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        lld op;</span><br><span class="line">        cin&gt;&gt;op;</span><br><span class="line">        <span class="built_in">Insert</span>(op);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(n==<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        cout&lt;&lt;heap[<span class="number">1</span>];</span><br><span class="line">        ret <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    lld sum=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n<span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        lld tem=<span class="built_in">Top</span>();</span><br><span class="line">        <span class="built_in">Pop</span>();</span><br><span class="line">        tem+=<span class="built_in">Top</span>();</span><br><span class="line">        <span class="built_in">Pop</span>();</span><br><span class="line">        sum+=tem;</span><br><span class="line">        <span class="built_in">Insert</span>(tem);</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;sum;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="25-2-7"><a href="#25-2-7" class="headerlink" title="25.2.7"></a>25.2.7</h2><h4 id="P1111-修复公路"><a href="#P1111-修复公路" class="headerlink" title="P1111 修复公路"></a>P1111 修复公路</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> lld long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> lf double</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> endl <span class="string">&#x27;\n&#x27;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> llu unsigned long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ci const int</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> clld const long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> cllu const unsigned long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> mo (1e9+7)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pi (acos(-1))</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> gc() getchar()</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> repp(i,a,b) for(lld i=(a);i&lt;=(b);i++)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> repm(i,a,b) for(lld i=(a);i&gt;=(b);i--)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ret return</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ct continue</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> <span class="keyword">elif</span> <span class="keyword">else</span> <span class="keyword">if</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pii pair<span class="string">&lt;int,int&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> mp make_pair</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> srepp(i,a) for(auto i=a.begin();i!=a.end();i++)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> srepm(i,a) for(auto i=--a.end();i!=--a.begin();i--)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> vi vector <span class="string">&lt;int&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> vii vector <span class="string">&lt;pair&lt;int,int&gt;</span>&gt;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> vlld vector <span class="string">&lt;long long&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> st struct</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> M 1003</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> P</span></span><br><span class="line"><span class="type">int</span> n,m,fa[M],cnt,maxt;</span><br><span class="line">vector&lt;pair&lt;<span class="type">int</span>,pair&lt;<span class="type">int</span>,<span class="type">int</span>&gt;&gt;&gt;a;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> i)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(i==fa[i])ret i;</span><br><span class="line">    <span class="built_in">ret</span>(fa[i]=<span class="built_in">get</span>(fa[i]));</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">merge</span><span class="params">(<span class="type">int</span> i,<span class="type">int</span> j)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    fa[<span class="built_in">get</span>(i)]=<span class="built_in">get</span>(j);<span class="comment">//别忘了是要根与根合并,一个get()都不能少</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    cin&gt;&gt;n&gt;&gt;m;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    fa[i]=i;</span><br><span class="line">    a.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">0</span>,<span class="built_in">mp</span>(<span class="number">0</span>,<span class="number">0</span>)));</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> x,y,t;</span><br><span class="line">        cin&gt;&gt;x&gt;&gt;y&gt;&gt;t;</span><br><span class="line">        a.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(t,<span class="built_in">mp</span>(x,y)));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">sort</span>(a.<span class="built_in">begin</span>()<span class="number">+1</span>,a.<span class="built_in">end</span>());<span class="comment">//按时间来排序处理就好</span></span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> t=a[i].first;</span><br><span class="line">        <span class="type">int</span> x=(a[i].second).first;</span><br><span class="line">        <span class="type">int</span> y=(a[i].second).second;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">get</span>(x)!=<span class="built_in">get</span>(y))</span><br><span class="line">        &#123;</span><br><span class="line">            cnt++;</span><br><span class="line">            <span class="built_in">merge</span>(x,y);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(cnt==n<span class="number">-1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            cout&lt;&lt;t;</span><br><span class="line">            ret <span class="number">0</span>;</span><br><span class="line">        &#125;&#125;</span><br><span class="line">    cout&lt;&lt;<span class="number">-1</span>;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="25-2-11-cf1003-c1-贪心-每个数都在符合要求的情况下尽量保持最小-无法满足就退出"><a href="#25-2-11-cf1003-c1-贪心-每个数都在符合要求的情况下尽量保持最小-无法满足就退出" class="headerlink" title="25.2.11 cf1003 c1(贪心,每个数都在符合要求的情况下尽量保持最小,无法满足就退出)"></a>25.2.11 cf1003 c1(贪心,每个数都在符合要求的情况下尽量保持最小,无法满足就退出)</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> n,m,b,a[<span class="number">200003</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="type">int</span> t;</span><br><span class="line">    cin&gt;&gt;t;</span><br><span class="line">    <span class="keyword">while</span>(t--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">bool</span> biao=<span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> n,m;</span><br><span class="line">        cin&gt;&gt;n&gt;&gt;m;</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">        cin&gt;&gt;a[i];</span><br><span class="line">        cin&gt;&gt;b;</span><br><span class="line">        a[<span class="number">1</span>]=<span class="built_in">min</span>(a[<span class="number">1</span>],b-a[<span class="number">1</span>]);</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">2</span>,n)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> t1=<span class="built_in">max</span>(b-a[i],a[i]);</span><br><span class="line">            <span class="type">int</span> t2=<span class="built_in">min</span>(b-a[i],a[i]);</span><br><span class="line">            <span class="keyword">if</span>(a[i<span class="number">-1</span>]&lt;=t2)a[i]=t2;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(a[i<span class="number">-1</span>]&gt;t1)</span><br><span class="line">                &#123;</span><br><span class="line">                    biao=<span class="number">0</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                a[i]=t1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(biao)</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;YES&quot;</span>&lt;&lt;endl;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;No&quot;</span>&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="25-2-13P1048-采药"><a href="#25-2-13P1048-采药" class="headerlink" title="25.2.13P1048 采药"></a>25.2.13P1048 采药</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> w[M],v[M];</span><br><span class="line"><span class="type">int</span> dp[M][M*<span class="number">10</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="type">int</span> t,m;</span><br><span class="line">    cin&gt;&gt;t&gt;&gt;m;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    &#123;</span><br><span class="line">        cin&gt;&gt;w[i]&gt;&gt;v[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">repp</span>(j,<span class="number">1</span>,t)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[i][j]=dp[i<span class="number">-1</span>][j];<span class="comment">//先考虑不放入,继承(跨度较大)</span></span><br><span class="line">            <span class="keyword">if</span>(j&gt;=w[i])<span class="comment">//总时间大于这个时即可以尝试放入</span></span><br><span class="line">            dp[i][j]=<span class="built_in">max</span>(dp[i][j],dp[i<span class="number">-1</span>][j-w[i]]+v[i]);<span class="comment">//由子问题最优解递推,如果可以增值就放入</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;dp[m][t];    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="25-2-15"><a href="#25-2-15" class="headerlink" title="25.2.15"></a>25.2.15</h4><p>P1616<strong>十年oi一场空,不开longlong见祖宗</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">lld a[<span class="number">10003</span>],b[<span class="number">10003</span>],dp[<span class="number">10000003</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    lld t,m;</span><br><span class="line">    cin&gt;&gt;t&gt;&gt;m;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    cin&gt;&gt;a[i]&gt;&gt;b[i];</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">repp</span>(j,a[i],t)</span><br><span class="line">        dp[j]=<span class="built_in">max</span>(dp[j],dp[j-a[i]]+b[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;dp[t];</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="25-4-16"><a href="#25-4-16" class="headerlink" title="25.4.16"></a>25.4.16</h2><h4 id="P3870-开关"><a href="#P3870-开关" class="headerlink" title="P3870 开关"></a>P3870 开关</h4><p>使用异或即可**(改变为一定与当前状态不同的另一种状态,即^1)**</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> M=<span class="number">1e5</span><span class="number">+10</span>;</span><br><span class="line">st tree&#123;</span><br><span class="line">lld l,r,dat;</span><br><span class="line"><span class="type">bool</span> cg;</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> l(p) t[p].l</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> r(p) t[p].r</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> dat(p) t[p].dat</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> cg(p) t[p].cg</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> len(p) (t[p].r-t[p].l+1)</span></span><br><span class="line">&#125;t[M*<span class="number">4</span>];</span><br><span class="line">lld n,m,a[M];</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">pushup</span><span class="params">(lld p)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">dat</span>(p)=<span class="built_in">dat</span>(<span class="built_in">ls</span>(p))+<span class="built_in">dat</span>(<span class="built_in">rs</span>(p));</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">spread</span><span class="params">(lld p)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span>(t[p].cg)</span><br><span class="line">&#123;</span><br><span class="line">t[<span class="built_in">ls</span>(p)].dat=<span class="built_in">len</span>(<span class="built_in">ls</span>(p))-t[<span class="built_in">ls</span>(p)].dat;</span><br><span class="line">t[<span class="built_in">rs</span>(p)].dat=<span class="built_in">len</span>(<span class="built_in">rs</span>(p))-t[<span class="built_in">rs</span>(p)].dat;</span><br><span class="line">t[<span class="built_in">ls</span>(p)].cg^=t[p].cg;t[<span class="built_in">rs</span>(p)].cg^=t[p].cg;</span><br><span class="line">t[p].cg=<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">build</span><span class="params">(lld p,lld l,lld r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">l</span>(p)=l;<span class="built_in">r</span>(p)=r;<span class="built_in">cg</span>(p)=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">if</span>(l==r)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">dat</span>(p)=<span class="number">0</span>;ret;</span><br><span class="line">&#125;</span><br><span class="line">lld mid=(l+r)&gt;&gt;<span class="number">1</span>;</span><br><span class="line"><span class="built_in">build</span>(<span class="built_in">ls</span>(p),l,mid);</span><br><span class="line"><span class="built_in">build</span>(<span class="built_in">rs</span>(p),mid<span class="number">+1</span>,r);</span><br><span class="line"><span class="built_in">pushup</span>(p);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">change</span><span class="params">(lld p,lld l,lld r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span>(l&lt;=<span class="built_in">l</span>(p)&amp;&amp;r&gt;=<span class="built_in">r</span>(p))</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">dat</span>(p)=<span class="built_in">len</span>(p)-<span class="built_in">dat</span>(p);</span><br><span class="line"><span class="built_in">cg</span>(p)^=<span class="number">1</span>;</span><br><span class="line">ret;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">spread</span>(p);</span><br><span class="line">lld mid=(<span class="built_in">l</span>(p)+<span class="built_in">r</span>(p))&gt;&gt;<span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span>(l&lt;=mid) <span class="built_in">change</span>(<span class="built_in">ls</span>(p),l,r);</span><br><span class="line"><span class="keyword">if</span>(r&gt;mid) <span class="built_in">change</span>(<span class="built_in">rs</span>(p),l,r);</span><br><span class="line"><span class="built_in">pushup</span>(p);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">lld <span class="title">ask</span><span class="params">(lld p,lld l,lld r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">lld ans=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">if</span>(l&lt;=<span class="built_in">l</span>(p)&amp;&amp;r&gt;=<span class="built_in">r</span>(p))</span><br><span class="line">&#123;</span><br><span class="line"><span class="function">ret <span class="title">dat</span><span class="params">(p)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">spread</span>(p);</span><br><span class="line">lld mid=(<span class="built_in">l</span>(p)+<span class="built_in">r</span>(p))&gt;&gt;<span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span>(l&lt;=mid)ans+=<span class="built_in">ask</span>(<span class="built_in">ls</span>(p),l,r);</span><br><span class="line"><span class="keyword">if</span>(r&gt;mid)ans+=<span class="built_in">ask</span>(<span class="built_in">rs</span>(p),l,r);</span><br><span class="line">ret ans;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">cin&gt;&gt;n&gt;&gt;m;</span><br><span class="line"><span class="built_in">build</span>(<span class="number">1</span>,<span class="number">1</span>,n);</span><br><span class="line"><span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">&#123;</span><br><span class="line">lld op,l,r;cin&gt;&gt;op&gt;&gt;l&gt;&gt;r;</span><br><span class="line"><span class="keyword">if</span>(op==<span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">change</span>(<span class="number">1</span>,l,r);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;</span><br><span class="line">cout&lt;&lt;<span class="built_in">ask</span>(<span class="number">1</span>,l,r)&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">ios::<span class="built_in">sync_with_stdio</span>(<span class="number">0</span>);cin.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line"><span class="built_in">solve</span>();</span><br><span class="line">ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建备忘</title>
      <link href="/2025/01/18/hexo%E4%BD%BF%E7%94%A8/"/>
      <url>/2025/01/18/hexo%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>[toc]</p><h2 id="hexo博客最常用的套路"><a href="#hexo博客最常用的套路" class="headerlink" title="hexo博客最常用的套路"></a>hexo博客最常用的套路</h2><ol><li><code>hexo g; hexo s</code>本地预览，再更改，直到满意为止</li><li><code>hexo d</code>或者<code>hexo g; hexo d</code>远程部署。</li><li>hexo new “title” # 默认是post，如果博客名称有空格需要用双引号包裹起来</li></ol><h4 id="Front-matter"><a href="#Front-matter" class="headerlink" title="Front-matter"></a>Front-matter</h4><p>Front-matter 是 markdown 文件<strong>最上方</strong>以 — 分隔的區域，用於指定個別檔案的變數。</p><p>Page Front-matter 用於 頁面 配置<br>Post Front-matter 用於 文章頁 配置<br>如果標注可選的參數，可根據自己需要添加，不用全部都寫在 markdown 裏</p><h4 id="Page-Front-matter-md渲染好像放在开头才有效果"><a href="#Page-Front-matter-md渲染好像放在开头才有效果" class="headerlink" title="Page Front-matter(md渲染好像放在开头才有效果)"></a>Page Front-matter(md渲染好像放在开头才有效果)</h4><p><img src="/./hexo%E4%BD%BF%E7%94%A8/5b3a26a09ff1908363efb29e78f6c2fc.png" alt="5b3a26a09ff1908363efb29e78f6c2fc"></p><p><img src="/./hexo%E4%BD%BF%E7%94%A8/2f0c8e78306edad277656d22ef1c220f.png" alt="2f0c8e78306edad277656d22ef1c220f"></p><h4 id="Post-Front-matter"><a href="#Post-Front-matter" class="headerlink" title="Post Front-matter"></a>Post Front-matter</h4><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title:</span><br><span class="line">date:</span><br><span class="line">updated:</span><br><span class="line">tags:</span><br><span class="line">categories:</span><br><span class="line">keywords:</span><br><span class="line">description:</span><br><span class="line">top<span class="emphasis">_img:</span></span><br><span class="line"><span class="emphasis">comments:</span></span><br><span class="line"><span class="emphasis">cover:</span></span><br><span class="line"><span class="emphasis">toc:</span></span><br><span class="line"><span class="emphasis">toc_</span>number:</span><br><span class="line">toc<span class="emphasis">_style_</span>simple:</span><br><span class="line">copyright:</span><br><span class="line">copyright<span class="emphasis">_author:</span></span><br><span class="line"><span class="emphasis">copyright_</span>author<span class="emphasis">_href:</span></span><br><span class="line"><span class="emphasis">copyright_</span>url:</span><br><span class="line">copyright<span class="emphasis">_info:</span></span><br><span class="line"><span class="emphasis">mathjax:</span></span><br><span class="line"><span class="emphasis">katex:</span></span><br><span class="line"><span class="emphasis">aplayer:</span></span><br><span class="line"><span class="emphasis">highlight_</span>shrink:</span><br><span class="line">aside:</span><br><span class="line">abcjs:</span><br><span class="line"><span class="section">noticeOutdate:</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><p><img src="/./hexo%E4%BD%BF%E7%94%A8/c702dece8b2fa2b5e5d8eab28dc60763.png" alt="c702dece8b2fa2b5e5d8eab28dc60763"></p><h2 id="标签页"><a href="#标签页" class="headerlink" title="标签页"></a>标签页</h2><p>標籤頁<strong>文件名</strong>不一定是 tags, 例子中的 tags 只是一個示例.記得添加 type: “tags”</p><p>1.前往你的 Hexo 的根目錄</p><p>2.輸入 <strong>hexo new page tags</strong></p><p>3.你會找到 source&#x2F;tags&#x2F;index.md 這個文件</p><p>修改這個文件：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 標籤</span><br><span class="line">date: 2018-01-05 00:00:00</span><br><span class="line">type: &#x27;tags&#x27;</span><br><span class="line">orderby: random</span><br><span class="line"><span class="section">order: 1</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><p><img src="/./hexo%E4%BD%BF%E7%94%A8/e416034ea913cfc279af6527b4174a3d.png" alt="e416034ea913cfc279af6527b4174a3d"></p><h2 id="分类页"><a href="#分类页" class="headerlink" title="分类页"></a>分类页</h2><p><strong>hexo new page categories</strong></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 分類</span><br><span class="line">date: 2018-01-05 00:00:00</span><br><span class="line"><span class="section">type: &#x27;categories&#x27;</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><h2 id="友链"><a href="#友链" class="headerlink" title="友链"></a>友链</h2><p><strong>hexo new page link</strong></p><p>在 Hexo 根目錄中的 source&#x2F;_data（如果沒有 _data 文件夾，請自行創建），創建一個文件 link.yml</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">class_name:</span> <span class="string">友情鏈接</span></span><br><span class="line">  <span class="attr">class_desc:</span> <span class="string">那些人，那些事</span></span><br><span class="line">  <span class="attr">link_list:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Hexo</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://hexo.io/zh-tw/</span></span><br><span class="line">      <span class="attr">avatar:</span> <span class="string">https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg</span></span><br><span class="line">      <span class="attr">descr:</span> <span class="string">快速、簡單且強大的網誌框架</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">class_name:</span> <span class="string">網站</span></span><br><span class="line">  <span class="attr">class_desc:</span> <span class="string">值得推薦的網站</span></span><br><span class="line">  <span class="attr">link_list:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Youtube</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://www.youtube.com/</span></span><br><span class="line">      <span class="attr">avatar:</span> <span class="string">https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png</span></span><br><span class="line">      <span class="attr">descr:</span> <span class="string">視頻網站</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Weibo</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://www.weibo.com/</span></span><br><span class="line">      <span class="attr">avatar:</span> <span class="string">https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png</span></span><br><span class="line">      <span class="attr">descr:</span> <span class="string">中國最大社交分享平台</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Twitter</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://twitter.com/</span></span><br><span class="line">      <span class="attr">avatar:</span> <span class="string">https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png</span></span><br><span class="line">      <span class="attr">descr:</span> <span class="string">社交分享平台</span></span><br></pre></td></tr></table></figure><p>從 4.0.0 開始，支持從遠程加載友情鏈接，遠程拉取只支持 json。</p><p><strong>注意： 選擇遠程加載後，本地生成的方法會無效。</strong></p><p>在 source&#x2F;link&#x2F;index.md 這個文件的 front-matter 添加遠程鏈接</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink_url: xxxxx</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;class_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;友情鏈接&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;class_desc&quot;</span><span class="punctuation">:</span> <span class="string">&quot;那些人，那些事&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;link_list&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Hexo&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;link&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://hexo.io/zh-tw/&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;avatar&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;descr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;快速、簡單且強大的網誌框架&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;class_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;網站&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;class_desc&quot;</span><span class="punctuation">:</span> <span class="string">&quot;值得推薦的網站&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;link_list&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Youtube&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;link&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://www.youtube.com/&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;avatar&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;descr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;視頻網站&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Weibo&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;link&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://www.weibo.com/&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;avatar&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;descr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;中國最大社交分享平台&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Twitter&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;link&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://twitter.com/&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;avatar&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;descr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;社交分享平台&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure><h2 id="子页面"><a href="#子页面" class="headerlink" title="子页面"></a>子页面</h2><p>子頁面也是普通的頁面，你只需要 hexo n page xxxxx 創建你的頁面就行</p><p>然後使用標簽外掛 gallery，具體用法請查看對應的內容。</p><p><strong>如果你想要使用 &#x2F;photo&#x2F;ohmygirl 這樣的鏈接顯示你的圖片內容.你可以把創建好的 ohmygirl 整個文件夾移到 photo 文件夾裏去</strong></p><h2 id="404-頁面"><a href="#404-頁面" class="headerlink" title="404 頁面"></a>404 頁面</h2><p>主題內置了一個簡單的 404 頁面，可在設置中開啟</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A simple 404 page</span></span><br><span class="line"><span class="attr">error_404:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">subtitle:</span> <span class="string">&#x27;頁面沒有找到&#x27;</span></span><br><span class="line">  <span class="attr">background:</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
