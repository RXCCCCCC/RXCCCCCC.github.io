<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>顽强拼搏记录</title>
      <link href="/2025/03/23/%E9%A1%BD%E5%BC%BA%E6%8B%BC%E6%90%8F%E8%AE%B0%E5%BD%95/"/>
      <url>/2025/03/23/%E9%A1%BD%E5%BC%BA%E6%8B%BC%E6%90%8F%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>本篇记录博主自己顽强拼搏的时刻以供回念.<strong>(没有含金量,全是刺激性)</strong></p><p>这也许,就是我为什么又菜又爱玩吧.</p><p>此致.</p><p>2025.3.23</p><h2 id="大工之星第二场"><a href="#大工之星第二场" class="headerlink" title="大工之星第二场"></a>大工之星第二场</h2><p>开头ev录屏忘开了,中间又花了近1h去做了个志愿活动(雾</p><p><img src="/./%E9%A1%BD%E5%BC%BA%E6%8B%BC%E6%90%8F%E8%AE%B0%E5%BD%95/ab0a68dd90d0f528e83e053c6fc546fb.png" alt="ab0a68dd90d0f528e83e053c6fc546fb"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>算法比赛后记</title>
      <link href="/2025/03/18/%E7%AE%97%E6%B3%95%E6%AF%94%E8%B5%9B%E5%90%8E%E8%AE%B0/"/>
      <url>/2025/03/18/%E7%AE%97%E6%B3%95%E6%AF%94%E8%B5%9B%E5%90%8E%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>唐b杂事</title>
      <link href="/2025/03/17/%E5%94%90b%E6%9D%82%E4%BA%8B/"/>
      <url>/2025/03/17/%E5%94%90b%E6%9D%82%E4%BA%8B/</url>
      
        <content type="html"><![CDATA[<p>[toc]</p><h1 id="算法学习方面"><a href="#算法学习方面" class="headerlink" title="算法学习方面"></a>算法学习方面</h1><p>1.vscode调试跟coderunner插件配置文件是分开来的,要想用vscode自带调试设置为c++20,必须在tasks.json文件中的args里面添加一个”-std&#x3D;c++2<strong>a”,</strong>(注意逗号,双引号和a).</p><p>2.玛德用<strong>宏定义</strong>(纯文本替换卧槽忘了<strong>运算顺序</strong>)被干死了,再用宏定义我就是煞笔.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>麦麦--qq聊天机器人部署备忘</title>
      <link href="/2025/03/17/%E9%BA%A6%E9%BA%A6-qq%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E9%83%A8%E7%BD%B2%E5%A4%87%E5%BF%98/"/>
      <url>/2025/03/17/%E9%BA%A6%E9%BA%A6-qq%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E9%83%A8%E7%BD%B2%E5%A4%87%E5%BF%98/</url>
      
        <content type="html"><![CDATA[<h1 id="重启电脑后如何启动"><a href="#重启电脑后如何启动" class="headerlink" title="重启电脑后如何启动"></a>重启电脑后如何启动</h1><h2 id="启动QQ"><a href="#启动QQ" class="headerlink" title="启动QQ"></a>启动QQ</h2><h2 id="打开compass，启动数据库"><a href="#打开compass，启动数据库" class="headerlink" title="打开compass，启动数据库"></a>打开compass，启动数据库</h2><h2 id="打开bot文件夹，启动终端"><a href="#打开bot文件夹，启动终端" class="headerlink" title="打开bot文件夹，启动终端"></a>打开bot文件夹，启动终端</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bot\\Scripts\\activate</span><br><span class="line">nb run</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>嗯看个几把教程,身为新手,无法分辨教程是不是全对,又搜不到,该问的时候就正确的去发问,真几把别不好意思.</strong></p><p>多看看新的教程,评价好的,完整的教程,甚至是视频教程(虽然慢点但是最稳,能看见所有步)</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>算法学习有感</title>
      <link href="/2025/03/13/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%9C%89%E6%84%9F/"/>
      <url>/2025/03/13/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>[toc]</p><p>一定要有能自主配好<strong>成熟</strong>环境的能力,总会遇到<strong>到新环境要调整</strong>的时候</p><ul><li><p>一定要能独立完成(<strong>包括不看资料(除非纯生涩那种)</strong>)编码——可以参考别人的代码，但是<strong>写的时候要独立</strong>。凡是参考过别人代码的题，隔一段时间之后<strong>要重写</strong></p></li><li><p>多问几个为什么，不要在知识点和代码里遗留那些：好像这样就能过了的问题，AC不是一个题做完的标志，<strong>想明白的才是</strong>。</p></li><li><p>写题解——题解应该包含的内容：<strong>英文题的大概意思</strong>、大体思路、<strong>“我是哪里没想到”，产生错误提交的原因(别只会总结对的不会纠正错的,那样遇到错遇到不会又出错误思路怎么办)</strong></p></li></ul><p><img src="/./%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%9C%89%E6%84%9F/image-20250313181037086.png" alt="image-20250313181037086"></p><p><img src="/./%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%9C%89%E6%84%9F/image-20250313181112291.png" alt="image-20250313181112291"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>一个一个计算机网络笔记</title>
      <link href="/2025/03/08/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
      <url>/2025/03/08/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="一个一个计算机网络的笔记"><a href="#一个一个计算机网络的笔记" class="headerlink" title="一个一个计算机网络的笔记"></a>一个一个计算机网络的笔记</h1><p>[TOC]</p><h2 id="TCP-IP-网络模型有哪几层？"><a href="#TCP-IP-网络模型有哪几层？" class="headerlink" title="TCP&#x2F;IP 网络模型有哪几层？"></a>TCP&#x2F;IP 网络模型有哪几层？</h2><p>问大家，为什么要有 TCP&#x2F;IP 网络模型？</p><p>对于同一台设备上的进程间通信，有很多种方式，比如有管道、消息队列、共享内存、信号等方式，而对于不同设备上的进程间通信，就需要网络通信，而设备是多样性的，所以<strong>要兼容多种多样的设备</strong>，就协商出了一套通用的网络协议。</p><p>这个网络协议是<strong>分层</strong>的，每一层都有各自的作用和职责，接下来就根据「 TCP&#x2F;IP 网络模型」分别对每一层进行介绍。</p><h2 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h2><p>最上层的，也是我们能<strong>直接接触到的</strong>就是应用层（Application Layer），我们电脑或手机使用的应用软件都是在应用层实现。那么，当两个不同设备的应用需要通信的时候，应用就把应用数据传给下一层，也就是<strong>传输层</strong>。</p><p>所以，应用层只需要<strong>专注于为用户提供应用功能</strong>，比如 HTTP、FTP、Telnet、DNS、SMTP等。</p><p>应用层是不用去关心数据是如何传输的，就类似于，我们寄快递的时候，只需要把包裹交给快递员，由他负责运输快递，我们不需要关心快递是如何被运输的。</p><p>而且应用层是工作在操作系统中的<strong>用户态</strong>，传输层及以下则工作在<strong>内核态</strong>。</p><h2 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h2><p>应用层的数据包会传给传输层，传输层（Transport Layer）是为应用层提供网络支持的。</p><p>在传输层会有两个传输协议，分别是 TCP 和 UDP。</p><p>TCP 的全称叫<strong>传输控制协议（Transmission Control Protocol）</strong>，大部分应用使用的正是 TCP <strong>传输层</strong>协议，比如 <strong>HTTP 应用层协议</strong>。TCP 相比 UDP 多了很多特性，比如流量控制、超时重传、拥塞控制等，这些都是为了保证数据包能可靠地传输给对方。</p><p>UDP 相对来说就很简单，简单到只负责发送数据包，不保证数据包是否能抵达对方，但它实时性相对更好，传输效率也高。当然，UDP 也可以实现可靠传输，把 TCP 的特性在应用层上实现就可以，不过要实现一个商用的可靠 UDP 传输协议，也不是一件简单的事情。</p><p>应用需要传输的数据可能会非常大，如果直接传输就不好控制，因此当传输层的数据包大小<strong>超过 MSS（TCP 最大报文段长度）</strong> ，就要将<strong>数据包分块</strong>，这样即使中途有一个分块丢失或损坏了，只需要重新发送这一个分块，而不用重新发送整个数据包。在 TCP 协议中，我们把每个分块称为一个 <strong>TCP 段（TCP Segment）</strong>。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP%E6%AE%B5.png" alt="img"></p><p>当设备作为接收方时，传输层则要负责<strong>把数据包传给应用</strong>，但是一台设备上可能会有<strong>很多应用在接收或者传输</strong>数据，因此需要<strong>用一个编号将应用区分</strong>开来，这个编号就是<strong>端口</strong>。</p><p>比如 <strong>80 端口通常是 Web 服务器</strong>用的，<strong>22 端口通常是远程登录服务器</strong>用的。而对于浏览器（客户端）中的<strong>每个标签栏都是一个独立的进程</strong>，操作系统会为这些进程<strong>分配临时的端口号</strong>。</p><p>由于传输层的<strong>报文</strong>中会<strong>携带端口号</strong>，因此接收方可以识别出该报文是发送给哪个应用。</p><h2 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h2><p>传输层可能大家刚接触的时候，会认为它负责将数据从一个设备传输到另一个设备，事实上它<strong>并不负责</strong>。</p><p>实际场景中的网络环节是错综复杂的，中间有各种各样的线路和分叉路口，如果一个设备的数据要传输给另一个设备，就需要在<strong>各种各样的路径和节点进行选择</strong>，而传输层的设计理念是简单、高效、专注，如果传输层还负责这一块功能就有点违背设计原则了。</p><p>也就是说，我们不希望传输层协议处理太多的事情，只需要<strong>服务好应用</strong>即可，让其作为应用间数据传输的媒介，帮助实现<strong>应用到应用的通信</strong>，而<strong>实际的传输功能</strong>就交给<strong>下一</strong>层，也就是<strong>网络层</strong>（Internet Layer）。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E5%B1%82.png" alt="img"></p><p><strong>网络层</strong>最常使用的是 <strong>IP 协议（Internet Protocol）</strong>，IP 协议会<strong>将传输层的报文作为数据部分</strong>，再<strong>加上 IP 包头组装成 IP 报文</strong>，如果 <strong>IP 报文大小超过 MTU（以太网中一般为 1500 字节）<strong>就会再次进行</strong>分片</strong>，得到一个即将发送到网络的 IP 报文。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/12.jpg" alt="img"></p><p>网络层负责将数据从一个设备<strong>传输到另一个</strong>设备，世界上那么多设备，又该如何找到对方呢？因此，网络层需要有<strong>区分设备的编号</strong>。</p><p>我们一般用 IP 地址给设备进行编号，对于 IP<strong>v4</strong> 协议， IP 地址共 <strong>32 位</strong>，<strong>分成了四段</strong>（比如，192.168.100.1），<strong>每段是 8 位</strong>。只有一个单纯的 IP 地址虽然做到了区分设备，但是寻址起来就特别麻烦，全世界那么多台设备，难道一个一个去匹配？这显然不科学。</p><p>因此，需要将 IP 地址分成两种意义：</p><p>一个是<strong>网络号</strong>，负责标识该 IP 地址是属于<strong>哪个「子网」<strong>的；<br>一个是</strong>主机号</strong>，负责标识<strong>同一「子网」下的不同主机</strong>；<br>怎么分的呢？这需要<strong>配合子网掩码</strong>才能算出 IP 地址 的网络号和主机号。</p><p>举个例子，比如 10.100.122.0&#x2F;24，后面的**&#x2F;24表示就是 255.255.255.0 子网掩码**，<strong>255.255.255.0 二进制是「11111111-11111111-11111111-00000000」</strong>，大家数数一共多少个1？不用数了，是 24 个1，为了<strong>简化子网掩码的表示，用&#x2F;24代替255.255.255.0</strong>。</p><p>知道了子网掩码，该怎么计算出网络地址和主机地址呢？</p><p>将 10.100.122.2 和 255.255.255.0 进行<strong>按位与</strong>运算，就可以<strong>得到网络号</strong>，如下图：</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/16.jpg" alt="img"></p><p>将 255.255.255.0 <strong>取反后</strong>与IP地址进行进行<strong>按位与运算</strong>，就可以得到<strong>主机号</strong>。</p><p> 那么在寻址的过程中，先匹配到<strong>相同的网络号</strong>（表示要找到<strong>同一个子网</strong>），才会去找对应的主机。</p><p>除了<strong>寻址能力</strong>， IP 协议还有另一个重要的能力就是<strong>路由</strong>。实际场景中，两台设备并不是用一条网线连接起来的，而是<strong>通过很多网关、路由器、交换机等众多网络设备连接</strong>起来的，那么就会形成很多条网络的路径，因此当数据包到达一个网络节点，就需要<strong>通过路由算法</strong>决定下一步走哪条路径。</p><p>路由器寻址工作中，就是要<strong>找到目标地址的子网</strong>，找到后<strong>进而把数据包转发</strong>给对应的网络内。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/17.jpg" alt="IP地址的网络号"></p><p>所以，IP 协议的寻址作用是告诉我们<strong>去往下一个目的地该朝哪个方向</strong>走，路由则是<strong>根据「下一个目的地」选择路径</strong>。寻址更像在导航，路由更像在操作方向盘。</p><h2 id="网络接口层"><a href="#网络接口层" class="headerlink" title="网络接口层"></a>网络接口层</h2><p>生成了 <strong>IP 头部</strong>之后，接下来要交给<strong>网络接口层（Link Layer）<strong>在 IP 头部的前面</strong>加上 MAC 头部</strong>，并<strong>封装成数据帧（Data frame）发送到网络上</strong>。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3%E5%B1%82.png" alt="img"></p><p>IP 头部中的接收方 IP 地址表示<strong>网络包的目的地</strong>，通过这个地址我们就可以判断要将包发到哪里，但在以太网的世界中，这个思路是行不通的。</p><p>什么是以太网呢？<strong>电脑上的以太网接口，Wi-Fi接口，以太网交换机、路由器上的千兆，万兆以太网口，还有网线，它们都是以太网的组成部分。<strong>以太网就是一种</strong>在「局域网」内</strong>，把附近的设备连接起来，使它们之间可以进行通讯的技术。</p><p>以太网在判断网络包目的地时和 IP 的方式不同，因此<strong>必须采用相匹配的方式</strong>才能在以太网中将包发往目的地，而 MAC 头部就是干这个用的，所以，<strong>在以太网进行通讯要用到 MAC 地址</strong>。</p><p>MAC 头部是以太网使用的头部，它<strong>包含了接收方和发送方的 MAC 地址</strong>等信息，我们可以<strong>通过 ARP 协议获取对方</strong>的 MAC 地址。</p><p>所以说，网络接口层主要<strong>为网络层提供「链路级别」传输的服务</strong>，负责在以太网、WiFi 这样的<strong>底层网络上发送</strong>原始数据包，<strong>工作在网卡这个层次</strong>，使用 MAC 地址来标识网络上的设备。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>综上所述，TCP&#x2F;IP 网络通常是由上到下分成 4 层，分别是应用层，传输层，网络层和网络接口层。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/tcpip%E5%8F%82%E8%80%83%E6%A8%A1%E5%9E%8B.drawio.png" alt="img"></p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E5%B0%81%E8%A3%85.png" alt="img"></p><p>网络接口层的传输单位是<strong>帧（frame）</strong>，IP 层的传输单位是<strong>包（packet）</strong>，TCP 层的传输单位是<strong>段（segment）</strong>，HTTP 的传输单位则是<strong>消息或报文（message）</strong>。但这些名词并没有什么本质的区分，可以<strong>统称为数据包</strong>。</p><h2 id="2-2-键入网址到网页显示，期间发生了什么？"><a href="#2-2-键入网址到网页显示，期间发生了什么？" class="headerlink" title="2.2 键入网址到网页显示，期间发生了什么？"></a>2.2 键入网址到网页显示，期间发生了什么？</h2><p>以下图较简单的<strong>网络拓扑模型</strong>作为例子，探究探究其间发生了什么</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/2.jpg" alt="简单的网络模型"></p><h2 id="孤单小弟-——-HTTP"><a href="#孤单小弟-——-HTTP" class="headerlink" title="孤单小弟 —— HTTP"></a>孤单小弟 —— HTTP</h2><blockquote><p>浏览器做的第一步工作是解析 URL</p></blockquote><p>首先浏览器做的第一步工作就是要<strong>对 URL 进行解析</strong>，从而<strong>生成发送给 Web 服务器的请求信息</strong>。</p><p>让我们看看一条长长的 URL 里的各个元素的代表什么，见下图：</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/3.jpg" alt="URL 解析"></p><p>所以图中的长长的 URL 实际上是<strong>请求服务器</strong>里的文件资源。</p><h4 id="要是上图中的蓝色部分-URL-元素都省略了，那应该是请求哪个文件呢？"><a href="#要是上图中的蓝色部分-URL-元素都省略了，那应该是请求哪个文件呢？" class="headerlink" title="要是上图中的蓝色部分 URL 元素都省略了，那应该是请求哪个文件呢？"></a>要是上图中的蓝色部分 URL 元素都省略了，那应该是请求哪个文件呢？</h4><p>当<strong>没有路径名时</strong>，就代表<strong>访问根目录下</strong>事先设置的默认文件，也就是 &#x2F;index.html 或者 &#x2F;default.html 这些文件，这样就不会发生混乱了。</p><h4 id="生产-HTTP-请求信息"><a href="#生产-HTTP-请求信息" class="headerlink" title="生产 HTTP 请求信息"></a>生产 HTTP 请求信息</h4><p>对 URL 进行解析之后，浏览器<strong>确定了 Web 服务器和文件名</strong>，接下来就是根<strong>据这些信息来生成 HTTP 请求</strong>消息了。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/4.jpg" alt="HTTP 的消息格式"></p><p>一个孤单 HTTP 数据包表示：“我这么一个小小的数据包，没亲没友，直接发到浩瀚的网络，谁会知道我呢？谁能载我一程呢？谁能保护我呢？我的目的地在哪呢？”。充满各种疑问的它，没有停滞不前，依然踏上了征途！</p><h2 id="真实地址查询-——-DNS"><a href="#真实地址查询-——-DNS" class="headerlink" title="真实地址查询 —— DNS"></a>真实地址查询 —— DNS</h2><p>通过浏览器解析 URL 并生成 HTTP 消息后，需要<strong>委托操作系统将消息发送给 Web 服务器</strong>。</p><p>但在发送之前，还有一项工作需要完成，那就是<strong>查询服务器域名对应的 IP 地址</strong>，因为委托操作系统发送消息时，必须提供通信对象的 IP 地址。</p><p>比如我们打电话的时候，必须要知道对方的电话号码，但由于电话号码难以记忆，所以通常我们会将对方电话号 + 姓名保存在通讯录里。</p><p>所以，<strong>有一种服务器就专门保存了 Web 服务器域名与 IP 的对应关系</strong>，它就是 <strong>DNS 服务器</strong>。</p><h4 id="域名的层级关系"><a href="#域名的层级关系" class="headerlink" title="域名的层级关系"></a>域名的层级关系</h4><p>DNS 中的域名都是用句点来分隔的，比如 <a href="http://www.server.com,这里的句点代表了**不同层次之间的界限**./">www.server.com，这里的句点代表了**不同层次之间的界限**。</a></p><p>在域名中，<strong>越靠右的位置表示其层级越高</strong>。</p><p>毕竟域名是外国人发明，所以思维和中国人相反，比如说一个城市地点的时候，<strong>外国喜欢从小到大的方式</strong>顺序说起（如 XX 街道 XX 区 XX 市 XX 省），而中国则喜欢从大到小的顺序（如 XX 省 XX 市 XX 区 XX 街道）。</p><p>实际上域名最后还有一个点，比如 <a href="http://www.server.com.,这个最后的一个点代表**根域名**./">www.server.com.，这个最后的一个点代表**根域名**。</a></p><p>也就是，. 根域是在最顶层，它的下一层就是 .com 顶级域，再下面是 server.com。</p><p>所以域名的层级关系类似一个<strong>树状结构</strong>：</p><p><strong>根 DNS 服务器</strong>（.）<br><strong>顶级域 DNS 服务器</strong>（.com）<br><strong>权威 DNS 服务器</strong>（server.com）</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/5.jpg" alt="DNS 树状结构"></p><p><strong>根域</strong>的 DNS 服务器信息保存在<strong>互联网中所有的 DNS 服务器中</strong>。</p><p>这样一来，任何 DNS 服务器就都可以<strong>找到并访问根域 DNS 服务器</strong>了。</p><p>因此，客户端只要能够找到<strong>任意一台 DNS 服务器</strong>，就可以通过它找到根域 DNS 服务器，然后再一路顺藤摸瓜找到位于下层的某台目标 DNS 服务器。</p><h4 id="域名解析的工作流程"><a href="#域名解析的工作流程" class="headerlink" title="域名解析的工作流程"></a>域名解析的工作流程</h4><p>客户端首先会发出一个 <strong>DNS 请求</strong>，问 <a href="http://www.server.com/">www.server.com</a> 的 <strong>IP 是啥</strong>，并发给<strong>本地 DNS 服务器</strong>（也就是客户端的 <strong>TCP&#x2F;IP 设置中填写的 DNS 服务器地址</strong>）。<br>本地域名服务器收到客户端的请求后，如果<strong>缓存里</strong>的表格能找到 <a href="http://www.server.com,则它**直接返回/">www.server.com，则它**直接返回</a>** IP 地址。如果没有，本地 DNS 会去问它的根域名服务器：“老大， 能告诉我 <a href="http://www.server.com/">www.server.com</a> 的 IP 地址吗？” 根域名服务器是<strong>最高层次</strong>的，它不直接用于域名解析，但能指明一条道路。<br>根 DNS 收到来自本地 DNS 的请求后，发现后置是 .com，说：“<a href="http://www.server.com/">www.server.com</a> 这个域名归 .com 区域管理”，我给你 .com 顶级域名服务器地址给你，你去问问它吧。”<br>本地 DNS 收到顶级域名服务器的地址后，发起请求问“老二， 你能告诉我 <a href="http://www.server.com/">www.server.com</a> 的 IP 地址吗？”<br>顶级域名服务器说：“我给你负责 <a href="http://www.server.com/">www.server.com</a> 区域的权威 DNS 服务器的地址，你去问它应该能问到”。<br>本地 DNS 于是转向问权威 DNS 服务器：“老三，<a href="http://www.server.com对应的IP是啥呀？”">www.server.com对应的IP是啥呀？”</a> server.com 的权威 DNS 服务器，它是<strong>域名解析结果的原出处</strong>。为啥叫权威呢？就是我的域名我做主。<br>权威 DNS 服务器<strong>查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS</strong>。<br>本地 DNS <strong>再将 IP 地址返回客户端</strong>，客户端和目标建立连接。<br>至此，我们完成了 DNS 的解析过程。现在总结一下，整个过程我画成了一个图。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/6.jpg" alt="域名解析的工作流程"></p><p>DNS 域名解析的过程蛮有意思的，整个过程就和我们日常生活中找人问路的过程类似，<strong>只指路不带路</strong>。</p><p>那是不是每次解析域名都要经过那么多的步骤呢？</p><p>当然不是了，还有缓存这个东西的嘛。</p><p>浏览器会先看自身有没有对这个域名的缓存，如果有，就直接返回，如果没有，就去问<strong>操作系统</strong>，操作系统也会去看自己的缓存，如果有，就直接返回，如果没有，再去 <strong>hosts 文件</strong>看，也没有，<strong>才会去问「本地 DNS 服务器」</strong>。</p><p>数据包表示：“DNS 老大哥厉害呀，找到了目的地了！我还是很迷茫呀，我要发出去，接下来我需要谁的帮助呢?”</p><h2 id="指南好帮手-——-协议栈"><a href="#指南好帮手-——-协议栈" class="headerlink" title="指南好帮手 —— 协议栈"></a>指南好帮手 —— 协议栈</h2><p>通过 DNS 获取到 IP 后，就可以把 HTTP 的<strong>传输</strong>工作交给操作系统中的<strong>协议栈</strong>。</p><p>协议栈的内部分为几个部分，分别承担不同的工作。上下关系是有一定的规则的，上面的部分会向下面的部分委托工作，下面的部分收到委托的工作并执行。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/7.jpg" alt="img"></p><p>应用程序（浏览器）通过<strong>调用 Socket 库</strong>，来委托协议栈工作。协议栈的上半部分有两块，分别是<strong>负责收发数据的 TCP 和 UDP 协议</strong>，这两个传输协议会接受应用层的<strong>委托执行收发数据</strong>的操作。</p><p>协议栈的下面一半是用 <strong>IP 协议控制网络包收发操作</strong>，在互联网上传数据时，数据会被切分成一块块的网络包，而将网络包<strong>发送给对方</strong>的操作就是由 <strong>IP 负责</strong>的。</p><p>此外 IP 中还包括 ICMP 协议和 ARP 协议。</p><p>ICMP 用于告知网络包传送过程中产生的<strong>错误以及各种控制信息</strong>。<br>ARP 用于<strong>根据 IP 地址查询相应的以太网 MAC 地址</strong>。</p><p>IP 下面的网卡驱动程序负责控制网卡硬件，而最下面的网卡则负责完成<strong>实际的收发操作</strong>，也就是对网线中的信号执行发送和接收操作。</p><p>数据包看了这份指南表示：“原来我需要那么多大佬的协助啊，那我先去找找 TCP 大佬！”</p><h2 id="可靠传输-——-TCP"><a href="#可靠传输-——-TCP" class="headerlink" title="可靠传输 —— TCP"></a>可靠传输 —— TCP</h2><p><strong>HTTP</strong> 是<strong>基于 TCP 协议传输</strong>的，所以在这我们先了解下 TCP 协议。</p><h4 id="TCP-包头格式"><a href="#TCP-包头格式" class="headerlink" title="TCP 包头格式"></a>TCP 包头格式</h4><p>TCP <strong>报文头部</strong>的格式</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/8.jpg" alt="TCP 包头格式"></p><p>首先，<strong>源端口号</strong>和<strong>目标端口号</strong>是不可少的，如果没有这两个端口号，数据就不知道应该发给哪个应用。</p><p>接下来有<strong>包的序号</strong>，这个是为了解决包乱序的问题。</p><p>还有应该有的是<strong>确认号</strong>，目的是确认发出去对方是否有收到。如果<strong>没有收到就应该重新发送，直到送达</strong>，这个是为了<strong>解决丢包的问题</strong>。</p><p>接下来还有一些<strong>状态位</strong>。例如 <strong>SYN 是发起一个连接</strong>，<strong>ACK 是回复</strong>，<strong>RST 是重新连接</strong>，<strong>FIN 是结束连接</strong>等。TCP 是面向连接的，因而双方要维护连接的状态，这些带状态位的包的发送，会引起双方的状态变更。</p><p>还有一个重要的就是<strong>窗口大小</strong>。TCP 要做流量控制，通信双方<strong>各声明一个窗口（缓存大小）</strong>，标识自己<strong>当前能够的处理能力</strong>，别发送的太快，撑死我，也别发的太慢，饿死我。</p><p>除了做流量控制以外，TCP还会做<strong>拥塞控制</strong>，对于真正的通路堵车不堵车，它无能为力，唯一能做的就是控制自己，也即<strong>控制发送的速度</strong>。不能改变世界，就改变自己嘛。</p><h4 id="TCP-传输数据之前，要先三次握手建立连接"><a href="#TCP-传输数据之前，要先三次握手建立连接" class="headerlink" title="TCP 传输数据之前，要先三次握手建立连接"></a>TCP 传输数据之前，要先三次握手建立连接</h4><p>在 <strong>HTTP 传输数据</strong>之前，首先需要 <strong>TCP 建立连接</strong>，TCP 连接的建立，通常称为<strong>三次握手</strong>。</p><p>这个所谓的「连接」，只是双方计算机里维护一个状态机，在连接建立的过程中，双方的状态变化时序图就像这样。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.drawio.png" alt="TCP 三次握手"></p><p>一开始，客户端和服务端<strong>都处于 CLOSED 状态</strong>。<strong>先是服务端主动监听某个端口</strong>，处于 <strong>LISTEN 状态</strong>。</p><p>然后<strong>客户端主动发起连接 SYN</strong>，之后<strong>处于 SYN-SENT 状态</strong>。</p><p>服务端收到发起的连接，<strong>返回 SYN</strong>，并且 <strong>ACK 客户端的 SYN</strong>，之后处于 <strong>SYN-RCVD 状态</strong>。</p><p>客户端收到<strong>服务端发送的 SYN 和 ACK</strong> 之后，<strong>发送对 SYN 确认的 ACK</strong>，之后<strong>处于 ESTABLISHED 状态</strong>，因为它<strong>一发一收成功</strong>了。</p><p>服务端收到 <strong>ACK 的 ACK</strong> 之后，<strong>处于 ESTABLISHED 状态</strong>，因为它也<strong>一发一收</strong>了。</p><p>所以三次握手目的是<strong>保证双方都有发送和接收</strong>的能力。</p><h4 id="如何查看-TCP-的连接状态？"><a href="#如何查看-TCP-的连接状态？" class="headerlink" title="如何查看 TCP 的连接状态？"></a>如何查看 TCP 的连接状态？</h4><p>TCP 的连接状态查看，在 Linux 可以通过 <strong>netstat -napt 命令</strong>查看。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/10.jpg" alt="TCP 连接状态查看"></p><h4 id="TCP-分割数据"><a href="#TCP-分割数据" class="headerlink" title="TCP 分割数据"></a>TCP 分割数据</h4><p>如果 HTTP 请求消息比较长，超过了 MSS 的长度，这时 TCP 就需要把 HTTP 的数据拆解成一块块的数据发送，而不是一次性发送所有数据。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/11.jpg" alt="MTU 与 MSS"></p><p>MTU：一个<strong>网络包的最大长度</strong>，以太网中一般为 1500 字节。<br>MSS：<strong>除去 IP 和 TCP 头部之后</strong>，一个网络包所能容纳的 TCP 数据的最大长度。<br>数据会被以 MSS 的长度为单位进行拆分，拆分出来的每一块数据都会被放进<strong>单独的网络包中</strong>。也就是在每个被拆分的数据加上 TCP 头信息，然后交给 IP 模块来发送数据。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/12-1741610449767-37.jpg" alt="数据包分割"></p><h4 id="TCP-报文生成"><a href="#TCP-报文生成" class="headerlink" title="TCP 报文生成"></a>TCP 报文生成</h4><p>TCP 协议里面会有两个端口，一个是<strong>浏览器监听的端口</strong>（通常是<strong>随机生成</strong>的），一个是 <strong>Web 服务器监听的端口</strong>（<strong>HTTP</strong> 默认端口号是 <strong>80</strong>， <strong>HTTPS</strong> 默认端口号是 <strong>443</strong>）。</p><p>在双方建立了连接后，<strong>TCP 报文</strong>中的<strong>数据部分</strong>就是存放 <strong>HTTP 头部 + 数据</strong>，组装好 TCP 报文之后，就需交给下面的网络层处理。</p><p>至此，网络包的报文如下图。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/13.jpg" alt="TCP 层报文"></p><p>此时，遇上了 TCP 的 数据包激动表示：“太好了，碰到了可靠传输的 TCP 传输，它给我加上 TCP 头部，我不再孤单了，安全感十足啊！有大佬可以保护我的可靠送达！但我应该往哪走呢？”</p><h2 id="远程定位-——-IP"><a href="#远程定位-——-IP" class="headerlink" title="远程定位 —— IP"></a>远程定位 —— IP</h2><p>TCP 模块在执行<strong>连接、收发、断开</strong>等各阶段操作时，<strong>都需要委托 IP 模块</strong>将数据封装成网络包发送给通信对象。</p><h4 id="IP-包头格式"><a href="#IP-包头格式" class="headerlink" title="IP 包头格式"></a>IP 包头格式</h4><p>IP 报文头部的格式：</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/14.jpg" alt="IP 包头格式"></p><p>在 IP 协议里面需要有<strong>源地址 IP</strong> 和 <strong>目标地址 IP</strong>：</p><p>源地址IP，即是<strong>客户端输出</strong>的 IP 地址；<br>目标地址，即通过 DNS 域名解析得到的 <strong>Web 服务器 IP</strong>。<br>因为 HTTP 是经过 TCP 传输的，所以在 <strong>IP 包头的协议号</strong>，要填<strong>写为 06（十六进制）</strong>，<strong>表示协议为 TCP</strong>。</p><p>假设客户端有<strong>多个网卡</strong>，就会有<strong>多个 IP 地址</strong>，那 IP 头部的<strong>源地址</strong>应该选择哪个 IP 呢？</p><p>当存在多个网卡时，在填写源地址 IP 时，就需要判断到底应该填写哪个地址。这个判断相当于在多块网卡中判断<strong>应该使用哪个一块网卡</strong>来发送包。</p><p>这个时候就需要<strong>根据路由表规则</strong>，来判断哪一个网卡作为源地址 IP。</p><p>在 Linux 操作系统，我们可以使用 route -n 命令查看当前系统的路由表。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/15.jpg" alt="路由表"></p><p>举个例子，根据上面的路由表，我们假设 Web 服务器的目标地址是 192.168.10.200。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/16-1741610779790-46.jpg" alt="路由规则判断"></p><p>1.首先先和<strong>第一条目的子网掩码（Genmask）<strong>进行 <strong>与运算</strong>，得到结果为 192.168.10.0，但是第一个条目的 Destination 是 192.168.3.0，两者不一致所以匹配失败。<br>2.再与第二条目的子网掩码进行 与运算，得到的结果为 192.168.10.0，与第二条目的 Destination 192.168.10.0 匹配成功，所以将使用 eth1 网卡的 IP 地址作为 <strong>IP 包头的源地址</strong>。<br>那么假设 Web 服务器的目标地址是 10.100.20.100，那么依然</strong>依照上面的路由表规则判断</strong>，判断后的结果是和第三条目匹配。</p><p>第三条目比较特殊，它<strong>目标地址和子网掩码都是 0.0.0.0</strong>，这表示<strong>默认网关</strong>，如果其他所有条目都无法匹配，就会<strong>自动匹配这一行</strong>。并且后续就把包发给路由器，<strong>Gateway 即是路由器的 IP 地址</strong>。</p><p>至此，网络包的报文如下图。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/17-1741680701446-49.jpg" alt="IP 层报文"></p><p>此时，加上了 IP 头部的数据包表示 ：“有 IP 大佬给我指路了，感谢 IP 层给我加上了 IP 包头，让我有了<strong>远程定位的能力</strong>！不会害怕在浩瀚的互联网迷茫了！可是目的地好远啊，我下一站应该去哪呢？”</p><h2 id="两点传输-——-MAC"><a href="#两点传输-——-MAC" class="headerlink" title="两点传输 —— MAC"></a>两点传输 —— MAC</h2><p>生成了 IP 头部之后，接下来网络包还需要<strong>在 IP 头部的前面加上 MAC 头部</strong></p><h4 id="MAC-包头格式"><a href="#MAC-包头格式" class="headerlink" title="MAC 包头格式"></a>MAC 包头格式</h4><p>MAC 头部<strong>是以太网使用的头部</strong>，它包含了<strong>接收方和发送方的 MAC 地址</strong>等信息。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/18.jpg" alt="MAC 包头格式"></p><p>在 MAC 包头里需要发送方 MAC 地址和接收方目标 MAC 地址，用于两点之间的传输。</p><p>一般在 TCP&#x2F;IP 通信里，<strong>MAC 包头的协议类型</strong>只使用：</p><p><strong>0800 ： IP 协议</strong><br><strong>0806 ： ARP 协议</strong></p><h4 id="MAC-发送方和接收方如何确认"><a href="#MAC-发送方和接收方如何确认" class="headerlink" title="MAC 发送方和接收方如何确认?"></a>MAC 发送方和接收方如何确认?</h4><p><strong>发送方</strong>的 MAC 地址获取就比较简单了，MAC 地址是<strong>在网卡生产时写入到 ROM 里</strong>的，只要将这个值读取出来写入到 MAC 头部就可以了。</p><p><strong>接收方</strong>的 MAC 地址就有点复杂了，只要告诉以太网对方的 MAC 的地址，以太网就会帮我们把包发送过去，那么很显然这里应该填写对方的 MAC 地址。</p><p>所以先得搞清楚应该把包发给谁，这个只要查一下路由表就知道了。在路由表中找到相匹配的条目，然后把包发给 Gateway 列中的 IP 地址就可以了。</p><h4 id="既然知道要发给谁，该如何获取对方的-MAC-地址呢？"><a href="#既然知道要发给谁，该如何获取对方的-MAC-地址呢？" class="headerlink" title="既然知道要发给谁，该如何获取对方的 MAC 地址呢？"></a>既然知道要发给谁，该如何获取对方的 MAC 地址呢？</h4><p>不知道对方 MAC 地址？不知道就喊呗。</p><p>此时就需要 ARP 协议帮我们找到路由器的 MAC 地址。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/19.jpg" alt="ARP 广播"></p><p>ARP 协议会在以太网中以广播的形式，对以太网所有的设备喊出：“<strong>这个 IP 地址是谁的</strong>？请把你的 <strong>MAC 地址告诉我”</strong>。</p><p>然后就会有人回答：“这个 IP 地址是我的，我的 MAC 地址是 XXXX”。</p><p>如果对方和自己<strong>处于同一个子网中</strong>，那么通过上面的操作就可以得到对方的 MAC 地址。然后，我们将这个 MAC 地址写入 MAC 头部，MAC 头部就完成了。</p><h4 id="好像每次都要广播获取，这不是很麻烦吗？"><a href="#好像每次都要广播获取，这不是很麻烦吗？" class="headerlink" title="好像每次都要广播获取，这不是很麻烦吗？"></a>好像每次都要广播获取，这不是很麻烦吗？</h4><p>放心，在后续操作系统会把本次查询结果放到一块叫做 <strong>ARP 缓存的内存空间</strong>留着以后用，不过<strong>缓存的时间就几分钟</strong>。</p><p>也就是说，在发包时：</p><p><strong>先查询 ARP 缓存</strong>，如果其中已经保存了对方的 MAC 地址，就不需要发送 ARP 查询，直接使用 ARP 缓存中的地址。<br>而当 ARP 缓存中不存在对方 MAC 地址时，则<strong>发送 ARP 广播</strong>查询。</p><h4 id="查看-ARP-缓存内容"><a href="#查看-ARP-缓存内容" class="headerlink" title="查看 ARP 缓存内容"></a>查看 ARP 缓存内容</h4><p>在 Linux 系统中，我们可以使用 arp -a 命令来查看 ARP 缓存的内容。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/20.jpg" alt="ARP 缓存内容"></p><h4 id="MAC-报文生成"><a href="#MAC-报文生成" class="headerlink" title="MAC 报文生成"></a>MAC 报文生成</h4><p>至此，网络包的报文如下图。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/21.jpg" alt="MAC 层报文"></p><p>此时，加上了 MAC 头部的数据包万分感谢，说道 ：“感谢 MAC 大佬，我知道我下一步要去哪了！我现在有很多头部兄弟，相信我可以到达最终的目的地！”。 带着众多头部兄弟的数据包，终于准备要出门了。</p><h2 id="出口-——-网卡"><a href="#出口-——-网卡" class="headerlink" title="出口 —— 网卡"></a>出口 —— 网卡</h2><p>网络包只是<strong>存放在内存中的一串二进制数字</strong>信息，没有办法直接发送给对方。因此，我们需要将<strong>数字信息转换为电信号</strong>，才能<strong>在网线上传输</strong>，也就是说，这才是真正的数据发送过程。</p><p><strong>负责执行这一操作的是网卡</strong>，要控制网卡还需要靠网卡驱动程序。</p><p>网卡驱动获取网络包之后，会将其<strong>复制到网卡内的缓存区中</strong>，接着会在其<strong>开头加上报头和起始帧分界符</strong>，在<strong>末尾加上用于检测错误的帧校验序列</strong>。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E6%95%B0%E6%8D%AE%E5%8C%85.drawio.png" alt="数据包"></p><ul><li>起始帧分界符是一个用来表示包起始位置的标记</li><li>末尾的 FCS（帧校验序列）用来检查包传输过程是否有损坏</li></ul><p>最后<strong>网卡会将包转为电信号</strong>，通过网线发送出去。</p><p>唉，真是不容易，发一个包，真是历经千辛万苦。致此，一个带有许多头部的数据终于踏上寻找目的地的征途了！</p><h2 id="送别者-——-交换机"><a href="#送别者-——-交换机" class="headerlink" title="送别者 —— 交换机"></a>送别者 —— 交换机</h2><p>下面来看一下包是如何通过交换机的。交换机的设计是<strong>将网络包原样转发到目的地</strong>。<strong>交换机工作在 MAC 层</strong>，也称为二层网络设备。</p><h4 id="交换机的包接收操作"><a href="#交换机的包接收操作" class="headerlink" title="交换机的包接收操作"></a>交换机的包接收操作</h4><p>首先，电信号到达网线接口，交换机里的模块进行接收，接下来<strong>交换机里的模块将电信号转换为数字信号</strong>。</p><p>然后<strong>通过包末尾的 FCS 校验错误</strong>，如果没问题则放到缓冲区。这部分操作基本和计算机的网卡相同，但交换机的工作方式和网卡不同。</p><p>计算机的网卡本身具有 MAC 地址，并通过核对收到的包的接收方 MAC 地址判断<strong>是不是发给自己的</strong>，如果不是发给自己的则丢弃；相对地，<strong>交换机的端口不核对接收方 MAC 地址</strong>，而是<strong>直接接收所有的包并存放到缓冲区中</strong>。因此，和网卡不同，交换机的<strong>端口</strong>不具有 MAC 地址。</p><p>将包存入缓冲区后，接下来需要查询一下<strong>这个包的接收方 MAC 地址是否已经在 MAC 地址表中有记录</strong>了。</p><p><strong>交换机的 MAC 地址</strong>表主要包含两个信息：</p><ul><li>一个是<strong>设备的 MAC 地址</strong>，</li><li>另一个是该设备<strong>连接在交换机的哪个端口</strong>上。</li></ul><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/23.jpg" alt="交换机的 MAC 地址表"></p><p>举个例子，如果<strong>收到的包的接收方 MAC 地址</strong>为 00-02-B3-1C-9C-F9，则<strong>与图中表中的第 3 行匹配，根据端口列的信息</strong>，可知这个地址位于 3 号端口上，然后就可以<strong>通过交换电路将包发送到相应的端口</strong>了。</p><p>所以，<strong>交换机根据 MAC 地址表查找 MAC 地址，然后将信号发送到相应的端口</strong>。</p><h4 id="当-MAC-地址表找不到指定的-MAC-地址会怎么样？"><a href="#当-MAC-地址表找不到指定的-MAC-地址会怎么样？" class="headerlink" title="当 MAC 地址表找不到指定的 MAC 地址会怎么样？"></a>当 MAC 地址表找不到指定的 MAC 地址会怎么样？</h4><p>地址表中找不到指定的 MAC 地址。这可能是因为具有该地址的设备还没有向交换机发送过包，或者这个设备一段时间没有工作导致地址被从地址表中删除了。</p><p>这种情况下，交换机无法判断应该把包转发到哪个端口，只能将包转<strong>发到除了源端口之外的所有端口上</strong>，<strong>无论该设备连接在哪个端口上都能收到这个包</strong>。</p><p>这样做不会产生什么问题，因为以太网的设计本来就是将包发送到整个网络的，然后<strong>只有相应的接收者才接收包，而其他设备则会忽略这个包</strong>。</p><p>有人会说：“这样做会发送多余的包，会不会造成网络拥塞呢？”</p><p>其实完全不用过于担心，因为发送了包之后目标设备会作出响应，只要返回了响应包，交换机就可以将它的地址写入 MAC 地址表，下次也就不需要把包发到所有端口了。</p><p>局域网中<strong>每秒可以传输上千个包，多出一两个包并无大碍</strong>。</p><p>此外，如果接收方 MAC 地址是一个<strong>广播地址</strong>，那么交换机会将包发送到除源端口之外的所有端口。</p><p><strong>以下两个属于广播地址：</strong></p><p><strong>MAC 地址中的 FF:FF:FF:FF:FF:FF</strong><br><strong>IP 地址中的 255.255.255.255</strong><br>数据包通过交换机转发抵达了路由器，准备要<strong>离开土生土长的子网</strong>了。此时，数据包和交换机离别时说道：“感谢交换机兄弟，帮我转发到出境的大门，我要出远门啦！”</p><h4 id="出境大门-——-路由器"><a href="#出境大门-——-路由器" class="headerlink" title="出境大门 —— 路由器"></a>出境大门 —— 路由器</h4><p>路由器与交换机的区别</p><p>网络包经过交换机之后，现在<strong>到达了路由器，并在此被转发到下一个路由器或目标设备</strong>。</p><p>这一步转发的工作原理和交换机类似，也是通过<strong>查表判断包转发的目标</strong>。</p><p>不过在具体的操作过程上，路由器和交换机是有区别的。</p><p>因为<strong>路由器是基于 IP 设计</strong>的，俗称三层网络设备，<strong>路由器的各个端口都具有 MAC 地址和 IP 地址</strong>；<br>而<strong>交换机是基于以太网设计</strong>的，俗称二层网络设备，交换机的<strong>端口不具有 MAC 地址</strong>。</p><h4 id="路由器基本原理"><a href="#路由器基本原理" class="headerlink" title="路由器基本原理"></a>路由器基本原理</h4><p>路由器的<strong>端口具有 MAC 地址</strong>，因此它<strong>就能够成为以太网的发送方和接收方</strong>；同时<strong>还具有 IP 地址</strong>，从这个意义上来说，它和计算机的网卡是一样的。</p><p>当转发包时，首先路由器端口会接收发给自己的以太网包，然后路由表查询转发目标，再由相应的端口作为发送方将<strong>以太网包</strong>发送出去。</p><h4 id="路由器的包接收操作"><a href="#路由器的包接收操作" class="headerlink" title="路由器的包接收操作"></a>路由器的包接收操作</h4><p>首先，<strong>电信号到达网线接口</strong>部分，路由器中的模块会<strong>将电信号转成数字信号</strong>，然后通过<strong>包末尾的 FCS 进行错误校验</strong>。</p><p>如果<strong>没问题则检查 MAC 头部中的接收方 MAC 地址</strong>，看看<strong>是不是发给自己的包</strong>，如果是就放到接收缓冲区中，否则就丢弃这个包。</p><p>总的来说，路由器的端口都具有 MAC 地址，只接收与自身地址匹配的包，遇到不匹配的包则直接丢弃。</p><h4 id="查询路由表确定输出端口"><a href="#查询路由表确定输出端口" class="headerlink" title="查询路由表确定输出端口"></a>查询路由表确定输出端口</h4><p>完成包接收操作之后，路由器就会<strong>去掉包开头的 MAC 头部</strong>。</p><p><strong>MAC 头部的作用就是将包送达路由器</strong>，其中的<strong>接收方 MAC 地址就是路由器端口的 MAC 地址</strong>。因此，当包到达路由器之后，MAC 头部的任务就完成了，于是 MAC 头部就会被丢弃。</p><p>接下来，路由器会根据 MAC 头部后方的 <strong>IP 头部中的内容进行包的转发操作</strong>。</p><p>转发操作分为几个阶段，<strong>首先是查询路由表</strong>判断转发目标。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/24.jpg" alt="路由器转发"></p><p>具体的工作流程根据上图，举个例子。</p><p>假设地址为 10.10.1.101 的计算机要向地址为 192.168.1.100 的服务器发送一个包，这个包先到达图中的路由器。</p><p>判断转发目标的第一步，就是<strong>根据包的接收方 IP 地址查询路由表中的目标地址栏</strong>，以找到相匹配的记录。</p><p>路由匹配和前面讲的一样，<strong>每个条目的子网掩码和 192.168.1.100 IP 做 &amp; 与运算后</strong>，得到的结果与<strong>对应条目的目标地址</strong>进行<strong>匹配</strong>，如果匹配就会<strong>作为候选转发目标</strong>，如果不匹配就继续与下个条目进行路由匹配。</p><p>如第二条目的子网掩码 255.255.255.0 与 192.168.1.100 IP 做 &amp; 与运算后，得到结果是 192.168.1.0 ，这与第二条目的目标地址 192.168.1.0 匹配，该第二条目记录就会被作为转发目标。</p><p>实在找不到匹配路由时，就会<strong>选择默认路由</strong>，路由表中<strong>子网掩码为 0.0.0.0 的记录表示「默认路由」</strong>。</p><h5 id="路由器的发送操作"><a href="#路由器的发送操作" class="headerlink" title="路由器的发送操作"></a>路由器的发送操作</h5><p>接下来就会进入包的发送操作。</p><p>首先，我们需要<strong>根据路由表的网关列判断对方的地址</strong>。</p><p>如果网关是一个 IP 地址，则<strong>这个IP 地址就是我们要转发到的目标地址</strong>，还未抵达终点，还需继续需要路由器转发。<br>如果网关为空，则 <strong>IP 头部中的接收方 IP 地址</strong>就是要转发到的目标地址，也是就终于找到 IP 包头里的目标地址了，说明<strong>已抵达终点</strong>。<br>知道对方的 IP 地址之后，接下来需要<strong>通过 ARP 协议根据 IP 地址查询 MAC 地址</strong>，并将<strong>查询的结果作为接收方 MAC 地址</strong>。</p><p><strong>路由器也有 ARP 缓存</strong>，因此首先会在 ARP 缓存中查询，如果找不到则发送 ARP 查询请求。</p><p>接下来是<strong>发送方 MAC 地址字段</strong>，这里<strong>填写输出端口的 MAC 地址</strong>。还有一个<strong>以太类型字段，填写 0800 （十六进制）表示 IP 协议</strong>。</p><p>网络包完成后，接下来会将其转换成电信号并通过端口发送出去。这一步的工作过程和计算机也是相同的。</p><p>发送出去的网络包会<strong>通过交换机到达下一个路由器</strong>。由于<strong>接收方 MAC 地址</strong>就是下一个路由器的地址，所以<strong>交换机会根据这一地址将包传输到下一个路由器</strong>。</p><p>接下来，下一个路由器会将包转发给再下一个路由器，经过层层转发之后，网络包就到达了最终的目的地。</p><p>不知你发现了没有，在网络包传输的过程中**，源 IP 和目标 IP 始终是不会变<strong>的，<strong>一直变化的是 MAC 地址</strong>，因为</strong>需要 MAC 地址在以太网内进行两个设备之间的包传输**。</p><p>数据包通过多个路由器道友的帮助，在网络世界途经了很多路程，最终抵达了目的地的城门！城门值守的路由器，发现了这个小兄弟数据包原来是找城内的人，于是它就将数据包送进了城内，再经由城内的交换机帮助下，最终转发到了目的地了。数据包感慨万千的说道：“多谢这一路上，各路大侠的相助！”</p><h2 id="互相扒皮-——-服务器-与-客户端"><a href="#互相扒皮-——-服务器-与-客户端" class="headerlink" title="互相扒皮 —— 服务器 与 客户端"></a>互相扒皮 —— 服务器 与 客户端</h2><p>数据包抵达了服务器，服务器肯定高兴呀，正所谓有朋自远方来，不亦乐乎？</p><p>服务器高兴的不得了，于是开始扒数据包的皮！就好像你收到快递，能不兴奋吗？</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/25.jpg" alt="网络分层模型"></p><p>数据包抵达服务器后，服务器会先扒开数据包的 MAC 头部，查看<strong>是否和服务器自己的 MAC 地址符合</strong>，符合就将包收起来。</p><p>接着继续扒开数据包的 IP 头，发现 IP 地址符合，根据 IP 头中协议项，知道自己上层是 TCP 协议。</p><p>于是**，扒开 TCP 的头，里面有序列号**，需要看一看这个<strong>序列包是不是我想要的，如果是就放入缓存中然后返回一个 ACK，如果不是就丢弃</strong>。<strong>TCP头部里面还有端口号， HTTP 的服务器正在监听这个端口号</strong>。</p><p>于是，<strong>服务器</strong>自然就<strong>知道是 HTTP 进程想要这个包，于是就将包发给 HTTP 进程</strong>。</p><p>服务器的 HTTP 进程看到，原来这个请求是要访问一个页面，于是就把这个网页封装在 HTTP 响应报文里。</p><p>HTTP 响应报文也需要穿上 TCP、IP、MAC 头部，不过这次是<strong>源地址是服务器 IP 地址，目的地址是客户端 IP 地址</strong>。</p><p>穿好头部衣服后，从网卡出去，交由交换机转发到出城的路由器，路由器就把<strong>响应数据包</strong>发到了下一个路由器，就这样跳啊跳。</p><p>最后跳到了客户端的城门把守的路由器，路由器扒开 IP 头部发现是要找城内的人，于是又把包发给了城内的交换机，再由交换机转发到客户端。</p><p>客户端收到了服务器的响应数据包后，同样也非常的高兴，客户能拆快递了！</p><p>于是，客户端开始扒皮，把收到的数据包的<strong>皮扒剩 HTTP 响应报文后，交给浏览器去渲染页面</strong>，一份特别的数据包快递，就这样显示出来了！</p><p>最后，<strong>客户端要离开了，向服务器发起了 TCP 四次挥手</strong>，至此双方的连接就断开了。</p><h2 id="读者问答"><a href="#读者问答" class="headerlink" title="读者问答"></a>读者问答</h2><p>读者问：“笔记本的是自带交换机的吗？交换机现在我还不知道是什么”</p><p>笔记本不是交换机，<strong>交换机通常是2个网口以上</strong>。</p><p>现在家里的路由器其实有了交换机的功能了。交换机可以简单理解成<strong>一个设备</strong>，三台电脑网线接到这个设备，这三台电脑就可以互相通信了，交换机嘛，交换数据这么理解就可以。</p><p>读者问：“如果知道你电脑的mac地址，我可以直接给你发消息吗？”</p><p>Mac地址<strong>只能是两个设备之间传递时使用</strong>的，如果你要从大老远给我发消息，是<strong>离不开 IP</strong> 的。</p><p>读者问：“请问公网服务器的 Mac 地址是在什么时机通过什么方式获取到的？我看 arp 获取Mac地址只能获取到内网机器的 Mac 地址吧？”</p><p>在发送数据包时，如果<strong>目标主机不是本地局域网，填入的MAC地址是路由器</strong>，也就是把数据包转发给路由器，路由器一直转发下一个路由器，<strong>直到转发到目标主机的路由器</strong>，发现 IP 地址是<strong>自己局域网内的主机</strong>，就会 arp 请求<strong>获取目标主机的 MAC 地址，从而转发到这个服务器主机</strong>。</p><p>转发的过程中，<strong>源IP地址和目标IP地址是不会变的（前提：没有使用 NAT 网络的）</strong>，<strong>源 MAC 地址和目标 MAC 地址是会变化的</strong>。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>wordle,启动!</title>
      <link href="/2025/02/27/wordle/"/>
      <url>/2025/02/27/wordle/</url>
      
        <content type="html"><![CDATA[<p><strong>我上传了一个wordle小游戏,在右上角的游戏栏目下.</strong></p><p><strong>也可点此游玩</strong></p><html lang="en"><head>    <meta charset="UTF-8">    <meta name="viewport" content="width=device-width, initial-scale=1.0">    <style>        /* 定义全局 CSS 变量 --green，设置为绿色 */        :root {            --green: #2ecc71;        }        /* 设置 #wordle-link 元素的样式 */        #wordle-link {            font-size: larger;            display: block;            margin: 30px auto -20px;            width: fit-content;            padding: 18px 26px;            border: none;            border-radius: 40px;            background-color: var(--green);            color: white;            font-weight: bold;            cursor: pointer;            transition: background-color 0.2s;            position: relative;            top: -20px;            text-align: center;            text-decoration: none;        }        /* 当鼠标悬停在 #wordle-link 元素上时，更改背景颜色 */        #wordle-link:hover {            background-color: #558850;        }    </style></head><body>    <!-- 创建一个指向 Wordle 游戏的链接 -->    <a href="https://rxcccccc.github.io/game/word-games/" id="wordle-link">wordle,启动!</a></body></html>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>进行一个一个比赛的补</title>
      <link href="/2025/02/16/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/"/>
      <url>/2025/02/16/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/</url>
      
        <content type="html"><![CDATA[<p>[toc]</p><p>本页面记录了本人在<strong>赛时</strong>和赛后的思路和相关问题以及对应解法,总结和反思</p><h1 id="cf"><a href="#cf" class="headerlink" title="cf"></a>cf</h1><h2 id="2025cccc选拔赛"><a href="#2025cccc选拔赛" class="headerlink" title="2025cccc选拔赛"></a>2025cccc选拔赛</h2><p>后记:一定要用成熟的版本,不要临时求变,一定要<strong>巩固好最稳定的做法</strong>.前几题考不到什么大的知识点的,主要都是侧重思维,因此一定要<strong>注意</strong>自己解法<strong>在思维上的投入(太少太暴力模拟肯定不对)</strong>,肯定<strong>不能过于直观</strong>,一般一定是错的,要不在此基础上优化,要不就全部推翻用<strong>新思路</strong>.<strong>确定了绝对不可能</strong>的思路那就一定要<strong>及时完全跳出</strong>来,不要再被拖累了,每次都是先想到了巨麻烦的做法但是又总不即时抛弃掉,<strong>脑子跳不出来想</strong>.</p><p><strong>B 放烟花</strong></p><p>Bob喜欢放烟花，他购买了两个烟花发射装置和大量的发射炮弹。</p><p>两个装置同时开启。第一个装置每隔 $a$ 分钟（即开启后 $a, 2 \cdot a, 3 \cdot a, \dots$ 分钟）发射一次烟花。第二个装置每隔 $b$ 分钟（即开启后 $b, 2 \cdot b, 3 \cdot b, \dots$ 分钟）发射一次烟花。</p><p>每个烟花在发射后的 $m + 1$ 分钟内都可以在天空中看到，也就是说，如果一个烟花是在装置开启后的 $x$ 分钟后发射的，那么从 $x$ 到 $x + m$ （包括首尾两分钟）的每一分钟都可以看到该烟花。如果一个烟花在另一个烟花 $m$ 分钟后发射，则两个烟花都将在一分钟内可见。</p><p>天空中最多可以同时看到多少枚烟花？</p><p>第一行包含一个整数 $t$ $(1 \le t \le 10^4)$ 代表测试用例数。</p><p>每个测试用例的第一行也是唯一一行包含整数 $a$, $b$, $m$ $(1 \le a, b, m \le 10^{18})$ 代表第一个装置、第二个装置的发射频率和烟花在天空中可见的时间。</p><p>赛时思路:画图直观枚举叠加,发现数据量过大暴力不行,又想到差分(?),根据样例分析直接盲猜*2做差的结论(?),后面才发现只需求出各个区间内最多后再相加即可</p><p>正确思路:由于<strong>时间无限</strong>,直接求出<strong>各个区间内最多</strong>后再相加即可,总有重叠的时候.</p><p>反思:不要直观画图了真就完全只按直观的来,肯定要在图上思考优化的,甚至是推翻图重做,思路一定要打开,别把前面的题就想得那么难直接<strong>硬套太多大知识点</strong>.可以根据样例分析而思考相应做法,但样例分析肯定不会完全透出正解甚至可能完全跟正解做法不相关(<strong>纯暴力模拟</strong>),绝对<strong>不能完全依靠据此猜得的各种结论和做法</strong>.</p><p><strong>C 自然语言处理</strong></p><p>Alice 觉得无聊，于是决定用五个字母 $\texttt{a}$ , $\texttt{b}$ , $\texttt{c}$ , $\texttt{d}$ , $\texttt{e}$ 创造一种简单的语言。字母分为两类：</p><ul><li>元音 — 字母 $\texttt{a}$ 和 $\texttt{e}$ 。用 $\textsf{V}$ 表示。</li><li>辅音 — 字母 $\texttt{b}$ , $\texttt{c}$ 和 $\texttt{d}$ 。用 $\textsf{C}$ 表示。</li></ul><p>这种语言中有两种类型的音节： $\textsf{CV}$ （辅音后接元音）或 $\textsf{CVC}$ （元音前后均有辅音）。例如， $\texttt{ba}$ , $\texttt{ced}$ , $\texttt{bab}$ 是音节，但 $\texttt{aa}$ , $\texttt{eda}$ , $\texttt{baba}$ 不是。</p><p>语言的单词由音节序列构成。Alice 写了一个单词，但她不知道如何将其分割为音节。请帮助她分割单词。</p><p>例如，给定单词 $\texttt{bacedbab}$ ，应分割为 $\texttt{ba.ced.bab}$ （点 $\texttt{.}$ 表示音节边界）。</p><p><strong>Input</strong></p><p>输入包含多个测试用例。</p><p>第一行包含一个整数 $t$ $(1 \leq t \leq 100)$ 代表测试用例数量。接下来描述每个测试用例。</p><p>每个测试用例的第一行包含一个整数 $n$ $(1 \leq n \leq 2 \cdot 10^5)$ 代表单词长度。</p><p>每个测试用例的第二行包含一个由 $n$ 个小写拉丁字母组成的字符串，代表要分割的单词。</p><p>所有给定的单词均为合法单词，即仅使用字母 $\texttt{a}$, $\texttt{b}$, $\texttt{c}$, $\texttt{d}$, $\texttt{e}$，且每个单词由若干音节构成。</p><p>保证所有测试用例的 $n$ 之和不超过 $2 \cdot 10^5$。</p><p><strong>Output</strong></p><p>对每个测试用例，输出一个字符串，通过在相邻音节间插入点 .. 来表示分割后的结果。</p><p>若存在多种可能的分割方式，输出任意一种即可。输入保证至少存在一种有效分割。</p><p>赛时思路:一个个找出来再分割,不同的分割情况相应深搜再剪枝叶(?),后面发现数据量太大不可能,随后观察题意,才发现有元音处一定有分割,于是想先找出所有元音位置后再进行分割然后再深搜剪枝(?),发现肯定也太麻烦,后面对比两种形式后才发现元音前一定是辅音,只要在辅音前加.则一定会满足题意(题目保证了一定有一种解法,这样一定最优,无论哪种分割都不会漏点)</p><p>正解思路:对比两种形式后才发现元音前一定是辅音,**只要在辅音前加.**则一定会满足题意(<strong>题目保证</strong>了一定有一种解法,这样一定最优,无论哪种分割都不会漏点)</p><p>反思:要学会观察题意对比,找出<strong>共通点</strong>从而形成<strong>简单而符合题意的思路</strong>.</p><p>代码问题:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">repp</span>(i,<span class="number">0</span>,s.<span class="built_in">length</span>()<span class="number">-1</span>)<span class="comment">//循环中用了s.length,由于字符串被修改且长度变化,这里循环次数也是会变化的</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(s[i]==<span class="string">&#x27;a&#x27;</span>||s[i]==<span class="string">&#x27;e&#x27;</span>)</span><br><span class="line">            &#123;</span><br><span class="line">            s.<span class="built_in">insert</span>(i<span class="number">-1</span>,<span class="string">&quot;.&quot;</span>);<span class="comment">//就在该位置插入,然后把后面字符串往后推</span></span><br><span class="line">            i++;<span class="comment">//关键一步,如果插入了,则肯定要再向后推(s长度变化了),不然这里指向的位置就不再是原来的a/e</span></span><br><span class="line">        &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        s.<span class="built_in">erase</span>(s.<span class="built_in">begin</span>());<span class="comment">//前面肯定会有一个多的点,要删除掉</span></span><br></pre></td></tr></table></figure><p>D 数学平均数</p><p>Bob 有一个由 $n$ 个整数组成的数组 $a$。让我们用 $k$ 表示这些元素的数学平均数（注意 $k$ 有可能不是整数）。</p><p>由 $n$ 个元素组成的数组的数学平均数是所有元素之和除以这些元素的个数（即和除以 $n$）。</p><p>Bob 希望从 $a$ 中恰好删除两个元素，这样剩下的 $(n - 2)$ 个元素的数学平均数仍然等于 $k$。</p><p>你的任务是计算有多少对位置 $[i, j]$ (i &lt; j)$，使得删除这些位置上的两个元素后，剩下的 $(n - 2)$ 个元素的数学平均数仍然等于 $k$（即等于原数组 $a中 n个元素的数学平均数）。</p><p>赛时思路:求出所有i-n的数的出现次数,一开始想用二分,但发现<strong>并非有序</strong>,但发现<strong>空间太大</strong>,无法完成,又尝试用map映射,但发现也是空间太大,实现不了**(本质都没变)**,</p><p>正解思路:先处理好1-n的数的出现次数,再倒序遍历,对于遍历到每个数时减去一次该数相应求平均数对象的出现次数,再加和<strong>即为1-i-1的数的出现次数</strong>.</p><p>反思:使用任何方法时要<strong>注意限制条件再使用</strong>,换方法时一定要注意自己<strong>本质变没变</strong>.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> a[(<span class="type">int</span>)<span class="number">2e5</span><span class="number">+3</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> t;cin&gt;&gt;t;</span><br><span class="line">    <span class="keyword">while</span>(t--)&#123;</span><br><span class="line">        <span class="type">int</span> n;cin&gt;&gt;n;</span><br><span class="line">        unordered_map&lt;<span class="type">int</span>,<span class="type">int</span>&gt;s;<span class="comment">//记得每次都要重新开,得hu</span></span><br><span class="line">        lld sum=<span class="number">0</span>;lld ans=<span class="number">0</span>;</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">1</span>,n)&#123;</span><br><span class="line">            cin&gt;&gt;a[i];</span><br><span class="line">            sum+=a[i];</span><br><span class="line">            s[a[i]]++;</span><br><span class="line">        &#125;</span><br><span class="line">        lf k0=(sum*<span class="number">2.0</span>/n);lld k=(sum*<span class="number">2.0</span>/n);</span><br><span class="line">        <span class="keyword">if</span>(k0!=k)</span><br><span class="line">        &#123;</span><br><span class="line">            cout&lt;&lt;<span class="number">0</span>&lt;&lt;endl;ct;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">repm</span>(i,n,<span class="number">1</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="type">int</span> a0=k-a[i];s[a[i]]--;</span><br><span class="line">                ans+=(s[a0]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="牛客"><a href="#牛客" class="headerlink" title="牛客"></a>牛客</h1><h2 id="2025-牛客寒假训练1"><a href="#2025-牛客寒假训练1" class="headerlink" title="2025 牛客寒假训练1"></a>2025 牛客寒假训练1</h2><p><strong>G调整出1-n序列(贪心+排序)</strong></p><p>题意：给一个数组，每次操作可以使一个元素加1，另一个元素减1，问变成排列的最小操作次数。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">lld a[<span class="number">100003</span>],sum,ans;<span class="type">bool</span> biao[<span class="number">100003</span>];vi dai,que;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="comment">//int t;cin&gt;&gt;t;</span></span><br><span class="line">    <span class="comment">//while(t--)&#123; &#125;</span></span><br><span class="line">    lld n;cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        cin&gt;&gt;a[i];</span><br><span class="line">        sum+=a[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(sum!=(n*(n<span class="number">+1</span>))/<span class="number">2</span>)<span class="comment">//加减操作不应影响整个的和</span></span><br><span class="line">    &#123;cout&lt;&lt;<span class="number">-1</span>;ret <span class="number">0</span>;&#125;</span><br><span class="line">    <span class="built_in">sort</span>(a<span class="number">+1</span>,a<span class="number">+1</span>+n);</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        ans+=<span class="built_in">abs</span>(a[i]-i);<span class="comment">//对应操作</span></span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;ans/<span class="number">2</span>;<span class="comment">//取一半即可,加减操作次数相同</span></span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>E 双生双宿之错</strong></p><p>题意：给定一个数组，每次操作可以使得一个元素加1或者减1，问最小操作几次可以变成双生数组，即元素种类数为2、且出现次数相同。</p><p><strong>中位数定理</strong>：给定一个数组，每次操作<strong>加1或者减1</strong>，将所有元素<strong>变成相同</strong>的<strong>最小操作次数</strong>则是<strong>将所有元素变成中位数即可。</strong>(出现浮点数时整左右<strong>哪个整数都行</strong>,因为左右两半的<strong>数个数是相同的</strong>)</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/2a4d15a25658e929758cd4a0a0afd4d2.png" alt="2a4d15a25658e929758cd4a0a0afd4d2"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">lld a[<span class="number">100003</span>],x,y,t,n;</span><br><span class="line"><span class="function">lld <span class="title">check</span><span class="params">(lld xx,lld yy)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    lld sum=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(xx!=yy)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">1</span>,t)</span><br><span class="line">        sum+=<span class="built_in">abs</span>(a[i]-xx);</span><br><span class="line">        <span class="built_in">repp</span>(i,t<span class="number">+1</span>,n)</span><br><span class="line">        sum+=<span class="built_in">abs</span>(a[i]-yy);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span><span class="comment">//如果相等分类讨论取最小</span></span><br><span class="line">    &#123;</span><br><span class="line">        lld sum1=<span class="number">0</span>;lld sum2=<span class="number">0</span>;</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">1</span>,t)</span><br><span class="line">        sum1+=<span class="built_in">abs</span>(a[i]-(xx<span class="number">-1</span>));</span><br><span class="line">        <span class="built_in">repp</span>(i,t<span class="number">+1</span>,n)</span><br><span class="line">        sum1+=<span class="built_in">abs</span>(a[i]-yy);</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">1</span>,t)</span><br><span class="line">        sum2+=<span class="built_in">abs</span>(a[i]-xx);</span><br><span class="line">        <span class="built_in">repp</span>(i,t<span class="number">+1</span>,n)</span><br><span class="line">        sum2+=<span class="built_in">abs</span>(a[i]-(yy<span class="number">+1</span>));</span><br><span class="line">        sum=<span class="built_in">min</span>(sum1,sum2);</span><br><span class="line">    &#125;</span><br><span class="line">    ret sum;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="type">int</span> k;cin&gt;&gt;k;<span class="comment">//记得把t换了</span></span><br><span class="line">    <span class="keyword">while</span>(k--)&#123; </span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        cin&gt;&gt;a[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">sort</span>(a<span class="number">+1</span>,a<span class="number">+1</span>+n);<span class="comment">//排好序</span></span><br><span class="line">    t=n&gt;&gt;<span class="number">1</span>;<span class="comment">//拆一半</span></span><br><span class="line">    <span class="keyword">if</span>(t&amp;<span class="number">1</span>)<span class="comment">//如何取中位数</span></span><br><span class="line">    &#123;</span><br><span class="line">        x=a[(t<span class="number">+1</span>)/<span class="number">2</span>];</span><br><span class="line">        y=a[(n+t<span class="number">+1</span>)/<span class="number">2</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        x=a[t/<span class="number">2</span>];<span class="comment">//不统一方向取,尽量防止相等</span></span><br><span class="line">        y=a[(n+t<span class="number">+1</span>)/<span class="number">2</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;<span class="built_in">check</span>(x, y)&lt;&lt;endl;<span class="comment">//输出处理结果</span></span><br><span class="line">    &#125;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>M 数值膨胀之美</strong></p><p><strong>STL、排序，枚举</strong></p><p>要使得数组极差变小，显然需要先让最小值乘以 2 ，然后是<strong>次小值</strong>，……，以此类推。</p><p>新的区间一定会<strong>包含</strong>这个区间.我们需要在<strong>每次乘以 2 操作后计算</strong>数组的极差。</p><p>我们需要一个容器：每次操作可以在容器中<strong>删除</strong>一个数，<strong>并插入</strong>一个数。符合条件的容器有许多，我们选择的是 <strong>multiset</strong></p><p>时间复杂度 O(nlogn)。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">a</span><span class="params">(n + <span class="number">1</span>)</span></span>;<span class="comment">//用于读取</span></span><br><span class="line">    vector&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; b;<span class="comment">//用于形成映射并排序</span></span><br><span class="line">    multiset&lt;<span class="type">int</span>&gt; st;<span class="comment">//用于读取两个最值</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">        cin &gt;&gt; a[i];</span><br><span class="line">        b.<span class="built_in">push_back</span>(&#123;a[i], i&#125;);<span class="comment">//用&#123;&#125;也能造pair</span></span><br><span class="line">        st.<span class="built_in">insert</span>(a[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">sort</span>(b.<span class="built_in">begin</span>(),b.<span class="built_in">end</span>());<span class="comment">//排序,从而读取原序列第k小值位置</span></span><br><span class="line">    multiset&lt;<span class="type">int</span>&gt;::iterator it;<span class="comment">//先定义迭代器及其类型(auto 在 if里面不给用)</span></span><br><span class="line">    <span class="keyword">auto</span> [l, r] = b[<span class="number">0</span>];<span class="comment">//读取pair的一种方法</span></span><br><span class="line">    <span class="keyword">if</span>(( it=st.<span class="built_in">find</span>(l))!=st.<span class="built_in">end</span>())st.<span class="built_in">erase</span>(it);<span class="comment">//只能删除一个数一次,不然还得数多少个,麻烦</span></span><br><span class="line">    st.<span class="built_in">insert</span>(l * <span class="number">2</span>);</span><br><span class="line">    <span class="type">int</span> ans = *st.<span class="built_in">rbegin</span>() - *st.<span class="built_in">begin</span>();<span class="comment">//逆向即末尾数</span></span><br><span class="line">    l = r;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;[_, i] : b)&#123;<span class="comment">//当用不上一个元素时,可用_代替(记得打上&amp;)</span></span><br><span class="line">        <span class="keyword">if</span>(i &gt;= l &amp;&amp; i &lt;= r) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = l - <span class="number">1</span>; j &gt;= i; j--)&#123;</span><br><span class="line">            <span class="keyword">if</span>((it=st.<span class="built_in">find</span>(a[j]))!=st.<span class="built_in">end</span>())st.<span class="built_in">erase</span>(it);</span><br><span class="line">            st.<span class="built_in">insert</span>(a[j] * <span class="number">2</span>);</span><br><span class="line">            ans = <span class="built_in">min</span>(ans, *st.<span class="built_in">rbegin</span>() - *st.<span class="built_in">begin</span>());</span><br><span class="line">        &#125;</span><br><span class="line">        l = <span class="built_in">min</span>(l, i);</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = r + <span class="number">1</span>; j &lt;= i; j++)&#123;</span><br><span class="line">            <span class="keyword">if</span>((it=st.<span class="built_in">find</span>(a[j]))!=st.<span class="built_in">end</span>())st.<span class="built_in">erase</span>(it);</span><br><span class="line">            st.<span class="built_in">insert</span>(a[j] * <span class="number">2</span>);</span><br><span class="line">            ans = <span class="built_in">min</span>(ans, *st.<span class="built_in">rbegin</span>() - *st.<span class="built_in">begin</span>());</span><br><span class="line">        &#125;</span><br><span class="line">        r = <span class="built_in">max</span>(r, i);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="H-井然有序之窗"><a href="#H-井然有序之窗" class="headerlink" title="H 井然有序之窗"></a>H 井然有序之窗</h2><p><strong>构造、贪心</strong></p><p><strong>从小到大考虑(后面选择合理的前提条件)<strong>1-n的数.在</strong>符合要求(l&lt;i,此时选择l更大的或者更小的都不会影响最终结果,因为都是符合这个要求的了)<strong>的</strong>所有</strong>区间中选择 **r 最小的区间给使用掉(限制最严的)**一定不会使得答案变劣。</p><p>可以使用<strong>优先队列维护右端点最小的区间</strong>，对区间<strong>先按左端点排序后</strong>，可以从前往后将左端点小于i的<strong>区间加入优先队列(所以这个优先队列里的一直都能用,因为i是从小到大考虑的,i增大后放进来的数也肯定还满足l&lt;i)</strong>，然后<strong>取出右端点最小</strong>的区间。</p><p>若右端点小于i ，理论上应该将这个区间丢弃，找到第一个满足右端点大于等于i的区间，但由<strong>于区间不能浪费(据题意每个区间内都要有一个数)</strong>，因此此时一定无解。若<strong>优先队列为空，同样无解</strong>。</p><p>我的写法</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">st qujian&#123;</span><br><span class="line">    <span class="type">int</span> l;<span class="type">int</span> r;<span class="type">int</span> hao;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">cmp</span><span class="params">(qujian a,qujian b)</span><span class="comment">//用于sort比较的</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(a.l!=b.l)</span><br><span class="line">    <span class="built_in">ret</span>(a.l&lt;b.l);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">ret</span>(a.r&lt;b.r);</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">bool</span> <span class="keyword">operator</span> &lt;(qujian a,qujian b)<span class="comment">//用于pq特殊比较的</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(a.l!=b.l)</span><br><span class="line">    <span class="built_in">ret</span>(a.l&gt;b.l);<span class="comment">//记得反过来</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">ret</span>(a.r&gt;b.r);</span><br><span class="line">&#125;</span><br><span class="line">lld n,a[M];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">signed</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="number">0</span>);cin.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin&gt;&gt;n;<span class="type">int</span> nowhao=<span class="number">0</span>;<span class="comment">//标记第几个区间</span></span><br><span class="line">    pq&lt;qujian&gt;q;</span><br><span class="line">    deque&lt;qujian&gt;<span class="built_in">s</span>(n);<span class="comment">//提前预定好空间,从而下面可借助引用&amp;读取输入</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;[l, r, i] : s)&#123;</span><br><span class="line">        cin &gt;&gt; l &gt;&gt; r;</span><br><span class="line">        i = ++nowhao;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">sort</span>(s.<span class="built_in">begin</span>(),s.<span class="built_in">end</span>(),cmp);</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(s.<span class="built_in">size</span>()&amp;&amp;s.<span class="built_in">front</span>().l&lt;=i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">auto</span> [l,r,hao]=s.<span class="built_in">front</span>();s.<span class="built_in">pop_front</span>();<span class="comment">//因为不可以浪费,所以这里直接弹出就好</span></span><br><span class="line">            q.<span class="built_in">push</span>(&#123;r,l,hao&#125;);<span class="comment">//注意只在这边反着存入,读取时仍然按原定义的l,r顺序读取</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(q.<span class="built_in">empty</span>())&#123;<span class="comment">//特判空的1情况</span></span><br><span class="line">            cout&lt;&lt;<span class="number">-1</span>;ret <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">auto</span> tt=q.<span class="built_in">top</span>();</span><br><span class="line">        <span class="keyword">if</span>(i&gt;tt.l)<span class="comment">//肯定不符合要求了</span></span><br><span class="line">        &#123;</span><br><span class="line">            cout&lt;&lt;<span class="number">-1</span>;</span><br><span class="line">            ret <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            a[tt.hao]=i;q.<span class="built_in">pop</span>();<span class="comment">//在a的指定位置填入i</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)cout&lt;&lt;a[i]&lt;&lt;<span class="string">&#x27; &#x27;</span>;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>佬写的</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    deque&lt;array&lt;<span class="type">int</span>, 3&gt;&gt; <span class="built_in">a</span>(n);</span><br><span class="line">    <span class="type">int</span> pt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;[l, r, i] : a)&#123;</span><br><span class="line">        cin &gt;&gt; l &gt;&gt; r;</span><br><span class="line">        i = ++pt;</span><br><span class="line">    &#125;</span><br><span class="line">    ranges::<span class="built_in">sort</span>(a);</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">ans</span><span class="params">(n + <span class="number">1</span>)</span></span>;</span><br><span class="line">    set&lt;array&lt;<span class="type">int</span>, 3&gt;&gt; st;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">        <span class="keyword">while</span>(a.<span class="built_in">size</span>() &amp;&amp; a[<span class="number">0</span>][<span class="number">0</span>] &lt;= i)&#123;</span><br><span class="line">            <span class="keyword">auto</span> [l, r, i] = a[<span class="number">0</span>];</span><br><span class="line">            st.<span class="built_in">insert</span>(&#123;r, l, i&#125;);</span><br><span class="line">            a.<span class="built_in">pop_front</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(!st.<span class="built_in">size</span>())&#123;</span><br><span class="line">            cout &lt;&lt; <span class="number">-1</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">auto</span> [r, l, j] = *st.<span class="built_in">begin</span>();</span><br><span class="line">        st.<span class="built_in">erase</span>(st.<span class="built_in">begin</span>());</span><br><span class="line">        <span class="keyword">if</span>(r &lt; i)&#123;</span><br><span class="line">            cout &lt;&lt; <span class="number">-1</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ans[j] = i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">        cout &lt;&lt; ans[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="2025牛客寒假训练营"><a href="#2025牛客寒假训练营" class="headerlink" title="2025牛客寒假训练营"></a>2025牛客寒假训练营</h1><h2 id="D-字符串里串"><a href="#D-字符串里串" class="headerlink" title="D 字符串里串"></a>D 字符串里串</h2><p>思维、贪心</p><p>有一个比较容易想到的结论，如果从前往后第i&gt;1个字符在i之后的第j个位置还出现过，那么选择这[1,i]]前缀作为子串，子序列就把[1,i-1]和[j,j]拼在一起，就得到了两个不一样的子串。</p><p>然后把字符串翻转一下，再跑一次即可（<strong>从前往后和从后往前都是可以的不要只有一个方向</strong>）。</p><p>注意 “aba” 的<strong>答案是 0 ，而不是 1</strong> ，之前 std 也错了，不过数据里没有这个，所以不考虑这一个也能过。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    string s;</span><br><span class="line">    cin &gt;&gt; s;</span><br><span class="line">    s = <span class="string">&quot; &quot;</span> + s;</span><br><span class="line">    set&lt;<span class="type">int</span>&gt; st;</span><br><span class="line">    <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = n; i &gt;= <span class="number">2</span>; i--)&#123;</span><br><span class="line">        <span class="keyword">if</span>(st.<span class="built_in">count</span>(s[i]))&#123; ans = <span class="built_in">max</span>(ans, i);<span class="keyword">break</span>;&#125;<span class="comment">//看后面有没有这个字母,仅在此可break</span></span><br><span class="line">        st.<span class="built_in">insert</span>(s[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    st = <span class="built_in">set</span>&lt;<span class="type">int</span>&gt;();<span class="comment">//可以清空</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n - <span class="number">1</span>; i++)&#123;<span class="comment">//另一个方向</span></span><br><span class="line">        <span class="keyword">if</span>(st.<span class="built_in">count</span>(s[i])) ans = <span class="built_in">max</span>(ans, n - i + <span class="number">1</span>);</span><br><span class="line">        st.<span class="built_in">insert</span>(s[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="C-字符串外串"><a href="#C-字符串外串" class="headerlink" title="C 字符串外串"></a>C 字符串外串</h2><p>构造</p><p>首先要知道D题的结论，那我们需要构造一个字符串使得<strong>从前往后看和从后往前看可爱度都是</strong>m的字符串。</p><p>那就思考一下<strong>类似回文串的构造</strong>方法。</p><p>如果 n超过了m的两倍，意味着前m个和后m个字母<strong>对称</strong>（”abcdefcba”），<strong>中间n-2m</strong>个字母在<strong>整个字符串中全都只能出现一次</strong>，总共需要的字母种类是m+n-2m&#x3D;n-m，很显然字母种类不能超过26个。</p><p>如果 n不超过m的两倍，意味着前 n-m个和后n-m个字母对称（”abczzzcba”），<strong>中间字母任意</strong>，总共需要的字母种类是n-m，很显然字母种类不能超过26个。</p><p>按上述情况<strong>分类讨论构造</strong>一下即可。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> m,n;</span><br><span class="line">string s;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="type">int</span> t;</span><br><span class="line">    cin&gt;&gt;t;</span><br><span class="line">    <span class="keyword">while</span>(t--)</span><br><span class="line">    &#123;</span><br><span class="line">        cin&gt;&gt;n&gt;&gt;m;string ans=<span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="keyword">if</span>(n-m&gt;<span class="number">26</span>||n&lt;=m)&#123;</span><br><span class="line">            cout&lt;&lt;<span class="string">&quot;NO&quot;</span>&lt;&lt;endl;ct;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            cout&lt;&lt;<span class="string">&quot;YES&quot;</span>&lt;&lt;endl;</span><br><span class="line">            <span class="keyword">if</span>(n&gt;=<span class="number">2</span>*m)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">repp</span>(i,<span class="number">0</span>,m<span class="number">-1</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    ans+=(<span class="string">&#x27;a&#x27;</span>+i);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="built_in">repp</span>(i,m,n-m<span class="number">-1</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    ans+=(<span class="string">&#x27;a&#x27;</span>+i);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="built_in">repm</span>(i,m<span class="number">-1</span>,<span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    ans+=(<span class="string">&#x27;a&#x27;</span>+i);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">repp</span>(i,<span class="number">0</span>,n-m<span class="number">-1</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    ans+=(<span class="string">&#x27;a&#x27;</span>+i);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="built_in">repp</span>(i,<span class="number">1</span>,<span class="number">2</span>*m-n)</span><br><span class="line">                &#123;</span><br><span class="line">                    ans+=(<span class="string">&#x27;a&#x27;</span>+n-m<span class="number">-1</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="built_in">repm</span>(i,n-m<span class="number">-1</span>,<span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    ans+=(<span class="string">&#x27;a&#x27;</span>+i);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="H-一起画很大的圆！"><a href="#H-一起画很大的圆！" class="headerlink" title="H 一起画很大的圆！"></a>H 一起画很大的圆！</h2><p><strong>构造、计算几何，贪心</strong></p><p>三个不共线的点确定一个圆。</p><p>如果这三个点<strong>越接近一条直线，这个圆最大(三点共线的时候这个最大)</strong>。</p><p>那么我们需要在边界上找三个点使得最接近一条直线，<strong>猜一下有一个点会在边角</strong>，那剩下的点就不难确定了。</p><p><strong>横着</strong>可以找到一个答案是<img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/9d846c00bf84543b285250dadf59bd12.png" alt="9d846c00bf84543b285250dadf59bd12">，但如果<img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/c32aae6022cbfb5e60918fcde1845b07.png" alt="c32aae6022cbfb5e60918fcde1845b07"> 的话，就应该<strong>竖着</strong>找。</p><p>显然<strong>在长边取两个点越接近直线斜边也越长</strong></p><p>我们应当使得斜边尽可能的大，同时斜边所对的角尽可能的接近0 或 <img src="https://www.nowcoder.com/equation?tex=180%20%5E%7B%5Ccirc%7D" alt="180 ^{\circ}">。前者很好实现，令长边上的一点<strong>位于角落</strong>（如图中点A，短边上点尽可能靠近角落（如图中点C)，此时可使得AB所对的锐角角度最小(相切那个是<strong>角度最大</strong>)。</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/D2B5CA33BD970F64A6301FA75AE2EB22.png" alt="alt"></p><p>再<strong>移动一位</strong>取B</p><p><img src="/./%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E6%AF%94%E8%B5%9B%E7%9A%84%E8%A1%A5/D2B5CA33BD970F64A6301FA75AE2EB22-1742044851694-89.png" alt="alt"></p><p>那么关键就在于<strong>找出长边</strong>了.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记</title>
      <link href="/2025/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2025/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习教程"><a href="#机器学习教程" class="headerlink" title="机器学习教程"></a>机器学习教程</h1><p>[TOC]</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Machine-learning-logo-1.webp" alt="img"></p><p>机器学习（Machine Learning）是人工智能（AI）的一个<strong>分支</strong>，它使计算机系统能够<strong>利用数据和算法自动学习和改进</strong>其性能。</p><p>机器学习是让**机器通过经验（数据）**来做决策和预测。</p><p>机器学习已经广泛应用于许多领域，包括推荐系统、图像识别、语音识别、金融分析等。</p><p>举个例子，通过机器学习，汽车可以学习如何识别交通标志、行人和障碍物，以实现自动驾驶。</p><h2 id="机器学习与传统编程的区别"><a href="#机器学习与传统编程的区别" class="headerlink" title="机器学习与传统编程的区别"></a>机器学习与传统编程的区别</h2><p>在传统的编程方法中，程序员会编写一系列规则或指令，告诉计算机如何执行任务。而在机器学习中，程序员并不是直接编写所有规则，而是<strong>训练计算机</strong>从数据中<strong>自动</strong>学习和推断模式。具体的差异可以总结如下：</p><ul><li><strong>传统编程：</strong> 程序员<strong>定义明确</strong>的规则和逻辑，计算机根据这些规则执行任务。</li><li><strong>机器学习：</strong> 计算机<strong>通过数据”学习”<strong>模式，<strong>生成模型</strong>并</strong>基于这些模式</strong>进行<strong>预测</strong>或<strong>决策</strong>。</li></ul><p>举个简单的例子，假设我们要训练一个模型来识别猫和狗的图片。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/68747470733a2f2f6d69726f25674a6b50587563386f672e676966.gif" alt="img"></p><p>在传统编程中，程序员需要手动定义哪些特征可以区分猫和狗（如耳朵形状、鼻子形状等），而在机器学习中，程序员只需要提供大量带标签的图片数据，计算机会<strong>自动学习</strong>如何区分猫和狗。</p><hr><h2 id="常见机器学习任务"><a href="#常见机器学习任务" class="headerlink" title="常见机器学习任务"></a>常见机器学习任务</h2><ul><li><strong>回归问题</strong>：<strong>预测连续</strong>值，例如房价预测。</li><li><strong>分类问题</strong>：将样本<strong>分为不同类别</strong>，例如垃圾邮件检测。</li><li><strong>聚类问题</strong>：将数据<strong>自动分组</strong>，例如客户细分。</li><li><strong>降维问题</strong>：将数据<strong>降到低维</strong>度，例如主成分分析（PCA）。</li></ul><hr><h2 id="机器学习常见算法"><a href="#机器学习常见算法" class="headerlink" title="机器学习常见算法"></a>机器学习常见算法</h2><p><strong>监督学习：</strong></p><ul><li><strong>线性</strong>回归（Linear Regression）</li><li><strong>逻辑</strong>回归（Logistic Regression）</li><li>支持向量机（SVM）</li><li>K-近邻算法（KNN）</li><li>决策树（Decision Tree）</li><li>随机森林（Random Forest）</li></ul><p><strong>无监督学习：</strong></p><ul><li>K-均值聚类（K-Means Clustering）</li><li>主成分分析（PCA）</li></ul><p><strong>深度学习：</strong></p><ul><li>神经网络（Neural Networks）</li><li>卷积神经网络（CNN）</li><li>循环神经网络（RNN）</li></ul><h1 id="机器学习简介"><a href="#机器学习简介" class="headerlink" title="机器学习简介"></a>机器学习简介</h1><h2 id="机器学习是如何工作的？"><a href="#机器学习是如何工作的？" class="headerlink" title="机器学习是如何工作的？"></a>机器学习是如何工作的？</h2><p>机器学习通过让计算机<strong>从大量数据</strong>中学习模式和规律来做出决策和预测。</p><ul><li>首先，<strong>收集并准备数据</strong>，然后选择一个<strong>合适的算法</strong>来训练模型。</li><li>然后，模型通过<strong>不断优化参数</strong>，<strong>最小化预测错误</strong>，直到能准确地对新数据进行预测。</li><li>最后，模型<strong>部署</strong>到实际应用中，<strong>实时</strong>做出预测或决策，并根据新的数据进行更新。</li></ul><p>机器学习是一个<strong>迭代</strong>过程，可能需要<strong>多次调整</strong>模型参数和特征选择，以提高模型的性能。</p><p>下面这张图展示了机器学习的基本流程：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/how-does-machine-learning-work.png" alt="img"></p><ol><li><strong>Labeled Data（标记数据）：</strong>：图中蓝色区域显示了标记数据，这些数据包括了不同的几何形状（如六边形、正方形、三角形）。</li><li><strong>Model Training（模型训练）：</strong>：在这个阶段，机器学习<strong>算法分析</strong>数据的特征，并学习如何根据这些特征来预测标签。</li><li><strong>Test Data（测试数据）：</strong>：图中深绿色区域显示了测试数据，包括一个正方形和一个三角形。</li><li><strong>Prediction（预测）：</strong>：模型使用从训练数据中学到的规则来预测测试数据的标签。在图中，模型预测了测试数据中的正方形和三角形。</li><li><strong>Evaluation（评估）：</strong>：预测结果与测试数据的真实标签进行<strong>比较</strong>，以评估模型的准确性。</li></ol><p>机器学习的工作流程可以大致分为以下几个步骤：</p><h3 id="1-数据收集"><a href="#1-数据收集" class="headerlink" title="1. 数据收集"></a>1. 数据收集</h3><ul><li><strong>收集数据</strong>：这是机器学习项目的第一步，涉及收集相关数据。数据可以来自数据库、文件、网络或实时数据流。</li><li><strong>数据类型</strong>：可以是结构化数据（如表格数据）或非结构化数据（如文本、图像、视频）。</li></ul><h3 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h3><ul><li><strong>清洗数据</strong>：处理缺失值、异常值、错误和重复数据。</li><li><strong>特征工程</strong>：<strong>选择</strong>有助于模型学习的最相关特征，可能包括创建新特征或转换现有特征。</li><li><strong>数据标准化&#x2F;归一化</strong>：调整数据的尺度，使其在<strong>同一范围</strong>内，有助于某些算法的性能。</li></ul><h3 id="3-选择模型"><a href="#3-选择模型" class="headerlink" title="3. 选择模型"></a>3. 选择模型</h3><ul><li><strong>确定问题类型</strong>：根据<strong>问题的性质</strong>（分类、回归、聚类等）<strong>选择合适</strong>的机器学习<strong>模型</strong>。</li><li><strong>选择算法</strong>：基于问题类型和数据特性，选择<strong>一个或多个算法</strong>进行实验。</li></ul><h3 id="4-训练模型"><a href="#4-训练模型" class="headerlink" title="4. 训练模型"></a>4. 训练模型</h3><ul><li><strong>划分数据集</strong>：将数据分为<strong>训练</strong>集、<strong>验证</strong>集和<strong>测试</strong>集。</li><li><strong>训练</strong>：使用训练集上的数据来训练模型，调整模型参数以<strong>最小化损失函数</strong>。</li><li><strong>验证</strong>：使用验证集来调整模型参数，<strong>防止过拟合</strong>。</li></ul><h3 id="5-评估模型"><a href="#5-评估模型" class="headerlink" title="5. 评估模型"></a>5. 评估模型</h3><ul><li><strong>性能指标</strong>：使用<strong>测试集来评估</strong>模型的性能，常用的指标包括<strong>准确</strong>率、<strong>召回</strong>率、<strong>F1分数</strong>等。</li><li><strong>交叉验证</strong>：一种评估模型泛化能力的技术，通过将数据<strong>分成多个子集</strong>进行训练和验证。</li></ul><h3 id="6-模型优化"><a href="#6-模型优化" class="headerlink" title="6. 模型优化"></a>6. 模型优化</h3><ul><li><strong>调整超参数</strong>：超参数是<strong>学习过程之前设置的</strong>参数，如学习率、树的深度等，可以通过网格搜索、随机搜索或贝叶斯优化等方法来调整。</li><li><strong>特征选择</strong>：可能需要<strong>重新</strong>评估和选择特征，以提高模型性能。</li></ul><h3 id="7-部署模型"><a href="#7-部署模型" class="headerlink" title="7. 部署模型"></a>7. 部署模型</h3><ul><li><strong>集成到应用</strong>：将训练好的模型集成到实际应用中，如网站、移动应用或软件中。</li><li><strong>监控和维护</strong>：持续监控模型的性能，并根据新数据更新模型。</li></ul><h3 id="8-反馈循环"><a href="#8-反馈循环" class="headerlink" title="8. 反馈循环"></a>8. 反馈循环</h3><ul><li><strong>持续学习</strong>：机器学习模型可以设计为随着时间的推移自动从新数据中学习，以适应变化。</li></ul><h3 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h3><ul><li><strong>损失函数</strong>：一个衡量模型<strong>预测与实际结果差异</strong>的函数，模型训练的<strong>目标是最小化</strong>这个函数。</li><li><strong>优化算法</strong>：如梯度下降，用于找到最小化损失函数的参数值。</li><li><strong>正则化</strong>：一种技术，通过<strong>添加惩罚项</strong>来防止模型过拟合。</li></ul><p>机器学习的工作流程是迭代的，可能需要多次调整和优化以达到最佳性能。此外，随着数据的积累和算法的发展，机器学习模型可以变得更加精确和高效。</p><h2 id="机器学习的类型"><a href="#机器学习的类型" class="headerlink" title="机器学习的类型"></a>机器学习的类型</h2><p>机器学习主要分为以下三种类型：</p><h3 id="1-监督学习（Supervised-Learning）"><a href="#1-监督学习（Supervised-Learning）" class="headerlink" title="1. 监督学习（Supervised Learning）"></a>1. <strong>监督学习（Supervised Learning）</strong></h3><ul><li><strong>定义：</strong> 监督学习是指使用<strong>带标签的数据</strong>进行训练，模型通过学习输入数据与标签之间的<strong>关系</strong>，来做出预测或分类。</li><li><strong>应用：</strong> <strong>分类</strong>（如垃圾邮件识别）、<strong>回归</strong>（如房价预测）。</li><li><strong>例子：</strong> 线性回归、决策树、支持向量机（SVM）。</li></ul><h3 id="2-无监督学习（Unsupervised-Learning）"><a href="#2-无监督学习（Unsupervised-Learning）" class="headerlink" title="2. 无监督学习（Unsupervised Learning）"></a>2. <strong>无监督学习（Unsupervised Learning）</strong></h3><ul><li><strong>定义：</strong> 无监督学习使用<strong>没有标签</strong>的数据，模型试图在数据中发现潜在的结构或模式。</li><li><strong>应用：</strong> <strong>聚类</strong>（如客户分群）、<strong>降维</strong>（如数据可视化）。</li><li><strong>例子：</strong> K-means 聚类、主成分分析（PCA）。</li></ul><h3 id="3-强化学习（Reinforcement-Learning）"><a href="#3-强化学习（Reinforcement-Learning）" class="headerlink" title="3. 强化学习（Reinforcement Learning）"></a>3. <strong>强化学习（Reinforcement Learning）</strong></h3><ul><li><strong>定义：</strong> 强化学习通过与环境互动，智能体<strong>在试错中学习</strong>最佳策略，以最大化长期回报。每次行动后，系统会<strong>收到奖励或惩罚</strong>，来指导行为的改进。</li><li><strong>应用：</strong> 游戏AI（如AlphaGo）、自动驾驶、机器人控制。</li><li><strong>例子：</strong> Q-learning、深度Q网络（DQN）。</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/The-main-types-of-machine-learning-Main-approaches-include-classification-and-regression.png" alt="img"></p><p>这三种机器学习类型各有其应用场景和优势，监督学习适用于<strong>有明确标签的数据</strong>，无监督学习适用于<strong>探索数据内在结构</strong>，而强化学习适用于需要通过试错来学习最优策略的场景。</p><h2 id="机器学习的应用领域"><a href="#机器学习的应用领域" class="headerlink" title="机器学习的应用领域"></a>机器学习的应用领域</h2><ul><li><strong>推荐系统：</strong> 例如，抖音推荐你可能感兴趣的视频，淘宝推荐你可能会购买的商品，网易云音乐推荐你喜欢的音乐。</li><li><strong>自然语言处理（NLP）：</strong> 机器学习在语音识别、机器翻译、情感分析、聊天机器人等方面的应用。例如，Google 翻译、Siri 和智能客服等。</li><li><strong>计算机视觉：</strong> 机器学习在图像识别、物体检测、面部识别、自动驾驶等领域有广泛应用。例如，自动驾驶汽车通过摄像头和传感器识别周围的障碍物，识别行人和其他车辆。</li><li><strong>金融分析：</strong> 机器学习在股市预测、信用评分、欺诈检测等金融领域具有重要应用。例如，银行利用机器学习检测信用卡交易中的欺诈行为。</li><li><strong>医疗健康：</strong> 机器学习帮助医生诊断疾病、发现药物副作用、预测病情发展等。例如，IBM 的 Watson 系统帮助医生分析患者的病历数据，提供诊断和治疗建议。</li><li><strong>游戏和娱乐：</strong> 机器学习不仅用于游戏中的智能对手，还应用于游戏设计、动态难度调整等方面。例如，AlphaGo 使用深度学习技术战胜了围棋世界冠军。</li></ul><h2 id="机器学习的未来"><a href="#机器学习的未来" class="headerlink" title="机器学习的未来"></a>机器学习的未来</h2><p>随着数据量的爆炸式增长和计算能力的提升，机器学习的应用将继续扩展，带来更加智能和高效的系统。例如：</p><ul><li><strong>强化学习：</strong> 使计算机能够在没有明确指导的情况下通过试错来解决复杂问题。例如，AlphaGo 和 OpenAI 的 Dota 2 游戏 AI 都使用了强化学习。</li><li><strong>自监督学习：</strong> 目前的机器学习模型通常<strong>需要大量带标签</strong>的数据来进行训练，而自监督学习则能够在没有标签的数据下学习更有效的表示。</li><li><strong>深度学习：</strong> <strong>深度</strong>学习是机器学习中的一个分支，主要关注<strong>神经网络的应用</strong>，它已经在图像识别、自然语言处理等方面取得了突破性进展。未来，深度学习将继续推动人工智能的发展。</li></ul><h1 id="机器学习如何工作"><a href="#机器学习如何工作" class="headerlink" title="机器学习如何工作"></a>机器学习如何工作</h1><p>机器学习（Machine Learning, ML）的核心思想是让计算机能够通过<strong>数据学习</strong>，并从中推断出规律或模式，而不依赖于显式编写的规则或代码。</p><p>简单来说，机器学习的工作流程是让机器<strong>通过历史数据自动改进</strong>其决策和预测能力。</p><p>机器学习的工作流程可以简化为以下几个步骤：</p><ol><li><strong>收集数据</strong>：准备包含特征和标签的数据。</li><li><strong>选择模型</strong>：根据任务选择合适的机器学习算法。</li><li><strong>训练模型</strong>：让模型通过数据学习模式，最小化误差。</li><li><strong>评估与验证</strong>：通过测试集评估模型性能，并进行优化。</li><li><strong>部署模型</strong>：将训练好的模型应用到实际场景中进行预测。</li><li><strong>持续改进</strong>：随着新数据的产生，模型需要定期更新和优化。</li></ol><p>这个过程能够让计算机从经验中自动学习，并在各种任务中做出越来越准确的预测。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/machine-learning-how-machine-learning-work.png" alt="img"></p><p>我们可以从以下几个方面来理解机器学习是如何工作的：</p><h3 id="1-数据输入：数据是学习的基础"><a href="#1-数据输入：数据是学习的基础" class="headerlink" title="1. 数据输入：数据是学习的基础"></a>1. 数据输入：数据是学习的基础</h3><p>机器学习的<strong>第一步是数据收集</strong>。没有数据，机器学习模型无法进行训练。数据通常包括”输入<strong>特征</strong>“和”<strong>标签</strong>“：</p><ul><li><strong>输入特征（Features）：</strong> 这些是模型用来做预测或分类的信息。例如，在房价预测问题中，输入特征可以是房子的面积、地理位置、卧室数量等。</li><li><strong>标签（Labels）：</strong> 标签是我们<strong>想要预测或分类的结果</strong>，通常是一个数字或类别。例如，在房价预测问题中，标签是房子的价格。</li></ul><p>机器学习模型的目标是从数据中找出输入特征与标签之间的关系，基于这些关系做出预测。</p><h3 id="2-模型选择：选择合适的学习算法"><a href="#2-模型选择：选择合适的学习算法" class="headerlink" title="2. 模型选择：选择合适的学习算法"></a>2. 模型选择：选择合适的学习算法</h3><p>机器学习<strong>模型</strong>（<strong>也叫做算法</strong>）是帮助计算机学习数据并进行预测的工具。根据数据的性质和任务的不同，常见的机器学习模型包括：</p><ul><li><strong>监督学习模型：</strong> 给定带有标签的数据，模型通过学习输入和标签之间的关系来做预测。例如，<strong>线性回归</strong>、<strong>逻辑回归</strong>、<strong>支持向量机（SVM）</strong> 和 <strong>决策树</strong>。</li><li><strong>无监督学习模型：</strong> 没有标签的数据，模型通过探索数据中的结构或模式来进行学习。例如，<strong>K-means 聚类</strong>、<strong>主成分分析（PCA）</strong>。</li><li><strong>强化学习模型：</strong> 模型在与环境互动的过程中，通过奖励和惩罚来学习最佳行为。例如，<strong>Q-learning</strong>、<strong>深度强化学习</strong>（Deep Q-Networks, DQN）。</li></ul><h3 id="3-训练过程：让模型从数据中学习"><a href="#3-训练过程：让模型从数据中学习" class="headerlink" title="3. 训练过程：让模型从数据中学习"></a>3. 训练过程：让模型从数据中学习</h3><p>在训练阶段，模型通过历史数据”学习”输入和标签之间的关系，通常通过最小化一个损失函数（Loss Function）来优化模型的参数。训练过程可以概括为以下步骤：</p><ul><li><strong>初始状态：</strong> 模型从<strong>随机值开始</strong>。比如，神经网络的权重是随机初始化的。</li><li><strong>计算预测：</strong> 对于每个输入，模型会做出一个预测。这是通过将输入数据传递给模型，计算得到输出。</li><li><strong>计算误差（损失）：</strong> 误差是指模型预测的输出与实际标签之间的差异。例如，对于回归问题，误差可以通过均方误差（MSE）来衡量。</li><li><strong>优化模型：</strong> 通过反向传播（在神经网络中）或梯度下降等优化算法，<strong>不断调整模型的参数</strong>（如神经网络的权重），使得误差最小化。这个过程就是<strong>训练</strong>，直到模型能够在训练数据上做出比较准确的预测。</li></ul><h3 id="4-验证与评估：测试模型的性能"><a href="#4-验证与评估：测试模型的性能" class="headerlink" title="4. 验证与评估：测试模型的性能"></a>4. 验证与评估：测试模型的性能</h3><p>训练过程完成后，我们需要<strong>评估</strong>模型的性能。为了<strong>避免模型过度拟合</strong>训练数据，我们将数据分为<strong>训练集</strong>和<strong>测试集</strong>，其中：</p><ul><li><strong>训练集：</strong> 用于训练模型的部分数据。</li><li><strong>测试集：</strong> 用于评估模型性能的部分数据，通常不参与训练过程。</li></ul><p>常见的评估指标包括：</p><ul><li><strong>准确率（Accuracy）：</strong> 分类问题中正确分类的比例。</li><li><strong>均方误差（MSE）：</strong> <strong>回归</strong>问题中，预测值与真实值差的<strong>平方的平均值</strong>。</li><li><strong>精确率（Precision）与召回率（Recall）：</strong> 用于二分类问题，尤其是<strong>类别不平衡</strong>时。</li><li><strong>F1分数：</strong> <strong>精确率</strong>与<strong>召回率</strong>的<strong>调和平均数</strong>，综合考虑分类器的表现。</li></ul><h3 id="5-优化与调整：提高模型的精度"><a href="#5-优化与调整：提高模型的精度" class="headerlink" title="5. 优化与调整：提高模型的精度"></a>5. 优化与调整：提高模型的精度</h3><p>如果模型在测试集上的表现不理想，可能需要进一步优化。这通常包括：</p><ul><li><strong>调整超参数（Hyperparameters）：</strong> 比如<strong>学习率</strong>、<strong>正则化系数</strong>、<strong>树的深度</strong>等。这些超参数影响模型的学习能力。</li><li><strong>模型选择与融合：</strong> 尝试不同的模型或模型融合（比如集成学习方法，如随机森林、XGBoost 等）来提高精度。</li><li><strong>数据增强：</strong> <strong>扩展</strong>训练数据集，比如对图像进行旋转、翻转等操作，帮助模型提高泛化能力。</li></ul><h3 id="6-模型部署与预测：实际应用"><a href="#6-模型部署与预测：实际应用" class="headerlink" title="6. 模型部署与预测：实际应用"></a>6. 模型部署与预测：实际应用</h3><p>一旦模型在训练和测试数据上表现良好，就可以将模型部署到实际应用中：</p><ul><li><strong>模型部署：</strong> 将训练好的模型嵌入到应用程序、网站、服务器等系统中，供用户使用。</li><li><strong>实时预测：</strong> 在实际环境中，新的数据输入到模型中，模型根据之前学习到的模式进行实时预测或分类。</li></ul><h3 id="7-持续学习与模型更新："><a href="#7-持续学习与模型更新：" class="headerlink" title="7. 持续学习与模型更新："></a>7. 持续学习与模型更新：</h3><p>机器学习系统通常不是一次性完成的。在实际应用中，随着时间的推移，新的数据会不断产生，因此，模型需要定期更新和再训练，以保持其预测能力。这可以通过<strong>在线学习</strong>、<strong>迁移学习</strong>等方法来实现。</p><h1 id="机器学习基础概念"><a href="#机器学习基础概念" class="headerlink" title="机器学习基础概念"></a>机器学习基础概念</h1><p>在学习机器学习时，理解其核心基础概念至关重要。</p><p>这些基础概念帮助我们理解数据如何输入到模型中、模型如何学习、以及如何评估模型的表现。</p><p>接下来，我们将详细讲解几个机器学习中的基本概念：</p><ul><li><strong>训练集、测试集和验证集</strong>：帮助训练、评估和调优模型。</li><li><strong>特征与标签</strong>：特征是<strong>输入</strong>，标签是模型预测的<strong>目标</strong>。</li><li><strong>模型与算法</strong>：模型是<strong>通过算法训练得到</strong>的，算法帮助模型<strong>学习</strong>数据中的<strong>模式</strong>。</li><li><strong>监督学习、无监督学习和强化学习</strong>：三种常见的学习方式，分别用于不同的任务。</li><li><strong>过拟合与欠拟合</strong>：两种常见的问题，<strong>影响</strong>模型的<strong>泛化</strong>能力。</li><li><strong>训练误差与测试误差</strong>：反映模型是否能适应数据，并进行有效预测。</li><li><strong>评估指标</strong>：衡量模型好坏的标准，根据任务选择合适的指标。</li></ul><p>这些基础概念是理解和应用机器学习的基础，掌握它们是进一步学习的关键。</p><h3 id="训练集、测试集和验证集"><a href="#训练集、测试集和验证集" class="headerlink" title="训练集、测试集和验证集"></a>训练集、测试集和验证集</h3><ul><li><strong>训练集（Training Set）：</strong> 训练集是用于训练机器学习模型的数据集，它包含输入特征和对应的标签（在监督学习中）。模型通过学习训练集中的数据来调整参数，逐步提高预测的准确性。</li><li><strong>测试集（Test Set）：</strong> 测试集用于<strong>评估</strong>训练好的模型的<strong>性能</strong>。测试集中的数据不参与模型的训练，模型使用它来进行预测，并与真实标签进行比较，帮助我们了解模型在<strong>未见过</strong>的数据上的<strong>表现</strong>。</li><li><strong>验证集（Validation Set）：</strong> 验证集用于在训练过程中<strong>调整</strong>模型的<strong>超参数</strong>（如学习率、正则化参数等）。它通常被用于模型<strong>调优</strong>，帮助选择<strong>最佳的模型参数</strong>，<strong>避免过</strong>拟合。验证集的作用是对模型进行<strong>监控和调试</strong>。</li></ul><p><strong>总结：</strong></p><ul><li>训练集用于训练模型。</li><li>测试集用于<strong>评估</strong>模型的<strong>最终</strong>性能。</li><li>验证集用于模型<strong>调优</strong>。</li></ul><h3 id="特征（Features）和标签（Labels）"><a href="#特征（Features）和标签（Labels）" class="headerlink" title="特征（Features）和标签（Labels）"></a>特征（Features）和标签（Labels）</h3><ul><li><strong>特征（Features）：</strong> 特征是<strong>输入数据的不同属性</strong>，模型使用这些特征来做出预测或分类。例如，在房价预测中，特征可能包括房子的面积、地理位置、卧室数量等。</li><li><strong>标签（Labels）：</strong> 标签是机器学习任务中的<strong>目标变量</strong>，模型要预测的结果。对于监督学习任务，标签通常是已知的。例如，在房价预测中，标签就是房子的实际价格。</li></ul><p><strong>总结：</strong></p><ul><li>特征是模型输入的数据。</li><li>标签是模型需要预测的输出。</li></ul><h3 id="模型（Model）与算法（Algorithm）"><a href="#模型（Model）与算法（Algorithm）" class="headerlink" title="模型（Model）与算法（Algorithm）"></a>模型（Model）与算法（Algorithm）</h3><ul><li><strong>模型（Model）：</strong> 模型是通过学习数据中的模式而构建的<strong>数学结构</strong>。它接受输入特征，经过一系列计算和转化，输出一个预测结果。常见的模型有线性回归、决策树、神经网络等。</li><li><strong>算法（Algorithm）：</strong> 算法是实现机器学习的<strong>步骤或规则</strong>，它定义了模型如何从数据中学习。常见的算法有梯度下降法、随机森林、K近邻算法等。算法帮助模型调整其参数以最小化预测误差。</li></ul><p><strong>总结：</strong></p><ul><li>模型是<strong>学习到的结果</strong>，它可以用来进行预测。</li><li>算法是<strong>训练模型的过程</strong>，帮助模型从数据中学习。</li></ul><h3 id="监督学习、无监督学习和强化学习"><a href="#监督学习、无监督学习和强化学习" class="headerlink" title="监督学习、无监督学习和强化学习"></a>监督学习、无监督学习和强化学习</h3><ul><li><strong>监督学习（Supervised Learning）：</strong> 在监督学习中，训练数据包含<strong>已知的标签</strong>。模型通过学习输入特征与标签之间的关系来进行预测或分类。监督学习的目标是最小化预测错误，使模型能够在新数据上做出准确的预测。<ul><li><strong>例子：</strong> 线性回归、逻辑回归、支持向量机（SVM）、决策树。</li></ul></li><li><strong>无监督学习（Unsupervised Learning）：</strong> 无监督学习中，训练数据没有标签，模型通过分析输入数据中的结构或模式来进行学习。目标是发现数据的<strong>潜在规律</strong>，常见的任务包括聚类、降维等。<ul><li><strong>例子：</strong> K-means 聚类、主成分分析（PCA）。</li></ul></li><li><strong>强化学习（Reinforcement Learning）：</strong> 强化学习是让智能体（Agent）通过与环境（Environment）的互动，采取行动并<strong>根据奖励或惩罚来学习最优</strong>策略。智能体的目标是通过最大化长期奖励来优化行为。<ul><li><strong>例子：</strong> AlphaGo、自动驾驶、游戏AI。</li></ul></li></ul><p><strong>总结：</strong></p><ul><li>监督学习：有标签的训练数据，任务是预测或分类。</li><li>无监督学习：没有标签的训练数据，任务是发现数据中的模式或结构。</li><li>强化学习：通过与环境互动，智能体根据奖励和惩罚进行学习。</li></ul><h3 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h3><ul><li><strong>过拟合（Overfitting）：</strong> 过拟合是指模型<strong>在训练数据上</strong>表现非常好，但<strong>在测试数据上</strong>表现很差。这通常发生在模型<strong>复杂度过高</strong>、<strong>参数过多</strong>，导致模型”记住”了<strong>训练数据中的</strong>噪声或偶然性，而<strong>不具备泛化</strong>能力。过拟合的模型<strong>无法有效应对新</strong>数据。</li><li><strong>欠拟合（Underfitting）：</strong> 欠拟合是指模型在训练数据上和测试数据上<strong>都表现不佳</strong>，通常是因为模型<strong>过于简单</strong>，无法捕捉数据中的复杂模式。欠拟合的模型无法从数据中学习到有用的规律。</li></ul><p><strong>解决方法：</strong></p><ul><li>过拟合：可以通过<strong>简化</strong>模型、<strong>增加</strong>训练数据或使用<strong>正则化</strong>等方法来缓解。</li><li>欠拟合：可以通过增加模型复杂度或使用更复杂的算法来改进。</li></ul><h3 id="训练与测试误差"><a href="#训练与测试误差" class="headerlink" title="训练与测试误差"></a>训练与测试误差</h3><ul><li><strong>训练误差（Training Error）：</strong> 训练误差是模型在训练数据上的表现，反映了模型是否能够很好地适应训练数据。如果训练误差很大，可能说明模型不够复杂，<strong>欠</strong>拟合；如果训练误差很小，可能说明模型太复杂，容易<strong>过</strong>拟合。</li><li><strong>测试误差（Test Error）：</strong> 测试误差是模型在未见过的数据上的表现，反映了模型的泛化能力。测试误差应当与训练误差相匹配，若测试误差<strong>远高</strong>于训练误差，通常是<strong>过</strong>拟合。</li></ul><p><strong>总结：</strong></p><ul><li>训练误差和测试误差的差距可以帮助我们判断模型的适应性。</li><li>理想的情况是训练误差和测试误差<strong>都较小</strong>，并且<strong>相对接近</strong>。</li></ul><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>根据任务的不同，机器学习模型的评估指标也不同。以下是常用的一些评估指标：</p><ul><li><strong>准确率（Accuracy）：</strong> <strong>分类</strong>任务中，<strong>正确</strong>分类的样本占总样本的比例。</li><li><strong>精确率（Precision）和召回率（Recall）：</strong> 主要用于处理<strong>不平衡</strong>数据集，精确率衡量的是被模型预<strong>测为正类</strong>的样本中，有多少是<strong>真正的正类</strong>；召回率衡量的是<strong>所有实际</strong>正类<strong>中</strong>，有多少被模型<strong>正确识别</strong>为正类。</li><li><strong>F1 分数：</strong> 精确率与召回率的<strong>调和</strong>平均数，用于综合考虑模型的表现。</li><li><strong>均方误差（MSE）：</strong> 回归任务中，预测值与真实值之间差异的平方的平均值。</li></ul><p><strong>总结：</strong><br>评估指标帮助我们衡量模型的表现，选择最合适的指标可以根据任务的需求来进行。</p><h1 id="深度学习-Deep-Learning-入门——基本概念"><a href="#深度学习-Deep-Learning-入门——基本概念" class="headerlink" title="深度学习(Deep Learning)入门——基本概念"></a>深度学习(Deep Learning)入门——基本概念</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a><strong>引言</strong></h2><p>本文是该系列文章中的第一篇，旨在介绍深度学习基础概念、<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95&zhida_source=entity">优化算法</a>、 调参基本思路、正则化方式等，后续文章将关注深度学习在<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&zhida_source=entity">自然语言处理</a>、语音识别、和计算机视觉领域的应用。</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a><strong>基本概念</strong></h2><p>深度学习是为了解决表示学习难题而被提出的。本节，我们介绍这些深度学习相关的基本概念。</p><p><strong>表示学习（representation learning）</strong> 机器学习旨在自动地学到从数据的<strong>表示</strong>（representation）到数据的<strong>标记</strong>（label）的<strong>映射</strong>。随着机器学习算法的日趋成熟，人们发现，在某些领域（如图像、语音、文本等），如何从数据中<strong>提取合适的表示</strong>成为整个任务的瓶颈所在，而数据表示的好坏直接影响后续学习任务（所谓garbage in，garbage out）。与其依赖人类专家<strong>设计手工特征</strong>（难设计还不见得好用），表示学习希望能从数据中<strong>自动地</strong>学到从数据的原始形式到数据的表示之间的映射。</p><p><strong>深度学习（deep learning，DL）</strong> 表示学习的理想很丰满，但实际中人们发现从数据的原始形式<strong>直接学得</strong>数据表示这件事<strong>很难</strong>。深度学习是<strong>目前最成功的表示学习方法</strong>，因此，目前国际表示学习大会（ICLR）的绝大部分论文都是关于深度学习的。深度学习是把表示学习的任务划分成几个小目标，先从数据的原始形式中先学习<strong>比较低级</strong>的表示，再<strong>从低级</strong>表示<strong>学得比较高级</strong>的表示。这样，每个小目标比较容易达到，综合起来我们就完成表示学习的任务。这类似于算法设计思想中的<strong>分治法</strong>（<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=divide-and-conquer&zhida_source=entity">divide-and-conquer</a>）。</p><p><strong>深度神经网络（deep neural networks，DNN）</strong> 深度学习<strong>目前几乎唯一行之有效</strong>的实现形式。简单的说，深度神经网络就是<strong>很深的</strong>神经网络。我们利用<strong>网络中逐层对特征进行加工</strong>的特性，<strong>逐渐从低级特征提取高级</strong>特征。除了深度神经网络之外，有学者在探索其他深度学习的实现形式，比如深度森林。</p><p>深度神经网络目前的成功取决于三大推动因素。1. <strong>大数据</strong>。当数据量小时，很难从数据中学得合适的表示，而传统算法+<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B&zhida_source=entity">特征工程</a>往往能取得很好的效果；2. <strong>计算能力</strong>。<strong>大的数据</strong>和<strong>大的网络</strong>需要有足够的快的计算能力才能使得模型的应用成为可能。3. <strong>算法创新</strong>。现在很多<strong>算法设计</strong>关注在如何使网络更好地训练、更快地运行、取得更好的性能。</p><p><strong>多层感知机（multi-layer perceptrons，MLP）</strong> <strong>多层</strong>由<strong>全连接层</strong>组成的深度神经网络。<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA&zhida_source=entity">多层感知机</a>的<strong>最后一层</strong>全连接层实质上是一个<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8&zhida_source=entity">线性分类器</a>，而其他部分则是为这个线性分类器<strong>学习一个合适的数据表示</strong>，使倒数第二层的特征<strong>线性可分</strong>。</p><p>**激活函数（activation function）**神经网络的必要组成部分。如果没有激活函数，多次线性运算的堆叠仍然是一个线性运算，即不管用再多层实质只起到了一层神经网络的作用。一个好的激活函数应满足以下性质。1. <strong>不会饱和</strong>。sigmoid和tanh激活函数在两侧尾端会有饱和现象，这会使导数在这些区域接近零，从而阻碍网络的训练。2. <strong>零均值</strong>。ReLU<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=5&q=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&zhida_source=entity">激活函数</a>的输出均值不为零，这会影响网络的训练。3. <strong>容易计算</strong>。</p><p><strong>迁移学习（transfer learning）</strong> 深度学习下的迁移学习旨在<strong>利用源任务数据辅助</strong>目标任务数据下的学习。迁移学习适用于源任务数据比目标任务数据多，并且源任务中学习得到的低层特征可以帮助目标任务的学习的情形。在<strong>计算机视觉</strong>领域，最常用的<strong>源任务数据是ImageNet</strong>。对ImageNet预训练模型的<strong>利用</strong>通常有两种方式。1. <strong>固定特征提取器</strong>。用ImageNett预训练模型提取目标任务数据的高层特征。2. <strong>微调（fine-tuning）</strong>。以ImageNet预训练模型作为目标任务模型的<strong>初始化初始化权值</strong>，之后<strong>在目标任务数据上进行微调</strong>。</p><p><strong>多任务学习（multi-task learning）</strong> 与其针对每个任务训练一个小网络，深度学习下的多任务学习旨在训<strong>练一个大网络以同时完成全部任务</strong>。这些任务中用于提取<strong>低层特征</strong>的层是<strong>共享</strong>的，之后产生分支，各任务拥有各自的若干层用于完成其任务。多任务学习适用于多个任务共享低层特征，并且各个任务的数据很相似的情况。</p><p><strong>端到端学习（end-to-end learning）</strong> 深度学习下的端到端学习旨在通过一个深度神经网络<strong>直接学习从数据的原始形式到数据的标记的映射</strong>。端到端学习并不应该作为我们的一个追求目标，是否要采用端到端学习的一个重要考虑因素是：有没有足够的数据对应端到端的过程，以及我们有没有一些领域知识能够用于整个系统中的一些模块。</p><h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a><strong>优化算法</strong></h2><p>在<strong>网络结构</strong>确定之后，我们需要对网络的<strong>权值</strong>（weights）进行优化。本节，我们介绍优化深度神经网络的基本思想。</p><p><strong>梯度下降（<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=gradient+descent&zhida_source=entity">gradient descent</a>，GD）</strong> 想象你去野足但却迷了路，在漆黑的深夜你一个人被困住山谷中，你<strong>知道谷底是出口</strong>但是天太黑了根本看不清楚路。于是你确定采取一个<strong>贪心</strong>(greedy)算法：先试探在当前位置往<strong>哪个方向走下降最快（即梯度方向）</strong>，再朝着这个方向走<strong>一小步</strong>，重复这个过程直到你到达谷底。这就是梯度下降的基本思想。</p><p><a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95&zhida_source=entity">梯度下降算法</a>的性能大致取决于三个因素。1. <strong>初始位置</strong>。如果你初始位置就离谷底很近，自然很容易走到谷底。2. <strong>山谷地形</strong>。如果山谷是“九曲十八弯”，很有可能你在里面绕半天都绕不出来。3. <strong>步长</strong>。你每步迈多大，当你步子迈太小，很可能你走半天也没走多远，而当你步子迈太大，一不小心就容易撞到旁边的悬崖峭壁，或者错过了谷底。</p><p><strong>误差反向传播（error back-propagation，BP）</strong> 结合微积分中<strong>链式法则</strong>和算法设计中<strong>动态规划思想</strong>用于计算梯度。 直接用纸笔推导出中间某一层的梯度的数学表达式是很困难的，但<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99&zhida_source=entity">链式法则</a>告诉我们，一旦我们知道<strong>后一层的梯度</strong>，再结合后一层<strong>对当前层的导数</strong>，我们就可以得到<strong>当前层的梯度</strong>。<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92&zhida_source=entity">动态规划</a>是一个<strong>高效计算所有梯度</strong>的实现技巧，通过由<strong>高层往低层逐层计算梯度</strong>，避免了对高层梯度的重复计算。</p><p><strong>滑动平均（moving average）</strong> 要前进的方向<strong>不再由当前梯度方向完全</strong>决定，而是最近<strong>几次梯度方向的滑动平均</strong>。利用滑动平均思想的优化算法有带动量（momentum）的SGD、<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=Nesterov%E5%8A%A8%E9%87%8F&zhida_source=entity">Nesterov动量</a>、Adam（ADAptive Momentum estimation）等。</p><p><strong><a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E8%87%AA%E9%80%82%E5%BA%94%E6%AD%A5%E9%95%BF&zhida_source=entity">自适应步长</a></strong> 自适应地确定权值<strong>每一维的步长</strong>。当某一维持续<strong>震荡</strong>时，我们希望这一维的步长小一些；当某一维<strong>一直沿着相同</strong>的方向前进时，我们希望这一维的步长大一些。利用自适应步长思想的优化算法有AdaGrad、RMSProp、Adam等。</p><p><strong>学习率衰减</strong> 当开始训练时，<strong>较大</strong>的学习率可以使你在参数空间有<strong>更大范围的探索</strong>；当优化接近<strong>收敛</strong>时，我们需要<strong>小一些的学习率</strong>使权值更接近<strong>局部最优</strong>点。</p><p><strong>深度神经网络优化的困难</strong> 有学者指出，在很高维的空间中，局部最优是比较少的，而大部分梯度为零的点是<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E9%9E%8D%E7%82%B9&zhida_source=entity">鞍点</a>。平原区域的鞍点会使梯度在<strong>很长一段时间内都接近零</strong>，这会使得拖慢优化过程。</p><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a><strong>初始化</strong></h2><p><strong>权值初始化对网络优化至关重要</strong>。早年深度神经网络无法有效训练的一个重要原因就是早期人们对初始化不太重视。本节，我们介绍几个适用于深度神经网络的初始化方法。</p><p><strong>初始化的基本思想</strong> <strong>方差不变</strong>，即设法对权值进行初始化，使得各层神经元的<strong>方差保持不变</strong>。</p><p><strong>Xavier初始化</strong> 从高斯分布或<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83&zhida_source=entity">均匀分布</a>中对权值进行采样，使得<strong>权值的方差是1&#x2F;n</strong>，其中n是输入神经元的个数。该推导假设<strong>激活函数是线性</strong>的。</p><p><strong>He初始化&#x2F;MSRA初始化</strong> 从高斯分布或均匀分布中对权值进行采样，使得权值的<strong>方差是2&#x2F;n</strong>。该推导假设<strong>激活函数是ReLU</strong>。因为ReLU<strong>会将小于0的神经元置零</strong>，大致上会使一半的神经元置零，所以为了弥补丢失的这部分信息，<strong>方差要乘以2</strong>。</p><p><strong>批量规范化（batch-normalization，BN）</strong> 每层<strong>显式</strong>地对神经元的激活值做<strong>规范化</strong>，使其具有零均值和单位方差。批量规范化使<strong>激活值的分布固定</strong>下来，这样可以使各层<strong>更加独立地进行学习</strong>。批量规范化可以使得网络<strong>对初始化和学习率不太敏感</strong>。此外，批量规范化<strong>有些许正则化</strong>的作用，但不要用其作为正则化手段。</p><h2 id="偏差-方差（bias-variance）"><a href="#偏差-方差（bias-variance）" class="headerlink" title="偏差&#x2F;方差（bias&#x2F;variance）"></a><strong>偏差&#x2F;方差（bias&#x2F;variance）</strong></h2><p>优化完成后，你发现网络的表现不尽如人意，这时诊断网络处于高偏差&#x2F;<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E9%AB%98%E6%96%B9%E5%B7%AE&zhida_source=entity">高方差</a>状态是对你下一步<strong>调参方向的重要指导</strong>。与经典机器学习算法有所不同，因为深度神经网络通常要<strong>处理非常高维</strong>的特征，所以网络可能<strong>同时处于高偏差&#x2F;高方差</strong>的状态，即在<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4&zhida_source=entity">特征空间</a>的一些区域网络处于高偏差，而在另一些区域处于高方差。本节，我们对偏差&#x2F;方差作一简要介绍。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/v2-2d443d459279b52f130fa3096c7e2673_1440w.jpg" alt="img"></p><p><strong>偏差</strong> 偏差度量了网络的<strong>训练集误差</strong>和<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%AF%AF%E5%B7%AE&zhida_source=entity"><strong>贝叶斯误差</strong></a>（即能达到的<strong>最优误差</strong>）的差距。高偏差的网络有很高的训练集误差，说明网络对数据中隐含的一般规律还没有学好。当网络处于高偏差时，通常有以下几种解决方案。<strong>1. 训练更大的网络</strong>。网络<strong>越大</strong>，对数据<strong>潜在</strong>规律的<strong>拟合能力越强</strong>。<strong>2. 更多的训练轮数</strong>。通常训练<strong>时间越久</strong>，对训练集的<strong>拟合能力越强</strong>。<strong>3. 改变网络结构</strong>。不同的网络<strong>结构</strong>对训练集的<strong>拟合能力</strong>有所不同。</p><p><strong>方差</strong> 方差度量了网络的<strong>验证集误差</strong>和<strong>训练集误差</strong>的差距。<strong>高</strong>方差的网络<strong>学习能力太强</strong>，把训练集中<strong>自身独有</strong>的一些特点<strong>也当作一般规律</strong>学得，使网络<strong>不能很好的泛化</strong>（generalize）到<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E9%AA%8C%E8%AF%81%E9%9B%86&zhida_source=entity">验证集</a>。当网络处于高方差时，通常有以下几种解决方案。<strong>1. 更多的数据</strong>。这是对高方差问题<strong>最行之有效</strong>的解决方案。<strong>2. 正则化</strong>。<strong>3. 改变网络结构</strong>。不同的网络结构对方差也会有影响。</p><h2 id="正则化（regularization）"><a href="#正则化（regularization）" class="headerlink" title="正则化（regularization）"></a><strong>正则化（regularization）</strong></h2><p>正则化是<strong>解决高方差</strong>问题的重要方案之一。本节，我们将对常用<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95&zhida_source=entity">正则化方法</a>做一介绍。</p><p><strong><a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=8&q=%E6%AD%A3%E5%88%99%E5%8C%96&zhida_source=entity">正则化</a>的基本思想</strong> 正则化的基本思想是使网络的<strong>有效</strong>大小<strong>变小</strong>。网络变小之后，网络的<strong>拟合能力随之降低</strong>，这会使网络不容易<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E8%BF%87%E6%8B%9F%E5%90%88&zhida_source=entity">过拟合</a>到训练集。</p><p><strong>L2正则化</strong> L2正则化倾向于使网络的<strong>权值接近0</strong>。这会使<strong>前</strong>一层神经元对<strong>后</strong>一层神经元的<strong>影响降低</strong>，使网络<strong>变得简单</strong>，降低网络的有效大小，降低网络的拟合能力。L2正则化实质上是<strong>对权值做线性衰减</strong>，所以<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=4&q=L2%E6%AD%A3%E5%88%99%E5%8C%96&zhida_source=entity">L2正则化</a>也被称为权值衰减（weight decay）。</p><p><strong>随机失活（dropout）</strong> 在训练时，随机失活随机选择一部分神经元，使其置零，不参与本次优化迭代。随机失活减少了每次参与优化迭代的神经元数目，使网络的有效大小变小。随机失活的作用有两点。<strong>1. 降低神经元之间耦合</strong>。因为神经元<strong>会被随机置零</strong>，所以每个神经元不能依赖于其他神经元，这会迫使每个神经元自身要能提取到合适的特征。<strong>2. 网络集成</strong>。随机失活可以看作在训练时<strong>每次迭代定义出一个新的网络</strong>，这些网络共享权值。在测试时的网络是这些网络的集成。</p><p><strong>数据扩充（<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=data+augmentation&zhida_source=entity">data augmentation</a>）</strong> 这实质是获得更多数据的方法。当收集数据很昂贵，或者我们拿到的是第二手数据，数据就这么多时，我们<strong>从现有数据中扩充</strong>生成更多数据，用<strong>生成的“伪造”<strong>数据当作更多的真实数据进行训练。以图像数据做分类任务为例，把图像水平翻转、移动一定位置、旋转一定角度、或做一点色彩变化等，这些操作通常都不会影响这幅图像对应的标记。并且你可以尝试</strong>这些操作的组合</strong>，理论上讲，你可以通过这些组合得到无穷多的训练样本。</p><p><strong>早停（early stopping）</strong> 随着训练的进行，当你发现验证集误差不再变化或者开始上升时，<strong>提前停止</strong>训练。</p><h2 id="调参技巧"><a href="#调参技巧" class="headerlink" title="调参技巧"></a><strong>调参技巧</strong></h2><p>深度神经网络涉及<strong>很多的超参数</strong>，如<strong>学习率大小</strong>、<strong>L2正则化系数</strong>、<strong>动量大小</strong>、<strong>批量大小</strong>、<strong>隐层神经元数目</strong>、<strong>层数</strong>、<strong>学习率衰减率</strong>等。本节，我们介绍调参的基本技巧。</p><p><strong>随机搜索</strong> 由于你<strong>事先并不知道</strong>哪些超参数对你的问题更重要，因此随机搜索通常是比<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2&zhida_source=entity">网格搜索</a>（grid search）更有效的<strong>调参策略</strong>。</p><p><strong>对数空间搜索</strong> 对于<strong>隐层神经元数目</strong>和<strong>层数</strong>，可以直接<strong>从均匀分布采样</strong>进行搜索。而对于<strong>学习率</strong>、L2<a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=2&q=%E6%AD%A3%E5%88%99%E5%8C%96%E7%B3%BB%E6%95%B0&zhida_source=entity">正则化系数</a>、和<strong>动量</strong>，在<strong>对数空间搜索</strong>更加有效。例如：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">learning_rate = 10 ** random.uniform(-5, -1)  # From 1e-5 to 1e-1</span><br><span class="line">weight_decay = 10 ** random.uniform(-7, -1)   # From 1e-7 to 1e-1</span><br><span class="line">momentum = 1 - 10 ** random.uniform(-3, -1)   # From 0.9 to 0.999</span><br></pre></td></tr></table></figure><h2 id="实现技巧"><a href="#实现技巧" class="headerlink" title="实现技巧"></a><strong>实现技巧</strong></h2><p><strong><a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E5%9B%BE%E5%BD%A2%E5%A4%84%E7%90%86%E5%8D%95%E5%85%83&zhida_source=entity">图形处理单元</a>（graphics processing units, GPU）</strong> 深度神经网络的<strong>高效实现工具</strong>。简单来说，CPU擅长<strong>串行、复杂</strong>的运算，而GPU擅长<strong>并行、简单</strong>的运算。深度神经网络中的<strong>矩阵运算都十分简单</strong>，但计算<strong>量巨大</strong>。因此，GPU无疑具有非常强大的优势。</p><p><strong><a href="https://zhida.zhihu.com/search?content_id=4813287&content_type=Article&match_order=1&q=%E5%90%91%E9%87%8F%E5%8C%96&zhida_source=entity">向量化</a>（vectorization）</strong> 代码<strong>提速</strong>的<strong>基本技巧</strong>。能<strong>少写</strong>一个for循环就少写一个，能<strong>少做</strong>一次矩阵运算就少做一次。实质是尽量将<strong>多次标量运算转化为一次向量</strong>运算；将<strong>多次向量</strong>运算转化为<strong>一次矩阵</strong>运算。因为矩阵运算<strong>可以并行</strong>，这将会比多次单独运算快很多。</p><h1 id="计算机视觉基础"><a href="#计算机视觉基础" class="headerlink" title="计算机视觉基础"></a>计算机视觉基础</h1><h2 id="一、计算机视觉概述"><a href="#一、计算机视觉概述" class="headerlink" title="一、计算机视觉概述"></a>一、计算机视觉概述</h2><p>1、计算机视觉的背景知识<br>对计算机视觉的第一印象：<strong>用计算机代替人类的眼睛，模仿人类视觉去完成各项任务</strong>。</p><p>计算机视觉（Computer Vision） 是一门研究<strong>如何使机器“看</strong>”的科学，也可以看作是研究如何使人工系统<strong>从图像或多维</strong>数据中**“感知”**的科学。</p><p>终极目标：计算机视觉成为机器认知世界的基础，终极目的是使得计算机能够像人一样“看懂世界”。<br>2、计算机视觉与人类视觉的关系</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/b4c77b970b584ed38304b329b367e9c9.png" alt="在这里插入图片描述"></p><h2 id="二、计算机视觉的基本原理"><a href="#二、计算机视觉的基本原理" class="headerlink" title="二、计算机视觉的基本原理"></a>二、计算机视觉的基本原理</h2><p>1、计算机视觉的处理对象</p><h3 id="数字图像"><a href="#数字图像" class="headerlink" title="数字图像"></a>数字图像</h3><p>又称为数码图像或数位图像；<br>是用一个<strong>数字矩阵</strong>来表达客观物体的图像；<br>是由模拟图像数字化得到的；<br>是一个<strong>离散采样点</strong>的集合，每个点具<strong>有其各自的属性</strong>；<br>是<strong>以像素为基本元素</strong>的图像；<br>可以用数字计算机或数字电路存储和处理的图像。<br>数字图像处理包括的内容:</p><p>图像变换；<br>图像增强；<br>图像恢复；<br>图像压缩编码；<br>图像分割；<br>图像分析与描述；<br>图像的识别分类。<br>2、计算机视觉的工作原理<br>图像数字化的两个过程：</p><p><strong>采样</strong>是将空间上连续的图像<strong>变换成离散的点</strong>，采样<strong>频率越高</strong>，还原的图像<strong>越真实</strong>。<br><strong>量化</strong>是将采样出来的像素点<strong>转换成离散的数量值，<strong>一幅数字图像中不同灰度值的个数称为</strong>灰度等级</strong>，级数<strong>越大</strong>，图像<strong>越清晰</strong>。<br>计算机视觉的基础工作原理：<br><strong>①构造多层神经网络 –&gt; ②较低层识别初级的图像特征 –&gt; ③若干底层特征组成更上一层特征 –&gt; ④通过多个层级的组合 –&gt; ⑤最终在顶层做出分类</strong></p><p>3、计算机视觉的关键技术<br><strong>图像分类</strong>：给定一组各自被标记为单一类别的图像，对一组新的测试图像的类别进行预测，并测量预测的准确性结果。</p><p><strong>目标检测</strong>：给定一张图像，让计算机找出其中所有目标的位置，并给出每个目标的具体类别。</p><p><strong>语义分割</strong>：将整个图像分成像素组，然后对像素组进行标记和分类；语义分割是在语义上理解图中每个像素是什么，还须确定每个物体的边界。如一张“人驾驶摩托车行驶在林间小道上”的图片</p><p><strong>实例分割</strong>：在语义分割的基础上进行，将多个重叠物体和不同背景的复杂景象进行分类；同时确定对象的边界、差异和彼此之间的关系。</p><p><strong>视频分类</strong>：分类的对象是由多帧图像构成的、包含语音数据、运动信息等的视频对象需要理解每帧图像包含内容，还需要知道上下文关联信息。</p><p><strong>人体关键点检测</strong>：通过人体关键节点的组合和追踪来识别人的运动和行为对于描述人体姿态，预测人体行为至关重要。</p><p><strong>场景文字识别</strong>：在图像背景复杂、分辨率低下、字体多样、分布随意等情况下，将图像信息转化为文字序列的过程。</p><p><strong>目标跟踪任务</strong>：在特定场景跟踪某一个或多个特定感兴趣对象的过程。</p><p>三、图像分类基础</p><h4 id="1、图像分类的定义"><a href="#1、图像分类的定义" class="headerlink" title="1、图像分类的定义"></a>1、图像分类的定义</h4><p><code>图像分类的定义</code>：图像分类的核心是从给定的分类集合中给图像<strong>分配一个标签</strong>。 </p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/7873a95f36a0446aafe811ef71deaf8b.png" alt="在这里插入图片描述"></p><p>图像分类：<br>1、根据大类、小类加标签<br>2、可以多单个或多个标签<br>3、不同的标签粒度和个数会形成不同的分类任务</p><p>2、图像分类的类别<br>单标签与多标签分类的区别<br>单标签：数据样本<strong>属于一个大类</strong>的；数据进行分类后用可以用一个值代表；单标签内有二分类（两个选项）和多分类（多个选项）；例子：单标签三个样本的二分类整形（0&#x2F;1）输出为：[0,1,0]。</p><p>多标签数据样本可以<strong>划分到几个大的不冲突主题类别</strong>中；在大主题中分别可以进行二分类和多分类问题；例子：多标签(假设为两个标签)三个样本的二分类整形输出为：[[0,1], [0,0],[1,1]]。</p><p>跨物种语义级别的图像分类定义</p><p>在不同物种层次上识别不同类别的对象，如猫狗分类；<br>各个类别之间属于<strong>不同</strong>的物种或大类，往往具有 <strong>较大</strong>的类间方差，而<strong>类内</strong>具有 <strong>较小</strong>的类内方差；<br>多类别图像分类由传统的特征提取方法转到数据驱动的深度学习方向来，取得了较大进展。<br> 子类细粒度图像分类的定义</p><p>子类细粒度分类相较于跨物种图像分类难度更大；<br>是一个大类中的子类的分类，如不同鸟的分类等；<br>在区分出基本类别的基础上，进行更精细的子类划分；<br>由于图像之间具有更加相似的外观和特征，受采集过程中存在干扰影响，导致数据呈现类间差异性大，类内间差异小，分类难度也更高。<img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/9592d4d1d8f444ca8dcf3ac1bd58dcc5.png" alt="在这里插入图片描述"></p><p><strong>多标签图像分类的定义</strong></p><ul><li>给每个样本一系列的目标标签，表示的是样本各属性且不相互排斥的，预测出一个概念集合；</li><li>标签数量较大且复杂；</li><li>标签的标准<strong>很难统一</strong>，且往往类标之间相互依赖并不独立；</li><li>标注的标签并<strong>不能完美覆盖</strong>所有概念面；</li><li>标签往往较短语义少，理解困难。</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/e3c61b5dac6c4e80a2e02d907618f293.png" alt="在这里插入图片描述"></p><p>图像分类会遇到的问题<br>1、一张图片包含的<strong>信息内容太多</strong>，不好分类<br>2、有些类别的<strong>图片太少</strong>，比如罕见害虫</p><p>3、图像分类遇到的挑战<br>虽然图像分类在大赛上的正确率已经接近极限，但在实际工程应用中，面临诸多挑战。如：类别不均衡；数据集小；巨大的类内差异；实际应用环境复杂</p><p>4、图像分类的常用数据集与网络<br>图像分类的常用数据集：CIFAR-10</p><p>介绍</p><p>CIFAR-10 ：一个用于识别普适物体的<strong>小型</strong>图像数据集；<br>包含6万张大小为32 x 32的彩色图像；<br>共有10个类，每类有6000张图；<br>共5万张图组成训练集合，训练集合中每一类均等且有5000张图；<br>共1万张图组成测试集合，测试集合中每一类均等且有1000张图；<br>10个类别：飞机（ airplane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）；<br><strong>类</strong>是<strong>完全互斥</strong>的：在⼀个类别中出现的图⽚不会出现在其它类中。使用的相关神经网络：LeNet-5、AlexNet<br>LeNet-5：是<strong>最早的卷积神经网络</strong>之一; 1998年第一次将LeNet-5应用到图像分类上，在手写数字识别任务中取得了巨大成功; LeNet-5通过连续使用卷积和池化层的组合提取图像特征,总共5层：3层卷积和2层全连接，池化层未计入层数; LeNet-5是卷积神经网络的开篇大作，完成了卷积神经网络从无到有的突破。<br>AlexNet：AlexNet将LeNet的思想发扬光大，把CNN的基本原理应用到了很深很宽的网络中。成功使用ReLU作为CNN的激活函数，并验证其效果优异；训练时使用数据增强和Dropout<strong>随机忽略</strong>一部分神经元，以避免模型过拟合，提升泛化能力；在CNN中使用重叠的最大池化，提升了特征的丰富性；提出了LRN层，增强了模型的泛化能力。</p><p>5、图像分类的典型应用<br>图像分类在图片搜索引擎中的应用</p><p>应用图像分类技术可以开发各种图片搜索引擎；<br>图片搜索引擎能通过用户上传图片，应用图像分类技术，识别出图片的内容并进行分类；<br>搜索互联网上与这张图片相同或相似信息的其他图片资源进行校对和匹配，识别图片的内容并提供相关信息。<br> 图像分类在垃圾分类中的应用-智能环卫</p><p>为了破解传统分类投放模式可能存在的乱扔垃圾等问题，可在传统垃圾分类投放站点部署摄像头进行智能化改造；<br>阿里云提出“智能环卫”产品，提供垃圾分类投放点AI智能检测分析功能；<br>有效针对垃圾桶内的未破袋垃圾包、残余垃圾袋等进行检测和识别，检测效率高，真实环境下检测准确率超过95%。<br>四、目标检测基础<br>目标检测：框出来，用坐标表示</p><p>1、目标检测的定义<br>目标检测的定义：目标检测就是识别图中<strong>有哪些物体</strong>，确定他们的类别并标出各自在图中的位置。目标检测模型读取该图片；寻找识别出图中的物体目标，对其进行定位，框起和标注。</p><p>图像分类与目标检测的区别<br>图像分类：<strong>整幅图像</strong>经过识别后被分类为单一的标签。<br>目标检测：除了识别出图像中的<strong>一个或多个</strong>目标，还需要找出目标在图像中的<strong>具体位置</strong>。</p><p>2、目标检测的评估指标<br>交并比：IoU</p><p>真实边界框：训练集中，<strong>人工标注的</strong>物体边界框；<br>预测边界框：模型预测到的物体边界框；<br>交并比：在分子项中，是真实边界框和预测边界框<strong>重叠的区域</strong>（Intersection）。分母是一个并集（Union），或者更简单地说，是由预测边界框和真实边界框所包括的区域。两者相除就得到了最终的得分<br>精确度（Precision）指目标检测模型判断该图片为正类，该图片<strong>确实是正类的概率</strong>；</p><p>和召回率（Recall）是指的是一个分类器能把<strong>所有的正类都找出来</strong>的能力；</p><p>平均精度值：mAP ：mAP，mean Average Precision, 即<strong>各类别平均</strong>精度均值；mAP是把每个类别的AP都单独拿出来，然后计算所有类别AP的平均值，代表着对检测到的目标平均精度的一个综合评价。每一个类别都可以根据Recall和Precision绘制一条曲线，那么<strong>AP就是该曲线下的面积</strong>，而mAP则是多个类别AP的平均值，这个值介于0到1之间。mAP是目标检测算法里最重要的一个评估指标。</p><p>3、目标检测遇到的挑战<br>目标<strong>数</strong>量问题：在图片输入模型前不清楚图片中有多少个目标，无法知道正确的输出数量。<br>目标<strong>大小</strong>问题：目标的大小不一致,甚至一些目标仅有十几个像素大小，占原始图像中非常小的比例。<br>如何建模：需要同时处理目标定位以及目标物体识别分类这两个问题。</p><p>4、目标检测的常用数据集与网络<br>目标检测的常用数据集：PASCAL VOC</p><p>PASCAL VOC ：一个常用于目标检测的小型图像数据集；<br>包含11530张彩色图像，标定了27450个目标识别区域；<br>从初始4个类发展成最终的20个类；<br>在整个数据集中，平均每张图片有2.4个目标；<br>20个类别：<br>动物：人、鸟、猫、狗、牛、马、羊；<br>运载工具：飞机、自行车、船、巴士、汽车、摩托车、火车；<br>物品：瓶子、椅子、餐桌、盆栽、沙发、电视机。</p><p>使用的相关神经网络：CenterNet</p><p>CenterNet结构优雅简单，直接检测目标的中心点和大小;<br>CenterNet把目标检测任务看作三个部分：寻找物体的中心点；计算物体中心点的偏移量；分析物体的大小;<br>CenterNet检测速度和精度相比于先前的框架都有明显且可观的提高，尤其是与著名的目标检测网络YOLOv3作比较，在相同速度的条件下，CenterNet的精度比YOLOv3提高了大约4个点。<br>5、目标检测的典型应用<br>智慧交通是目标检测的一个重要应用领域，主要包括如下场景：</p><p>检测各种交通异常事件，如车辆占用应急车道、车辆驾驶员的驾驶行为等；<br>第一时间将异常事件上报给交管部门，提高处理效率。<br>目标检测在智慧交通中的应用-智慧眼</p><p>通过目标检测算法，对道路视频图像进行分析；<br>根据分析车流量，调整红绿灯配时策略，提升交通通行能力。<br> 五、图像分割基础<br>“抠图软件”的操作流程？</p><p>1、选中图片中的目标主体<br>2、对主体的边界进行分割<br>3、主体与背景分离，突出显示主体<br>1、图像分割的定义<br>图像分割就是把图像分成若干个特定的、具有独特性质的区域并提出感兴趣目标的技术和过程；</p><p>图像分割包括：语义分割、实例分割和全景分割。</p><p>图像作为分割算法的输入，输出一组区域；</p><p>区域可以表示为一种掩码（灰度或颜色），其中每个部分被分配一个唯一的<strong>颜色或灰度值</strong>来代表它</p><p>2、图像分割的类别<br>语义分割的定义：</p><p>语义分割是在<strong>像素级别上</strong>的分类，属于<strong>同一类的像素</strong>都要被归为一类；<br>语义分割是从像素级别来理解图像的。<br>实例分割的定义：</p><p>实例分割比语义分割更进一步；<br>对于语义分割来说，只要将所有同类别（猫、狗）的像素都归为一类；<br>实例分割还要在具体类别（猫、狗）像素的基础上<strong>区分开不同的实例</strong>（短毛猫、虎斑猫、贵宾犬、柯基犬）。<br>全景分割的定义</p><p>全景分割是<strong>语义和实例分割的相结合</strong>；<br>每个像素都被分配一个类（比如：狗），如果一个类有多个实例，则可知道该像素属于该类的哪个实例（贵宾犬&#x2F;柯基犬）。<br>3、图像分割遇到的挑战<br>分割<strong>边缘不准</strong>：因为相邻临的像素对应感受野内的图像信息太过相似导致。</p><p>样本<strong>质量不一</strong>：样本中的目标物体具有多姿态、多视角问题，会出现物体之间的遮挡和重叠；<br>受场景光照影响，样本质量参差不齐。</p><p>标注成本高：对于数据样本的标注成本非常高，而且标注质量难以保证不含有噪声。</p><p>4、图像分割的常用数据集与网络<br>图像分割的常用数据集：COCO</p><p>COCO：一个常用于图像分割的大型图像数据集；<br>包含33万张彩色图像，标定了50万个目标实例；<br>具有80个目标类、91个物品类以及25万个人物关键点标注；<br>每张图片包含5个描述；<br>每一类的图像多，利于提升识别更多类别位于特定场景的能力；<br>类别包括： person(人) 、bicycle(自行车) 、car(汽车) 、motorbike(摩托车) 、aeroplane(飞机) 、bus(公共汽车) 、train(火车)、truck(卡车) 、boat(船) 、traffic light(信号灯) 、fire hydrant(消防栓) 、stop sign(停车标志) 、parking meter(停车计费器) 、bench(长凳) 、bird(鸟) 、cat(猫) 、dog(狗) 、horse(马) 、sheep(羊) 、cow(牛) 等等。<br>使用的相关神经网络：FCN</p><p><strong>FCN全卷积神经网络</strong>是图像分割的基础网络;<br>全卷积神经网络，顾名思义网络里的所有层<strong>都是卷积层</strong>；<br>卷积神经网络卷到最后特征图尺寸和分辨率越来越小，不适合做图像分割，为解决此问题FCN引入- 上采样的方法，卷积完之后再上采样到大尺寸图；<br>为避免层数不断叠加后原图的信息丢失得比较多，FCN引入一个跳层结构，把前面的层特征引过来- 进行叠加；<br>FCN实现了端到端的网络<br>端到端学习是一种解决问题的思路，与之对应的是多步骤解决问题，也就是将一个问题拆分为多个步骤分步解决，而端到端是由输入端的数据直接得到输出端的结果。</p><p>5、图像分割的典型应用<br>图像分割在抠图软件中的应用</p><p>应用图像分割技术可以开发各种抠图软件；<br>用户在软件平台上传图片，应用图像分割技术，分辨出图片具有独特特征的区域并进行边缘识别分割；<br>返回给用户经过图像分割处理的结果图片。<br>图像分割在智能证件照中的应用</p><p>基于智能视觉生产的人像分割能力，阿里云为用户提供证件照的智能制作与编辑能力；<br>自动从上传的生活照中分割出人像区域，精确到像素级别的分割保证证件照的专业性与准确性，将生活照完美转换成专业证件照。</p><h1 id="NLP-自然语言处理-—-NLP入门指南"><a href="#NLP-自然语言处理-—-NLP入门指南" class="headerlink" title="[NLP] 自然语言处理 — NLP入门指南"></a>[NLP] 自然语言处理 — NLP入门指南</h1><p><strong>NLP的全称是Natuarl Language Processing</strong>，中文意思是自然语言处理，是人工智能领域的一个重要方向</p><p>自然语言处理（NLP）的一个最伟大的方面是跨越多个领域的计算研究，从人工智能到计算语言学的多个计算研究领域都在研究计算机与人类语言之间的相互作用。它主要关注计算机如何准确并快速地处理大量的自然语言语料库。什么是自然语言语料库？它是用现实世界语言表达的语言学习，是从文本和语言与另一种语言的关系中理解一组抽象规则的综合方法。</p><p>人类语言是抽象的信息符号，其中蕴含着丰富的语义信息，人类可以很轻松地理解其中的含义。而<strong>计算机只能处理数值化的信息</strong>，无法直接理解人类语言，所以需要将人类语言进行<strong>数值化转换</strong>。不仅如此，人类间的沟通交流是有上下文信息的，这对于计算机也是巨大的挑战。</p><p>我们首先来看看NLP的任务类型，如下图所示：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/158d23180094de856f74843b80f131a0.png" alt="img"></p><p> 主要划分为了四大类：</p><p>类别到序列<br>序列到类别<br>同步的序列到序列<br>异步的序列到序列<br>其中“类别”可以理解为是<strong>标签或者分类</strong>，而“序列”可以理解为是<strong>一段文本或者一个数组</strong>。简单概况NLP的任务就是从<strong>一种数据类型转换成另一种数据类型</strong>的过程，这与绝大多数的机器学习模型相同或者类似，所以掌握了NLP的<strong>技术栈</strong>就等于掌握了机器学习的技术栈。</p><p>传统方式和深度学习方式 NLP 对比</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1393176ebd38c6650f9d65f2f7685e65.png" alt="img"></p><h2 id="NLP的预处理"><a href="#NLP的预处理" class="headerlink" title="NLP的预处理"></a>NLP的预处理</h2><p>为了能够完成上述的NLP任务，我们需要一些预处理，是NLP任务的基本流程。预处理包括：收集语料库、文本清洗、分词、去掉停用词（可选）、标准化和特征提取等。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/f5471c37eaf151e4bb48c07e1935d5d2.png" alt="img"></p><p>图中红色的部分就是NLP任务的预处理流程，有别于其它机器学习任务的流程</p><p><strong>英文 NLP 语料预处理的 6 个步骤</strong></p><ol><li><a href="https://easyai.tech/ai-definition/tokenization/">分词 – Tokenization</a></li><li><a href="https://easyai.tech/ai-definition/stemming-lemmatisation/">词干提取</a> – <a href="https://easyai.tech/ai-definition/stemming-lemmatisation/">Stemming</a></li><li><a href="https://easyai.tech/ai-definition/stemming-lemmatisation/">词形还原</a> – Lemmatization</li><li><a href="https://easyai.tech/ai-definition/part-of-speech/">词性标注 – Parts of Speech</a></li><li><a href="https://easyai.tech/ai-definition/ner/">命名实体识别 – NER</a></li><li>分块 – Chunking</li></ol><p><strong>中文 NLP 语料预处理的 4 个步骤</strong></p><ol><li><a href="https://easyai.tech/ai-definition/tokenization/">中文分词 – Chinese Word Segmentation</a></li><li><a href="https://easyai.tech/ai-definition/part-of-speech/">词性标注 – Parts of Speech</a></li><li><a href="https://easyai.tech/ai-definition/ner/">命名实体识别 – NER</a></li><li>去除停用词</li></ol><h2 id="第1步：收集您的数据—语料库"><a href="#第1步：收集您的数据—语料库" class="headerlink" title="第1步：收集您的数据—语料库"></a>第1步：收集您的数据—语料库</h2><p>对于NLP任务来说，没有大量高质量的语料，就是巧妇难为无米之炊，是无法工作的。</p><p>而获取语料的途径有很多种，最常见的方式就是<strong>直接下载开源</strong>的语料库，如：维基百科的语料库。</p><p>但这样<strong>开源</strong>的语料库一般都<strong>无法满足业务的个性化</strong>需要，所以就需要<strong>自己动手开发爬虫</strong>去<strong>抓取特定</strong>的内容，这也是一种获取语料库的途径。当然，每家互联网公司根据自身的业务，也都会有大量的语料数据，如：用户评论、电子书、商品描述等等，都是很好的语料库。</p><p>示例数据源</p><p>每个机器学习问题都从数据开始，例如电子邮件，帖子或推文列表。常见的文字信息来源包括：</p><p>产品评论（在亚马逊，Yelp和各种应用商店）<br>用户生成的内容（推文，Facebook帖子，StackOverflow问题）<br>故障排除（客户请求，支持服务单，聊天记录）<br>现在，数据对于互联网公司来说就是<strong>石油</strong>，其中蕴含着巨大的商业价值。所以，小伙伴们在日常工作中一定要养成收集数据的习惯，遇到<strong>好的语料库一定要记得备份</strong>（当然是在合理合法的条件下），它将会对你解决问题提供巨大的帮助。</p><p>第2步：清理数据 — 文本清洗<br>我们遵循的首要规则是：“您的模型将永远与您的数据一样好。”</p><p>数据科学家的关键技能之一是了解<strong>下一步是应该对模型还是数据</strong>进行处理。一个好的经验法则是<strong>首先查看数据然后进行清理</strong>。一个<strong>干净的数据集</strong>将允许模型学习有意义的功能，而不是过度匹配无关的噪音。</p><p>我们通过不同的途径获取到了想要的语料库之后，接下来就需要对其进行清洗。因为很多的语料数据是无法直接使用的，其中包含了大量的无用符号、特殊的文本结构。</p><p>数据类型分为：</p><p><strong>结构化</strong>数据：关系型数据、json等<br><strong>半结构化</strong>数据：XML、HTML等<br><strong>非结构化****数据：Word、PDF、文本、日志等<br>需要将原始的语料数据</strong>转化成易于处理<strong>的格式，一般在处理HTML、XML时，会使用Python的</strong>lxml库**，功能非常丰富且易于使用。对一些日志或者纯文本的数据，我们可以使用<strong>正则表达式</strong>进行处理。</p><p>正则表达式是使用单个字符串来描述、匹配一系列符合某个句法规则的字符串。Python的示例代码如下：</p> <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment"># 定义中文字符的正则表达式</span></span><br><span class="line">re_han_default = re.<span class="built_in">compile</span>(<span class="string">&quot;([\u4E00-\u9FD5]+)&quot;</span>, re.U)</span><br><span class="line">sentence = <span class="string">&quot;我/爱/自/然/语/言/处/理&quot;</span></span><br><span class="line"><span class="comment"># 根据正则表达式进行切分</span></span><br><span class="line">blocks= re_han_default.split(sentence)</span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> blocks:</span><br><span class="line">    <span class="comment"># 校验单个字符是否符合正则表达式</span></span><br><span class="line">    <span class="keyword">if</span> blk <span class="keyword">and</span> re_han_default.<span class="keyword">match</span>(blk):</span><br><span class="line">        <span class="built_in">print</span>(blk)</span><br></pre></td></tr></table></figure><p>除了上述的内容之外，我们还需要注意中文的编码问题，在windows平台下中文的默认编码是<strong>GBK（gb2312）</strong>，而在linux平台下中文的默认编码是<strong>UTF-8</strong>。在执行NLP任务之前，我们需要统一不同来源语料的编码，避免各种莫名其妙的问题。</p><p>如果大家事前无法判断语料的编码，那么我推荐大家可以使用Python的<strong>chardet库来检测编码</strong>，简单易用。既支持命令行：chardetect somefile，也支持代码开发。</p><p>以下是用于清理数据的清单:</p><p><strong>删除所有不相关</strong>的字符，例如任何非字母数字字符<br>令牌化通过将其<strong>分割</strong>成单个的单词文本<br>删除不相关的单词，例如“@”twitter提及或网址<br>将所有字符<strong>转换为小写</strong>，以便将诸如“hello”，“Hello”和“HELLO”之类的单词视为相同<br>考虑将拼写错误或交替拼写的单词<strong>组合成单个表示</strong>（例如“cool”&#x2F;“kewl”&#x2F;“cooool”）<br>考虑<strong>词开还原</strong>（将诸如“am”，“are”和“is”之类的词语简化为诸如“be”之类的常见形式）<br>按照这些步骤并检查其他错误后，我们可以开始使用干净的标记数据来训练模型！</p><p>第3步：分词<br>中英文分词的3个典型区别</p><p> 区别1：分词方式不同，中文更难</p><p>英文有<strong>天然的空格</strong>作为分隔符，但是<strong>中文没有</strong>。所以如何切分是一个难点，再加上中文里一词多意的情况非常多，导致很容易出现歧义。下文中难点部分会详细说明。</p><p>区别2：英文单词有多种形态</p><p>英文单词存在丰富的变形变换。为了应对这些复杂的变换，英文NLP相比中文存在一些独特的处理步骤，我们称为<strong>词形还原（Lemmatization）<strong>和</strong>词干提取(Stemming)</strong>。中文则不需要</p><p>词性还原：does，done，doing，did 需要通过词性还原恢复成 do。</p><p>词干提取：cities，children，teeth 这些词，需要转换为 city，child，tooth”这些基本形态</p><p>区别3：中文分词需要考虑<strong>粒度问题</strong></p><p>例如「中国科学技术大学」就有很<strong>多种分法</strong>：</p><p>中国科学技术大学<br>中国 \ 科学技术 \ 大学<br>中国 \ 科学 \ 技术 \ 大学<br>粒度越大，表达的<strong>意思就越准确</strong>，但是也会导致<strong>召回比较少</strong>。所以中文需要不同的场景和要求选择不同的粒度。这个在英文中是没有的。</p><p>中文分词是一个比较大的课题，相关的知识点和技术栈非常丰富，可以说搞懂了中文分词就等于搞懂了大半个NLP。</p><p>中文分词的3大难点<br> 难点 1：<strong>没有统一的标准</strong></p><p>目前中文分词没有统一的标准，也没有公认的规范。不同的公司和组织各有各的方法和规则。</p><p>难点 2：<strong>歧义词如何切分</strong></p><p>例如「兵乓球拍卖完了」就有2种分词方式表达了2种不同的含义：</p><p>乒乓球 \ 拍卖 \ 完了<br>乒乓 \ 球拍 \ 卖 \ 完了<br>难点 3：<strong>新词的识别</strong></p><p>信息爆炸的时代，三天两头就会冒出来一堆新词，如何快速的识别出这些新词是一大难点。比如当年「蓝瘦香菇」大火，就需要快速识别。</p><p>中文分词经历了20多年的发展，克服了重重困难，取得了巨大的进步，大体可以划分成两个阶段，如下图所示：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/d5e1e3b77629124b9a5aff5cc0c0955f.png" alt="img"></p><p><strong>词典</strong>匹配与规则</p><p>优点：<strong>速度</strong>快、成本低</p><p>缺点：适应性不强，不同领域效果差异大</p><p>基本思想是基于词典匹配，将待分词的中文文本根据一定规则切分和调整，然后跟词典中的词语进行匹配，匹配成功则按照词典的词分词，匹配失败通过调整或者重新选择，如此反复循环即可。代表方法有基于正向最大匹配和基于逆向最大匹配及双向匹配法。</p><p>基于统计与机器学习</p><p>优点：<strong>适应性</strong>较强</p><p>缺点：成本较高，速度较慢</p><p>这类目前常用的是算法是HMM、CRF等算法，比如stanford、Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。</p><p>常见的分词器都是使用<strong>机器学习算法和词典相结合</strong>，一方面能够提高分词准确率，另一方面能够改善领域适应性。</p><p>目前，主流的中文分词技术采用的都是基于词典最大概率路径+未登录词识别（HMM）的方案，其中典型的代表就是<strong>jieba分词</strong>，一个热门的多语言中文分词包。</p><p>中文分词工具</p><p>下面排名根据 GitHub 上的 star 数排名：</p><p>Hanlp<br>Stanford 分词<br>ansj 分词器<br>哈工大 LTP<br>KCWS分词器<br>jieba<br>IK<br>清华大学THULAC<br>ICTCLAS<br>英文分词工具</p><p>Keras<br>Spacy<br>Gensim<br>NLTK<br>第4步：标准化<br>标准化是为了给后续的处理提供一些<strong>必要的基础数据</strong>，包括：去掉停用词、词汇表、训练数据等等。</p><p>当我们完成了分词之后，可以去掉停用词，如：“其中”、“况且”、“什么”等等，但这一步不是必须的，要根据实际业务进行选择，像关键词挖掘就需要去掉停用词，而像训练词向量就不需要。</p><p>词汇表是为语料库建立一个<strong>所有不重复词的列表</strong>，每个词对应一个<strong>索引值</strong>，并索引值不可以改变。词汇表的最大作用就是可以<strong>将词转化成一个向量</strong>，即One-Hot编码。</p><p>假设我们有这样一个词汇表：</p><p>我<br>爱<br>自然<br>语言<br>处理<br>那么，我们就可以得到如下的One-Hot编码：</p><p>我：  [1, 0, 0, 0, 0]<br>爱：  [0, 1, 0, 0, 0]<br>自然：[0, 0, 1, 0, 0]<br>语言：[0, 0, 0, 1, 0]<br>处理：[0, 0, 0, 0, 1]<br>这样我们就可以简单的将词转化成了计算机可以直接处理的数值化数据了。虽然One-Hot编码可以较好的完成部分NLP任务，但它的问题还是不少的。</p><p>当词汇表的维度特别大的时候，就会导致经过One-Hot编码后的词向量非常稀疏，同时One-Hot编码也缺少词的语义信息。由于这些问题，才有了后面大名鼎鼎的Word2vec，以及Word2vec的升级版BERT。</p><p>除了词汇表之外，我们在训练模型时，还需要提供训练数据。模型的学习可以大体分为两类：</p><p><strong>监督</strong>学习，在已知答案的标注数据集上，模型给出的预测结果<strong>尽可能接近真实</strong>答案，适合<strong>预测</strong>任务<br><strong>非监督</strong>学习，学习没有标注的数据，是要<strong>揭示</strong>关于数据隐藏结构的一些规律，适合<strong>描述</strong>任务<br>根据不同的学习任务，我们需要提供不同的标准化数据。一般情况下，标注数据的获取成本非常昂贵，非监督学习虽然不需要花费这样的成本，但在实际问题的解决上，主流的方式还<strong>选择监督学习，因为效果更好</strong>。</p><p>带标注的训练数据大概如下所示（情感分析的训练数据）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">距离 川沙 公路 较近 公交 指示 蔡陆线 麻烦 建议 路线 房间 较为简单__label__1</span><br><span class="line">商务 大床 房 房间 很大 床有 2M 宽 整体 感觉 经济 实惠 不错 !__label__1</span><br><span class="line">半夜 没 暖气 住 ! __label__0</span><br></pre></td></tr></table></figure><p>其中每一行就是一条训练样本，<code>__label__0</code>和<code>__label__1</code>是分类信息，其余的部分就是分词后的文本数据。</p><p>第5步：特征提取<br>为了能够更好的训练模型，我们需要将文本的原始特征<strong>转化成具体</strong>特征，转化的方式主要有两种：统计和Embedding。</p><p>原始特征：需要人类或者机器进行转化，如：文本、图像。</p><p>具体特征：已经被人类进行整理和分析，可以直接使用，如：物体的重要、大小。</p><p>NLP表示方式<br>目前常用的文本表示方式分为：</p><p><strong>离散</strong>式表示（Discrete Representation）；<br><strong>分布</strong>式表示（Distributed Representation）；</p><h2 id="离散式表示（Discrete-Representation）"><a href="#离散式表示（Discrete-Representation）" class="headerlink" title="离散式表示（Discrete Representation）"></a>离散式表示（Discrete Representation）</h2><p>One-Hot<br>One-Hot 编码又称为“独热编码”或“哑编码”，是最传统、最基础的词（或字）特征表示方法。这种编码将词（或字）<strong>表示成一个向量</strong>，该向量的<strong>维度是词典（或字典）的长度</strong>（该词典是通过语料库生成的），该向量中，<strong>当前词的位置的值为1</strong>，其余的位置为0。</p><p>文本使用one-hot 编码步骤：</p><p>根据<strong>语料库创建 词典</strong>（vocabulary），并创建词和索引的 <strong>映射</strong>（stoi，itos)；<br>将句子转换为用<strong>索引表示</strong>；<br>创建OneHot 编码器；<br>使用OneHot 编码器对句子<strong>进行编码</strong>；<br>One-Hot 编码的特点如下：</p><p>词向量长度是词典长度；<br>在向量中，该单词的索引位置的值为  1 ，其余的值都是  0<br>使用One-Hot 进行编码的文本，得到的矩阵是<strong>稀疏矩阵</strong><br>缺点：</p><p>不同词的向量表示互相正交，<strong>无法衡量</strong>不同词之间的<strong>关系</strong>；<br>该编码只能反映某个词<strong>是否在句中出现</strong>，无法衡量不同词的<strong>重要程度</strong>；<br>使用One-Hot 对文本进行编码后得到的是<strong>高维稀疏</strong>矩阵，会<strong>浪费计算和存储资源</strong>；<br>词袋模型（Bag Of Word，BOW）<br>例句：</p><p>Jane wants to go to Shenzhen.<br>Bob wants to go to Shanghai.<br>在词袋模型中不考虑语序和词法的信息，每个单词都是<strong>相互独立</strong>的，将词语放入一个“袋子”里，统计每个单词出现的频率。</p><p>词袋模型编码特点：</p><p>词袋模型是<strong>对文本</strong>（而不是字或词）进行编码；<br>编码后的向量长度是<strong>词典的长度</strong>；<br>该编码<strong>忽略词出现的次序</strong>；<br>在向量中，该单词的<strong>索引位置的值</strong>为单词在文本中出现的<strong>次数</strong>；如果索引位置的单词没有在文本中出现，则该值为  0 ；<br>缺点</p><p>该编码<strong>忽略词的位置信息</strong>，位置信息在文本中是一个很重要信息，词的位置不一样语义会有很大的差别（如 “猫爱吃老鼠” 和 “老鼠爱吃猫” 的编码一样）；<br>该编码方式虽然统计了词在文本中出现的次数，但仅仅通过“出现次数”这个属性<strong>无法区分常用词</strong>（如：“我”、“是”、“的”等）和<strong>关键词</strong>（如：“自然语言处理”、“NLP ”等）在文本中的<strong>重要程度</strong>；<br>TF-IDF（词频-逆文档频率）<br>为了解决词袋模型无法区分常用词（如：“是”、“的”等）和专有名词（如：“自然语言处理”、“NLP ”等）对文本的重要性的问题，TF-IDF 算法应运而生。</p><p>TF-IDF 全称是：term frequency–inverse document frequency 又称 词频-逆文本频率。其中：</p><p>统计的方式主要是计算词的词频（TF）和逆向文件频率（IDF）：</p><p>TF （Term Frequency ）：某个词<strong>在当前文本中</strong>出现的频率，频率高的词语或者是重要的词（如：“自然语言处理”）或者是常用词（如：“我”、“是”、“的”等）；<br>IDF （Inverse Document frequency ）：逆文本频率。文本频率是指：含有某个词的文本<strong>在整个语料库中</strong>所占的比例。逆文本频率是<strong>文本频率的倒数</strong>；<br>那么，每个词都会得到一个TF-IDF值，用来<strong>衡量它的重要程度</strong>，计算公式如下：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4b52b3a5ad29985b686e084588e842fb.png" alt="img"></p><p>优点</p><p>实现简单，算法容易理解且解释性较强；<br>从IDF 的计算方法可以看出常用词（如：“我”、“是”、“的”等）在<strong>语料库中</strong>的很多文章都会出现，故<strong>IDF的值会很小</strong>；而关键词（如：“自然语言处理”、“NLP ”等）只会在<strong>某领域</strong>的文章出现，IDF 的值会<strong>比较大</strong>；故：TF-IDF 在保留文章的重要词的同时可以过滤掉一些常见的、无关紧要的词；<br>缺点</p><p><strong>不能反映词的位置信息</strong>，在对关键词进行提取时，词的位置信息（如：标题、句首、句尾的词应该赋予更高的权重）；<br>IDF 是一种试图抑制噪声的加权，本身倾向于文本中频率比较小的词，这使得IDF 的精度不高；<br>TF-IDF <strong>严重依赖于语料库</strong>（尤其在训练同类语料库时，往往会掩盖一些同类型的关键词；如：在进行TF-IDF 训练时，语料库中的 娱乐 新闻较多，则与 娱乐 相关的关键词的权重就会偏低 ），因此需要选取质量高的语料库进行训练；</p><h2 id="分布式表示（Distributed-Representation"><a href="#分布式表示（Distributed-Representation" class="headerlink" title="分布式表示（Distributed Representation"></a>分布式表示（Distributed Representation</h2><p>理论基础：</p><p>1954年，Harris提出分布式假说（distributional hypothesis）奠定了这种方法的理论基础：A word’s meaning is given by the words that frequently appear close-by（上下文相似的词，其语义也相似）；<br>1957年，Firth对分布式假说做出进一步的阐述和明确：A word is characterized by the company it keeps（词的语义<strong>由其上下文决定</strong>）；<br>n-gram<br>n-gram 是一种 语言模型(Language Model, LM)。语言模型是一种基于概率的判别式模型，该模型的输入是一句话（单词的序列），输出的是这句话的概率，也就是这些单词的<strong>联合概率</strong>（joint probability）。（备注：语言模型就是判断一句话是不是正常人说的。）</p><p>共现矩阵（Co-Occurrence Matrix）<br>首先指定窗口大小，然后统计窗口（和对称窗口）内词语<strong>共同出现</strong>的<strong>次数</strong>作为词的向量（vector）。</p><p>语料库：</p><p>I like deep learning.<br>I like NLP.<br>I enjoy flying.<br>备注： 指定窗口大小为1（即：左右的 window_length&#x3D;1，相当于 bi-gram）统计数据如下：（I, like），（Iike, deep），（deep, learning），（learning, .），（I, like），（like, NLP），（NLP, .），（I, enjoy），（enjoy, flying）， （flying, .）。则语料库的共现矩阵如下表所示：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/8b07f13487955c6279acbecd802fbcf5.png" alt="img"></p><p>从以上的共现矩阵可以看出，单词  like 和  enjoy 都在单词  I 附件出现且统计数目大概相等，则它们在 <strong>语义 和 语法 上的含义大概相同</strong>。</p><p>优点</p><p>考虑了句子中<strong>词的顺序</strong>；<br>缺点</p><p>词表的长度很大，导致词的向量长度也很大；<br>共现矩阵也是稀疏矩阵（可以使用 SVD、PCA 等算法进行降维，但是计算量很大）；<br>Word2Vec<br>word2vec 模型是Google团队在2013年发布的 word representation 方法。该方法一出让 预训练词向量 的使用在NLP 领域遍地开花。</p><p>word2vec模型</p><p>word2vec有两种模型：CBOW 和 SKIP-GRAM；</p><ul><li>CBOW：利用上下文的词预测中心词；</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6a9e694f893c53f1d031e02b03a96dcb.png" alt="img"></p><ul><li>SKIP-GRAM：利用中心词预测上下文的词；</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/a9321a80eaf4aaf89f60ee6c94b3598e.png" alt="img"></p><p><strong>优点</strong></p><ol><li>考虑到词语的<strong>上下文</strong>，学习到了语义和语法的信息；</li><li>得到的词向量<strong>维度小</strong>，节省存储和计算资源；</li><li><strong>通用性</strong>强，可以应用到各种<em>NLP</em> 任务中；</li></ol><p><strong>缺点</strong></p><ol><li>词和向量是一对一的关系，无法解决多义词的问题；</li><li>word2vec是一种<strong>静态</strong>的模型，虽然通用性强，但无法真的特定的任务做动态优化；</li></ol><p>GloVe<br>GloVe 是斯坦福大学Jeffrey、Richard 等提供的一种词向量表示算法，GloVe 的全称是Global Vectors for Word Representation，是一个基于全局词频统计（count-based &amp; overall staticstics）的词表征（word representation）算法。该算法综合了global matrix factorization（全局矩阵分解） 和 local context window（局部上下文窗口） 两种方法的优点。</p><p>效果</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/89c700f1bbd4be29476d9139b05a9679.png" alt="img"></p><p>优点</p><p>考虑到词语的<strong>上下文</strong>、和全局语料库的信息，学习到了语义和语法的信息；<br>得到的词向量<strong>维度小</strong>，节省存储和计算资源；<br>通用性强，可以应用到各种NLP 任务中；<br>缺点</p><p>词和向量是一对一的关系，无法解决多义词的问题；<br>glove也是一种静态的模型，虽然通用性强，但无法真的特定的任务做动态优化；<br>ELMO<br>word2vec 和 glove 算法得到的词向量都是静态词向量（静态词向量会把多义词的语义进行融合，训练结束之后不会根据上下文进行改变），静态词向量无法解决多义词的问题（如：“我今天买了7斤苹果” 和 “我今天买了苹果7” 中的 苹果 就是一个多义词）。而ELMO模型进行训练的词向量可以<strong>解决多义词</strong>的问题。</p><p>ELMO 的全称是“ Embedding from Language Models ”，这个名字不能很好的反映出该模型的特点，提出ELMO 的论文题目可以更准确的表达出该算法的特点“ Deep contextualized word representation ”。</p><p>该算法的精髓是：<strong>用语言模型训练神经网络</strong>，在使用word embedding 时，单词已经具备上下文信息，这个时候神经网络可以根据上下文信息对word embedding 进行调整，这样经过调整之后的word embedding 更能表达在这个上下文中的具体含义，这就解决了静态词向量无法表示多义词的问题。</p><p>网络模型</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/b20fb10062f8b5e18f8e7ea973e17b6d.png" alt="img"></p><p>过程</p><p>上图中的结构使用<strong>字符级卷积神经网络（convolutional neural network, CNN）<strong>来将文本中的词转换成</strong>原始词向量（raw word vector）</strong> ；<br>将原始词向量输入双向语言模型中第一层 ；<br><strong>前向迭代</strong>中包含了该词以及该词之前的一些词汇或语境的信息（即<strong>上文</strong>）；<br><strong>后向迭代</strong>中包含了该词以及该词之后的一些词汇或语境的信息（即<strong>下文</strong>） ；<br>这两种迭代的信息组成了<strong>中间词向量（intermediate word vector）</strong>；<br>中间词向量被输入到模型的<strong>下一层</strong> ；<br>最终向量就是原始词向量和两个中间词向量的<strong>加权和</strong>；</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/0b2bd5a91dfe856b390172a2589311ac.png" alt="img"></p><p>如上图所示：</p><ul><li>使用glove训练的词向量中，与 play 相近的词大多与<strong>体育相关</strong>，这是因为语料中与play相关的语料多时体育领域的有关；</li><li>在使用elmo训练的词向量中，当 play 取 <strong>演出</strong> 的意思时，与其相近的也是 演出 相近的句子</li></ul><h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-e1576624094357.webp" alt="img"></p><p>PyTorch 是一个开源的机器学习库，主要用于进行<strong>计算机视觉（CV）</strong>、<strong>自然语言处理（NLP）</strong>、语音识别等领域的研究和开发。</p><p>PyTorch 以其灵活性和易用性而闻名，特别适合于<strong>深度学习</strong>研究和开发。</p><h2 id="谁适合阅读本教程？"><a href="#谁适合阅读本教程？" class="headerlink" title="谁适合阅读本教程？"></a>谁适合阅读本教程？</h2><p>只要您具备编程的基础知识，您就可以阅读本教程，学习 PyTorch 适合对深度学习和机器学习感兴趣的人，包括数据科学家、工程师、研究人员和学生。</p><h2 id="阅读本教程前，您需要了解的知识："><a href="#阅读本教程前，您需要了解的知识：" class="headerlink" title="阅读本教程前，您需要了解的知识："></a>阅读本教程前，您需要了解的知识：</h2><p>在您开始阅读本教程之前，您必须具备的基础知识包括 Python 编程、基础数学（线性代数、概率论、微积分）、机器学习的基本概念、神经网络知识，以及一定的英语阅读能力来查阅文档和资料。</p><ul><li><strong>编程基础</strong>：熟悉至少一种编程语言，尤其是 <a href="https://www.runoob.com/python3/python3-tutorial.html">Python</a>，因为 PyTorch 主要是用 Python 编写的。</li><li><strong>数学基础</strong>：了解线性代数、概率论和统计学、微积分等基础数学知识，这些是理解和实现机器学习算法的基石。</li><li><strong>机器学习基础</strong>：了解机器学习的基本概念，如监督学习、无监督学习、强化学习、模型评估指标（准确率、召回率、F1分数等）。</li><li><strong>深度学习基础</strong>：熟悉神经网络的基本概念，包括前馈神经网络、卷积神经网络（CNN）、循环神经网络（RNN）、长短期记忆网络（LSTM）等。</li><li><strong>计算机视觉和自然语言处理基础</strong>：如果你打算在这些领域应用 PyTorch，了解相关的背景知识会很有帮助。</li><li><strong>Linux&#x2F;Unix 基础</strong>：虽然不是必需的，但了解 Linux&#x2F;Unix 操作系统的基础知识可以帮助你更有效地使用命令行工具和脚本，特别是在数据预处理和模型训练中。</li><li><strong>英语阅读能力</strong>：由于许多文档、教程和社区讨论都是用英语进行的，具备一定的英语阅读能力将有助于你更好地学习和解决问题。</li></ul><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>下面的是 PyTorch 中一些<strong>基本的张量操作</strong>：如何创建随机张量、进行逐元素运算、访问特定元素以及计算总和和最大值。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 设置数据类型和设备</span></span><br><span class="line">dtype = torch.<span class="built_in">float</span>  <span class="comment"># 张量数据类型为浮点型</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)  <span class="comment"># 本次计算在 CPU 上进行</span></span><br><span class="line"><span class="comment"># 创建并打印两个随机张量 a 和 b</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">3</span>, device=device, dtype=dtype)  <span class="comment"># 创建一个 2x3 的随机张量</span></span><br><span class="line">b = torch.randn(<span class="number">2</span>, <span class="number">3</span>, device=device, dtype=dtype)  <span class="comment"># 创建另一个 2x3 的随机张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 a:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 b:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># 逐元素相乘并输出结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;a 和 b 的逐元素乘积:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a * b)</span><br><span class="line"><span class="comment"># 输出张量 a 所有元素的总和</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 a 所有元素的总和:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># 输出张量 a 中第 2 行第 3 列的元素（注意索引从 0 开始）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 a 第 2 行第 3 列的元素:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 输出张量 a 中的最大值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量 a 中的最大值:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">max</span>())</span><br></pre></td></tr></table></figure><p><strong>创建张量：</strong></p><ul><li><code>torch.randn(2, 3)</code> 创建一个 2 行 3 列的张量，<strong>填充随机数</strong>（遵循<strong>正态分布</strong>）。</li><li><code>device=device</code> 和 <code>dtype=dtype</code> 分别指定了<strong>计算设备</strong>（CPU 或 GPU）和<strong>数据类型</strong>（浮点型）。</li></ul><p><strong>张量操作：</strong></p><ul><li><code>a * b</code>：<strong>逐</strong>元素相乘。</li><li><code>a.sum()</code>：计算张量 <code>a</code> 所有元素的和。</li><li><code>a[1, 2]</code>：访问张量 <code>a</code> 第 2 行第 3 列的元素（注意索引从 0 开始）。</li><li><code>a.max()</code>：获取张量 <code>a</code> 中的最大值。</li></ul><p>输出：（<strong>每次</strong>运行时值会有所<strong>不同</strong>）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">张量 a:</span><br><span class="line">tensor([[-0.1460, -0.3490,  0.3705],</span><br><span class="line">        [-1.1141,  0.7661,  1.0823]])</span><br><span class="line">张量 b:</span><br><span class="line">tensor([[ 0.6901, -0.9663,  0.3634],</span><br><span class="line">        [-0.6538, -0.3728, -1.1323]])</span><br><span class="line">a 和 b 的逐元素乘积:</span><br><span class="line">tensor([[-0.1007,  0.3372,  0.1346],</span><br><span class="line">        [ 0.7284, -0.2856, -1.2256]])</span><br><span class="line">张量 a 所有元素的总和:</span><br><span class="line">tensor(0.6097)</span><br><span class="line">张量 a 第 2 行第 3 列的元素:</span><br><span class="line">tensor(1.0823)</span><br><span class="line">张量 a 中的最大值:</span><br><span class="line">tensor(1.0823)</span><br></pre></td></tr></table></figure><h1 id="PyTorch-简介"><a href="#PyTorch-简介" class="headerlink" title="PyTorch 简介"></a>PyTorch 简介</h1><p>PyTorch 是一个开源的 Python 机器学习库，<strong>基于 Torch 库</strong>，**底层由C++**实现，应用于人工智能领域，如计算机视觉和自然语言处理。</p><p>PyTorch 最初由 Meta Platforms 的人工智能研究团队开发，现在属 于Linux 基金会的一部分。</p><p>许多深度学习软件都是基于 PyTorch 构建的，包括特斯拉自动驾驶、Uber 的 Pyro、<strong>Hugging Face 的 Transformers</strong>、 PyTorch Lightning 和 Catalyst。</p><p><strong>PyTorch 主要有两大特征：</strong></p><ul><li>类似于 NumPy 的<strong>张量计算</strong>，能在 GPU 或 MPS 等硬件加速器上加速。</li><li>基于带自动微分系统的<strong>深度神经网络</strong>。</li></ul><p>PyTorch 包括 torch.autograd、torch.nn、torch.optim 等子模块。</p><p>PyTorch 包含多种损失函数，包括 MSE（均方误差 &#x3D; L2 范数）、交叉熵损失和负熵似然损失（对分类器有用）等。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1567769062953.png" alt="img"></p><h2 id="PyTorch-特性"><a href="#PyTorch-特性" class="headerlink" title="PyTorch 特性"></a>PyTorch 特性</h2><ul><li><strong>动态计算图（Dynamic Computation Graphs）</strong>： PyTorch 的计算图是动态的，这意味着它们<strong>在运行时构建</strong>，并且<strong>可以随时改变</strong>。这为实验和调试提供了极大的灵活性，因为开发者可以逐行执行代码，查看中间结果。</li><li><strong>自动微分（Automatic Differentiation）</strong>： PyTorch 的自动微分系统允许开发者轻松地<strong>计算梯度</strong>，这对于训练深度学习模型至关重要。它通过反向传播算法自动计算出<strong>损失函数对模型参数的梯度</strong>。</li><li><strong>张量计算（Tensor Computation）</strong>： PyTorch 提供了类似于 NumPy 的张量操作，这些操作可以在 CPU 和 GPU 上执行，从而<strong>加速</strong>计算过程。张量是 PyTorch 中的<strong>基本数据结构</strong>，用于存储和操作数据。</li><li><strong>丰富的 API</strong>： PyTorch 提供了大量的<strong>预定义层</strong>、<strong>损失函数</strong>和<strong>优化算法</strong>，这些都是构建深度学习模型的常用组件。</li><li><strong>多语言支持</strong>： PyTorch 虽然以 Python 为主要接口，但也提供了 C++ 接口，<strong>允许更底层</strong>的集成和控制。</li></ul><h3 id="动态计算图（Dynamic-Computation-Graph）"><a href="#动态计算图（Dynamic-Computation-Graph）" class="headerlink" title="动态计算图（Dynamic Computation Graph）"></a>动态计算图（Dynamic Computation Graph）</h3><p>PyTorch 最显著的特点之一是其<strong>动态计算图</strong>的机制。</p><p>与 TensorFlow 的静态计算图（graph）不同，PyTorch 在执行时构建计算图，这意味着在每次计算时，图都会根据输入数据的形状自动变化。</p><p><strong>动态计算图的优点：</strong></p><ul><li>更加<strong>灵活</strong>，特别适用于需要条件判断或递归的场景。</li><li>方便调试和修改，能够直接查看中间结果。</li><li>更接近 Python 编程的风格，易于上手。</li></ul><h3 id="张量（Tensor）与自动求导（Autograd）"><a href="#张量（Tensor）与自动求导（Autograd）" class="headerlink" title="张量（Tensor）与自动求导（Autograd）"></a>张量（Tensor）与自动求导（Autograd）</h3><p>PyTorch 中的核心数据结构是 <strong>张量（Tensor）</strong>，它是一个<strong>多维矩阵</strong>，可以在 CPU 或 GPU 上高效地进行计算。张量的操作**支持自动求导（Autograd）**机制，使得在反向传播过程中自动计算梯度，这对于深度学习中的梯度下降优化算法至关重要。</p><p><strong>张量（Tensor）：</strong></p><ul><li>支持在 <strong>CPU 和 GPU 之间</strong>进行切换。</li><li>提供了类似 NumPy 的接口，支持元素级运算。</li><li>支持自动求导，可以方便地进行梯度计算。</li></ul><p><strong>自动求导（Autograd）：</strong></p><ul><li>PyTorch 内置的自动求导引擎，能够<strong>自动追踪</strong>所有张量的操作，并在反向传播时计算梯度。</li><li>通过 <code>requires_grad</code> 属性，<strong>可以指定</strong>张量需要计算梯度。</li><li>支持高效的反向传播，适用于神经网络的训练。</li></ul><h3 id="模型定义与训练"><a href="#模型定义与训练" class="headerlink" title="模型定义与训练"></a>模型定义与训练</h3><p>PyTorch 提供了 <code>torch.nn</code> 模块，允许用户通过继承 <code>nn.Module</code> 类来定义神经网络模型。使用 <code>forward</code> 函数指定前向传播，自动反向传播（通过 <code>autograd</code>）和梯度计算也由 PyTorch 内部处理。</p><p><strong>神经网络模块（torch.nn）：</strong></p><ul><li>提供了<strong>常用的层</strong>（如线性层、卷积层、池化层等）。</li><li>支持定义复杂的神经网络架构（包括多输入、多输出的网络）。</li><li>兼容与优化器（如 <code>torch.optim</code>）一起使用。</li></ul><h3 id="GPU-加速"><a href="#GPU-加速" class="headerlink" title="GPU 加速"></a>GPU 加速</h3><p>PyTorch 完全支持在 GPU 上运行，以加速深度学习模型的训练。通过<strong>简单的 <code>.to(device)</code> 方法</strong>，用户可以将模型和张量转移到 GPU 上进行计算。PyTorch <strong>支持多 GPU</strong> 训练，能够利用 <strong>NVIDIA CUDA</strong> 技术显著提高计算效率。</p><p><strong>GPU 支持：</strong></p><ul><li>自动选择 GPU 或 CPU。</li><li>支持通过 CUDA 加速运算。</li><li>支持多 GPU 并行计算（<code>DataParallel</code> 或 <code>torch.distributed</code>）。</li></ul><h3 id="生态系统与社区支持"><a href="#生态系统与社区支持" class="headerlink" title="生态系统与社区支持"></a>生态系统与社区支持</h3><p>PyTorch 作为一个开源项目，拥有一个庞大的社区和生态系统。它不仅在学术界得到了广泛的应用，也在工业界，特别是在计算机视觉、自然语言处理等领域中得到了广泛部署。PyTorch 还提供了许多与深度学习相关的工具和库，如：</p><ul><li><strong>torchvision</strong>：用于<strong>计算机视觉任务</strong>的<strong>数据集</strong>和<strong>模型</strong>。</li><li><strong>torchtext</strong>：用于<strong>自然语言处理任务</strong>的<strong>数据集</strong>和<strong>预处理工具</strong>。</li><li><strong>torchaudio</strong>：用于音频处理的工具包。</li><li><strong>PyTorch Lightning</strong>：一种<strong>简化</strong> PyTorch 代码的<strong>高层库</strong>，专注于研究和实验的快速迭代。</li></ul><h3 id="与其他框架的对比"><a href="#与其他框架的对比" class="headerlink" title="与其他框架的对比"></a>与其他框架的对比</h3><p>PyTorch 由于其灵活性、易用性和社区支持，已经成为很多深度学习研究者和开发者的首选框架。</p><p><strong>TensorFlow vs PyTorch：</strong></p><ul><li>PyTorch 的动态计算图使得它更加灵活，适合<strong>快速实验和研究</strong>；而 TensorFlow 的静态计算图在生产环境中更具优化空间。</li><li>PyTorch 在调试时更加方便，TensorFlow 则在部署上更加成熟，支持更广泛的硬件和平台。</li><li>近年来，TensorFlow 也引入了动态图（如 TensorFlow 2.x），使得两者在功能上趋于接近。</li><li>其他深度学习框架，如 Keras、Caffe 等也有一定应用，但 PyTorch 由于其灵活性、易用性和社区支持，已经成为很多深度学习研究者和开发者的首选框架。</li></ul><table><thead><tr><th align="left">特性</th><th align="left"><strong>TensorFlow</strong></th><th align="left"><strong>PyTorch</strong></th></tr></thead><tbody><tr><td align="left"><strong>开发公司</strong></td><td align="left">Google</td><td align="left">Facebook (FAIR)</td></tr><tr><td align="left"><strong>计算图类型</strong></td><td align="left">静态计算图（定义后再执行）</td><td align="left">动态计算图（定义即执行）</td></tr><tr><td align="left"><strong>灵活性</strong></td><td align="left">低（计算图在编译时构建，不易修改）</td><td align="left">高（计算图在执行时动态创建，易于修改和调试）</td></tr><tr><td align="left"><strong>调试</strong></td><td align="left">较难（需要使用 <code>tf.debugging</code> 或外部工具调试）</td><td align="left">容易（可以<strong>直接在 Python 中进行调试</strong>）</td></tr><tr><td align="left"><strong>易用性</strong></td><td align="left">低（较复杂，API 较多，学习曲线较陡峭）</td><td align="left">高（API 简洁，语法更加接近 Python，容易上手）</td></tr><tr><td align="left"><strong>部署</strong></td><td align="left">强（支持广泛的硬件，如 TensorFlow Lite、TensorFlow.js）</td><td align="left">较弱（部署工具和平台相对较少，虽然有 TensorFlow 支持）</td></tr><tr><td align="left"><strong>社区支持</strong></td><td align="left">很强（成熟且庞大的社区，广泛的教程和文档）</td><td align="left">很强（社区活跃，特别是在学术界，快速发展的生态）</td></tr><tr><td align="left"><strong>模型训练</strong></td><td align="left">支持分布式训练，支持多种设备（如 CPU、GPU、TPU）</td><td align="left">支持分布式训练，支持多 GPU、CPU 和 TPU</td></tr><tr><td align="left"><strong>API 层级</strong></td><td align="left">高级API：Keras；低级API：TensorFlow Core</td><td align="left">高级API：TorchVision、TorchText 等；低级API：Torch</td></tr><tr><td align="left"><strong>性能</strong></td><td align="left">高（优化方面成熟，适合生产环境）</td><td align="left">高（适合<strong>研究</strong>和<strong>原型</strong>开发，生产性能也在提升）</td></tr><tr><td align="left"><strong>自动求导</strong></td><td align="left">通过 <code>tf.GradientTape</code> 实现动态求导（较复杂）</td><td align="left">通过 <code>autograd</code> 动态求导（更简洁和直观）</td></tr><tr><td align="left"><strong>调优与可扩展性</strong></td><td align="left">强（支持在多平台上运行，如 TensorFlow Serving 等）</td><td align="left">较弱（虽然在学术和实验环境中表现优越，但生产环境支持相对较少）</td></tr><tr><td align="left"><strong>框架灵活性</strong></td><td align="left">较低（TensorFlow 2.x 引入了动态图特性，但仍不完全灵活）</td><td align="left">高（动态图带来更高的灵活性）</td></tr><tr><td align="left"><strong>支持多种语言</strong></td><td align="left">支持多种语言（Python, C++, Java, JavaScript, etc.）</td><td align="left">主要支持 Python（但也有 C++ API）</td></tr><tr><td align="left"><strong>兼容性与迁移</strong></td><td align="left">TensorFlow 2.x 与旧版本兼容性较好</td><td align="left">与 TensorFlow 兼容性差，迁移较难</td></tr></tbody></table><p>PyTorch 是一个强大且灵活的深度学习框架，适合学术研究和工业应用。它的动态计算图、自动求导机制、GPU 加速等特点，使得其成为深度学习研究和实验中不可或缺的工具。</p><h1 id="PyTorch-基础"><a href="#PyTorch-基础" class="headerlink" title="PyTorch 基础"></a>PyTorch 基础</h1><p>PyTorch 是一个开源的深度学习框架，以其灵活性和动态计算图而广受欢迎。</p><p>PyTorch 主要有以下几个基础概念：张量（Tensor）、自动求导（Autograd）、神经网络模块（nn.Module）、优化器（optim）等。</p><ul><li><strong>张量（Tensor）</strong>：PyTorch 的核心数据结构，支持多维数组，并可以在 CPU 或 GPU 上进行加速计算。</li><li><strong>自动求导（Autograd）</strong>：PyTorch 提供了自动求导功能，可以轻松计算模型的梯度，便于进行反向传播和优化。</li><li><strong>神经网络（nn.Module）</strong>：PyTorch 提供了简单且强大的 API 来构建神经网络模型，可以方便地进行前向传播和模型定义。</li><li><strong>优化器（Optimizers）</strong>：使用优化器（如 Adam、SGD 等）来更新模型的参数，使得损失最小化。</li><li><strong>设备（Device）</strong>：可以将模型和张量移动到 GPU 上以加速计算。</li></ul><h2 id="张量（Tensor）"><a href="#张量（Tensor）" class="headerlink" title="张量（Tensor）"></a>张量（Tensor）</h2><p>张量（Tensor）是 PyTorch 中的核心数据结构，用于存储和操作多维数组。</p><p>张量可以视为一个多维数组，支持加速计算的操作。</p><p>在 PyTorch 中，张量的概念类似于 NumPy 中的数组，但是 PyTorch 的张量可以运行在不同的设备上，比如 CPU 和 GPU，这使得它们非常适合于进行大规模并行计算，特别是在深度学习领域。</p><ul><li><strong>维度（Dimensionality）</strong>：张量的维度指的是<strong>数据的多维数组结构</strong>。例如，一个标量（0维张量）是一个单独的数字，一个向量（1维张量）是一个一维数组，一个矩阵（2维张量）是一个二维数组，以此类推。</li><li><strong>形状（Shape）</strong>：张量的形状是指每个维度上的大小。例如，一个形状为<code>(3, 4)</code>的张量意味着它有3行4列。</li><li><strong>数据类型（Dtype）</strong>：张量中的数据类型定义了存储每个元素所需的内存大小和解释方式。PyTorch支持多种数据类型，包括整数型（如<code>torch.int8</code>、<code>torch.int32</code>）、浮点型（如<code>torch.float32</code>、<code>torch.float64</code>）和布尔型（<code>torch.bool</code>）。</li></ul><p><strong>张量创建：</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个 2x3 的全 0 张量</span></span><br><span class="line">a = torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 创建一个 2x3 的全 1 张量</span></span><br><span class="line">b = torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># 创建一个 2x3 的随机数张量</span></span><br><span class="line">c = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="comment"># 从 NumPy 数组创建张量</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">numpy_array = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tensor_from_numpy = torch.from_numpy(numpy_array)</span><br><span class="line"><span class="built_in">print</span>(tensor_from_numpy)</span><br><span class="line"><span class="comment"># 在指定设备（CPU/GPU）上创建张量</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">d = torch.randn(<span class="number">2</span>, <span class="number">3</span>, device=device)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br></pre></td></tr></table></figure><p><strong>常用张量操作：</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 张量相加</span></span><br><span class="line">e = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">f = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(e + f)</span><br><span class="line"><span class="comment"># 逐元素乘法(不同于矩阵乘法)</span></span><br><span class="line"><span class="built_in">print</span>(e * f)</span><br><span class="line"><span class="comment"># 张量的转置</span></span><br><span class="line">g = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(g.t())  <span class="comment"># 或者 g.transpose(0, 1)</span></span><br><span class="line"><span class="comment"># 张量的形状</span></span><br><span class="line"><span class="built_in">print</span>(g.shape)  <span class="comment"># 返回形状</span></span><br></pre></td></tr></table></figure><h3 id="张量与设备"><a href="#张量与设备" class="headerlink" title="张量与设备"></a>张量与设备</h3><p>PyTorch 张量可以存在于不同的设备上，包括CPU和GPU，你可以将张量<strong>移动到 GPU 上</strong>以加速计算：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if torch.cuda.is_available():</span><br><span class="line">    tensor_gpu = tensor_from_list.to(&#x27;cuda&#x27;)  # 将张量移动到GPU</span><br></pre></td></tr></table></figure><h3 id="梯度和自动微分"><a href="#梯度和自动微分" class="headerlink" title="梯度和自动微分"></a>梯度和自动微分</h3><p>PyTorch的张量<strong>支持自动微分</strong>，这是深度学习中的关键特性。当你创建一个<strong>需要梯度</strong>的张量时，PyTorch可以<strong>自动计算其梯度</strong>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个需要梯度的张量</span></span><br><span class="line">tensor_requires_grad = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 进行一些操作</span></span><br><span class="line">tensor_result = tensor_requires_grad * <span class="number">2</span></span><br><span class="line"><span class="comment"># 计算梯度</span></span><br><span class="line">tensor_result.backward()</span><br><span class="line"><span class="built_in">print</span>(tensor_requires_grad.grad)  <span class="comment"># 输出梯度</span></span><br></pre></td></tr></table></figure><h3 id="内存和性能"><a href="#内存和性能" class="headerlink" title="内存和性能"></a>内存和性能</h3><p>PyTorch 张量还提供了一些内存管理功能，比如.clone()、.detach() 和 .to() 方法，它们可以帮助你优化内存使用和提高性能。</p><hr><h2 id="自动求导（Autograd）"><a href="#自动求导（Autograd）" class="headerlink" title="自动求导（Autograd）"></a>自动求导（Autograd）</h2><p>自动求导（Automatic Differentiation，简称Autograd）是深度学习框架中的一个核心特性，它允许计算机自动计算数学函数的导数。</p><p>在深度学习中，自动求导主要用于两个方面：<strong>一是在训练神经网络时计算梯度</strong>，<strong>二是进行反向传播算法的实现</strong>。</p><p>自动求导基于链式法则（Chain Rule），这是一个用于计算复杂函数导数的数学法则。链式法则表明，复合函数的导数是其各个组成部分导数的乘积。在深度学习中，模型<strong>通常是由许多层组成的复杂函数</strong>，自动求导能够高效地计算这些层的梯度。</p><p><strong>动态图与静态图：</strong></p><ul><li><strong>动态图（Dynamic Graph）</strong>：在动态图中，计算图在运行时动态构建。每次执行操作时，计算图都会更新，这使得调试和修改模型变得更加容易。PyTorch使用的是<strong>动态图</strong>。</li><li><strong>静态图（Static Graph）</strong>：在静态图中，计算图在开始执行之前构建完成，并且不会改变。TensorFlow最初使用的是静态图，但后来也支持动态图。</li></ul><p>PyTorch 提供了自动求导功能，通过 autograd 模块来自动计算梯度。</p><p>torch.Tensor 对象有一个 <strong>requires_grad</strong> 属性，用于指示<strong>是否需要计算该张量的梯度</strong>。</p><p>当你创建一个 requires_grad&#x3D;True 的张量时，PyTorch 会自动跟踪所有对它的操作，以便在之后计算梯度。</p><p>创建需要梯度的张量:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个需要计算梯度的张量</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># 执行某些操作</span></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure><h3 id="反向传播（Backpropagation）"><a href="#反向传播（Backpropagation）" class="headerlink" title="反向传播（Backpropagation）"></a>反向传播（Backpropagation）</h3><p>一旦定义了计算图，可以通过 <strong>.backward()</strong> 方法来计算梯度。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">out.backward()</span><br><span class="line"><span class="comment"># 查看 x 的梯度</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><p>在神经网络训练中，<strong>自动求导</strong>主要用于实现<strong>反向传播</strong>算法。</p><p>反向传播是一种通过<strong>计算损失函数关于网络参数的梯度</strong>来训练神经网络的方法。在<strong>每次迭代</strong>中，网络的<strong>前向</strong>传播会<strong>计算输出和损失</strong>，然后反向传播会<strong>计算损失关于每个参数的梯度</strong>，并<strong>使用这些梯度来更新</strong>参数。</p><h3 id="停止梯度计算"><a href="#停止梯度计算" class="headerlink" title="停止梯度计算"></a>停止梯度计算</h3><p>如果你不希望某些张量的梯度被计算（例如，当你<strong>不需要反向传播</strong>时），可以使用 <strong>torch.no_grad()</strong> 或设置 <strong>requires_grad&#x3D;False</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 torch.no_grad() 禁用梯度计算</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y = x * <span class="number">2</span></span><br></pre></td></tr></table></figure><h2 id="神经网络（nn-Module）"><a href="#神经网络（nn-Module）" class="headerlink" title="神经网络（nn.Module）"></a>神经网络（nn.Module）</h2><p>神经网络是一种<strong>模仿人脑神经元连接的计算模型</strong>，由多层节点（神经元）组成，用于学习数据之间的复杂模式和关系。</p><p>神经网络通过调整<strong>神经元之间的连接权重</strong>来优化预测结果，这一过程涉及前向传播、损失计算、反向传播和参数更新。</p><p>神经网络的类型包括<strong>前馈神经网络</strong>、<strong>卷积神经网络（CNN）</strong>、<strong>循环神经网络（RNN）<strong>和</strong>长短期记忆网络（LSTM）</strong>，它们在图像识别、语音处理、自然语言处理等多个领域都有广泛应用。</p><p>PyTorch 提供了一个非常方便的接口来<strong>构建神经网络模</strong>型，即 <strong>torch.nn.Module</strong>。</p><p>我们可以继承 nn.Module 类并定义自己的网络层。</p><p>创建一个简单的神经网络：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 定义一个简单的全连接神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 输入层到隐藏层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 隐藏层到输出层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># ReLU 激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建网络实例</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"><span class="comment"># 打印模型结构</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><p><strong>训练过程：</strong></p><ol><li><strong>前向传播（Forward Propagation）</strong>： 在前向传播阶段，<strong>输入</strong>数据通过网络层传递，每层<strong>应用权重和激活函数</strong>，<strong>直到产生输出</strong>。</li><li><strong>计算损失（Calculate Loss）</strong>： 根据网络的<strong>输出</strong>和<strong>真实</strong>标签，计算<strong>损失函数的值</strong>。</li><li><strong>反向传播（Backpropagation）</strong>： 反向传播利用<strong>自动求导</strong>技术计算损失函数关于每个参数的<strong>梯度</strong>。</li><li><strong>参数更新（Parameter Update）</strong>： 使用<strong>优化器</strong>根据梯度更新网络的权重和偏置。</li><li><strong>迭代（Iteration）</strong>： 重复上述过程，直到模型在训练数据上的性能达到满意的水平。</li></ol><h3 id="前向传播与损失计算"><a href="#前向传播与损失计算" class="headerlink" title="前向传播与损失计算"></a>前向传播与损失计算</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机输入</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = model(x)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="comment"># 定义损失函数（例如均方误差 MSE）</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"><span class="comment"># 假设目标值为 1</span></span><br><span class="line">target = torch.randn(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure><h3 id="优化器（Optimizers）"><a href="#优化器（Optimizers）" class="headerlink" title="优化器（Optimizers）"></a>优化器（Optimizers）</h3><p>优化器在训练过程中更新神经网络的参数，以减少损失函数的值。</p><p>PyTorch 提供了多种优化器，例如 <strong>SGD、Adam</strong> 等。</p><p>使用优化器进行参数更新：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义优化器（使用 Adam 优化器）</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 训练步骤</span></span><br><span class="line">optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">optimizer.step()  <span class="comment"># 更新参数</span></span><br></pre></td></tr></table></figure><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>训练模型是机器学习和深度学习中的核心过程，旨在通过大量数据学习模型参数，以便模型能够对新的、未见过的数据做出准确的预测。</p><p>训练模型通常包括以下几个步骤：</p><ol><li><strong>数据准备</strong>：<ul><li>收集和处理数据，包括清洗、标准化和归一化。</li><li>将数据分为训练集、验证集和测试集。</li></ul></li><li><strong>定义模型</strong>：<ul><li>选择<strong>模型架构</strong>，例如决策树、神经网络等。</li><li><strong>初始化模型参数</strong>（权重和偏置）。</li></ul></li><li><strong>选择损失函数</strong>：<ul><li><strong>根据任务类型</strong>（如分类、回归）选择合适的损失函数。</li></ul></li><li><strong>选择优化器</strong>：<ul><li><strong>选择一个优化算法</strong>，如SGD、Adam等，来更新模型参数。</li></ul></li><li><strong>前向传播</strong>：<ul><li>在每次迭代中，将输入数据<strong>通过模型传递</strong>，计算预测输出。</li></ul></li><li><strong>计算损失</strong>：<ul><li>使用<strong>损失函数评估</strong>预测输出与真实标签之间的差异。</li></ul></li><li><strong>反向传播</strong>：<ul><li>利用自动求导计算损失相对于模型参数的梯度。</li></ul></li><li><strong>参数更新</strong>：<ul><li>根据计算出的梯度和优化器的策略更新模型参数。</li></ul></li><li><strong>迭代优化</strong>：<ul><li><strong>重复步骤5-8</strong>，直到模型在验证集上的性能不再提升或达到预定的迭代次数。</li></ul></li><li><strong>评估和测试</strong>：<ul><li>使用测试集评估模型的最终性能，确保模型没有过拟合。</li></ul></li><li><strong>模型调优</strong>：<ul><li>根据模型在测试集上的表现进行调参，如改变学习率、增加正则化等。</li></ul></li><li><strong>部署模型</strong>：<ul><li>将训练好的模型部署到生产环境中，用于实际的预测任务。</li></ul></li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 1. 定义一个简单的神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 输入层到隐藏层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 隐藏层到输出层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># ReLU 激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 2. 创建模型实例</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"><span class="comment"># 3. 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.MSELoss()  <span class="comment"># 均方误差损失函数</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)  <span class="comment"># Adam 优化器</span></span><br><span class="line"><span class="comment"># 4. 假设我们有训练数据 X 和 Y</span></span><br><span class="line">X = torch.randn(<span class="number">10</span>, <span class="number">2</span>)  <span class="comment"># 10 个样本，2 个特征</span></span><br><span class="line">Y = torch.randn(<span class="number">10</span>, <span class="number">1</span>)  <span class="comment"># 10 个目标值</span></span><br><span class="line"><span class="comment"># 5. 训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 训练 100 轮</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清空之前的梯度</span></span><br><span class="line">    output = model(X)  <span class="comment"># 前向传播</span></span><br><span class="line">    loss = criterion(output, Y)  <span class="comment"># 计算损失</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="comment"># 每 10 轮输出一次损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/100], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>在每 10 轮，程序会输出当前的损失值，帮助我们跟踪模型的<strong>训练进度</strong>。随着训练的进行，损失值应该会逐渐降低，表示模型在不断学习并优化其参数。</p><p>训练模型是一个迭代的过程，需要不断地调整和优化，直到达到满意的性能。这个过程涉及到大量的实验和调优，目的是使模型在新的、未见过的数据上也能有良好的泛化能力。</p><hr><h2 id="设备（Device）"><a href="#设备（Device）" class="headerlink" title="设备（Device）"></a>设备（Device）</h2><p>PyTorch 允许你将模型和数据移动到 GPU 上进行加速。</p><p>使用 <strong>torch.device</strong> 来指定计算设备。</p><p>将模型和数据移至 GPU:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># 将模型移动到设备</span></span><br><span class="line">model.to(device)</span><br><span class="line"><span class="comment"># 将数据移动到设备</span></span><br><span class="line">X = X.to(device)</span><br><span class="line">Y = Y.to(device)</span><br></pre></td></tr></table></figure><h1 id="PyTorch-张量（Tensor）"><a href="#PyTorch-张量（Tensor）" class="headerlink" title="PyTorch 张量（Tensor）"></a>PyTorch 张量（Tensor）</h1><p>张量是一个多维数组，可以是标量、向量、矩阵或更高维度的数据结构。</p><p>在 PyTorch 中，张量（Tensor）是<strong>数据的核心表示形式</strong>，类似于 NumPy 的多维数组，但具有更强大的功能，例如<strong>支持 GPU 加速</strong>和自动梯度计算。</p><p>张量支持多种数据类型（整型、浮点型、布尔型等）。</p><p>张量可以存储在 CPU 或 GPU 中，GPU 张量可显著加速计算。</p><p>下图展示了不同维度的张量（Tensor）在 PyTorch 中的表示方法：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1__D5ZvufDS38WkhK9rK32hQ.jpg" alt="img"></p><p><strong>说明：</strong></p><ul><li><strong>1D Tensor &#x2F; Vector（一维张量&#x2F;向量）:</strong> 最基本的张量形式，可以看作是一个数组，图中的例子是一个包含 10 个元素的向量。</li><li><strong>2D Tensor &#x2F; Matrix（二维张量&#x2F;矩阵）:</strong> 二维数组，通常用于表示矩阵，图中的例子是一个 4x5 的矩阵，包含了 20 个元素。</li><li><strong>3D Tensor &#x2F; Cube（三维张量&#x2F;立方体）:</strong> 三维数组，可以看作是由多个矩阵堆叠而成的立方体，图中的例子展示了一个 3x4x5 的立方体，其中每个 5x5 的矩阵代表立方体的一个”层”。</li><li><strong>4D Tensor &#x2F; Vector of Cubes（四维张量&#x2F;立方体向量）:</strong> 四维数组，可以看作是由多个立方体组成的向量，图中的例子没有具体数值，但可以理解为一个<strong>包含多个 3D 张量的集合</strong>。</li><li><strong>5D Tensor &#x2F; Matrix of Cubes（五维张量&#x2F;立方体矩阵）:</strong> 五维数组，可以看作是由多个4D张量组成的矩阵，图中的例子同样没有具体数值，但可以理解为一个包含多个 4D 张量的集合。</li></ul><h2 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h2><p>张量创建的方式有：</p><table><thead><tr><th align="left"><strong>方法</strong></th><th align="left"><strong>说明</strong></th><th align="left"><strong>示例代码</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.tensor(data)</code></td><td align="left"><strong>从 Python 列表或 NumPy 数组</strong>创建张量。</td><td align="left"><code>x = torch.tensor([[1, 2], [3, 4]])</code></td></tr><tr><td align="left"><code>torch.zeros(size)</code></td><td align="left">创建一个<strong>全为零</strong>的张量。</td><td align="left"><code>x = torch.zeros((2, 3))</code></td></tr><tr><td align="left"><code>torch.ones(size)</code></td><td align="left">创建一个<strong>全为 1</strong> 的张量。</td><td align="left"><code>x = torch.ones((2, 3))</code></td></tr><tr><td align="left"><code>torch.empty(size)</code></td><td align="left">创建一个<strong>未初始化</strong>的张量。</td><td align="left"><code>x = torch.empty((2, 3))</code></td></tr><tr><td align="left"><code>torch.rand(size)</code></td><td align="left">创建一个<strong>服从均匀分布的随机张量</strong>，值在 <code>[0, 1)</code>。</td><td align="left"><code>x = torch.rand((2, 3))</code></td></tr><tr><td align="left"><code>torch.randn(size)</code></td><td align="left">创建一个<strong>服从正态分布的随机张量</strong>，均值为 0，标准差为 1。</td><td align="left"><code>x = torch.randn((2, 3))</code></td></tr><tr><td align="left"><code>torch.arange(start, end, step)</code></td><td align="left">创建一个<strong>一维序列</strong>张量，类似于 Python 的 <code>range</code>。</td><td align="left"><code>x = torch.arange(0, 10, 2)</code></td></tr><tr><td align="left"><code>torch.linspace(start, end, steps)</code></td><td align="left">创建一个在指定范围内<strong>等间隔</strong>的序列张量。</td><td align="left"><code>x = torch.linspace(0, 1, 5)</code></td></tr><tr><td align="left"><code>torch.eye(size)</code></td><td align="left">创建一个<strong>单位矩阵</strong>（对角线为 1，其他为 0）。</td><td align="left"><code>x = torch.eye(3)</code></td></tr><tr><td align="left"><code>torch.from_numpy(ndarray)</code></td><td align="left"><strong>将 NumPy 数组转换</strong>为张量。</td><td align="left"><code>x = torch.from_numpy(np.array([1, 2, 3]))</code></td></tr></tbody></table><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tensor = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure><p>如果你有一个 NumPy 数组，可以使用 torch.from_numpy() 将其转换为张量：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np_array = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor = torch.from_numpy(np_array)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3])</span><br></pre></td></tr></table></figure><p>创建 2D 张量（矩阵）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tensor_2d = torch.tensor([</span><br><span class="line">    [-<span class="number">9</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">0</span>, <span class="number">12</span>, <span class="number">8</span>, <span class="number">6</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">23</span>, -<span class="number">6</span>, <span class="number">45</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">22</span>, <span class="number">3</span>, -<span class="number">1</span>, <span class="number">72</span>, <span class="number">6</span>]</span><br><span class="line">])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;2D Tensor (Matrix):\n&quot;</span>, tensor_2d)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor_2d.shape)  <span class="comment"># 形状</span></span><br></pre></td></tr></table></figure><p>其他维度的创建：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 3D 张量（立方体）</span></span><br><span class="line">tensor_3d = torch.stack([tensor_2d, tensor_2d + <span class="number">10</span>, tensor_2d - <span class="number">5</span>])  <span class="comment"># 堆叠 3 个 2D 张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;3D Tensor (Cube):\n&quot;</span>, tensor_3d)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor_3d.shape)  <span class="comment"># 形状</span></span><br><span class="line"><span class="comment"># 创建 4D 张量（向量的立方体）</span></span><br><span class="line">tensor_4d = torch.stack([tensor_3d, tensor_3d + <span class="number">100</span>])  <span class="comment"># 堆叠 2 个 3D 张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;4D Tensor (Vector of Cubes):\n&quot;</span>, tensor_4d)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor_4d.shape)  <span class="comment"># 形状</span></span><br><span class="line"><span class="comment"># 创建 5D 张量（矩阵的立方体）</span></span><br><span class="line">tensor_5d = torch.stack([tensor_4d, tensor_4d + <span class="number">1000</span>])  <span class="comment"># 堆叠 2 个 4D 张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;5D Tensor (Matrix of Cubes):\n&quot;</span>, tensor_5d)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor_5d.shape)  <span class="comment"># 形状</span></span><br></pre></td></tr></table></figure><h2 id="张量的属性"><a href="#张量的属性" class="headerlink" title="张量的属性"></a>张量的属性</h2><p>张量的属性如下表：</p><table><thead><tr><th align="left"><strong>属性</strong></th><th align="left"><strong>说明</strong></th><th align="left"><strong>示例</strong></th></tr></thead><tbody><tr><td align="left"><code>.shape</code></td><td align="left">获取张量的形状</td><td align="left"><code>tensor.shape</code></td></tr><tr><td align="left"><code>.size()</code></td><td align="left">获取张量的形状</td><td align="left"><code>tensor.size()</code></td></tr><tr><td align="left"><code>.dtype</code></td><td align="left">获取张量的<strong>数据类型</strong></td><td align="left"><code>tensor.dtype</code></td></tr><tr><td align="left"><code>.device</code></td><td align="left">查看张量<strong>所在的设备</strong> (CPU&#x2F;GPU)</td><td align="left"><code>tensor.device</code></td></tr><tr><td align="left"><code>.dim()</code></td><td align="left">获取张量的<strong>维度数</strong></td><td align="left"><code>tensor.dim()</code></td></tr><tr><td align="left"><code>.requires_grad</code></td><td align="left">是否启用<strong>梯度</strong>计算</td><td align="left"><code>tensor.requires_grad</code></td></tr><tr><td align="left"><code>.numel()</code></td><td align="left">获取张量中的<strong>元素总数</strong></td><td align="left"><code>tensor.numel()</code></td></tr><tr><td align="left"><code>.is_cuda</code></td><td align="left">检查张量<strong>是否在 GPU 上</strong></td><td align="left"><code>tensor.is_cuda</code></td></tr><tr><td align="left"><code>.T</code></td><td align="left">获取张量的<strong>转置</strong>（适用于 <strong>2D 张量</strong>）</td><td align="left"><code>tensor.T</code></td></tr><tr><td align="left"><code>.item()</code></td><td align="left">获取<strong>单元素张量的值</strong></td><td align="left"><code>tensor.item()</code></td></tr><tr><td align="left"><code>.is_contiguous()</code></td><td align="left">检查张量<strong>是否连续</strong>存储</td><td align="left"><code>tensor.is_contiguous()</code></td></tr></tbody></table><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个 2D 张量</span></span><br><span class="line">tensor = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 张量的属性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor:\n&quot;</span>, tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor.shape)  <span class="comment"># 获取形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Size:&quot;</span>, tensor.size())  <span class="comment"># 获取形状（另一种方法）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Data Type:&quot;</span>, tensor.dtype)  <span class="comment"># 数据类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Device:&quot;</span>, tensor.device)  <span class="comment"># 设备</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Dimensions:&quot;</span>, tensor.dim())  <span class="comment"># 维度数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Total Elements:&quot;</span>, tensor.numel())  <span class="comment"># 元素总数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Requires Grad:&quot;</span>, tensor.requires_grad)  <span class="comment"># 是否启用梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Is CUDA:&quot;</span>, tensor.is_cuda)  <span class="comment"># 是否在 GPU 上</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Is Contiguous:&quot;</span>, tensor.is_contiguous())  <span class="comment"># 是否连续存储</span></span><br><span class="line"><span class="comment"># 获取单元素值</span></span><br><span class="line">single_value = torch.tensor(<span class="number">42</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Single Element Value:&quot;</span>, single_value.item())</span><br><span class="line"><span class="comment"># 转置张量</span></span><br><span class="line">tensor_T = tensor.T</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Transposed Tensor:\n&quot;</span>, tensor_T)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">         [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">Shape: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Data <span class="type">Type</span>: torch.float32</span><br><span class="line">Device: cpu</span><br><span class="line">Dimensions: <span class="number">2</span></span><br><span class="line">Total Elements: <span class="number">6</span></span><br><span class="line">Requires Grad: <span class="literal">False</span></span><br><span class="line">Is CUDA: <span class="literal">False</span></span><br><span class="line">Is Contiguous: <span class="literal">True</span></span><br><span class="line">Single Element Value: <span class="number">42</span></span><br><span class="line">Transposed Tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">4.</span>],</span><br><span class="line">         [<span class="number">2.</span>, <span class="number">5.</span>],</span><br><span class="line">         [<span class="number">3.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure><h2 id="张量的操作"><a href="#张量的操作" class="headerlink" title="张量的操作"></a>张量的操作</h2><p>张量操作方法说明如下。</p><h4 id="基础操作："><a href="#基础操作：" class="headerlink" title="基础操作："></a>基础操作：</h4><table><thead><tr><th align="left"><strong>操作</strong></th><th align="left"><strong>说明</strong></th><th align="left"><strong>示例代码</strong></th></tr></thead><tbody><tr><td align="left"><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code></td><td align="left">元素级加法、减法、乘法、除法。</td><td align="left"><code>z = x + y</code></td></tr><tr><td align="left"><code>torch.matmul(x, y)</code></td><td align="left">矩阵乘法。</td><td align="left"><code>z = torch.matmul(x, y)</code></td></tr><tr><td align="left"><code>torch.dot(x, y)</code></td><td align="left">向量点积（仅适用于 1D 张量）。</td><td align="left"><code>z = torch.dot(x, y)</code></td></tr><tr><td align="left"><code>torch.sum(x)</code></td><td align="left">求和。</td><td align="left"><code>z = torch.sum(x)</code></td></tr><tr><td align="left"><code>torch.mean(x)</code></td><td align="left">求均值。</td><td align="left"><code>z = torch.mean(x)</code></td></tr><tr><td align="left"><code>torch.max(x)</code></td><td align="left">求最大值。</td><td align="left"><code>z = torch.max(x)</code></td></tr><tr><td align="left"><code>torch.min(x)</code></td><td align="left">求最小值。</td><td align="left"><code>z = torch.min(x)</code></td></tr><tr><td align="left"><code>torch.argmax(x, dim)</code></td><td align="left">返回<strong>最大值的索引</strong>（指定维度）。</td><td align="left"><code>z = torch.argmax(x, dim=1)</code></td></tr><tr><td align="left"><code>torch.softmax(x, dim)</code></td><td align="left">计算 softmax（指定维度）。</td><td align="left"><code>z = torch.softmax(x, dim=1)</code></td></tr></tbody></table><h4 id="形状操作"><a href="#形状操作" class="headerlink" title="形状操作"></a><strong>形状操作</strong></h4><table><thead><tr><th align="left"><strong>操作</strong></th><th align="left"><strong>说明</strong></th><th align="left"><strong>示例代码</strong></th></tr></thead><tbody><tr><td align="left"><code>x.view(shape)</code></td><td align="left">改变张量的<strong>形状</strong>（<strong>不改变数据</strong>）。</td><td align="left"><code>z = x.view(3, 4)</code></td></tr><tr><td align="left"><code>x.reshape(shape)</code></td><td align="left">类似于 <code>view</code>，但更灵活。</td><td align="left"><code>z = x.reshape(3, 4)</code></td></tr><tr><td align="left"><code>x.t()</code></td><td align="left"><strong>转置</strong>矩阵。</td><td align="left"><code>z = x.t()</code></td></tr><tr><td align="left"><code>x.unsqueeze(dim)</code></td><td align="left">在<strong>指定维度</strong>添加一个维度。</td><td align="left"><code>z = x.unsqueeze(0)</code></td></tr><tr><td align="left"><code>x.squeeze(dim)</code></td><td align="left">去掉指定维度为 1 的维度。</td><td align="left"><code>z = x.squeeze(0)</code></td></tr><tr><td align="left"><code>torch.cat((x, y), dim)</code></td><td align="left">按指定维度连接多个张量。</td><td align="left"><code>z = torch.cat((x, y), dim=1)</code></td></tr></tbody></table><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个 2D 张量</span></span><br><span class="line">tensor = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始张量:\n&quot;</span>, tensor)</span><br><span class="line"><span class="comment"># 1. **索引和切片操作**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【索引和切片】&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;获取第一行:&quot;</span>, tensor[<span class="number">0</span>])  <span class="comment"># 获取第一行</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;获取第一行第一列的元素:&quot;</span>, tensor[<span class="number">0</span>, <span class="number">0</span>])  <span class="comment"># 获取特定元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;获取第二列的所有元素:&quot;</span>, tensor[:, <span class="number">1</span>])  <span class="comment"># 获取第二列所有元素</span></span><br><span class="line"><span class="comment"># 2. **形状变换操作**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【形状变换】&quot;</span>)</span><br><span class="line">reshaped = tensor.view(<span class="number">3</span>, <span class="number">2</span>)  <span class="comment"># 改变张量形状为 3x2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;改变形状后的张量:\n&quot;</span>, reshaped)</span><br><span class="line">flattened = tensor.flatten()  <span class="comment"># 将张量展平成一维</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;展平后的张量:\n&quot;</span>, flattened)</span><br><span class="line"><span class="comment"># 3. **数学运算操作**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【数学运算】&quot;</span>)</span><br><span class="line">tensor_add = tensor + <span class="number">10</span>  <span class="comment"># 张量加法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量加 10:\n&quot;</span>, tensor_add)</span><br><span class="line">tensor_mul = tensor * <span class="number">2</span>  <span class="comment"># 张量乘法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量乘 2:\n&quot;</span>, tensor_mul)</span><br><span class="line">tensor_sum = tensor.<span class="built_in">sum</span>()  <span class="comment"># 计算所有元素的和</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量元素的和:&quot;</span>, tensor_sum.item())</span><br><span class="line"><span class="comment"># 4. **与其他张量的操作**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【与其他张量操作】&quot;</span>)</span><br><span class="line">tensor2 = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;另一个张量:\n&quot;</span>, tensor2)</span><br><span class="line">tensor_dot = torch.matmul(tensor, tensor2.T)  <span class="comment"># 张量矩阵乘法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵乘法结果:\n&quot;</span>, tensor_dot)</span><br><span class="line"><span class="comment"># 5. **条件判断和筛选**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【条件判断和筛选】&quot;</span>)</span><br><span class="line">mask = tensor &gt; <span class="number">3</span>  <span class="comment"># 创建一个布尔掩码</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;大于 3 的元素的布尔掩码:\n&quot;</span>, mask)</span><br><span class="line">filtered_tensor = tensor[tensor &gt; <span class="number">3</span>]  <span class="comment"># 筛选出符合条件的元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;大于 3 的元素:\n&quot;</span>, filtered_tensor)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">原始张量:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">         [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">【索引和切片】</span><br><span class="line">获取第一行: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">获取第一行第一列的元素: tensor(<span class="number">1.</span>)</span><br><span class="line">获取第二列的所有元素: tensor([<span class="number">2.</span>, <span class="number">5.</span>])</span><br><span class="line">【形状变换】</span><br><span class="line">改变形状后的张量:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">         [<span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">         [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">展平后的张量:</span><br><span class="line"> tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line">【数学运算】</span><br><span class="line">张量加 <span class="number">10</span>:</span><br><span class="line"> tensor([[<span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>],</span><br><span class="line">         [<span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>]])</span><br><span class="line">张量乘 <span class="number">2</span>:</span><br><span class="line"> tensor([[ <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>],</span><br><span class="line">         [ <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">12.</span>]])</span><br><span class="line">张量元素的和: <span class="number">21.0</span></span><br><span class="line">【与其他张量操作】</span><br><span class="line">另一个张量:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">矩阵乘法结果:</span><br><span class="line"> tensor([[ <span class="number">6.</span>,  <span class="number">6.</span>],</span><br><span class="line">         [<span class="number">15.</span>, <span class="number">15.</span>]])</span><br><span class="line">【条件判断和筛选】</span><br><span class="line">大于 <span class="number">3</span> 的元素的布尔掩码:</span><br><span class="line"> tensor([[<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">         [ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>]])</span><br><span class="line">大于 <span class="number">3</span> 的元素:</span><br><span class="line"> tensor([<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br></pre></td></tr></table></figure><h2 id="张量的-GPU-加速"><a href="#张量的-GPU-加速" class="headerlink" title="张量的 GPU 加速"></a>张量的 GPU 加速</h2><p>将张量转移到 GPU：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line">x = torch.tensor([1.0, 2.0, 3.0], device=device)</span><br></pre></td></tr></table></figure><p>检查 GPU 是否可用：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available()  # 返回 True 或 False</span><br></pre></td></tr></table></figure><hr><h2 id="张量与-NumPy-的互操作"><a href="#张量与-NumPy-的互操作" class="headerlink" title="张量与 NumPy 的互操作"></a>张量与 NumPy 的互操作</h2><p>张量与 NumPy 的互操作如下表所示：</p><table><thead><tr><th align="left"><strong>操作</strong></th><th align="left"><strong>说明</strong></th><th align="left"><strong>示例代码</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.from_numpy(ndarray)</code></td><td align="left">将 NumPy 数组转换为张量。</td><td align="left"><code>x = torch.from_numpy(np_array)</code></td></tr><tr><td align="left"><code>x.numpy()</code></td><td align="left">将张量<strong>转换为 NumPy 数组</strong>（仅限 <strong>CPU 张量</strong>）。</td><td align="left"><code>np_array = x.numpy()</code></td></tr></tbody></table><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 1. NumPy 数组转换为 PyTorch 张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1. NumPy 转为 PyTorch 张量&quot;</span>)</span><br><span class="line">numpy_array = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy 数组:\n&quot;</span>, numpy_array)</span><br><span class="line"><span class="comment"># 使用 torch.from_numpy() 将 NumPy 数组转换为张量</span></span><br><span class="line">tensor_from_numpy = torch.from_numpy(numpy_array)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;转换后的 PyTorch 张量:\n&quot;</span>, tensor_from_numpy)</span><br><span class="line"><span class="comment"># 修改 NumPy 数组，观察张量的变化（共享内存）</span></span><br><span class="line">numpy_array[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;修改后的 NumPy 数组:\n&quot;</span>, numpy_array)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;PyTorch 张量也会同步变化:\n&quot;</span>, tensor_from_numpy)</span><br><span class="line"><span class="comment"># 2. PyTorch 张量转换为 NumPy 数组</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n2. PyTorch 张量转为 NumPy 数组&quot;</span>)</span><br><span class="line">tensor = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;PyTorch 张量:\n&quot;</span>, tensor)</span><br><span class="line"><span class="comment"># 使用 tensor.numpy() 将张量转换为 NumPy 数组</span></span><br><span class="line">numpy_from_tensor = tensor.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;转换后的 NumPy 数组:\n&quot;</span>, numpy_from_tensor)</span><br><span class="line"><span class="comment"># 修改张量，观察 NumPy 数组的变化（共享内存）</span></span><br><span class="line">tensor[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">77</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;修改后的 PyTorch 张量:\n&quot;</span>, tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy 数组也会同步变化:\n&quot;</span>, numpy_from_tensor)</span><br><span class="line"><span class="comment"># 3. 注意：不共享内存的情况（需要复制数据）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n3. 使用 clone() 保证独立数据&quot;</span>)</span><br><span class="line">tensor_independent = torch.tensor([[<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>], [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]], dtype=torch.float32)</span><br><span class="line">numpy_independent = tensor_independent.clone().numpy()  <span class="comment"># 使用 clone 复制数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始张量:\n&quot;</span>, tensor_independent)</span><br><span class="line">tensor_independent[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># 修改张量数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;修改后的张量:\n&quot;</span>, tensor_independent)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy 数组（不会同步变化）:\n&quot;</span>, numpy_independent)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> NumPy 转为 PyTorch 张量</span><br><span class="line">NumPy 数组:</span><br><span class="line"> [[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line">转换后的 PyTorch 张量:</span><br><span class="line"> tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">         [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">修改后的 NumPy 数组:</span><br><span class="line"> [[<span class="number">100</span>   <span class="number">2</span>   <span class="number">3</span>]</span><br><span class="line"> [  <span class="number">4</span>   <span class="number">5</span>   <span class="number">6</span>]]</span><br><span class="line">PyTorch 张量也会同步变化:</span><br><span class="line"> tensor([[<span class="number">100</span>,   <span class="number">2</span>,   <span class="number">3</span>],</span><br><span class="line">         [  <span class="number">4</span>,   <span class="number">5</span>,   <span class="number">6</span>]])</span><br><span class="line"><span class="number">2.</span> PyTorch 张量转为 NumPy 数组</span><br><span class="line">PyTorch 张量:</span><br><span class="line"> tensor([[ <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">         [<span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>]])</span><br><span class="line">转换后的 NumPy 数组:</span><br><span class="line"> [[ <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span>]</span><br><span class="line"> [<span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span>]]</span><br><span class="line">修改后的 PyTorch 张量:</span><br><span class="line"> tensor([[<span class="number">77.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">         [<span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>]])</span><br><span class="line">NumPy 数组也会同步变化:</span><br><span class="line"> [[<span class="number">77.</span>  <span class="number">8.</span>  <span class="number">9.</span>]</span><br><span class="line"> [<span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span>]]</span><br><span class="line"><span class="number">3.</span> 使用 clone() 保证独立数据</span><br><span class="line">原始张量:</span><br><span class="line"> tensor([[<span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>]])</span><br><span class="line">修改后的张量:</span><br><span class="line"> tensor([[ <span class="number">0.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>]])</span><br><span class="line">NumPy 数组（不会同步变化）:</span><br><span class="line"> [[<span class="number">13.</span> <span class="number">14.</span> <span class="number">15.</span>]</span><br><span class="line"> [<span class="number">16.</span> <span class="number">17.</span> <span class="number">18.</span>]]</span><br></pre></td></tr></table></figure><h1 id="PyTorch-神经网络基础"><a href="#PyTorch-神经网络基础" class="headerlink" title="PyTorch 神经网络基础"></a>PyTorch 神经网络基础</h1><p>神经网络是一种模仿人脑处理信息方式的计算模型，它由许多相互连接的节点（神经元）组成，这些节点按层次排列。</p><p>神经网络的强大之处在于其能够<strong>自动</strong>从大量数据中学习复杂的模式和特征，<strong>无需人工设计特征提取器</strong>。</p><p>随着深度学习的发展，神经网络已经成为解决许多复杂问题的关键技术。</p><h3 id="神经元（Neuron）"><a href="#神经元（Neuron）" class="headerlink" title="神经元（Neuron）"></a>神经元（Neuron）</h3><p>神经元是神经网络的基本单元，它接收输入信号，通过<strong>加权求和后与偏置（bias）相加</strong>，然后<strong>通过激活函数处理以产生输出</strong>。</p><p>神经元的<strong>权重</strong>和<strong>偏置</strong>是网络学习过程中<strong>需要调整的参数</strong>。</p><p><strong>输入和输出:</strong></p><ul><li><strong>输入（Input）</strong>：输入是网络的起始点，可以是特征数据，如图像的像素值或文本的词向量。</li><li><strong>输出（Output）</strong>：输出是网络的终点，表示模型的预测结果，如分类任务中的类别标签。</li></ul><p>神经元接收<strong>多个输入</strong>（例如x1, x2, …, xn），如果输入的<strong>加权和大于激活阈值</strong>（activation potential），则<strong>产生二进制输出</strong>。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_upfpVueoUuKPkyX3PR3KBg.png" alt="img">神经元的输出可以看作是<strong>输入的加权和加上偏置（bias)</strong>，神经元的数学表示：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/f0b929045ae6eef23514bd7024be62f0.png" alt="img"></p><p>这里，<strong>wj</strong> 是权重，<strong>xj</strong> 是输入，而 <strong>Bias</strong> 是偏置项。</p><h3 id="层（Layer）"><a href="#层（Layer）" class="headerlink" title="层（Layer）"></a>层（Layer）</h3><p>输入层和输出层之间的层被称为隐藏层，层与层之间的连接密度和类型构成了网络的配置。</p><p>神经网络由多个层组成，包括：</p><ul><li><strong>输入层（Input Layer）</strong>：接收原始输入数据。</li><li><strong>隐藏层（Hidden Layer）</strong>：对输入数据进行处理，可以有多个隐藏层。</li><li><strong>输出层（Output Layer）</strong>：产生最终的输出结果。</li></ul><p>典型的神经网络架构:</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_3fA77_mLNiJTSgZFhYnU0Q3K5DV4.webp" alt="img"></p><h3 id="前馈神经网络（Feedforward-Neural-Network，FNN）"><a href="#前馈神经网络（Feedforward-Neural-Network，FNN）" class="headerlink" title="前馈神经网络（Feedforward Neural Network，FNN）"></a>前馈神经网络（Feedforward Neural Network，FNN）</h3><p>前馈神经网络（Feedforward Neural Network，FNN）是神经网络家族中的基本单元。</p><p>前馈神经网络特点是数据从输入层开始，<strong>经过一个或多个隐藏</strong>层，最后到达输出层，全过程没有循环或反馈。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/neural-net.png" alt="img"></p><p><strong>前馈神经网络的基本结构：</strong></p><ul><li><strong>输入层：</strong> 数据进入网络的入口点。输入层的每个节点代表一个输入特征。</li><li><strong>隐藏层：<strong>一个或多个层，用于捕获数据的</strong>非线性</strong>特征。每个隐藏层由多个神经元组成，每个神经元通过激活函数增加非线性能力。</li><li>**输出层：**输出网络的预测结果。<strong>节点数和问题类型相关</strong>，例如分类问题的输出节点数等于类别数。</li><li>**连接权重与偏置：**每个神经元的输入通过权重进行加权求和，并加上偏置值，然后通过激活函数传递。</li></ul><h3 id="循环神经网络（Recurrent-Neural-Network-RNN）"><a href="#循环神经网络（Recurrent-Neural-Network-RNN）" class="headerlink" title="循环神经网络（Recurrent Neural Network, RNN）"></a>循环神经网络（Recurrent Neural Network, RNN）</h3><p>循环神经网络（Recurrent Neural Network, RNN）络是一类<strong>专门处理序列数据</strong>的神经网络，能够捕获输入数据中<strong>时间或顺序信息的依赖</strong>关系。</p><p>RNN 的特别之处在于它具有”<strong>记忆</strong>能力”，可以在网络的隐藏状态中保存之前时间步的信息。</p><p>循环神经网络用于<strong>处理随时间变化的数据模式</strong>。</p><p>在 RNN 中，<strong>相同的层被用来接收</strong>输入参数，并在<strong>指定的神经网络中显示输出</strong>参数。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/0_xs3Dya3qQBx6IU7C.png" alt="img"></p><p>PyTorch 提供了强大的工具来构建和训练神经网络。</p><p>神经网络在 PyTorch 中是通过 <strong>torch.nn</strong> 模块来实现的。</p><p><strong>torch.nn</strong> 模块提供了各种网络层（如全连接层、卷积层等）、损失函数和优化器，让神经网络的构建和训练变得更加方便。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_3DUs-90altOgaBcVJ9LTGg.png" alt="img"></p><p>在 PyTorch 中，构建神经网络通常需要继承 nn.Module 类。</p><p>nn.Module 是所有神经网络模块的基类，你需要定义以下两个部分：</p><ul><li><strong><code>__init__()</code></strong>：定义网络层。</li><li><strong><code>forward()</code></strong>：定义数据的前向传播过程。</li></ul><p>简单的全连接神经网络（Fully Connected Network）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># 定义一个简单的神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个输入层到隐藏层的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 输入 2 个特征，输出 2 个特征</span></span><br><span class="line">        <span class="comment"># 定义一个隐藏层到输出层的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 输入 2 个特征，输出 1 个预测值    </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 前向传播过程</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># 使用 ReLU 激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)  <span class="comment"># 输出层</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"><span class="comment"># 打印模型</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SimpleNN(</span><br><span class="line">  (fc1): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc2): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>PyTorch 提供了许多常见的神经网络层，以下是几个常见的：</p><ul><li><strong><code>nn.Linear(in_features, out_features)</code></strong>：<strong>全连接层</strong>，输入 <code>in_features</code> 个特征，输出 <code>out_features</code> 个特征。</li><li><strong><code>nn.Conv2d(in_channels, out_channels, kernel_size)</code></strong>：<strong>2D 卷积层</strong>，用于图像处理。</li><li><strong><code>nn.MaxPool2d(kernel_size)</code></strong>：<strong>2D 最大池化</strong>层，用于降维。</li><li><strong><code>nn.ReLU()</code></strong>：ReLU <strong>激活函数</strong>，常用于隐藏层。</li><li><strong><code>nn.Softmax(dim)</code></strong>：Softmax 激活函数，<strong>通常用于输出层</strong>，适用于<strong>多类分类</strong>问题。</li></ul><h3 id="激活函数（Activation-Function）"><a href="#激活函数（Activation-Function）" class="headerlink" title="激活函数（Activation Function）"></a>激活函数（Activation Function）</h3><p>激活函数决定了神经元是否应该被激活。它们是<strong>非线性函数</strong>，使得神经网络能够学习和执行更复杂的任务。常见的激活函数包括：</p><ul><li><strong>Sigmoid</strong>：用于二分类问题，输出值在 0 和 1 之间。</li><li><strong>Tanh</strong>：输出值在 -1 和 1 之间，常用于输出层之前。</li><li><strong>ReLU</strong>（Rectified Linear Unit）：目前最流行的激活函数之一，定义为 <code>f(x) = max(0, x)</code>，有助于<strong>解决梯度消失问题</strong>。</li><li><strong>Softmax</strong>：常用于多分类问题的输出层，将输出转换为<strong>概率分布</strong>。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># ReLU 激活</span></span><br><span class="line">output = F.relu(input_tensor)</span><br><span class="line"><span class="comment"># Sigmoid 激活</span></span><br><span class="line">output = torch.sigmoid(input_tensor)</span><br><span class="line"><span class="comment"># Tanh 激活</span></span><br><span class="line">output = torch.tanh(input_tensor)</span><br></pre></td></tr></table></figure><h3 id="损失函数（Loss-Function）"><a href="#损失函数（Loss-Function）" class="headerlink" title="损失函数（Loss Function）"></a>损失函数（Loss Function）</h3><p>损失函数用于衡量模型的预测值与真实值之间的差异。</p><p>常见的损失函数包括：</p><ul><li><strong>均方误差（MSELoss）</strong>：<strong>回归</strong>问题常用，计算输出与目标值的<strong>平方差</strong>。</li><li><strong>交叉熵损失（CrossEntropyLoss）</strong>：<strong>分类</strong>问题常用，计算输出和真实标签之间的<strong>交叉熵</strong>。</li><li><strong>BCEWithLogitsLoss</strong>：<strong>二分类</strong>问题，结合了 Sigmoid 激活和二元交叉熵损失。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 均方误差损失</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"><span class="comment"># 交叉熵损失</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 二分类交叉熵损失</span></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br></pre></td></tr></table></figure><h3 id="优化器（Optimizer）"><a href="#优化器（Optimizer）" class="headerlink" title="优化器（Optimizer）"></a>优化器（Optimizer）</h3><p>优化器负责在训练过程中更新网络的权重和偏置。</p><p>常见的优化器包括：</p><ul><li><strong>SGD</strong>（随机梯度下降）</li><li><strong>Adam</strong>（自适应矩估计）</li><li><strong>RMSprop</strong>（均方根传播）</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 使用 SGD 优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 使用 Adam 优化器</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><h3 id="训练过程（Training-Process）"><a href="#训练过程（Training-Process）" class="headerlink" title="训练过程（Training Process）"></a>训练过程（Training Process）</h3><p>训练神经网络涉及以下步骤：</p><ol><li><strong>准备数据</strong>：通过 <code>DataLoader</code> 加载数据。</li><li><strong>定义损失函数和优化器</strong>。</li><li><strong>前向传播</strong>：计算模型的输出。</li><li><strong>计算损失</strong>：与目标进行比较，得到损失值。</li><li><strong>反向传播</strong>：通过 <code>loss.backward()</code> 计算梯度。</li><li><strong>更新参数</strong>：通过 <code>optimizer.step()</code> 更新模型的参数。</li><li><strong>重复上述步骤</strong>，直到达到预定的训练轮数。</li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设已经定义好了模型、损失函数和优化器</span></span><br><span class="line"><span class="comment"># 训练数据示例</span></span><br><span class="line">X = torch.randn(<span class="number">10</span>, <span class="number">2</span>)  <span class="comment"># 10 个样本，每个样本有 2 个特征</span></span><br><span class="line">Y = torch.randn(<span class="number">10</span>, <span class="number">1</span>)  <span class="comment"># 10 个目标标签</span></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 训练 100 轮</span></span><br><span class="line">    model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清除梯度</span></span><br><span class="line">    output = model(X)  <span class="comment"># 前向传播</span></span><br><span class="line">    loss = criterion(output, Y)  <span class="comment"># 计算损失</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新权重</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:  <span class="comment"># 每 10 轮输出一次损失</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/100], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="测试与评估"><a href="#测试与评估" class="headerlink" title="测试与评估"></a>测试与评估</h3><p>训练完成后，需要对模型进行测试和评估。</p><p>常见的步骤包括：</p><ul><li><strong>计算测试集的损失</strong>：测试模型在未见过的数据上的表现。</li><li><strong>计算准确率（Accuracy）</strong>：对于分类问题，计算正确预测的比例。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设你有测试集 X_test 和 Y_test</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 在评估过程中禁用梯度计算</span></span><br><span class="line">    output = model(X_test)</span><br><span class="line">    loss = criterion(output, Y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Test Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="神经网络类型"><a href="#神经网络类型" class="headerlink" title="神经网络类型"></a>神经网络类型</h3><ol><li><strong>前馈神经网络（Feedforward Neural Networks）</strong>：数据<strong>单向流动</strong>，从输入层到输出层，无反馈连接。</li><li><strong>卷积神经网络（Convolutional Neural Networks, CNNs）</strong>：适用于<strong>图像处理</strong>，使用卷积层<strong>提取空间特征</strong>。</li><li><strong>循环神经网络（Recurrent Neural Networks, RNNs）</strong>：适用于<strong>序列数据</strong>，如<strong>时间序列分析</strong>和<strong>自然语言处理</strong>，允许信息反馈循环。</li><li><strong>长短期记忆网络（Long Short-Term Memory, LSTM）</strong>：一种特殊的RNN，能够<strong>学习长期依赖关系</strong>。</li></ol><h1 id="PyTorch-第一个神经网络"><a href="#PyTorch-第一个神经网络" class="headerlink" title="PyTorch 第一个神经网络"></a>PyTorch 第一个神经网络</h1><p>本章节我们将介绍如何用 PyTorch 实现一个<strong>简单的前馈神经网络</strong>，完成一个<strong>二分类</strong>任务。</p><p>以下实例展示了如何使用 PyTorch 实现一个简单的神经网络进行二分类任务训练。</p><p>网络结构包括输入层、隐藏层和输出层，使用了 ReLU 激活函数和 Sigmoid 激活函数。</p><p>采用了均方误差损失函数和随机梯度下降优化器。</p><p>训练过程是通过前向传播、计算损失、反向传播和参数更新来逐步调整模型参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入PyTorch库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># 定义输入层大小、隐藏层大小、输出层大小和批量大小</span></span><br><span class="line">n_in, n_h, n_out, batch_size = <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># 创建虚拟输入数据和目标数据</span></span><br><span class="line">x = torch.randn(batch_size, n_in)  <span class="comment"># 随机生成输入数据</span></span><br><span class="line">y = torch.tensor([[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], </span><br><span class="line">                 [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>]])  <span class="comment"># 目标输出数据</span></span><br><span class="line"><span class="comment"># 创建顺序模型，包含线性层、ReLU激活函数和Sigmoid激活函数</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">   nn.Linear(n_in, n_h),  <span class="comment"># 输入层到隐藏层的线性变换</span></span><br><span class="line">   nn.ReLU(),            <span class="comment"># 隐藏层的ReLU激活函数</span></span><br><span class="line">   nn.Linear(n_h, n_out),  <span class="comment"># 隐藏层到输出层的线性变换</span></span><br><span class="line">   nn.Sigmoid()           <span class="comment"># 输出层的Sigmoid激活函数</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 定义均方误差损失函数和随机梯度下降优化器</span></span><br><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 学习率为0.01</span></span><br><span class="line"><span class="comment"># 执行梯度下降算法进行模型训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):  <span class="comment"># 迭代50次</span></span><br><span class="line">   y_pred = model(x)  <span class="comment"># 前向传播，计算预测值</span></span><br><span class="line">   loss = criterion(y_pred, y)  <span class="comment"># 计算损失</span></span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span>, epoch, <span class="string">&#x27;loss: &#x27;</span>, loss.item())  <span class="comment"># 打印损失值</span></span><br><span class="line">   optimizer.zero_grad()  <span class="comment"># 清零梯度</span></span><br><span class="line">   loss.backward()  <span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">   optimizer.step()  <span class="comment"># 更新模型参数</span></span><br></pre></td></tr></table></figure><p><strong>定义网络参数：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n_in, n_h, n_out, batch_size = 10, 5, 1, 10</span><br></pre></td></tr></table></figure><ul><li><code>n_in</code>：输入层大小为 10，即<strong>每个数据点有 10 个特征</strong>。</li><li><code>n_h</code>：隐藏层大小为 5，即<strong>隐藏层包含 5 个神经元</strong>。</li><li><code>n_out</code>：输出层大小为 1，即<strong>输出一个标量</strong>，表示<strong>二分类结果</strong>（0 或 1）。</li><li><code>batch_size</code>：每个批次包含 <strong>10 个样本</strong>。</li></ul><p>生成输入数据和目标数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(batch_size, n_in)  # 随机生成输入数据</span><br><span class="line">y = torch.tensor([[1.0], [0.0], [0.0], </span><br><span class="line">                 [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])  # 目标输出数据</span><br></pre></td></tr></table></figure><ul><li><code>x</code>：随机生成一个<strong>形状为 <code>(10, 10)</code> 的输入数据矩阵</strong>，表示 10 个样本，每个样本有 10 个特征。</li><li><code>y</code>：目标输出数据（标签），表示每个输入样本的类别标签（0 或 1），是一个 10×1 的张量。</li></ul><p><strong>定义神经网络模型：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">   nn.Linear(n_in, n_h),  # 输入层到隐藏层的线性变换</span><br><span class="line">   nn.ReLU(),            # 隐藏层的ReLU激活函数</span><br><span class="line">   nn.Linear(n_h, n_out),  # 隐藏层到输出层的线性变换</span><br><span class="line">   nn.Sigmoid()           # 输出层的Sigmoid激活函数</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><code>nn.Sequential</code> 用于<strong>按顺序定义</strong>网络层。</p><ul><li><code>nn.Linear(n_in, n_h)</code>：定义<strong>输入层到隐藏层的线性变换</strong>，<strong>输入特征</strong>是 10 个，<strong>隐藏层</strong>有 5 个神经元。</li><li><code>nn.ReLU()</code>：在隐藏层后<strong>添加 ReLU 激活函数</strong>，<strong>增加非线性</strong>。</li><li><code>nn.Linear(n_h, n_out)</code>：定义隐藏层到输出层的线性变换，<strong>输出为 1 个神经元</strong>。</li><li><code>nn.Sigmoid()</code>：输出层使用 <strong>Sigmoid 激活函数</strong>，将<strong>结果映射到 0 到 1 之间</strong>，用于二分类任务。</li></ul><p><strong>定义损失函数和优化器：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.MSELoss()  # 使用均方误差损失函数</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # 使用随机梯度下降优化器，学习率为 0.01</span><br></pre></td></tr></table></figure><p><strong>训练循环：</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):  <span class="comment"># 训练50轮</span></span><br><span class="line">   y_pred = model(x)  <span class="comment"># 前向传播，计算预测值</span></span><br><span class="line">   loss = criterion(y_pred, y)  <span class="comment"># 计算损失</span></span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span>, epoch, <span class="string">&#x27;loss: &#x27;</span>, loss.item())  <span class="comment"># 打印损失值</span></span><br><span class="line">   optimizer.zero_grad()  <span class="comment"># 清零梯度</span></span><br><span class="line">   loss.backward()  <span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">   optimizer.step()  <span class="comment"># 更新模型参数</span></span><br></pre></td></tr></table></figure><ul><li><code>for epoch in range(50)</code>：进行 <strong>50 次训练迭代</strong>。</li><li><code>y_pred = model(x)</code>：进行<strong>前向</strong>传播，使用<strong>当前模型参数计算输入数据 <code>x</code> 的预测值</strong>。</li><li><code>loss = criterion(y_pred, y)</code>：计算<strong>预测值和目标值 <code>y</code></strong> 之间的<strong>损失</strong>。</li><li><code>optimizer.zero_grad()</code>：<strong>清除上一轮</strong>训练时的<strong>梯度值</strong>。</li><li><code>loss.backward()</code>：<strong>反向</strong>传播，计算<strong>损失函数相对于模型参数的梯度</strong>。</li><li><code>optimizer.step()</code><strong>：根据计算出的梯度更新</strong>模型参数。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 定义输入层大小、隐藏层大小、输出层大小和批量大小</span></span><br><span class="line">n_in, n_h, n_out, batch_size = <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># 创建虚拟输入数据和目标数据</span></span><br><span class="line">x = torch.randn(batch_size, n_in)  <span class="comment"># 随机生成输入数据</span></span><br><span class="line">y = torch.tensor([[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], </span><br><span class="line">                  [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>]])  <span class="comment"># 目标输出数据</span></span><br><span class="line"><span class="comment"># 创建顺序模型，包含线性层、ReLU激活函数和Sigmoid激活函数</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Linear(n_in, n_h),  <span class="comment"># 输入层到隐藏层的线性变换</span></span><br><span class="line">    nn.ReLU(),            <span class="comment"># 隐藏层的ReLU激活函数</span></span><br><span class="line">    nn.Linear(n_h, n_out),  <span class="comment"># 隐藏层到输出层的线性变换</span></span><br><span class="line">    nn.Sigmoid()           <span class="comment"># 输出层的Sigmoid激活函数</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 定义均方误差损失函数和随机梯度下降优化器</span></span><br><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 学习率为0.01</span></span><br><span class="line"><span class="comment"># 用于存储每轮的损失值</span></span><br><span class="line">losses = []</span><br><span class="line"><span class="comment"># 执行梯度下降算法进行模型训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):  <span class="comment"># 迭代50次</span></span><br><span class="line">    y_pred = model(x)  <span class="comment"># 前向传播，计算预测值</span></span><br><span class="line">    loss = criterion(y_pred, y)  <span class="comment"># 计算损失</span></span><br><span class="line">    losses.append(loss.item())  <span class="comment"># 记录损失值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/50], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)  <span class="comment"># 打印损失值</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清零梯度</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新模型参数</span></span><br><span class="line"><span class="comment"># 可视化损失变化曲线</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">51</span>), losses, label=<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss Over Epochs&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 可视化预测结果与实际目标值对比</span></span><br><span class="line">y_pred_final = model(x).detach().numpy()  <span class="comment"># 最终预测值</span></span><br><span class="line">y_actual = y.numpy()  <span class="comment"># 实际值</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, batch_size + <span class="number">1</span>), y_actual, <span class="string">&#x27;o-&#x27;</span>, label=<span class="string">&#x27;Actual&#x27;</span>, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, batch_size + <span class="number">1</span>), y_pred_final, <span class="string">&#x27;x--&#x27;</span>, label=<span class="string">&#x27;Predicted&#x27;</span>, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Sample Index&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Value&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Actual vs Predicted Values&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/first_n_runoob_1.png" alt="img"></p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/a7d9d307c8172f34fc906368ab8cf0fe.png" alt="a7d9d307c8172f34fc906368ab8cf0fe"></p><h2 id="另外一个实例"><a href="#另外一个实例" class="headerlink" title="另外一个实例"></a>另外一个实例</h2><p>我们假设有一个二维数据集，目标是根据点的位置将它们<strong>分类到两个类别中</strong>（例如，红色和蓝色点）。</p><p>以下实例展示了如何使用神经网络完成<strong>简单的二分类</strong>任务，为更复杂的任务奠定了基础，通过 PyTorch 的<strong>模块化接口</strong>，神经网络的构建、训练和可视化都非常直观。</p><h3 id="1、数据准备"><a href="#1、数据准备" class="headerlink" title="1、数据准备"></a>1、数据准备</h3><p>首先，我们生成一些简单的二维数据：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 生成一些随机数据</span></span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">data = torch.randn(n_samples, <span class="number">2</span>)  <span class="comment"># 生成 100 个二维数据点</span></span><br><span class="line">labels = (data[:, <span class="number">0</span>]**<span class="number">2</span> + data[:, <span class="number">1</span>]**<span class="number">2</span> &lt; <span class="number">1</span>).<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)  <span class="comment"># 点在圆内为1，圆外为0</span></span><br><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line">plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], c=labels.squeeze(), cmap=<span class="string">&#x27;coolwarm&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Generated Data&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 2&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>数据说明：</strong></p><ul><li><code>data</code> 是输入的二维点，每个点有两个特征。</li><li><code>labels</code> 是目标分类，点在圆形区域内为 1，否则为 0。</li></ul><p>显示如下：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/fnn_runoob_1.png" alt="img"></p><h3 id="2、定义神经网络"><a href="#2、定义神经网络" class="headerlink" title="2、定义神经网络"></a>2、定义神经网络</h3><p>用 PyTorch 创建一个简单的前馈神经网络。</p><p>前馈神经网络使用了一层隐藏层，通过简单的线性变换和激活函数捕获数据的非线性模式。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义神经网络的层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">4</span>)  <span class="comment"># 输入层有 2 个特征，隐藏层有 4 个神经元</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">4</span>, <span class="number">1</span>)  <span class="comment"># 隐藏层输出到 1 个神经元（用于二分类）</span></span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = nn.Sigmoid()  <span class="comment"># 二分类激活函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># 使用 ReLU 激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.sigmoid(<span class="variable language_">self</span>.fc2(x))  <span class="comment"># 输出层使用 Sigmoid 激活函数</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = SimpleNN()</span><br></pre></td></tr></table></figure><h3 id="3、定义损失函数和优化器"><a href="#3、定义损失函数和优化器" class="headerlink" title="3、定义损失函数和优化器"></a>3、定义损失函数和优化器</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义二分类的损失函数和优化器</span></span><br><span class="line">criterion = nn.BCELoss()  <span class="comment"># 二元交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)  <span class="comment"># 使用随机梯度下降优化器</span></span><br></pre></td></tr></table></figure><h3 id="4、训练模型"><a href="#4、训练模型" class="headerlink" title="4、训练模型"></a>4、训练模型</h3><p>用数据训练模型，让它<strong>学会分类</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    outputs = model(data)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment"># 每 10 轮打印一次损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="5、测试模型并可视化结果"><a href="#5、测试模型并可视化结果" class="headerlink" title="5、测试模型并可视化结果"></a>5、测试模型并可视化结果</h3><p>我们测试模型，并在<strong>图像上绘制决策边界</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化决策边界</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_decision_boundary</span>(<span class="params">model, data</span>):</span><br><span class="line">    x_min, x_max = data[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span>, data[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = data[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span>, data[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    xx, yy = torch.meshgrid(torch.arange(x_min, x_max, <span class="number">0.1</span>), torch.arange(y_min, y_max, <span class="number">0.1</span>), indexing=<span class="string">&#x27;ij&#x27;</span>)</span><br><span class="line">    grid = torch.cat([xx.reshape(-<span class="number">1</span>, <span class="number">1</span>), yy.reshape(-<span class="number">1</span>, <span class="number">1</span>)], dim=<span class="number">1</span>)</span><br><span class="line">    predictions = model(grid).detach().numpy().reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, predictions, levels=[<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1</span>], cmap=<span class="string">&#x27;coolwarm&#x27;</span>, alpha=<span class="number">0.7</span>)</span><br><span class="line">    plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], c=labels.squeeze(), cmap=<span class="string">&#x27;coolwarm&#x27;</span>, edgecolors=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Decision Boundary&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(model, data)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epoch [10/100], Loss: 0.5247</span><br><span class="line">Epoch [20/100], Loss: 0.3142</span><br><span class="line">...</span><br><span class="line">Epoch [100/100], Loss: 0.0957</span><br></pre></td></tr></table></figure><p>图中显示了原始数据点（红色和蓝色），以及<strong>模型学习到的分类边界</strong>。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/fnn_runoob_3.png" alt="img"></p><h1 id="PyTorch-数据处理与加载"><a href="#PyTorch-数据处理与加载" class="headerlink" title="PyTorch 数据处理与加载"></a>PyTorch 数据处理与加载</h1><p>在 PyTorch 中，处理和加载数据是深度学习训练过程中的关键步骤。</p><p>为了高效地处理数据，PyTorch 提供了强大的工具，包括 <strong>torch.utils.data.Dataset</strong> 和 <strong>torch.utils.data.DataLoader</strong>，帮助我们<strong>管理数据集</strong>、<strong>批量加载</strong>和<strong>数据增强</strong>等任务。</p><p>PyTorch 数据处理与加载的介绍：</p><ul><li><strong>自定义 Dataset</strong>：通过<strong>继承 <code>torch.utils.data.Dataset</code> 来加载自己的</strong>数据集。</li><li><strong>DataLoader</strong>：<code>DataLoader</code> <strong>按批次加载</strong>数据，支持多线程加载并进行数据<strong>打乱</strong>。</li><li><strong>数据预处理与增强</strong>：使用 <code>torchvision.transforms</code> 进行常见的图像<strong>预处理和增强</strong>操作，提高模型的<strong>泛化能力</strong>。</li><li><strong>加载标准数据集</strong>：<code>torchvision.datasets</code> 提供了许多<strong>常见的数据集</strong>，简化了数据加载过程。</li><li><strong>多个数据源</strong>：通过组合多个 <code>Dataset</code> 实例来处理来自不同来源的数据。</li></ul><h2 id="自定义-Dataset"><a href="#自定义-Dataset" class="headerlink" title="自定义 Dataset"></a>自定义 Dataset</h2><p><strong>torch.utils.data.Dataset</strong> 是一个抽象类，允许你从自己的数据源中创建数据集。</p><p>我们需要继承该类并实现以下两个方法：</p><ul><li><code>__len__(self)</code>：返回数据集中的样本数量。</li><li><code>__getitem__(self, idx)</code>：通过索引返回一个样本。</li></ul><p>假设我们有一个简单的 CSV 文件或一些列表数据，我们可以通过继承 Dataset 类来创建自己的数据集。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="comment"># 自定义数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X_data, Y_data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化数据集，X_data 和 Y_data 是两个列表或数组</span></span><br><span class="line"><span class="string">        X_data: 输入特征</span></span><br><span class="line"><span class="string">        Y_data: 目标标签</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.X_data = X_data</span><br><span class="line">        <span class="variable language_">self</span>.Y_data = Y_data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回数据集的大小&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.X_data)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回指定索引的数据&quot;&quot;&quot;</span></span><br><span class="line">        x = torch.tensor(<span class="variable language_">self</span>.X_data[idx], dtype=torch.float32)  <span class="comment"># 转换为 Tensor</span></span><br><span class="line">        y = torch.tensor(<span class="variable language_">self</span>.Y_data[idx], dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">X_data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]  <span class="comment"># 输入特征</span></span><br><span class="line">Y_data = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># 目标标签</span></span><br><span class="line"><span class="comment"># 创建数据集实例</span></span><br><span class="line">dataset = MyDataset(X_data, Y_data)</span><br></pre></td></tr></table></figure><h2 id="使用-DataLoader-加载数据"><a href="#使用-DataLoader-加载数据" class="headerlink" title="使用 DataLoader 加载数据"></a>使用 DataLoader 加载数据</h2><p>DataLoader 是 PyTorch 提供的一个重要工具，用于从 Dataset 中**按批次（batch）**加载数据。</p><p>DataLoader 允许我们批量读取数据并进行多线程加载，从而提高训练效率。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment"># 创建 DataLoader 实例，batch_size 设置每次加载的样本数量</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 打印加载的数据</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Batch <span class="subst">&#123;batch_idx + <span class="number">1</span>&#125;</span>:&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Inputs: <span class="subst">&#123;inputs&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Labels: <span class="subst">&#123;labels&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li><strong><code>batch_size</code></strong>: 每次加载的<strong>样本数量</strong>。</li><li><strong><code>shuffle</code></strong>: <strong>是否</strong>对数据进行<strong>洗牌</strong>，通常训练时需要将数据打乱。</li><li><strong><code>drop_last</code></strong>: 如果数据集中的样本数<strong>不能被 <code>batch_size</code> 整除</strong>，设置为 <code>True</code> 时，<strong>丢弃最后一个</strong>不完整的 batch。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Batch <span class="number">1</span>:</span><br><span class="line">Inputs: tensor([[<span class="number">3.</span>, <span class="number">4.</span>], [<span class="number">1.</span>, <span class="number">2.</span>]])</span><br><span class="line">Labels: tensor([<span class="number">0.</span>, <span class="number">1.</span>])</span><br><span class="line">Batch <span class="number">2</span>:</span><br><span class="line">Inputs: tensor([[<span class="number">7.</span>, <span class="number">8.</span>], [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">Labels: tensor([<span class="number">0.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure><p>每次循环中，DataLoader 会返回一个批次的数据，包括<strong>输入特征（inputs）<strong>和</strong>目标标签（labels）</strong>。</p><h2 id="预处理与数据增强"><a href="#预处理与数据增强" class="headerlink" title="预处理与数据增强"></a>预处理与数据增强</h2><p>数据预处理和增强对于提高模型的性能至关重要。</p><p>PyTorch 提供了 torchvision.transforms 模块来进行常见的图像预处理和增强操作，如旋转、裁剪、归一化等。</p><p>常见的图像预处理操作:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="comment"># 定义数据预处理的流水线</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),  <span class="comment"># 将图像调整为 128x128</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为张量</span></span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># 加载图像</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;image.jpg&#x27;</span>)</span><br><span class="line"><span class="comment"># 应用预处理</span></span><br><span class="line">image_tensor = transform(image)</span><br><span class="line"><span class="built_in">print</span>(image_tensor.shape)  <span class="comment"># 输出张量的形状</span></span><br></pre></td></tr></table></figure><ul><li><strong><code>transforms.Compose()</code></strong>：将多个变换操作组合在一起。</li><li><strong><code>transforms.Resize()</code></strong>：调整图像大小。</li><li><strong><code>transforms.ToTensor()</code></strong>：将图像转换为 PyTorch 张量，值会被归一化到 <code>[0, 1]</code> 范围。</li><li><strong><code>transforms.Normalize()</code></strong>：标准化图像数据，通常使用预训练模型时需要进行标准化处理。</li></ul><h3 id="图像数据增强"><a href="#图像数据增强" class="headerlink" title="图像数据增强"></a>图像数据增强</h3><p>数据增强技术通过对训练数据进行随机变换，增加数据的多样性，帮助模型更好地泛化。例如，随机翻转、旋转、裁剪等。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(),  <span class="comment"># 随机水平翻转</span></span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),  <span class="comment"># 随机旋转 30 度</span></span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">128</span>),  <span class="comment"># 随机裁剪并调整为 128x128</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>这些数据增强方法可以通过 transforms.Compose() 组合使用，保证每个图像在训练时具有不同的变换。</p><h2 id="加载图像数据集"><a href="#加载图像数据集" class="headerlink" title="加载图像数据集"></a>加载图像数据集</h2><p>对于图像数据集，torchvision.datasets 提供了许多<strong>常见数据集</strong>（如 CIFAR-10、ImageNet、MNIST 等）以及用于加载图像数据的工具。</p><p>加载 MNIST 数据集:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="comment"># 定义预处理操作</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 对灰度图像进行标准化</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># 下载并加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"><span class="comment"># 创建 DataLoader</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 迭代训练数据</span></span><br><span class="line"><span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="built_in">print</span>(inputs.shape)  <span class="comment"># 每个批次的输入数据形状</span></span><br><span class="line">    <span class="built_in">print</span>(labels.shape)  <span class="comment"># 每个批次的标签形状</span></span><br></pre></td></tr></table></figure><ul><li><code>datasets.MNIST()</code> 会自动<strong>下载</strong> MNIST 数据集<strong>并加载</strong>。</li><li><code>transform</code> 参数允许我们对数据进行<strong>预处理</strong>。</li><li><code>train=True</code> 和 <code>train=False</code> 分别表示<strong>训练</strong>集和<strong>测试</strong>集。</li></ul><h2 id="用多个数据源（Multi-source-Dataset）"><a href="#用多个数据源（Multi-source-Dataset）" class="headerlink" title="用多个数据源（Multi-source Dataset）"></a>用多个数据源（Multi-source Dataset）</h2><p>如果你的数据集由多个文件、多个来源（例如多个图像文件夹）组成，可以通过<strong>继承 Dataset 类自定义加载多个</strong>数据源。</p><p>PyTorch 提供了 <strong>ConcatDataset 和 ChainDataset 等类</strong>来连接多个数据集。</p><p>例如，假设我们有多个图像文件夹的数据，可以将它们合并为一个数据集：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> ConcatDataset</span><br><span class="line"><span class="comment"># 假设 dataset1 和 dataset2 是两个 Dataset 对象</span></span><br><span class="line">combined_dataset = ConcatDataset([dataset1, dataset2])</span><br><span class="line">combined_loader = DataLoader(combined_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h1 id="PyTorch-线性回归"><a href="#PyTorch-线性回归" class="headerlink" title="PyTorch 线性回归"></a>PyTorch 线性回归</h1><p>线性回归是最基本的机器学习算法之一，用于<strong>预测一个连续值</strong>。</p><p>线性回归是一种<strong>简单且常见的回归分析方法</strong>，目的是通过<strong>拟合一个线性函数来预测输出</strong>。</p><p>对于一个简单的线性回归问题，模型可以表示为：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/lg-1.png" alt="img"></p><ul><li>y 是预测值（目标值）。</li><li>x1，x2，xn 是输入特征。</li><li>w1，w2，wn是待学习的权重（模型参数）。</li><li>b 是偏置项。</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Linear_regression.svg.png" alt="img"></p><p>在 PyTorch 中，线性回归模型可以通过<strong>继承 nn.Module 类</strong>来实现。我们将通过一个简单的示例来详细说明如何使用 PyTorch 实现线性回归模型。</p><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>我们首先准备一些假数据，用于训练我们的线性回归模型。这里，我们可以生成一个简单的线性关系的数据集，其中每个样本有两个特征 x1，x2。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 随机种子，确保每次运行结果一致</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line">X = torch.randn(<span class="number">100</span>, <span class="number">2</span>)  <span class="comment"># 100 个样本，每个样本 2 个特征</span></span><br><span class="line">true_w = torch.tensor([<span class="number">2.0</span>, <span class="number">3.0</span>])  <span class="comment"># 假设真实权重</span></span><br><span class="line">true_b = <span class="number">4.0</span>  <span class="comment"># 偏置项</span></span><br><span class="line">Y = X @ true_w + true_b + torch.randn(<span class="number">100</span>) * <span class="number">0.1</span>  <span class="comment"># 加入一些噪声</span></span><br><span class="line"><span class="comment"># 打印部分数据</span></span><br><span class="line"><span class="built_in">print</span>(X[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(Y[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.9269,  1.4873],</span><br><span class="line">        [ 0.9007, -2.1055],</span><br><span class="line">        [ 0.6784, -1.2345],</span><br><span class="line">        [-0.0431, -1.6047],</span><br><span class="line">        [-0.7521,  1.6487]])</span><br><span class="line">tensor([12.4460, -0.4663,  1.7666, -0.9357,  7.4781])</span><br></pre></td></tr></table></figure><p>这段代码创建了一个带有噪声的线性数据集，输入 X 为 100x2 的矩阵，每个样本有<strong>两个特征</strong>，输出 Y 由真实的权重和偏置生成，并<strong>加上了一些随机噪声</strong>。</p><h3 id="定义线性回归模型"><a href="#定义线性回归模型" class="headerlink" title="定义线性回归模型"></a>定义线性回归模型</h3><p>我们可以通过继承 <code>nn.Module</code> 来定义一个简单的线性回归模型。在 PyTorch 中，线性回归的核心是 <code>nn.Linear()</code> 层，它会自动处理权重和偏置的初始化。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearRegressionModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个线性层，输入为2个特征，输出为1个预测值</span></span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 输入维度2，输出维度1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x)  <span class="comment"># 前向传播，返回预测结果</span></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = LinearRegressionModel()</span><br></pre></td></tr></table></figure><p>这里的 <code>nn.Linear(2, 1)</code> 表示一个线性层，它有 2 个输入特征和 1 个输出。<code>forward</code> 方法定义了如何通过这个层进行前向传播。</p><h2 id="定义损失函数与优化器"><a href="#定义损失函数与优化器" class="headerlink" title="定义损失函数与优化器"></a>定义损失函数与优化器</h2><p>线性回归的常见损失函数是 <strong>均方误差损失（MSELoss）</strong>，用于衡量预测值与真实值之间的差异。PyTorch 中提供了现成的 MSELoss 函数。</p><p>我们将使用 <strong>SGD（随机梯度下降）</strong> 或 <strong>Adam</strong> 优化器来最小化损失函数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数（均方误差）</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"><span class="comment"># 优化器（使用 SGD 或 Adam）</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 学习率设置为0.01</span></span><br></pre></td></tr></table></figure><ul><li><strong><code>MSELoss</code></strong>：计算预测值与真实值的均方误差。</li><li><strong><code>SGD</code></strong>：使用随机梯度下降法更新参数。</li></ul><h3 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h3><p>在训练过程中，我们将执行以下步骤：</p><ol><li>使用输入数据 X 进行前向传播，得到预测值。</li><li>计算损失（预测值与实际值之间的差异）。</li><li>使用反向传播计算梯度。</li><li>更新模型参数（权重和偏置）。</li></ol><p>我们将<strong>训练模型 1000 轮</strong>，并在每 100 轮打印一次损失。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">num_epochs = <span class="number">1000</span>  <span class="comment"># 训练 1000 轮</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    predictions = model(X)  <span class="comment"># 模型输出预测值</span></span><br><span class="line">    loss = criterion(predictions.squeeze(), Y)  <span class="comment"># 计算损失（注意预测值需要压缩为1D）</span></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清空之前的梯度</span></span><br><span class="line">    loss.backward()  <span class="comment"># 计算梯度</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新模型参数</span></span><br><span class="line">    <span class="comment"># 打印损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/1000], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li><strong><code>predictions.squeeze()</code></strong>：我们在这里将模型的输出从 2D 张量压缩为 1D，因为目标值 <code>Y</code> 是一个一维数组。</li><li><strong><code>optimizer.zero_grad()</code></strong>：每次反向传播前需要清空之前的梯度。</li><li><strong><code>loss.backward()</code></strong>：计算梯度。</li><li><strong><code>optimizer.step()</code></strong>：更新权重和偏置。</li></ul><h3 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h3><p>训练完成后，我们可以通过查看<strong>模型的权重和偏置来评估模型的效果</strong>。我们还可以在新的数据上进行预测并<strong>与实际值进行比较</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看训练后的权重和偏置</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Predicted weight: <span class="subst">&#123;model.linear.weight.data.numpy()&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Predicted bias: <span class="subst">&#123;model.linear.bias.data.numpy()&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 在新数据上做预测</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 评估时不需要计算梯度</span></span><br><span class="line">    predictions = model(X)</span><br><span class="line"><span class="comment"># 可视化预测与实际值</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], Y, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;True values&#x27;</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], predictions, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Predictions&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li><strong><code>model.linear.weight.data</code></strong> 和 <strong><code>model.linear.bias.data</code></strong>：这些属性存储了模型的权重和偏置。</li><li><strong><code>torch.no_grad()</code></strong>：在评估模式下，不需要计算梯度，节省内存。</li></ul><h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p>在训练过程中，随着损失逐渐减小，我们希望最终的模型能够拟合我们生成的数据。通过查看训练后的权重和偏置，我们可以比较其与真实值（<code>true_w</code> 和 <code>true_b</code>）的差异。理论上，模型的输出权重应该接近 <code>true_w</code> 和 <code>true_b</code>。</p><p>在可视化的散点图中，蓝色点表示真实值，红色点表示模型的预测值。我们希望看到红色点与蓝色点尽可能接近，表明模型成功学习了数据的线性关系。</p><h1 id="PyTorch-卷积神经网络"><a href="#PyTorch-卷积神经网络" class="headerlink" title="PyTorch 卷积神经网络"></a>PyTorch 卷积神经网络</h1><p>PyTorch <strong>卷积神经网络 (Convolutional Neural Networks, CNN)</strong> 是一类<strong>专门用于处理具有网格状拓扑结构数据（如图像）<strong>的</strong>深度学习</strong>模型。</p><p>CNN 是<strong>计算机视觉任务（如图像分类、目标检测和分割）的核心技术。</strong></p><p>下面这张图展示了一个典型的卷积神经网络（CNN）的结构和工作流程，用于图像识别任务。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1686919918947.jpeg" alt="img"></p><p>在图中，CNN 的输出层给出了三个类别的概率：Donald（0.2）、Goofy（0.1）和Tweety（0.7），这表明网络认为输入图像最有可能是 Tweety。</p><p>以下是各个部分的简要说明：</p><ul><li><strong>输入图像（Input Image）</strong>：网络接收的原始图像数据。</li><li><strong>卷积（Convolution）</strong>：使用<strong>卷积核（Kernel）在输入图像上滑动，提取特征</strong>，<strong>生成特征图（Feature Maps）</strong>。</li><li><strong>池化（Pooling）</strong>：通常在卷积层之后，通过<strong>最大池化或平均池化减少特征图的尺寸</strong>，同时<strong>保留重要特征，生成池化特征图（Pooled Feature Maps）</strong>。</li><li><strong>特征提取（Feature Extraction）</strong>：通过<strong>多个</strong>卷积和池化层的组合，<strong>逐步提取</strong>图像的<strong>高级</strong>特征。</li><li><strong>展平层（Flatten Layer）</strong>：将多维的特征图<strong>转换为一维向量</strong>，以便输入到全连接层。</li><li><strong>全连接层（Fully Connected Layer）</strong>：类似于传统的神经网络层，用于将提取的特征映射到输出类别。</li><li><strong>分类（Classification）</strong>：网络的输出层，根据全连接层的输出进行分类。</li><li><strong>概率分布（Probabilistic Distribution）</strong>：输出层给出每个类别的概率，表示<strong>输入图像属于各个类别的可能性。</strong></li></ul><h3 id="卷积神经网络的基本结构"><a href="#卷积神经网络的基本结构" class="headerlink" title="卷积神经网络的基本结构"></a>卷积神经网络的基本结构</h3><p><strong>1、输入层（Input Layer）</strong></p><p>接收原始图像数据，图像通常被表示为一个<strong>三维数组</strong>，其中两个维度代表图像的宽度和高度，第三个维度代表<strong>颜色通道</strong>（例如，RGB图像有三个通道）。</p><p><strong>2、卷积层（Convolutional Layer）</strong></p><p>用卷积核<strong>提取局部特征</strong>，如边缘、纹理等。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-cnn-1.png" alt="img"></p><ul><li>x：输入图像。</li><li>k：卷积核（权重矩阵）。</li><li>b：偏置。</li></ul><p>**应用一组可学习的滤波器（或卷积核）**在输入图像上进行卷积操作，以提取局部特征。</p><p>每个滤波器在输入图像上滑动，生成一个特征图（Feature Map），表示滤波器在不同位置的激活。</p><p>卷积层可以有多个滤波器，每个滤波器生成一个特征图，所有特征图组成一个特征图集合。</p><h3 id="3、激活函数（Activation-Function）"><a href="#3、激活函数（Activation-Function）" class="headerlink" title="3、激活函数（Activation Function）"></a>3、激活函数（Activation Function）</h3><p>通常在<strong>卷积层之后应用非线性激活函数，如 ReLU</strong>（Rectified Linear Unit），以引入非线性特性，使网络能够<strong>学习更复杂</strong>的模式。</p><p>ReLU 函数定义为 ：<strong>f(x)&#x3D;max(0,x)</strong>，即如果输入<strong>小于 0 则输出 0，否则输出输入值</strong>。</p><p><strong>4、池化层（Pooling Layer）</strong></p><ul><li>用于<strong>降低特征图的空间维度</strong>，减少计算量和参数数量，同时保留最重要的特征信息。</li><li>最常见的池化操作是最大池化（Max Pooling）和平均池化（Average Pooling）。</li><li>最大池化选择区域内的最大值，而平均池化计算区域内的平均值。</li></ul><p><strong>5、归一化层（Normalization Layer，可选）</strong></p><ul><li>例如，局部响应归一化（Local Response Normalization, LRN）或批归一化（Batch Normalization）。</li><li>这些层有助于<strong>加速训练过程，提高模型的稳定性</strong>。</li></ul><p><strong>6、全连接层（Fully Connected Layer）</strong></p><ul><li>在 <strong>CNN 的末端</strong>，将前面层提取的特征图展平（Flatten）成一维向量，然后输入到全连接层。</li><li>全连接层的每个神经元都与前一层的所有神经元相连，<strong>用于综合特征并进行最终的分类或回归</strong>。</li></ul><p><strong>7、输出层（Output Layer）</strong></p><p>根据任务的不同，输出层可以有不同的形式。</p><p>对于分类任务，通常使用 <strong>Softmax 函数将输出转换为概率分布</strong>，表示输入属于各个类别的概率。</p><p><strong>8、损失函数（Loss Function）</strong></p><p>用于衡量模型预测<strong>与真实标签之间的差异</strong>。</p><p>常见的损失函数包括<strong>交叉熵损失（Cross-Entropy Loss）<strong>用于</strong>多分类</strong>任务，<strong>均方误差（Mean Squared Error, MSE）<strong>用于</strong>回归</strong>任务。</p><p><strong>9、优化器（Optimizer）</strong></p><p>用于<strong>根据损失函数的梯度更新</strong>网络的权重。常见的优化器包括随机梯度下降（SGD）、Adam、RMSprop等。</p><p><strong>10、正则化（Regularization，可选）</strong></p><p>包括 Dropout、L1&#x2F;L2 正则化等技术，用于<strong>防止模型过拟合</strong>。</p><p><strong>这些层可以堆叠形成更深的网络结构，以提高模型的学习能力。</strong></p><p>CNN 的深度和复杂性可以根据任务的需求进行调整。</p><h2 id="PyTorch-实现一个-CNN-实例"><a href="#PyTorch-实现一个-CNN-实例" class="headerlink" title="PyTorch 实现一个 CNN 实例"></a>PyTorch 实现一个 CNN 实例</h2><p>以下示例展示如何用 PyTorch 构建一个简单的 CNN 模型，用于 MNIST 数据集的数字分类。</p><p>主要步骤：</p><ul><li><strong>数据加载与预处理</strong>：使用 <code>torchvision</code> 加载和预处理 MNIST 数据。</li><li><strong>模型构建</strong>：定义卷积层、池化层和全连接层。</li><li><strong>训练</strong>：通过损失函数和优化器进行模型训练。</li><li><strong>评估</strong>：测试集上计算模型的准确率。</li><li><strong>可视化</strong>：展示部分测试样本及其预测结果。</li></ul><h3 id="1、导入必要库"><a href="#1、导入必要库" class="headerlink" title="1、导入必要库"></a>1、导入必要库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.optim as optim</span><br></pre></td></tr></table></figure><h3 id="2、数据加载"><a href="#2、数据加载" class="headerlink" title="2、数据加载"></a>2、数据加载</h3><p>使用 torchvision 提供的 MNIST 数据集，<strong>加载和预处理</strong>数据。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="3、定义-CNN-模型"><a href="#3、定义-CNN-模型" class="headerlink" title="3、定义 CNN 模型"></a>3、定义 CNN 模型</h3><p>使用 <strong>nn.Module</strong> 构建一个 CNN。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层：输入1通道，输出32通道，卷积核大小3x3</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义卷积层：输入32通道，输出64通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)  <span class="comment"># 输入大小 = 特征图大小 * 通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)  <span class="comment"># 10 个类别</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))  <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))  <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>) <span class="comment"># 展平操作</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))    <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)            <span class="comment"># 全连接层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleCNN()</span><br></pre></td></tr></table></figure><h3 id="4、定义损失函数与优化器"><a href="#4、定义损失函数与优化器" class="headerlink" title="4、定义损失函数与优化器"></a>4、定义损失函数与优化器</h3><p>使用交叉熵损失和随机梯度下降优化器。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()  # 多分类交叉熵损失</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # 学习率和动量</span><br></pre></td></tr></table></figure><h3 id="5、训练模型"><a href="#5、训练模型" class="headerlink" title="5、训练模型"></a>5、训练模型</h3><p>训练模型 5 个 epoch，每个 epoch 后输出训练损失。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">model.train()  <span class="comment"># 设为训练模式</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="6、测试模型"><a href="#6、测试模型" class="headerlink" title="6、测试模型"></a>6、测试模型</h3><p>在测试集上评估模型的准确率。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 评估时不需要计算梯度</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)  <span class="comment"># 预测类别</span></span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 数据加载与预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 定义 CNN 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># 输入1通道，输出32通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># 输入32通道，输出64通道</span></span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)  <span class="comment"># 展平后输入到全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)  <span class="comment"># 10 个类别</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))  <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))  <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>) <span class="comment"># 展平</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))    <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)            <span class="comment"># 最后一层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleCNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 定义损失函数与优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 多分类交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 模型训练</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(images)  <span class="comment"># 前向传播</span></span><br><span class="line">        loss = criterion(outputs, labels)  <span class="comment"># 计算损失</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">        loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 模型测试</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 关闭梯度计算</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="运行结果说明"><a href="#运行结果说明" class="headerlink" title="运行结果说明"></a>运行结果说明</h3><p><strong>1. 输出的训练损失</strong></p><p>代码中每个 epoch 会输出一次平均损失，例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Epoch [1/5], Loss: 0.2325</span><br><span class="line">Epoch [2/5], Loss: 0.0526</span><br><span class="line">Epoch [3/5], Loss: 0.0366</span><br><span class="line">Epoch [4/5], Loss: 0.0273</span><br><span class="line">Epoch [5/5], Loss: 0.0221</span><br></pre></td></tr></table></figure><p>**解释：**损失逐渐下降表明模型在逐步收敛。</p><p><strong>2. 测试集的准确率</strong></p><p>代码在测试集上输出<strong>最终的分类准确率</strong>，例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Test Accuracy: 98.96%</span><br></pre></td></tr></table></figure><p>**解释：**模型对 MNIST 测试集的分类准确率为 98.96%，对于简单的 CNN 模型来说是一个不错的结果。</p><h3 id="7、可视化结果"><a href="#7、可视化结果" class="headerlink" title="7、可视化结果"></a>7、可视化结果</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 数据加载与预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 定义 CNN 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># 输入1通道，输出32通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># 输入32通道，输出64通道</span></span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)  <span class="comment"># 展平后输入到全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)  <span class="comment"># 10 个类别</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))  <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))  <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)     <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>) <span class="comment"># 展平</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))    <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)            <span class="comment"># 最后一层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleCNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 定义损失函数与优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 多分类交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 模型训练</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(images)  <span class="comment"># 前向传播</span></span><br><span class="line">        loss = criterion(outputs, labels)  <span class="comment"># 计算损失</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">        loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 模型测试</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 关闭梯度计算</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 可视化测试结果</span></span><br><span class="line">dataiter = <span class="built_in">iter</span>(test_loader)</span><br><span class="line">images, labels = <span class="built_in">next</span>(dataiter)</span><br><span class="line">outputs = model(images)</span><br><span class="line">_, predictions = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">6</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    axes[i].imshow(images[i][<span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    axes[i].set_title(<span class="string">f&quot;Label: <span class="subst">&#123;labels[i]&#125;</span>\nPred: <span class="subst">&#123;predictions[i]&#125;</span>&quot;</span>)</span><br><span class="line">    axes[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="PyTorch-循环神经网络（RNN）"><a href="#PyTorch-循环神经网络（RNN）" class="headerlink" title="PyTorch 循环神经网络（RNN）"></a>PyTorch 循环神经网络（RNN）</h1><p><strong>循环神经网络（Recurrent Neural Networks, RNN）是一类神经网络架构</strong>，专门用于<strong>处理序列数据</strong>，能够<strong>捕捉时间序列或有序数据的动态信息</strong>，能够处理序列数据，如文本、时间序列或音频。</p><p>RNN 在自然语言处理（NLP）、语音识别、时间序列预测等任务中有着广泛的应用。</p><p>RNN 的关键特性是其能够保持隐状态（hidden state），使得网络能够记住先前时间步的信息，这对于处理序列数据至关重要。</p><h3 id="RNN-的基本结构"><a href="#RNN-的基本结构" class="headerlink" title="RNN 的基本结构"></a>RNN 的基本结构</h3><p>在传统的前馈神经网络（Feedforward Neural Network）中，数据是<strong>从输入层流向输出层</strong>的，而在 RNN 中，数据不仅沿着网络层级流动，还会在每个时间步骤上传播到当前的隐层状态，从而将之前的信息传递到下一个时间步骤。</p><p><strong>隐状态（Hidden State）：</strong> RNN <strong>通过隐状态来记住序列中的信息</strong>。</p><p>隐状态是通过上一时间步的隐状态和当前输入共同计算得到的。</p><p>公式：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-rnn-1.png" alt="img"></p><ul><li>ht：当前时刻的隐状态。</li><li>ht-1：<strong>前一时刻</strong>的隐状态。</li><li>Xt：<strong>当前</strong>时刻的<strong>输入</strong>。</li><li>Whh、Wxh：<strong>权重矩阵</strong>。</li><li>b：偏置项。</li><li>f：<strong>激活函数（如 Tanh 或 ReLU）</strong>。</li></ul><p><strong>输出（Output）：</strong> RNN 的输出<strong>不仅依赖当前的输入，还依赖于隐状态的历史信息</strong>。</p><p>公式：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-rnn-2.png" alt="img"></p><ul><li>yt：当前时刻的隐状态。</li><li>Why：当前时刻的隐状态。</li></ul><h3 id="RNN-如何处理序列数据"><a href="#RNN-如何处理序列数据" class="headerlink" title="RNN 如何处理序列数据"></a>RNN 如何处理序列数据</h3><p>循环神经网络（RNN）在处理序列数据时的展开（unfold）视图如下：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_dznTsiaHCvRc70fxWWEcgw.png" alt="img"></p><p>RNN 是一种处理序列数据的神经网络，它通过<strong>循环连接来处理序列中的每个元素</strong>，并在每个时间步传递信息，以下是图中各部分的说明：</p><ul><li><strong>输入序列（Xt, Xt-1, Xt+1, …）</strong>：图中的粉色圆圈代表输入序列中的各个元素，如Xt表示当前时间步的输入，Xt-1表示前一个时间步的输入，以此类推。</li><li><strong>隐藏状态（ht, ht-1, ht+1, …）</strong>：绿色矩形代表<strong>RNN的隐藏状态</strong>，它在每个时间步存储有关序列的信息。ht是当前时间步的隐藏状态，ht-1是前一个时间步的隐藏状态。</li><li><strong>权重矩阵（U, W, V）</strong>：<ul><li><code>U</code>：<strong>输入到隐藏状态的权重矩阵</strong>，用于将输入<code>Xt</code><strong>转换为隐藏状态的一部分</strong>。</li><li><code>W</code>：<strong>隐藏状态到隐藏状态</strong>的权重矩阵，用于将<strong>前一时间步的隐藏状态<code>ht-1</code>转换为当前</strong>时间步隐藏状态的一部分。</li><li><code>V</code>：<strong>隐藏状态到输出的权重矩阵</strong>，用于将隐藏状态<code>ht</code>转换为输出<code>Yt</code>。</li></ul></li><li><strong>输出序列（Yt, Yt-1, Yt+1, …）</strong>：蓝色圆圈代表RNN在每个时间步的输出，如Yt是当前时间步的输出。</li><li><strong>循环连接</strong>：RNN的特点是<strong>隐藏状态的循环连接</strong>，这允许网络在<strong>处理当前时间步的输入时考虑到之前时间步的信息</strong>。</li><li><strong>展开（Unfold）</strong>：图中展示了RNN在序列上的展开过程，这有助于理解RNN如何<strong>在时间上处理序列数据</strong>。在实际的RNN实现中，这些步骤是并行处理的，但在概念上，我们可以将其展开来理解信息是如何流动的。</li><li><strong>信息流动</strong>：信息从输入序列通过权重矩阵U传递到隐藏状态，然后通过权重矩阵W在时间步之间传递，最后通过权重矩阵V从隐藏状态传递到输出序列。</li></ul><hr><h2 id="PyTorch-中的-RNN-基础"><a href="#PyTorch-中的-RNN-基础" class="headerlink" title="PyTorch 中的 RNN 基础"></a>PyTorch 中的 RNN 基础</h2><p>在 PyTorch 中，RNN 可以用于构建复杂的序列模型。</p><p>PyTorch 提供了几种 RNN 模块，包括：</p><ul><li><code>torch.nn.RNN</code>：<strong>基本的RNN单元。</strong></li><li><code>torch.nn.LSTM</code>：**长短期记忆单元，**能够学习长期依赖关系。</li><li><code>torch.nn.GRU</code>：门控循环单元，是<strong>LSTM的简化版本</strong>，但通常更容易训练。</li></ul><p>使用 RNN 类时，您需要指定输入的维度、隐藏层的维度以及其他一些超参数。</p><h3 id="PyTorch-实现一个简单的-RNN-实例"><a href="#PyTorch-实现一个简单的-RNN-实例" class="headerlink" title="PyTorch 实现一个简单的 RNN 实例"></a>PyTorch 实现一个简单的 RNN 实例</h3><p>以下是一个简单的 PyTorch 实现例子，使用 RNN 模型来处理序列数据并进行分类。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集：字符序列预测（Hello -&gt; Elloh）</span></span><br><span class="line">char_set = <span class="built_in">list</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">char_to_idx = &#123;c: i <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(char_set)&#125;</span><br><span class="line">idx_to_char = &#123;i: c <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(char_set)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备</span></span><br><span class="line">input_str = <span class="string">&quot;hello&quot;</span></span><br><span class="line">target_str = <span class="string">&quot;elloh&quot;</span></span><br><span class="line">input_data = [char_to_idx[c] <span class="keyword">for</span> c <span class="keyword">in</span> input_str]</span><br><span class="line">target_data = [char_to_idx[c] <span class="keyword">for</span> c <span class="keyword">in</span> target_str]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为独热编码</span></span><br><span class="line">input_one_hot = np.eye(<span class="built_in">len</span>(char_set))[input_data]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为 PyTorch Tensor</span></span><br><span class="line">inputs = torch.tensor(input_one_hot, dtype=torch.float32)</span><br><span class="line">targets = torch.tensor(target_data, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型超参数</span></span><br><span class="line">input_size = <span class="built_in">len</span>(char_set)</span><br><span class="line">hidden_size = <span class="number">8</span></span><br><span class="line">output_size = <span class="built_in">len</span>(char_set)</span><br><span class="line">num_epochs = <span class="number">200</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 RNN 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.RNN(input_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, hidden</span>):</span><br><span class="line">        out, hidden = <span class="variable language_">self</span>.rnn(x, hidden)</span><br><span class="line">        out = <span class="variable language_">self</span>.fc(out)  <span class="comment"># 应用全连接层</span></span><br><span class="line">        <span class="keyword">return</span> out, hidden</span><br><span class="line"></span><br><span class="line">model = RNNModel(input_size, hidden_size, output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练 RNN</span></span><br><span class="line">losses = []</span><br><span class="line">hidden = <span class="literal">None</span>  <span class="comment"># 初始隐藏状态为 None</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    outputs, hidden = model(inputs.unsqueeze(<span class="number">0</span>), hidden)</span><br><span class="line">    hidden = hidden.detach()  <span class="comment"># 防止梯度爆炸</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = criterion(outputs.view(-<span class="number">1</span>, output_size), targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试 RNN</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    test_hidden = <span class="literal">None</span></span><br><span class="line">    test_output, _ = model(inputs.unsqueeze(<span class="number">0</span>), test_hidden)</span><br><span class="line">    predicted = torch.argmax(test_output, dim=<span class="number">2</span>).squeeze().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Input sequence: &quot;</span>, <span class="string">&#x27;&#x27;</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> input_data]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Predicted sequence: &quot;</span>, <span class="string">&#x27;&#x27;</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> predicted]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化损失</span></span><br><span class="line">plt.plot(losses, label=<span class="string">&quot;Training Loss&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Epoch&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;RNN Training Loss Over Epochs&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>代码解析：</strong></p><ol><li><strong>数据准备</strong>：<ul><li>使用字符序列 <code>hello</code>，并将其转化为独热编码。</li><li>目标序列为 <code>elloh</code>，即向右旋转一个字符。</li></ul></li><li><strong>模型构建</strong>：<ul><li>使用 <code>torch.nn.RNN</code> 创建循环神经网络。</li><li>加入全连接层 <code>torch.nn.Linear</code> 用于映射隐藏状态到输出。</li></ul></li><li><strong>训练部分</strong>：<ul><li>每一轮都计算损失并反向传播。</li><li>隐藏状态通过 <code>hidden.detach()</code> <strong>防止梯度爆炸</strong>。</li></ul></li><li><strong>测试部分</strong>：<ul><li>模型输出字符的预测结果。</li></ul></li><li><strong>可视化</strong>：<ul><li>用 Matplotlib 绘制训练损失的变化趋势。</li></ul></li></ol><p>假设你的模型训练良好，输出可能如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Epoch [20/200], Loss: 0.0013</span><br><span class="line">Epoch [40/200], Loss: 0.0003</span><br><span class="line">Epoch [60/200], Loss: 0.0002</span><br><span class="line">Epoch [80/200], Loss: 0.0001</span><br><span class="line">Epoch [100/200], Loss: 0.0001</span><br><span class="line">Epoch [120/200], Loss: 0.0001</span><br><span class="line">Epoch [140/200], Loss: 0.0001</span><br><span class="line">Epoch [160/200], Loss: 0.0001</span><br><span class="line">Epoch [180/200], Loss: 0.0001</span><br><span class="line">Epoch [200/200], Loss: 0.0001</span><br><span class="line">Input sequence:  hello</span><br></pre></td></tr></table></figure><p>从结果来看，图像显示损失逐渐减少，表明模型训练有效。</p><h1 id="PyTorch-数据集"><a href="#PyTorch-数据集" class="headerlink" title="PyTorch 数据集"></a>PyTorch 数据集</h1><p>在深度学习任务中，数据加载和处理是至关重要的一环。</p><p>PyTorch 提供了<strong>强大的数据加载和处理工具</strong>，主要包括：</p><ul><li><strong><code>torch.utils.data.Dataset</code></strong>：数据集的<strong>抽象类</strong>，需要<strong>自定义并实现 <code>__len__</code>（数据集大小）和 <code>__getitem__</code>（按索引获取样本）</strong>。</li><li><strong><code>torch.utils.data.TensorDataset</code></strong>：<strong>基于张量</strong>的数据集，适合<strong>处理数据-标签对</strong>，直接支持批处理和迭代。</li><li><strong><code>torch.utils.data.DataLoader</code></strong>：<strong>封装 Dataset 的迭代器</strong>，提供<strong>批处理、数据打乱、多线程加载</strong>等功能，便于数据输入模型训练。</li><li><strong><code>torchvision.datasets.ImageFolder</code></strong>：<strong>从文件夹加载图像数据</strong>，每个子文件夹代表一个类别，适用于图像分类任务。</li></ul><h3 id="PyTorch-内置数据集"><a href="#PyTorch-内置数据集" class="headerlink" title="PyTorch 内置数据集"></a>PyTorch 内置数据集</h3><p>PyTorch 通过 torchvision.datasets 模块提供了许多常用的数据集，例如：</p><ul><li><strong>MNIST</strong>：<strong>手写数字图像数据集</strong>，用于图像分类任务。</li><li><strong>CIFAR</strong>：包含 10 个类别、60000 张 32x32 的<strong>彩色图像数据集</strong>，用于图像分类任务。</li><li><strong>COCO</strong>：<strong>通用物体检测、分割、关键点检测数据集</strong>，包含超过 330k 个图像和 2.5M 个目标实例的大规模数据集。</li><li><strong>ImageNet</strong>：包含超过 1400 万张图像，用于<strong>图像分类和物体检测等任务</strong>。</li><li><strong>STL-10</strong>：包含 100k 张 96x96 的彩色图像数据集，<strong>用于图像分类任务</strong>。</li><li><strong>Cityscapes</strong>：包含 5000 张精细注释的城市街道场景图像，<strong>用于语义分割任务</strong>。</li><li><strong>SQUAD</strong>：<strong>用于机器阅读理解任务的数据集</strong>。</li></ul><p>以上数据集可以通过 <strong>torchvision.datasets 模块中的函数</strong>进行加载，也可以通过自定义的方式加载其他数据集。</p><h3 id="torchvision-和-torchtext"><a href="#torchvision-和-torchtext" class="headerlink" title="torchvision 和 torchtext"></a>torchvision 和 torchtext</h3><ul><li><strong>torchvision</strong>： 一个<strong>图形库，提供了图片数据处理相关的 API 和数据集接口</strong>，包括数据集加载函数和常用的图像变换。</li><li><strong>torchtext</strong>： <strong>自然语言处理工具包，提供了文本数据处理和建模的工具</strong>，包括数据预处理和数据加载的方式。</li></ul><hr><h2 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h2><p>Dataset 是 PyTorch 中用于数据集抽象的类。</p><p>自定义数据集需要继承 torch.utils.data.Dataset 并重写以下两个方法：</p><ul><li><code>__len__</code>：返回数据集的大小。</li><li><code>__getitem__</code>：按索引获取一个数据样本及其标签。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, labels</span>):</span><br><span class="line">        <span class="comment"># 数据初始化</span></span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 按索引返回数据和标签</span></span><br><span class="line">        sample = <span class="variable language_">self</span>.data[idx]</span><br><span class="line">        label = <span class="variable language_">self</span>.labels[idx]</span><br><span class="line">        <span class="keyword">return</span> sample, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">data = torch.randn(<span class="number">100</span>, <span class="number">5</span>)  <span class="comment"># 100 个样本，每个样本有 5 个特征</span></span><br><span class="line">labels = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">100</span>,))  <span class="comment"># 100 个标签，取值为 0 或 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化数据集</span></span><br><span class="line">dataset = MyDataset(data, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集大小:&quot;</span>, <span class="built_in">len</span>(dataset))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第 0 个样本:&quot;</span>, dataset[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据集大小: <span class="number">100</span></span><br><span class="line">第 <span class="number">0</span> 个样本: (tensor([-<span class="number">0.2006</span>,  <span class="number">0.7304</span>, -<span class="number">1.3911</span>, -<span class="number">0.4408</span>,  <span class="number">1.1447</span>]), tensor(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><h2 id="torch-utils-data-DataLoader"><a href="#torch-utils-data-DataLoader" class="headerlink" title="torch.utils.data.DataLoader"></a>torch.utils.data.DataLoader</h2><p>DataLoader 是 PyTorch 提供的<strong>数据加载器</strong>，用于批量加载数据集。</p><p>提供了以下功能：</p><ul><li><strong>批量加载</strong>：通过设置 <code>batch_size</code>。</li><li><strong>数据打乱</strong>：通过设置 <code>shuffle=True</code>。</li><li><strong>多线程加速</strong>：通过设置 <code>num_workers</code>。</li><li><strong>迭代访问</strong>：方便地按批次访问数据。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, labels</span>):</span><br><span class="line">        <span class="comment"># 数据初始化</span></span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 按索引返回数据和标签</span></span><br><span class="line">        sample = <span class="variable language_">self</span>.data[idx]</span><br><span class="line">        label = <span class="variable language_">self</span>.labels[idx]</span><br><span class="line">        <span class="keyword">return</span> sample, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">data = torch.randn(<span class="number">100</span>, <span class="number">5</span>)  <span class="comment"># 100 个样本，每个样本有 5 个特征</span></span><br><span class="line">labels = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">100</span>,))  <span class="comment"># 100 个标签，取值为 0 或 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化数据集</span></span><br><span class="line">dataset = MyDataset(data, labels)</span><br><span class="line"><span class="comment"># 实例化 DataLoader</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历 DataLoader</span></span><br><span class="line"><span class="keyword">for</span> batch_idx, (batch_data, batch_labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;批次 <span class="subst">&#123;batch_idx + <span class="number">1</span>&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;数据:&quot;</span>, batch_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;标签:&quot;</span>, batch_labels)</span><br><span class="line">    <span class="keyword">if</span> batch_idx == <span class="number">2</span>:  <span class="comment"># 仅显示前 3 个批次</span></span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">批次 <span class="number">1</span></span><br><span class="line">数据: tensor([[ <span class="number">0.4689</span>,  <span class="number">0.6666</span>, -<span class="number">1.0234</span>,  <span class="number">0.8948</span>,  <span class="number">0.4503</span>],</span><br><span class="line">        [ <span class="number">0.0273</span>, -<span class="number">0.4684</span>, -<span class="number">0.7762</span>,  <span class="number">0.7963</span>,  <span class="number">0.2168</span>],</span><br><span class="line">        [ <span class="number">1.0677</span>, -<span class="number">0.3502</span>, -<span class="number">0.9594</span>, -<span class="number">1.1318</span>, -<span class="number">0.2196</span>],</span><br><span class="line">        [-<span class="number">1.4989</span>,  <span class="number">0.0267</span>,  <span class="number">1.0405</span>, -<span class="number">0.7284</span>,  <span class="number">0.2335</span>],</span><br><span class="line">        [-<span class="number">0.5887</span>, -<span class="number">0.4934</span>,  <span class="number">1.6283</span>,  <span class="number">1.4638</span>,  <span class="number">0.0157</span>],</span><br><span class="line">        [-<span class="number">1.1047</span>, -<span class="number">0.6550</span>, -<span class="number">0.0381</span>,  <span class="number">0.3617</span>, -<span class="number">1.2792</span>],</span><br><span class="line">        [ <span class="number">0.3592</span>, -<span class="number">0.8264</span>,  <span class="number">0.0231</span>, -<span class="number">1.5508</span>,  <span class="number">0.6833</span>],</span><br><span class="line">        [-<span class="number">0.6835</span>,  <span class="number">0.6979</span>,  <span class="number">0.9048</span>, -<span class="number">0.4756</span>,  <span class="number">0.3003</span>],</span><br><span class="line">        [ <span class="number">1.1562</span>, -<span class="number">0.4516</span>, -<span class="number">1.2415</span>,  <span class="number">0.2859</span>,  <span class="number">0.5837</span>],</span><br><span class="line">        [ <span class="number">0.7937</span>,  <span class="number">1.5316</span>, -<span class="number">0.6139</span>,  <span class="number">0.7999</span>,  <span class="number">0.5506</span>]])</span><br><span class="line">标签: tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">批次 <span class="number">2</span></span><br><span class="line">数据: tensor([[-<span class="number">0.0388</span>, -<span class="number">0.3658</span>,  <span class="number">0.8993</span>, -<span class="number">1.5027</span>,  <span class="number">1.0738</span>],</span><br><span class="line">        [-<span class="number">0.6182</span>,  <span class="number">1.0684</span>, -<span class="number">2.3049</span>,  <span class="number">0.8338</span>,  <span class="number">0.1363</span>],</span><br><span class="line">        [-<span class="number">0.5289</span>,  <span class="number">0.1661</span>, -<span class="number">0.0349</span>,  <span class="number">0.2112</span>,  <span class="number">1.4745</span>],</span><br><span class="line">        [-<span class="number">0.3304</span>, -<span class="number">1.2114</span>, -<span class="number">0.2982</span>, -<span class="number">0.3006</span>,  <span class="number">0.5252</span>],</span><br><span class="line">        [-<span class="number">1.4394</span>, -<span class="number">0.3732</span>,  <span class="number">1.0281</span>,  <span class="number">0.5754</span>,  <span class="number">1.0081</span>],</span><br><span class="line">        [ <span class="number">0.8714</span>, -<span class="number">0.1945</span>, -<span class="number">0.2451</span>, -<span class="number">0.2879</span>, -<span class="number">2.0520</span>],</span><br><span class="line">        [ <span class="number">0.0235</span>,  <span class="number">0.4360</span>,  <span class="number">0.1233</span>,  <span class="number">0.0504</span>,  <span class="number">0.5908</span>],</span><br><span class="line">        [ <span class="number">0.5927</span>,  <span class="number">0.1785</span>, -<span class="number">0.9052</span>, -<span class="number">0.9012</span>,  <span class="number">0.8914</span>],</span><br><span class="line">        [ <span class="number">0.4693</span>,  <span class="number">0.5533</span>, -<span class="number">0.1903</span>,  <span class="number">0.0267</span>,  <span class="number">0.4077</span>],</span><br><span class="line">        [-<span class="number">1.1683</span>,  <span class="number">1.6699</span>, -<span class="number">0.4846</span>, -<span class="number">0.7404</span>,  <span class="number">0.3370</span>]])</span><br><span class="line">标签: tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">批次 <span class="number">3</span></span><br><span class="line">数据: tensor([[ <span class="number">0.2103</span>, -<span class="number">0.7839</span>,  <span class="number">1.4899</span>,  <span class="number">2.2749</span>, -<span class="number">0.7548</span>],</span><br><span class="line">        [-<span class="number">1.2836</span>,  <span class="number">1.0025</span>, -<span class="number">1.1162</span>, -<span class="number">0.4261</span>,  <span class="number">1.0690</span>],</span><br><span class="line">        [-<span class="number">0.7969</span>,  <span class="number">1.0418</span>, -<span class="number">0.7405</span>,  <span class="number">0.8766</span>,  <span class="number">0.2347</span>],</span><br><span class="line">        [-<span class="number">1.1071</span>,  <span class="number">1.8560</span>, -<span class="number">1.2979</span>, -<span class="number">0.8364</span>, -<span class="number">0.2925</span>],</span><br><span class="line">        [-<span class="number">1.0488</span>,  <span class="number">0.4802</span>, -<span class="number">0.6453</span>,  <span class="number">0.2009</span>,  <span class="number">0.5693</span>],</span><br><span class="line">        [ <span class="number">0.8883</span>,  <span class="number">0.4619</span>, -<span class="number">0.2087</span>,  <span class="number">0.2189</span>, -<span class="number">0.3708</span>],</span><br><span class="line">        [-<span class="number">1.4578</span>,  <span class="number">0.3629</span>,  <span class="number">1.8282</span>,  <span class="number">0.5353</span>, -<span class="number">1.1783</span>],</span><br><span class="line">        [-<span class="number">1.2813</span>,  <span class="number">0.5129</span>, -<span class="number">0.4598</span>, -<span class="number">0.2131</span>, -<span class="number">1.2804</span>],</span><br><span class="line">        [ <span class="number">1.7831</span>,  <span class="number">1.1730</span>, -<span class="number">0.2305</span>, -<span class="number">0.6550</span>,  <span class="number">0.1197</span>],</span><br><span class="line">        [-<span class="number">0.9384</span>, -<span class="number">0.0483</span>,  <span class="number">1.9626</span>,  <span class="number">0.3342</span>,  <span class="number">0.1700</span>]])</span><br><span class="line">标签: tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><h2 id="使用内置数据集"><a href="#使用内置数据集" class="headerlink" title="使用内置数据集"></a>使用内置数据集</h2><p>PyTorch 提供了多个常用数据集，存放在 <strong>torchvision 中</strong>，特别适合图像任务。</p><p>加载 MNIST 数据集:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转换为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练数据集</span></span><br><span class="line">train_dataset = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 DataLoader 加载数据</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看一个批次的数据</span></span><br><span class="line">data_iter = <span class="built_in">iter</span>(train_loader)</span><br><span class="line">images, labels = <span class="built_in">next</span>(data_iter)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;批次图像大小: <span class="subst">&#123;images.shape&#125;</span>&quot;</span>)  <span class="comment"># 输出形状为 [batch_size, 1, 28, 28]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;批次标签: <span class="subst">&#123;labels&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">批次图像大小: torch.Size([32, 1, 28, 28])</span><br><span class="line">批次标签: tensor([0, 4, 9, 8, 1, 3, 8, 1, 7, 2, 1, 1, 1, 2, 6, 3, 9, 7, 6, 9, 4, 9, 7, 1,</span><br><span class="line">        3, 7, 3, 0, 7, 7, 6, 7])</span><br></pre></td></tr></table></figure><hr><h2 id="Dataset-与-DataLoader-的自定义应用"><a href="#Dataset-与-DataLoader-的自定义应用" class="headerlink" title="Dataset 与 DataLoader 的自定义应用"></a>Dataset 与 DataLoader 的自定义应用</h2><p>以下是一个将 <strong>CSV 文件 作为数据源</strong>，并通过<strong>自定义 Dataset 和 DataLoader 读取数据</strong>。</p><p><strong>CSV 文件内容如下（下载<a href="https://static.jyshare.com/download/runoob_pytorch_data.csv">runoob_pytorch_data.csv</a>）：</strong></p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-dataset-21.png" alt="img"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义 CSV 数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CSVDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path</span>):</span><br><span class="line">        <span class="comment"># 读取 CSV 文件</span></span><br><span class="line">        <span class="variable language_">self</span>.data = pd.read_csv(file_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 使用 .iloc 明确基于位置索引</span></span><br><span class="line">        row = <span class="variable language_">self</span>.data.iloc[idx]</span><br><span class="line">        <span class="comment"># 将特征和标签分开</span></span><br><span class="line">        features = torch.tensor(row.iloc[:-<span class="number">1</span>].to_numpy(), dtype=torch.float32)  <span class="comment"># 特征</span></span><br><span class="line">        label = torch.tensor(row.iloc[-<span class="number">1</span>], dtype=torch.float32)  <span class="comment"># 标签</span></span><br><span class="line">        <span class="keyword">return</span> features, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化数据集和 DataLoader</span></span><br><span class="line">dataset = CSVDataset(<span class="string">&quot;runoob_pytorch_data.csv&quot;</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历 DataLoader</span></span><br><span class="line"><span class="keyword">for</span> features, label <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;特征:&quot;</span>, features)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;标签:&quot;</span>, label)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">特征: tensor([[ 1.2000,  2.1000, -3.0000],</span><br><span class="line">        [ 1.0000,  1.1000, -2.0000],</span><br><span class="line">        [ 0.5000, -1.2000,  3.3000],</span><br><span class="line">        [-0.3000,  0.8000,  1.2000]])</span><br><span class="line">标签: tensor([1., 0., 1., 0.])</span><br><span class="line">tianqixin@Mac-mini runoob-test % python3 test.py</span><br><span class="line">特征: tensor([[ 1.5000,  2.2000, -1.1000],</span><br><span class="line">        [ 2.1000, -3.3000,  0.0000],</span><br><span class="line">        [-2.3000,  0.4000,  0.7000],</span><br><span class="line">        [-0.3000,  0.8000,  1.2000]])</span><br><span class="line">标签: tensor([0., 1., 0., 0.])</span><br></pre></td></tr></table></figure><h1 id="PyTorch-数据转换"><a href="#PyTorch-数据转换" class="headerlink" title="PyTorch 数据转换"></a>PyTorch 数据转换</h1><p>在 PyTorch 中，数据转换（Data Transformation） 是一种在加载数据时<strong>对数据进行处理的机制</strong>，将原始数据<strong>转换成适合模型训练的格式</strong>，主要通过 <strong>torchvision.transforms</strong> 提供的工具完成。</p><p>数据转换不仅可以实现基本的<strong>数据预处理（如归一化、大小调整等）</strong>，还能帮助进行<strong>数据增强（如随机裁剪、翻转等）</strong>，提高模型的<strong>泛化能力</strong>。</p><h3 id="为什么需要数据转换？"><a href="#为什么需要数据转换？" class="headerlink" title="为什么需要数据转换？"></a>为什么需要数据转换？</h3><p><strong>数据预处理</strong>：</p><ul><li><strong>调整数据格式、大小和范围</strong>，使其<strong>适合模型输入</strong>。</li><li>例如，图像需要调整为固定大小、张量格式并归一化到 [0,1]。</li></ul><p><strong>数据增强</strong>：</p><ul><li>在训练时对数据进行<strong>变换</strong>，以<strong>增加多样性</strong>。</li><li>例如，通过随机旋转、翻转和裁剪增加数据样本的变种，<strong>避免过拟合</strong>。</li></ul><p><strong>灵活性</strong>：</p><ul><li>通过定义一系列转换操作，可以动态地对数据进行处理，简化数据加载的复杂度。</li></ul><p>在 PyTorch 中，torchvision.transforms 模块提供了多种用于图像处理的变换操作。</p><h3 id="基础变换操作"><a href="#基础变换操作" class="headerlink" title="基础变换操作"></a>基础变换操作</h3><table><thead><tr><th align="left">变换函数名称</th><th align="left">描述</th><th align="left">实例</th></tr></thead><tbody><tr><td align="left"><strong>transforms.ToTensor()</strong></td><td align="left">将<strong>PIL图像或NumPy数组转换为PyTorch张量</strong>，并自动<strong>将像素值归一化到 [0, 1]</strong>。</td><td align="left"><code>transform = transforms.ToTensor()</code></td></tr><tr><td align="left"><strong>transforms.Normalize(mean, std)</strong></td><td align="left">对图像进行<strong>标准化</strong>，使数据<strong>符合零均值和单位方差</strong>。</td><td align="left"><code>transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</code></td></tr><tr><td align="left"><strong>transforms.Resize(size)</strong></td><td align="left">调整图像尺寸，确保输入到网络的图像<strong>大小一致</strong>。</td><td align="left"><code>transform = transforms.Resize((256, 256))</code></td></tr><tr><td align="left"><strong>transforms.CenterCrop(size)</strong></td><td align="left"><strong>从图像中心裁剪指定大小的区域。</strong></td><td align="left"><code>transform = transforms.CenterCrop(224)</code></td></tr></tbody></table><p><strong>1、ToTensor</strong></p><p>将 PIL 图像或 NumPy 数组转换为 PyTorch 张量。</p><p><strong>同时将像素值从 [0, 255] 归一化为 [0, 1]。</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from torchvision import transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.ToTensor()</span><br></pre></td></tr></table></figure><p><strong>2、Normalize</strong></p><p>对数据进行<strong>标准化</strong>，使其符合特定的均值和标准差。</p><p><strong>通常用于图像数据</strong>，将其像素值归一化为零均值和单位方差。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Normalize(mean=[0.5], std=[0.5])  # 归一化到 [-1, 1]</span><br></pre></td></tr></table></figure><p><strong>3、Resize</strong></p><p>调整图像的大小。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Resize((128, 128))  # 将图像调整为 128x128</span><br></pre></td></tr></table></figure><p><strong>4、CenterCrop</strong></p><p>从图像中心裁剪指定大小的区域。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.CenterCrop(128)  # 裁剪 128x128 的区域</span><br></pre></td></tr></table></figure><h3 id="数据增强操作"><a href="#数据增强操作" class="headerlink" title="数据增强操作"></a>数据增强操作</h3><table><thead><tr><th align="left">变换函数名称</th><th align="left">描述</th><th align="left">实例</th></tr></thead><tbody><tr><td align="left"><strong>transforms.RandomHorizontalFlip(p)</strong></td><td align="left"><strong>随机水平翻转</strong>图像。</td><td align="left"><code>transform = transforms.RandomHorizontalFlip(p=0.5)</code></td></tr><tr><td align="left"><strong>transforms.RandomRotation(degrees)</strong></td><td align="left"><strong>随机旋转</strong>图像。</td><td align="left"><code>transform = transforms.RandomRotation(degrees=45)</code></td></tr><tr><td align="left"><strong>transforms.ColorJitter(brightness, contrast, saturation, hue)</strong></td><td align="left">调整图像的<strong>亮度、对比度、饱和度和色调</strong>。</td><td align="left"><code>transform = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)</code></td></tr><tr><td align="left"><strong>transforms.RandomCrop(size)</strong></td><td align="left"><strong>随机裁剪指定大小</strong>的区域。</td><td align="left"><code>transform = transforms.RandomCrop(224)</code></td></tr><tr><td align="left"><strong>transforms.RandomResizedCrop(size)</strong></td><td align="left"><strong>随机裁剪图像并调整到指定大小。</strong></td><td align="left"><code>transform = transforms.RandomResizedCrop(224)</code></td></tr></tbody></table><p><strong>1、RandomCrop</strong></p><p>从图像中随机裁剪指定大小。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.RandomCrop(128)</span><br></pre></td></tr></table></figure><p><strong>2、RandomHorizontalFlip</strong></p><p>以一定概率水平翻转图像。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.RandomHorizontalFlip(p=0.5)  # 50% 概率翻转</span><br></pre></td></tr></table></figure><p><strong>3、RandomRotation</strong></p><p>随机旋转一定角度。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.RandomRotation(degrees=30)  # 随机旋转 -30 到 +30 度</span><br></pre></td></tr></table></figure><p><strong>4、ColorJitter</strong></p><p>随机改变图像的亮度、对比度、饱和度或色调。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.ColorJitter(brightness=0.5, contrast=0.5)</span><br></pre></td></tr></table></figure><h3 id="组合变换"><a href="#组合变换" class="headerlink" title="组合变换"></a>组合变换</h3><table><thead><tr><th align="left">变换函数名称</th><th align="left">描述</th><th align="left">实例</th></tr></thead><tbody><tr><td align="left">transforms.Compose()</td><td align="left">将多个变换组合在一起，按照顺序依次应用。</td><td align="left"><code>transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), transforms.Resize((256, 256))])</code></td></tr></tbody></table><h3 id="自定义转换"><a href="#自定义转换" class="headerlink" title="自定义转换"></a>自定义转换</h3><p>如果 transforms 提供的功能无法满足需求，可以通过自定义类或函数实现。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomTransform</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 这里可以自定义任何变换逻辑</span></span><br><span class="line">        <span class="keyword">return</span> x * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">transform = CustomTransform()</span><br></pre></td></tr></table></figure><h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h2><h3 id="对图像数据集应用转换"><a href="#对图像数据集应用转换" class="headerlink" title="对图像数据集应用转换"></a>对图像数据集应用转换</h3><p>加载 MNIST 数据集，并应用转换。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment"># 定义转换</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.5</span>], std=[<span class="number">0.5</span>])</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 使用 DataLoader</span></span><br><span class="line">train_loader = DataLoader(dataset=train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 查看转换后的数据</span></span><br><span class="line"><span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;图像张量大小:&quot;</span>, images.size())  <span class="comment"># [batch_size, 1, 128, 128]</span></span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">图像张量大小: torch.Size([32, 1, 128, 128])</span><br></pre></td></tr></table></figure><h3 id="可视化转换效果"><a href="#可视化转换效果" class="headerlink" title="可视化转换效果"></a>可视化转换效果</h3><p>以下代码展示了原始数据和经过转换后的数据对比。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="comment"># 原始和增强后的图像可视化</span></span><br><span class="line">transform_augment = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform_augment)</span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">dataset</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        image, label = dataset[i]</span><br><span class="line">        axs[i].imshow(image.squeeze(<span class="number">0</span>), cmap=<span class="string">&#x27;gray&#x27;</span>)  <span class="comment"># 将 (1, H, W) 转为 (H, W)</span></span><br><span class="line">        axs[i].set_title(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br><span class="line">        axs[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">show_images(dataset)</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/pytorch-transforms-1.png" alt="img"></p><h1 id="Pytorch-torch-参考手册"><a href="#Pytorch-torch-参考手册" class="headerlink" title="Pytorch torch 参考手册"></a>Pytorch torch 参考手册</h1><p>PyTorch 软件包包含了用于多维张量的数据结构，并定义了在这些张量上执行的数学运算。此外，它还提供了许多实用工具，用于高效地序列化张量和任意类型的数据，以及其他有用的工具。</p><p>它还有一个 CUDA 版本，可以让你在计算能力 &gt;&#x3D; 3.0 的 NVIDIA GPU 上运行张量计算。</p><h2 id="PyTorch-torch-API-手册"><a href="#PyTorch-torch-API-手册" class="headerlink" title="PyTorch torch API 手册"></a>PyTorch torch API 手册</h2><table><thead><tr><th align="left">类别</th><th align="left">API</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">Tensors</td><td align="left"><code>is_tensor(obj)</code></td><td align="left">检查 <code>obj</code> 是否为 PyTorch 张量。</td></tr><tr><td align="left"></td><td align="left"><code>is_storage(obj)</code></td><td align="left">检查 <code>obj</code> 是否为 PyTorch 存储对象。</td></tr><tr><td align="left"></td><td align="left"><code>is_complex(input)</code></td><td align="left">检查 <code>input</code> 数据类型是否为复数数据类型。</td></tr><tr><td align="left"></td><td align="left"><code>is_conj(input)</code></td><td align="left">检查 <code>input</code> 是否为共轭张量。</td></tr><tr><td align="left"></td><td align="left"><code>is_floating_point(input)</code></td><td align="left">检查 <code>input</code> 数据类型是否为浮点数据类型。</td></tr><tr><td align="left"></td><td align="left"><code>is_nonzero(input)</code></td><td align="left">检查 <code>input</code> 是否为非零单一元素张量。</td></tr><tr><td align="left"></td><td align="left"><code>set_default_dtype(d)</code></td><td align="left">设置默认浮点数据类型为 <code>d</code>。</td></tr><tr><td align="left"></td><td align="left"><code>get_default_dtype()</code></td><td align="left">获取当前默认浮点 <code>torch.dtype</code>。</td></tr><tr><td align="left"></td><td align="left"><code>set_default_device(device)</code></td><td align="left">设置默认 <code>torch.Tensor</code> 分配的设备为 <code>device</code>。</td></tr><tr><td align="left"></td><td align="left"><code>get_default_device()</code></td><td align="left">获取默认 <code>torch.Tensor</code> 分配的设备。</td></tr><tr><td align="left"></td><td align="left"><code>numel(input)</code></td><td align="left">返回 <code>input</code> 张量中的元素总数。</td></tr><tr><td align="left">Creation Ops</td><td align="left"><code>tensor(data)</code></td><td align="left">通过复制 <code>data</code> 构造无自动梯度历史的张量。</td></tr><tr><td align="left"></td><td align="left"><code>sparse_coo_tensor(indices, values)</code></td><td align="left">在指定的 <code>indices</code> 处构造稀疏张量，具有指定的值。</td></tr><tr><td align="left"></td><td align="left"><code>as_tensor(data)</code></td><td align="left">将 <code>data</code> 转换为张量，共享数据并尽可能保留自动梯度历史。</td></tr><tr><td align="left"></td><td align="left"><code>zeros(size)</code></td><td align="left">返回一个用标量值 0 填充的张量，形状由 <code>size</code> 定义。</td></tr><tr><td align="left"></td><td align="left"><code>ones(size)</code></td><td align="left">返回一个用标量值 1 填充的张量，形状由 <code>size</code> 定义。</td></tr><tr><td align="left"></td><td align="left"><code>arange(start, end, step)</code></td><td align="left">返回一个 1-D 张量，包含从 <code>start</code> 到 <code>end</code> 的值，步长为 <code>step</code>。</td></tr><tr><td align="left"></td><td align="left"><code>rand(size)</code></td><td align="left">返回一个从 [0, 1) 区间均匀分布的随机数填充的张量。</td></tr><tr><td align="left"></td><td align="left"><code>randn(size)</code></td><td align="left">返回一个从标准正态分布填充的张量。</td></tr><tr><td align="left">Math operations</td><td align="left"><code>add(input, other, alpha)</code></td><td align="left">将 <code>other</code>（由 <code>alpha</code> 缩放）加到 <code>input</code> 上。</td></tr><tr><td align="left"></td><td align="left"><code>mul(input, other)</code></td><td align="left">将 <code>input</code> 与 <code>other</code> 相乘。</td></tr><tr><td align="left"></td><td align="left"><code>matmul(input, other)</code></td><td align="left">执行 <code>input</code> 和 <code>other</code> 的矩阵乘法。</td></tr><tr><td align="left"></td><td align="left"><code>mean(input, dim)</code></td><td align="left">计算 <code>input</code> 在维度 <code>dim</code> 上的均值。</td></tr><tr><td align="left"></td><td align="left"><code>sum(input, dim)</code></td><td align="left">计算 <code>input</code> 在维度 <code>dim</code> 上的和。</td></tr><tr><td align="left"></td><td align="left"><code>max(input, dim)</code></td><td align="left">返回 <code>input</code> 在维度 <code>dim</code> 上的最大值。</td></tr><tr><td align="left"></td><td align="left"><code>min(input, dim)</code></td><td align="left">返回 <code>input</code> 在维度 <code>dim</code> 上的最小值。</td></tr></tbody></table><h3 id="Tensor-创建"><a href="#Tensor-创建" class="headerlink" title="Tensor 创建"></a><strong>Tensor 创建</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.tensor(data, dtype, device, requires_grad)</code></td><td align="left">从数据创建张量。</td></tr><tr><td align="left"><code>torch.as_tensor(data, dtype, device)</code></td><td align="left">将数据转换为张量（共享内存）。</td></tr><tr><td align="left"><code>torch.from_numpy(ndarray)</code></td><td align="left">从 NumPy 数组创建张量（共享内存）。</td></tr><tr><td align="left"><code>torch.zeros(*size, dtype, device, requires_grad)</code></td><td align="left">创建全零张量。</td></tr><tr><td align="left"><code>torch.ones(*size, dtype, device, requires_grad)</code></td><td align="left">创建全一张量。</td></tr><tr><td align="left"><code>torch.empty(*size, dtype, device, requires_grad)</code></td><td align="left">创建未初始化的张量。</td></tr><tr><td align="left"><code>torch.arange(start, end, step, dtype, device, requires_grad)</code></td><td align="left">创建等差序列张量。</td></tr><tr><td align="left"><code>torch.linspace(start, end, steps, dtype, device, requires_grad)</code></td><td align="left">创建等间隔序列张量。</td></tr><tr><td align="left"><code>torch.logspace(start, end, steps, base, dtype, device, requires_grad)</code></td><td align="left">创建对数间隔序列张量。</td></tr><tr><td align="left"><code>torch.eye(n, m, dtype, device, requires_grad)</code></td><td align="left">创建单位矩阵。</td></tr><tr><td align="left"><code>torch.full(size, fill_value, dtype, device, requires_grad)</code></td><td align="left">创建填充指定值的张量。</td></tr><tr><td align="left"><code>torch.rand(*size, dtype, device, requires_grad)</code></td><td align="left">创建均匀分布随机张量（范围 [0, 1)）。</td></tr><tr><td align="left"><code>torch.randn(*size, dtype, device, requires_grad)</code></td><td align="left">创建标准正态分布随机张量。</td></tr><tr><td align="left"><code>torch.randint(low, high, size, dtype, device, requires_grad)</code></td><td align="left">创建整数随机张量。</td></tr><tr><td align="left"><code>torch.randperm(n, dtype, device, requires_grad)</code></td><td align="left">创建 0 到 n-1 的随机排列。</td></tr></tbody></table><hr><h3 id="Tensor-操作"><a href="#Tensor-操作" class="headerlink" title="Tensor 操作"></a><strong>Tensor 操作</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.cat(tensors, dim)</code></td><td align="left">沿指定维度连接张量。</td></tr><tr><td align="left"><code>torch.stack(tensors, dim)</code></td><td align="left">沿新维度堆叠张量。</td></tr><tr><td align="left"><code>torch.split(tensor, split_size, dim)</code></td><td align="left">将张量沿指定维度分割。</td></tr><tr><td align="left"><code>torch.chunk(tensor, chunks, dim)</code></td><td align="left">将张量沿指定维度分块。</td></tr><tr><td align="left"><code>torch.reshape(input, shape)</code></td><td align="left">改变张量的形状。</td></tr><tr><td align="left"><code>torch.transpose(input, dim0, dim1)</code></td><td align="left">交换张量的两个维度。</td></tr><tr><td align="left"><code>torch.squeeze(input, dim)</code></td><td align="left">移除大小为 1 的维度。</td></tr><tr><td align="left"><code>torch.unsqueeze(input, dim)</code></td><td align="left">在指定位置插入大小为 1 的维度。</td></tr><tr><td align="left"><code>torch.expand(input, size)</code></td><td align="left">扩展张量的尺寸。</td></tr><tr><td align="left"><code>torch.narrow(input, dim, start, length)</code></td><td align="left">返回张量的切片。</td></tr><tr><td align="left"><code>torch.permute(input, dims)</code></td><td align="left">重新排列张量的维度。</td></tr><tr><td align="left"><code>torch.masked_select(input, mask)</code></td><td align="left">根据布尔掩码选择元素。</td></tr><tr><td align="left"><code>torch.index_select(input, dim, index)</code></td><td align="left">沿指定维度选择索引对应的元素。</td></tr><tr><td align="left"><code>torch.gather(input, dim, index)</code></td><td align="left">沿指定维度收集指定索引的元素。</td></tr><tr><td align="left"><code>torch.scatter(input, dim, index, src)</code></td><td align="left">将 <code>src</code> 的值散布到 <code>input</code> 的指定位置。</td></tr><tr><td align="left"><code>torch.nonzero(input)</code></td><td align="left">返回非零元素的索引。</td></tr></tbody></table><hr><h3 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a><strong>数学运算</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.add(input, other)</code></td><td align="left">逐元素加法。</td></tr><tr><td align="left"><code>torch.sub(input, other)</code></td><td align="left">逐元素减法。</td></tr><tr><td align="left"><code>torch.mul(input, other)</code></td><td align="left">逐元素乘法。</td></tr><tr><td align="left"><code>torch.div(input, other)</code></td><td align="left">逐元素除法。</td></tr><tr><td align="left"><code>torch.matmul(input, other)</code></td><td align="left">矩阵乘法。</td></tr><tr><td align="left"><code>torch.pow(input, exponent)</code></td><td align="left">逐元素幂运算。</td></tr><tr><td align="left"><code>torch.sqrt(input)</code></td><td align="left">逐元素平方根。</td></tr><tr><td align="left"><code>torch.exp(input)</code></td><td align="left">逐元素指数函数。</td></tr><tr><td align="left"><code>torch.log(input)</code></td><td align="left">逐元素自然对数。</td></tr><tr><td align="left"><code>torch.sum(input, dim)</code></td><td align="left">沿指定维度求和。</td></tr><tr><td align="left"><code>torch.mean(input, dim)</code></td><td align="left">沿指定维度求均值。</td></tr><tr><td align="left"><code>torch.max(input, dim)</code></td><td align="left">沿指定维度求最大值。</td></tr><tr><td align="left"><code>torch.min(input, dim)</code></td><td align="left">沿指定维度求最小值。</td></tr><tr><td align="left"><code>torch.abs(input)</code></td><td align="left">逐元素绝对值。</td></tr><tr><td align="left"><code>torch.clamp(input, min, max)</code></td><td align="left">将张量值限制在指定范围内。</td></tr><tr><td align="left"><code>torch.round(input)</code></td><td align="left">逐元素四舍五入。</td></tr><tr><td align="left"><code>torch.floor(input)</code></td><td align="left">逐元素向下取整。</td></tr><tr><td align="left"><code>torch.ceil(input)</code></td><td align="left">逐元素向上取整。</td></tr></tbody></table><hr><h3 id="随机数生成"><a href="#随机数生成" class="headerlink" title="随机数生成"></a><strong>随机数生成</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.manual_seed(seed)</code></td><td align="left">设置随机种子。</td></tr><tr><td align="left"><code>torch.initial_seed()</code></td><td align="left">返回当前随机种子。</td></tr><tr><td align="left"><code>torch.rand(*size)</code></td><td align="left">创建均匀分布随机张量（范围 [0, 1)）。</td></tr><tr><td align="left"><code>torch.randn(*size)</code></td><td align="left">创建标准正态分布随机张量。</td></tr><tr><td align="left"><code>torch.randint(low, high, size)</code></td><td align="left">创建整数随机张量。</td></tr><tr><td align="left"><code>torch.randperm(n)</code></td><td align="left">返回 0 到 n-1 的随机排列。</td></tr></tbody></table><hr><h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a><strong>线性代数</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.dot(input, other)</code></td><td align="left">计算两个向量的点积。</td></tr><tr><td align="left"><code>torch.mm(input, mat2)</code></td><td align="left">矩阵乘法。</td></tr><tr><td align="left"><code>torch.bmm(input, mat2)</code></td><td align="left">批量矩阵乘法。</td></tr><tr><td align="left"><code>torch.eig(input)</code></td><td align="left">计算矩阵的特征值和特征向量。</td></tr><tr><td align="left"><code>torch.svd(input)</code></td><td align="left">计算矩阵的奇异值分解。</td></tr><tr><td align="left"><code>torch.inverse(input)</code></td><td align="left">计算矩阵的逆。</td></tr><tr><td align="left"><code>torch.det(input)</code></td><td align="left">计算矩阵的行列式。</td></tr><tr><td align="left"><code>torch.trace(input)</code></td><td align="left">计算矩阵的迹。</td></tr></tbody></table><hr><h3 id="设备管理"><a href="#设备管理" class="headerlink" title="设备管理"></a><strong>设备管理</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.cuda.is_available()</code></td><td align="left">检查 CUDA 是否可用。</td></tr><tr><td align="left"><code>torch.device(device)</code></td><td align="left">创建一个设备对象（如 <code>&#39;cpu&#39;</code> 或 <code>&#39;cuda:0&#39;</code>）。</td></tr><tr><td align="left"><code>torch.to(device)</code></td><td align="left">将张量移动到指定设备。</td></tr></tbody></table><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建张量</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y = torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 数学运算</span></span><br><span class="line">z = torch.add(x, <span class="number">1</span>)  <span class="comment"># 逐元素加 1</span></span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="comment"># 索引和切片</span></span><br><span class="line">mask = x &gt; <span class="number">1</span></span><br><span class="line">selected = torch.masked_select(x, mask)</span><br><span class="line"><span class="built_in">print</span>(selected)</span><br><span class="line"><span class="comment"># 设备管理</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    <span class="built_in">print</span>(x.device)</span><br></pre></td></tr></table></figure><h1 id="PyTorch-torch-nn-参考手册"><a href="#PyTorch-torch-nn-参考手册" class="headerlink" title="PyTorch torch.nn 参考手册"></a>PyTorch torch.nn 参考手册</h1><p>PyTorch 的 <code>torch.nn</code> 模块是<strong>构建和训练神经网络的核心模块</strong>，它提供了丰富的类和函数来定义和操作神经网络。</p><p>以下是 <code>torch.nn</code> 模块的一些关键组成部分及其功能：</p><p><strong>1、nn.Module 类</strong>：</p><ul><li><code>nn.Module</code> 是<strong>所有自定义神经网络模型的基类</strong>。用户通常会从这个类<strong>派生自己的模型类</strong>，并在其中<strong>定义网络层结构以及前向传播函数（forward pass）</strong>。</li></ul><p><strong>2、预定义层（Modules）</strong>：</p><ul><li>包括各种类型的层组件，例如<strong>卷积层（<code>nn.Conv1d</code>, <code>nn.Conv2d</code>, <code>nn.Conv3d</code>）</strong>、<strong>全连接层（<code>nn.Linear</code>）</strong>、**激活函数（<code>nn.ReLU</code>, <code>nn.Sigmoid</code>, <code>nn.Tanh</code>）**等。</li></ul><p><strong>3、容器类</strong>：</p><ul><li><code>nn.Sequential</code>：允许将多个层按顺序组合起来，<strong>形成简单的线性堆叠网络</strong>。</li><li><code>nn.ModuleList</code> 和 <code>nn.ModuleDict</code>：可以动态地存储和访问子模块，支持可变长度或命名的模块集合。</li></ul><p><strong>4、损失函数（Loss Functions）</strong>：</p><ul><li><code>torch.nn</code> 包含了一系列用于衡量模型预测与真实标签之间差异的<strong>损失函数</strong>，例如<strong>均方误差损失（<code>nn.MSELoss</code>）</strong>、**交叉熵损失（<code>nn.CrossEntropyLoss</code>）**等。</li></ul><p><strong>5、实用函数接口（Functional Interface）</strong>：</p><ul><li><strong><code>nn.functional</code>（通常简写为 <code>F</code>）</strong>，包含了许多可以<strong>直接作用于张量上</strong>的函数，它们实现了与层对象相同的功能，但不具有参数保存和更新的能力。例如，可以<strong>使用 <code>F.relu()</code> 直接进行 ReLU 操作</strong>，或者 <strong><code>F.conv2d()</code> 进行卷积操作</strong>。</li></ul><p><strong>6、初始化方法</strong>：</p><ul><li><code>torch.nn.init</code> 提供了一些常用的<strong>权重初始化策略</strong>，比如 <strong>Xavier 初始化 (<code>nn.init.xavier_uniform_()</code></strong>) 和 <strong>Kaiming 初始化 (<code>nn.init.kaiming_uniform_()</code>)</strong>，这些对于成功训练神经网络至关重要。</li></ul><hr><h2 id="PyTorch-torch-nn-模块参考手册"><a href="#PyTorch-torch-nn-模块参考手册" class="headerlink" title="PyTorch torch.nn 模块参考手册"></a>PyTorch torch.nn 模块参考手册</h2><h3 id="神经网络容器"><a href="#神经网络容器" class="headerlink" title="神经网络容器"></a><strong>神经网络容器</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.Module</code></td><td align="left">所有神经网络模块的基类。</td></tr><tr><td align="left"><code>torch.nn.Sequential(*args)</code></td><td align="left">按顺序组合多个模块。</td></tr><tr><td align="left"><code>torch.nn.ModuleList(modules)</code></td><td align="left">将子模块存储在列表中。</td></tr><tr><td align="left"><code>torch.nn.ModuleDict(modules)</code></td><td align="left">将子模块存储在字典中。</td></tr><tr><td align="left"><code>torch.nn.ParameterList(parameters)</code></td><td align="left">将参数存储在列表中。</td></tr><tr><td align="left"><code>torch.nn.ParameterDict(parameters)</code></td><td align="left">将参数存储在字典中。</td></tr></tbody></table><hr><h3 id="线性层"><a href="#线性层" class="headerlink" title="线性层"></a><strong>线性层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.Linear(in_features, out_features)</code></td><td align="left">全连接层。</td></tr><tr><td align="left"><code>torch.nn.Bilinear(in1_features, in2_features, out_features)</code></td><td align="left">双线性层。</td></tr></tbody></table><hr><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a><strong>卷积层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.Conv1d(in_channels, out_channels, kernel_size)</code></td><td align="left">一维卷积层。</td></tr><tr><td align="left"><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size)</code></td><td align="left">二维卷积层。</td></tr><tr><td align="left"><code>torch.nn.Conv3d(in_channels, out_channels, kernel_size)</code></td><td align="left">三维卷积层。</td></tr><tr><td align="left"><code>torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size)</code></td><td align="left">一维转置卷积层。</td></tr><tr><td align="left"><code>torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size)</code></td><td align="left">二维转置卷积层。</td></tr><tr><td align="left"><code>torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size)</code></td><td align="left">三维转置卷积层。</td></tr></tbody></table><hr><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a><strong>池化层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.MaxPool1d(kernel_size)</code></td><td align="left">一维最大池化层。</td></tr><tr><td align="left"><code>torch.nn.MaxPool2d(kernel_size)</code></td><td align="left">二维最大池化层。</td></tr><tr><td align="left"><code>torch.nn.MaxPool3d(kernel_size)</code></td><td align="left">三维最大池化层。</td></tr><tr><td align="left"><code>torch.nn.AvgPool1d(kernel_size)</code></td><td align="left">一维平均池化层。</td></tr><tr><td align="left"><code>torch.nn.AvgPool2d(kernel_size)</code></td><td align="left">二维平均池化层。</td></tr><tr><td align="left"><code>torch.nn.AvgPool3d(kernel_size)</code></td><td align="left">三维平均池化层。</td></tr><tr><td align="left"><code>torch.nn.AdaptiveMaxPool1d(output_size)</code></td><td align="left">一维自适应最大池化层。</td></tr><tr><td align="left"><code>torch.nn.AdaptiveAvgPool1d(output_size)</code></td><td align="left">一维自适应平均池化层。</td></tr><tr><td align="left"><code>torch.nn.AdaptiveMaxPool2d(output_size)</code></td><td align="left">二维自适应最大池化层。</td></tr><tr><td align="left"><code>torch.nn.AdaptiveAvgPool2d(output_size)</code></td><td align="left">二维自适应平均池化层。</td></tr><tr><td align="left"><code>torch.nn.AdaptiveMaxPool3d(output_size)</code></td><td align="left">三维自适应最大池化层。</td></tr><tr><td align="left"><code>torch.nn.AdaptiveAvgPool3d(output_size)</code></td><td align="left">三维自适应平均池化层。</td></tr></tbody></table><hr><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a><strong>激活函数</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.ReLU()</code></td><td align="left">ReLU 激活函数。</td></tr><tr><td align="left"><code>torch.nn.Sigmoid()</code></td><td align="left">Sigmoid 激活函数。</td></tr><tr><td align="left"><code>torch.nn.Tanh()</code></td><td align="left">Tanh 激活函数。</td></tr><tr><td align="left"><code>torch.nn.Softmax(dim)</code></td><td align="left">Softmax 激活函数。</td></tr><tr><td align="left"><code>torch.nn.LogSoftmax(dim)</code></td><td align="left">LogSoftmax 激活函数。</td></tr><tr><td align="left"><code>torch.nn.LeakyReLU(negative_slope)</code></td><td align="left">LeakyReLU 激活函数。</td></tr><tr><td align="left"><code>torch.nn.ELU(alpha)</code></td><td align="left">ELU 激活函数。</td></tr><tr><td align="left"><code>torch.nn.SELU()</code></td><td align="left">SELU 激活函数。</td></tr><tr><td align="left"><code>torch.nn.GELU()</code></td><td align="left">GELU 激活函数。</td></tr></tbody></table><hr><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a><strong>损失函数</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.MSELoss()</code></td><td align="left">均方误差损失。</td></tr><tr><td align="left"><code>torch.nn.L1Loss()</code></td><td align="left">L1 损失。</td></tr><tr><td align="left"><code>torch.nn.CrossEntropyLoss()</code></td><td align="left">交叉熵损失。</td></tr><tr><td align="left"><code>torch.nn.NLLLoss()</code></td><td align="left">负对数似然损失。</td></tr><tr><td align="left"><code>torch.nn.BCELoss()</code></td><td align="left">二分类交叉熵损失。</td></tr><tr><td align="left"><code>torch.nn.BCEWithLogitsLoss()</code></td><td align="left">带 Sigmoid 的二分类交叉熵损失。</td></tr><tr><td align="left"><code>torch.nn.KLDivLoss()</code></td><td align="left">KL 散度损失。</td></tr><tr><td align="left"><code>torch.nn.HingeEmbeddingLoss()</code></td><td align="left">铰链嵌入损失。</td></tr><tr><td align="left"><code>torch.nn.MultiMarginLoss()</code></td><td align="left">多分类间隔损失。</td></tr><tr><td align="left"><code>torch.nn.SmoothL1Loss()</code></td><td align="left">平滑 L1 损失。</td></tr></tbody></table><hr><h3 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a><strong>归一化层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.BatchNorm1d(num_features)</code></td><td align="left">一维批归一化层。</td></tr><tr><td align="left"><code>torch.nn.BatchNorm2d(num_features)</code></td><td align="left">二维批归一化层。</td></tr><tr><td align="left"><code>torch.nn.BatchNorm3d(num_features)</code></td><td align="left">三维批归一化层。</td></tr><tr><td align="left"><code>torch.nn.LayerNorm(normalized_shape)</code></td><td align="left">层归一化。</td></tr><tr><td align="left"><code>torch.nn.InstanceNorm1d(num_features)</code></td><td align="left">一维实例归一化层。</td></tr><tr><td align="left"><code>torch.nn.InstanceNorm2d(num_features)</code></td><td align="left">二维实例归一化层。</td></tr><tr><td align="left"><code>torch.nn.InstanceNorm3d(num_features)</code></td><td align="left">三维实例归一化层。</td></tr><tr><td align="left"><code>torch.nn.GroupNorm(num_groups, num_channels)</code></td><td align="left">组归一化。</td></tr></tbody></table><hr><h3 id="循环神经网络层"><a href="#循环神经网络层" class="headerlink" title="循环神经网络层"></a><strong>循环神经网络层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.RNN(input_size, hidden_size)</code></td><td align="left">简单 RNN 层。</td></tr><tr><td align="left"><code>torch.nn.LSTM(input_size, hidden_size)</code></td><td align="left">LSTM 层。</td></tr><tr><td align="left"><code>torch.nn.GRU(input_size, hidden_size)</code></td><td align="left">GRU 层。</td></tr><tr><td align="left"><code>torch.nn.RNNCell(input_size, hidden_size)</code></td><td align="left">简单 RNN 单元。</td></tr><tr><td align="left"><code>torch.nn.LSTMCell(input_size, hidden_size)</code></td><td align="left">LSTM 单元。</td></tr><tr><td align="left"><code>torch.nn.GRUCell(input_size, hidden_size)</code></td><td align="left">GRU 单元。</td></tr></tbody></table><hr><h3 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a><strong>嵌入层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.Embedding(num_embeddings, embedding_dim)</code></td><td align="left">嵌入层。</td></tr></tbody></table><hr><h3 id="Dropout-层"><a href="#Dropout-层" class="headerlink" title="Dropout 层"></a><strong>Dropout 层</strong></h3><table><thead><tr><th align="left"><strong>类&#x2F;函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.Dropout(p)</code></td><td align="left">Dropout 层。</td></tr><tr><td align="left"><code>torch.nn.Dropout2d(p)</code></td><td align="left">2D Dropout 层。</td></tr><tr><td align="left"><code>torch.nn.Dropout3d(p)</code></td><td align="left">3D Dropout 层。</td></tr></tbody></table><hr><h3 id="实用函数"><a href="#实用函数" class="headerlink" title="实用函数"></a><strong>实用函数</strong></h3><table><thead><tr><th align="left"><strong>函数</strong></th><th align="left"><strong>描述</strong></th></tr></thead><tbody><tr><td align="left"><code>torch.nn.functional.relu(input)</code></td><td align="left">应用 ReLU 激活函数。</td></tr><tr><td align="left"><code>torch.nn.functional.sigmoid(input)</code></td><td align="left">应用 Sigmoid 激活函数。</td></tr><tr><td align="left"><code>torch.nn.functional.softmax(input, dim)</code></td><td align="left">应用 Softmax 激活函数。</td></tr><tr><td align="left"><code>torch.nn.functional.cross_entropy(input, target)</code></td><td align="left">计算交叉熵损失。</td></tr><tr><td align="left"><code>torch.nn.functional.mse_loss(input, target)</code></td><td align="left">计算均方误差损失。</td></tr></tbody></table><hr><h3 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># 定义一个简单的神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">20</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建模型和输入</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">output = model(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h1 id="Transformer-模型"><a href="#Transformer-模型" class="headerlink" title="Transformer 模型"></a>Transformer 模型</h1><p>Transformer 模型是一种<strong>基于注意力机制的深度学习模型</strong>，最初由 Vaswani 等人在 2017 年的论文《Attention is All You Need》中提出。</p><p>Transformer <strong>彻底改变了自然语言处理（NLP）领域，并逐渐扩展到计算机视觉（CV）等领域</strong>。</p><p>Transformer 的核心思想是<strong>完全摒弃传统的循环神经网络（RNN）结构，仅依赖注意力机制来处理序列数据</strong>，从而实现<strong>更高的并行性</strong>和<strong>更快的训练速度</strong>。</p><p>以下是 Transformer 架构图，左边为编码器，右边为解码器。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Transformer_full_architecture.png" alt="img"></p><p>Transformer 模型由 编码器（Encoder） 和 解码器（Decoder） 两部分组成，每部分都由多层堆叠的相同模块构成。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/runoob-transformer-1.png" alt="img">编码器（Encoder）</p><p>编码器<strong>由 N 层相同的模块堆叠</strong>而成，每层包含两个子层：</p><ul><li><strong>多头自注意力机制（Multi-Head Self-Attention）：<strong>计算输入序列中每个词与其他词的</strong>相关性</strong>。</li><li><strong>前馈神经网络（Feed-Forward Neural Network）：<strong>对每个词进行独立的</strong>非线性变换</strong>。</li></ul><p>每个子层后面都接有 <strong>残差连接（Residual Connection）</strong> 和 <strong>层归一化（Layer Normalization）</strong>。</p><h3 id="解码器（Decoder）"><a href="#解码器（Decoder）" class="headerlink" title="解码器（Decoder）"></a>解码器（Decoder）</h3><p>解码器也由 N 层相同的模块堆叠而成，每层包含三个子层：</p><ul><li><strong>掩码多头自注意力机制（Masked Multi-Head Self-Attention）：<strong>计算输出序列中每个词与前面词的</strong>相关性（使用掩码防止未来信息泄露）</strong>。</li><li><strong>编码器-解码器注意力机制（Encoder-Decoder Attention）：<strong>计算输出序列</strong>与输入序列的相关性</strong>。</li><li><strong>前馈神经网络（Feed-Forward Neural Network）：<strong>对每个词进行独立的</strong>非线性变换</strong>。</li></ul><p>同样，每个子层后面都接有残差连接和层归一化。</p><p>在 Transformer 模型出现之前，NLP 领域的主流模型是基于 RNN 的架构，如长短期记忆网络（LSTM）和门控循环单元（GRU）。这些模型通过顺序处理输入数据来捕捉序列中的依赖关系，但存在以下问题：</p><ol><li><strong>梯度消失问题</strong>：<strong>长距离依赖关系难以捕捉</strong>。</li><li><strong>顺序计算的局限性</strong>：无法充分利用现代硬件的并行计算能力，训练效率低下。</li></ol><p>Transformer 通过引入自注意力机制解决了这些问题，允许模型同时处理整个输入序列，并动态地为序列中的每个位置分配不同的权重。</p><hr><h2 id="Transformer-的核心思想"><a href="#Transformer-的核心思想" class="headerlink" title="Transformer 的核心思想"></a>Transformer 的核心思想</h2><h3 id="1-自注意力机制（Self-Attention）"><a href="#1-自注意力机制（Self-Attention）" class="headerlink" title="1. 自注意力机制（Self-Attention）"></a>1. 自注意力机制（Self-Attention）</h3><p>自注意力机制是 Transformer 的核心组件。</p><p>自注意力机制允许模型在处理序列时，<strong>动态地为每个位置分配不同的权重</strong>，从而捕捉序列中任意两个位置之间的依赖关系。</p><ul><li><strong>输入表示</strong>：输入序列中的每个词（或标记）<strong>通过词嵌入（Embedding）转换为向量表示</strong>。</li><li><strong>注意力权重计算</strong>：通过<strong>计算查询（Query）、键（Key）和值（Value）之间的点积</strong>，得到每个词<strong>与其他词的相关性权重</strong>。</li><li><strong>加权求和</strong>：使用注意力权重对值（Value）进行<strong>加权求和</strong>，得到<strong>每个词的上下文表示</strong>。</li></ul><p>公式如下：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/192f2ceca2b507875d3c7eef6f0367e5.png" alt="192f2ceca2b507875d3c7eef6f0367e5"></p><p>其中：</p><ul><li>Q 是查询矩阵，K 是键矩阵，V是值矩阵。</li><li>dk是向量的维度，用于<strong>缩放点积，防止梯度爆炸</strong>。</li></ul><h3 id="多头注意力（Multi-Head-Attention）"><a href="#多头注意力（Multi-Head-Attention）" class="headerlink" title="多头注意力（Multi-Head Attention）"></a>多头注意力（Multi-Head Attention）</h3><p>为了捕捉更丰富的特征，Transformer 使用<strong>多头</strong>注意力机制。它将<strong>输入分成多个子空间</strong>，每个子空间独立计算注意力，<strong>最后将结果拼接</strong>起来。</p><ul><li><strong>多头注意力的优势</strong>：允许模型关注序列中不同的部分，例如语法结构、语义关系等。</li><li><strong>并行计算</strong>：多个注意力头可以并行计算，提高效率。</li></ul><h3 id="位置编码（Positional-Encoding）"><a href="#位置编码（Positional-Encoding）" class="headerlink" title="位置编码（Positional Encoding）"></a>位置编码（Positional Encoding）</h3><p>由于 Transformer <strong>没有显式的序列信息（如 RNN 中的时间步）</strong>，<strong>位置编码</strong>被用来<strong>为输入序列中的每个词添加位置信息</strong>。通常<strong>使用正弦和余弦函数生成位置编码</strong>：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/b92014de131411175caef93b06e4006f.png" alt="b92014de131411175caef93b06e4006f"></p><h2 id="Transformer-的应用"><a href="#Transformer-的应用" class="headerlink" title="Transformer 的应用"></a>Transformer 的应用</h2><ol><li><strong>自然语言处理（NLP）</strong>：<ul><li>机器翻译（如 Google Translate）</li><li>文本生成（如 GPT 系列模型）</li><li>文本分类、问答系统等。</li></ul></li><li><strong>计算机视觉（CV）</strong>：<ul><li>图像分类（如 Vision Transformer）</li><li>目标检测、图像生成等。</li></ul></li><li><strong>多模态任务</strong>：<ul><li>结合文本和图像的任务（如 CLIP、DALL-E）。</li></ul></li></ol><hr><h2 id="PyTorch-实现-Transformer"><a href="#PyTorch-实现-Transformer" class="headerlink" title="PyTorch 实现 Transformer"></a>PyTorch 实现 Transformer</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, model_dim, num_heads, num_layers, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(input_dim, model_dim)</span><br><span class="line">        <span class="variable language_">self</span>.positional_encoding = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1000</span>, model_dim))  <span class="comment"># 假设序列长度最大为1000</span></span><br><span class="line">        <span class="variable language_">self</span>.transformer = nn.Transformer(d_model=model_dim, nhead=num_heads, num_encoder_layers=num_layers)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(model_dim, output_dim)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        src_seq_length, tgt_seq_length = src.size(<span class="number">1</span>), tgt.size(<span class="number">1</span>)</span><br><span class="line">        src = <span class="variable language_">self</span>.embedding(src) + <span class="variable language_">self</span>.positional_encoding[:, :src_seq_length, :]</span><br><span class="line">        tgt = <span class="variable language_">self</span>.embedding(tgt) + <span class="variable language_">self</span>.positional_encoding[:, :tgt_seq_length, :]</span><br><span class="line">        transformer_output = <span class="variable language_">self</span>.transformer(src, tgt)</span><br><span class="line">        output = <span class="variable language_">self</span>.fc(transformer_output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">input_dim = <span class="number">10000</span>  <span class="comment"># 词汇表大小</span></span><br><span class="line">model_dim = <span class="number">512</span>    <span class="comment"># 模型维度</span></span><br><span class="line">num_heads = <span class="number">8</span>      <span class="comment"># 多头注意力头数</span></span><br><span class="line">num_layers = <span class="number">6</span>     <span class="comment"># 编码器和解码器层数</span></span><br><span class="line">output_dim = <span class="number">10000</span> <span class="comment"># 输出维度（通常与词汇表大小相同）</span></span><br><span class="line"><span class="comment"># 初始化模型、损失函数和优化器</span></span><br><span class="line">model = TransformerModel(input_dim, model_dim, num_heads, num_layers, output_dim)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 假设输入数据</span></span><br><span class="line">src = torch.randint(<span class="number">0</span>, input_dim, (<span class="number">10</span>, <span class="number">32</span>))  <span class="comment"># (序列长度, 批量大小)</span></span><br><span class="line">tgt = torch.randint(<span class="number">0</span>, input_dim, (<span class="number">20</span>, <span class="number">32</span>))  <span class="comment"># (序列长度, 批量大小)</span></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = model(src, tgt)</span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = criterion(output.view(-<span class="number">1</span>, output_dim), tgt.view(-<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 反向传播和优化</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Loss:&quot;</span>, loss.item())</span><br></pre></td></tr></table></figure><h1 id="PyTorch-构建-Transformer-模型"><a href="#PyTorch-构建-Transformer-模型" class="headerlink" title="PyTorch 构建 Transformer 模型"></a>PyTorch 构建 Transformer 模型</h1><p>Transformer 是现代机器学习中最强大的模型之一。</p><p>Transformer 模型是一种基于自注意力机制（Self-Attention） 的深度学习架构，它彻底改变了自然语言处理（NLP）领域，并成为现代深度学习模型（如 BERT、GPT 等）的基础。</p><p>Transformer 是现代 NLP 领域的核心架构，凭借其强大的长距离依赖建模能力和高效的并行计算优势，在语言翻译和文本摘要等任务中超越了传统的 长短期记忆 (LSTM) 网络。</p><h2 id="使用-PyTorch-构建-Transformer-模型"><a href="#使用-PyTorch-构建-Transformer-模型" class="headerlink" title="使用 PyTorch 构建 Transformer 模型"></a>使用 PyTorch 构建 Transformer 模型</h2><p><strong>构建 Transformer 模型的步骤如下：</strong></p><h3 id="1、导入必要的库和模块"><a href="#1、导入必要的库和模块" class="headerlink" title="1、导入必要的库和模块"></a>1、导入必要的库和模块</h3><p>导入 PyTorch 核心库、神经网络模块、优化器模块、数据处理工具，以及数学和对象复制模块，为定义模型架构、管理数据和训练过程提供支持。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> copy</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><code>torch</code>：PyTorch 的<strong>核心库</strong>，<strong>用于张量操作和自动求导</strong>。</li><li><code>torch.nn</code>：PyTorch 的<strong>神经网络模块</strong>，包含各<strong>种层和损失函数</strong>。</li><li><code>torch.optim</code>：<strong>优化算法模块</strong>，如 Adam、SGD 等。</li><li><code>math</code>：数学函数库，用于计算平方根等。</li><li><code>copy</code>：用于<strong>深度复制对象</strong>。</li></ul><h3 id="定义基本构建块：多头注意力、位置前馈网络、位置编码"><a href="#定义基本构建块：多头注意力、位置前馈网络、位置编码" class="headerlink" title="定义基本构建块：多头注意力、位置前馈网络、位置编码"></a>定义基本构建块：多头注意力、位置前馈网络、位置编码</h3><p><strong>多头注意力</strong>通过多个”注意力头”计算序列中每对位置之间的关系，能够捕捉输入序列的不同特征和模式。</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Figure_1_Multi_Head_Attention_source_image_created_by_author_653bad32f1.avif" alt="img"></p><p><strong>MultiHeadAttention 类封装了 Transformer 模型中常用的多头注意力机制</strong>，负责<strong>将输入拆分成多个注意力头</strong>，对每个注意力头施加注意力，然后将结果组合起来，这样模型就可以在不同尺度上捕捉输入数据中的各种关系，提高模型的表达能力。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % num_heads == <span class="number">0</span>, <span class="string">&quot;d_model必须能被num_heads整除&quot;</span>        </span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model    <span class="comment"># 模型维度（如512）</span></span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads <span class="comment"># 注意力头数（如8）</span></span><br><span class="line">        <span class="variable language_">self</span>.d_k = d_model // num_heads <span class="comment"># 每个头的维度（如64）        </span></span><br><span class="line">        <span class="comment"># 定义线性变换层（无需偏置）</span></span><br><span class="line">        <span class="variable language_">self</span>.W_q = nn.Linear(d_model, d_model) <span class="comment"># 查询变换</span></span><br><span class="line">        <span class="variable language_">self</span>.W_k = nn.Linear(d_model, d_model) <span class="comment"># 键变换</span></span><br><span class="line">        <span class="variable language_">self</span>.W_v = nn.Linear(d_model, d_model) <span class="comment"># 值变换</span></span><br><span class="line">        <span class="variable language_">self</span>.W_o = nn.Linear(d_model, d_model) <span class="comment"># 输出变换       </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算缩放点积注意力</span></span><br><span class="line"><span class="string">        输入形状：</span></span><br><span class="line"><span class="string">            Q: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">            K, V: 同Q</span></span><br><span class="line"><span class="string">        输出形状： (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 计算注意力分数（Q和K的点积）</span></span><br><span class="line">        attn_scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(<span class="variable language_">self</span>.d_k)       </span><br><span class="line">        <span class="comment"># 应用掩码（如填充掩码或未来信息掩码）</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn_scores = attn_scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)     </span><br><span class="line">        <span class="comment"># 计算注意力权重（softmax归一化）</span></span><br><span class="line">        attn_probs = torch.softmax(attn_scores, dim=-<span class="number">1</span>)      </span><br><span class="line">        <span class="comment"># 对值向量加权求和</span></span><br><span class="line">        output = torch.matmul(attn_probs, V)</span><br><span class="line">        <span class="keyword">return</span> output       </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">split_heads</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将输入张量分割为多个头</span></span><br><span class="line"><span class="string">        输入形状: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        输出形状: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, seq_length, d_model = x.size()</span><br><span class="line">        <span class="keyword">return</span> x.view(batch_size, seq_length, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">combine_heads</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将多个头的输出合并回原始形状</span></span><br><span class="line"><span class="string">        输入形状: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">        输出形状: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, _, seq_length, d_k = x.size()</span><br><span class="line">        <span class="keyword">return</span> x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, seq_length, <span class="variable language_">self</span>.d_model)       </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播</span></span><br><span class="line"><span class="string">        输入形状: Q/K/V: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        输出形状: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 线性变换并分割多头</span></span><br><span class="line">        Q = <span class="variable language_">self</span>.split_heads(<span class="variable language_">self</span>.W_q(Q)) <span class="comment"># (batch, heads, seq_len, d_k)</span></span><br><span class="line">        K = <span class="variable language_">self</span>.split_heads(<span class="variable language_">self</span>.W_k(K))</span><br><span class="line">        V = <span class="variable language_">self</span>.split_heads(<span class="variable language_">self</span>.W_v(V))        </span><br><span class="line">        <span class="comment"># 计算注意力</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.scaled_dot_product_attention(Q, K, V, mask)        </span><br><span class="line">        <span class="comment"># 合并多头并输出变换</span></span><br><span class="line">        output = <span class="variable language_">self</span>.W_o(<span class="variable language_">self</span>.combine_heads(attn_output))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><strong>多头注意力机制</strong>：将输入分割成多个头，每个头独立计算注意力，最后将结果合并。</li><li><strong>缩放点积注意力</strong>：计算查询和键的点积，缩放后使用 softmax 计算注意力权重，最后对值进行加权求和。</li><li><strong>掩码</strong>：用于屏蔽无效位置（如填充部分）。</li></ul><h3 id="位置前馈网络（Position-wise-Feed-Forward-Network）"><a href="#位置前馈网络（Position-wise-Feed-Forward-Network）" class="headerlink" title="位置前馈网络（Position-wise Feed-Forward Network）"></a>位置前馈网络（Position-wise Feed-Forward Network）</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFeedForward, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(d_model, d_ff)  <span class="comment"># 第一层全连接</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(d_ff, d_model)  <span class="comment"># 第二层全连接</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()  <span class="comment"># 激活函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 前馈网络的计算</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc2(<span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.fc1(x)))</span><br></pre></td></tr></table></figure><p>**前馈网络：**由两个全连接层和一个 ReLU 激活函数组成，用于进一步处理注意力机制的输出。</p><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>位置编码用于注入输入序列中<strong>每个 token 的位置信息</strong>。</p><p>使用<strong>不同频率的正弦和余弦函数来生成位置编码</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_seq_length</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        pe = torch.zeros(max_seq_length, d_model)  <span class="comment"># 初始化位置编码矩阵</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_seq_length, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)  <span class="comment"># 偶数位置使用正弦函数</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)  <span class="comment"># 奇数位置使用余弦函数</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe.unsqueeze(<span class="number">0</span>))  <span class="comment"># 注册为缓冲区</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 将位置编码添加到输入中</span></span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.pe[:, :x.size(<span class="number">1</span>)]</span><br></pre></td></tr></table></figure><h3 id="构建编码器块（Encoder-Layer）"><a href="#构建编码器块（Encoder-Layer）" class="headerlink" title="构建编码器块（Encoder Layer）"></a>构建编码器块（Encoder Layer）</h3><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Figure_2_The_Encoder_part_of_the_transformer_network_Source_image_from_the_original_paper_b0e3ac40fa.avif" alt="img"></p><p><strong>编码器层：<strong>包含</strong>一个自注意力机制和一个前馈网络</strong>，每个子层后接残差连接和层归一化。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads, d_ff, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = MultiHeadAttention(d_model, num_heads)  <span class="comment"># 自注意力机制</span></span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = PositionWiseFeedForward(d_model, d_ff)  <span class="comment"># 前馈网络</span></span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(d_model)  <span class="comment"># 层归一化</span></span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># Dropout       </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="comment"># 自注意力机制</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.self_attn(x, x, x, mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x + <span class="variable language_">self</span>.dropout(attn_output))  <span class="comment"># 残差连接和层归一化     </span></span><br><span class="line">        <span class="comment"># 前馈网络</span></span><br><span class="line">        ff_output = <span class="variable language_">self</span>.feed_forward(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x + <span class="variable language_">self</span>.dropout(ff_output))  <span class="comment"># 残差连接和层归一化</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="构建解码器模块"><a href="#构建解码器模块" class="headerlink" title="构建解码器模块"></a>构建解码器模块</h3><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Figure_3_The_Decoder_part_of_the_Transformer_network_Souce_Image_from_the_original_paper_b90d9e7f66.avif" alt="img"></p><p><strong>解码器层：<strong>包含</strong>一个自注意力机制、一个交叉注意力机制和一个前馈网络</strong>，每个子层后接残差连接和层归一化。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads, d_ff, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = MultiHeadAttention(d_model, num_heads)  <span class="comment"># 自注意力机制</span></span><br><span class="line">        <span class="variable language_">self</span>.cross_attn = MultiHeadAttention(d_model, num_heads)  <span class="comment"># 交叉注意力机制</span></span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = PositionWiseFeedForward(d_model, d_ff)  <span class="comment"># 前馈网络</span></span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(d_model)  <span class="comment"># 层归一化</span></span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.norm3 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># Dropout</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, enc_output, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="comment"># 自注意力机制</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.self_attn(x, x, x, tgt_mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x + <span class="variable language_">self</span>.dropout(attn_output))  <span class="comment"># 残差连接和层归一化     </span></span><br><span class="line">        <span class="comment"># 交叉注意力机制</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.cross_attn(x, enc_output, enc_output, src_mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x + <span class="variable language_">self</span>.dropout(attn_output))  <span class="comment"># 残差连接和层归一化  </span></span><br><span class="line">        <span class="comment"># 前馈网络</span></span><br><span class="line">        ff_output = <span class="variable language_">self</span>.feed_forward(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm3(x + <span class="variable language_">self</span>.dropout(ff_output))  <span class="comment"># 残差连接和层归一化</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="构建完整的-Transformer-模型"><a href="#构建完整的-Transformer-模型" class="headerlink" title="构建完整的 Transformer 模型"></a>构建完整的 Transformer 模型</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder_embedding = nn.Embedding(src_vocab_size, d_model)  <span class="comment"># 编码器词嵌入</span></span><br><span class="line">        <span class="variable language_">self</span>.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)  <span class="comment"># 解码器词嵌入</span></span><br><span class="line">        <span class="variable language_">self</span>.positional_encoding = PositionalEncoding(d_model, max_seq_length)  <span class="comment"># 位置编码</span></span><br><span class="line">        <span class="comment"># 编码器和解码器层</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)])</span><br><span class="line">        <span class="variable language_">self</span>.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)])</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(d_model, tgt_vocab_size)  <span class="comment"># 最终的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># Dropout</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_mask</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        <span class="comment"># 源掩码：屏蔽填充符（假设填充符索引为0）</span></span><br><span class="line">        <span class="comment"># 形状：(batch_size, 1, 1, seq_length)</span></span><br><span class="line">        src_mask = (src != <span class="number">0</span>).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 目标掩码：屏蔽填充符和未来信息</span></span><br><span class="line">        <span class="comment"># 形状：(batch_size, 1, seq_length, 1)</span></span><br><span class="line">        tgt_mask = (tgt != <span class="number">0</span>).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">3</span>)</span><br><span class="line">        seq_length = tgt.size(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 生成上三角矩阵掩码，防止解码时看到未来信息</span></span><br><span class="line">        nopeak_mask = (<span class="number">1</span> - torch.triu(torch.ones(<span class="number">1</span>, seq_length, seq_length), diagonal=<span class="number">1</span>)).<span class="built_in">bool</span>()</span><br><span class="line">        tgt_mask = tgt_mask &amp; nopeak_mask  <span class="comment"># 合并填充掩码和未来信息掩码</span></span><br><span class="line">        <span class="keyword">return</span> src_mask, tgt_mask</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        <span class="comment"># 生成掩码</span></span><br><span class="line">        src_mask, tgt_mask = <span class="variable language_">self</span>.generate_mask(src, tgt)</span><br><span class="line">        <span class="comment"># 编码器部分</span></span><br><span class="line">        src_embedded = <span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.positional_encoding(<span class="variable language_">self</span>.encoder_embedding(src)))</span><br><span class="line">        enc_output = src_embedded</span><br><span class="line">        <span class="keyword">for</span> enc_layer <span class="keyword">in</span> <span class="variable language_">self</span>.encoder_layers:</span><br><span class="line">            enc_output = enc_layer(enc_output, src_mask)</span><br><span class="line">        <span class="comment"># 解码器部分</span></span><br><span class="line">        tgt_embedded = <span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.positional_encoding(<span class="variable language_">self</span>.decoder_embedding(tgt)))</span><br><span class="line">        dec_output = tgt_embedded</span><br><span class="line">        <span class="keyword">for</span> dec_layer <span class="keyword">in</span> <span class="variable language_">self</span>.decoder_layers:</span><br><span class="line">            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)        </span><br><span class="line">        <span class="comment"># 最终输出</span></span><br><span class="line">        output = <span class="variable language_">self</span>.fc(dec_output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><strong>Transformer 模型</strong>：包含编码器和解码器部分，每个部分由多个层堆叠而成。</li><li><strong>掩码生成</strong>：用于屏蔽无效位置和未来信息。</li><li><strong>前向传播</strong>：依次通过编码器和解码器，最后通过全连接层输出。</li></ul><p>模型初始化参数说明：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Transformer(nn.Module):</span><br><span class="line">    def __init__(</span><br><span class="line">        self, </span><br><span class="line">        src_vocab_size,  # 源语言词汇表大小（如英文单词数）</span><br><span class="line">        tgt_vocab_size,  # 目标语言词汇表大小（如中文单词数）</span><br><span class="line">        d_model=512,     # 模型维度（每个词向量的长度）</span><br><span class="line">        num_heads=8,     # 多头注意力的头数</span><br><span class="line">        num_layers=6,    # 编码器/解码器的堆叠层数</span><br><span class="line">        d_ff=2048,       # 前馈网络隐藏层维度</span><br><span class="line">        max_seq_length=100, # 最大序列长度（用于位置编码）</span><br><span class="line">        dropout=0.1      # Dropout概率</span><br><span class="line">    ):</span><br></pre></td></tr></table></figure><h3 id="训练-PyTorch-Transformer-模型"><a href="#训练-PyTorch-Transformer-模型" class="headerlink" title="训练 PyTorch Transformer 模型"></a>训练 PyTorch Transformer 模型</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">src_vocab_size = <span class="number">5000</span>  <span class="comment"># 源词汇表大小</span></span><br><span class="line">tgt_vocab_size = <span class="number">5000</span>  <span class="comment"># 目标词汇表大小</span></span><br><span class="line">d_model = <span class="number">512</span>  <span class="comment"># 模型维度</span></span><br><span class="line">num_heads = <span class="number">8</span>  <span class="comment"># 注意力头数量</span></span><br><span class="line">num_layers = <span class="number">6</span>  <span class="comment"># 编码器和解码器层数</span></span><br><span class="line">d_ff = <span class="number">2048</span>  <span class="comment"># 前馈网络内层维度</span></span><br><span class="line">max_seq_length = <span class="number">100</span>  <span class="comment"># 最大序列长度</span></span><br><span class="line">dropout = <span class="number">0.1</span>  <span class="comment"># Dropout 概率</span></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)</span><br><span class="line"><span class="comment"># 生成随机数据</span></span><br><span class="line">src_data = torch.randint(<span class="number">1</span>, src_vocab_size, (<span class="number">64</span>, max_seq_length))  <span class="comment"># 源序列</span></span><br><span class="line">tgt_data = torch.randint(<span class="number">1</span>, tgt_vocab_size, (<span class="number">64</span>, max_seq_length))  <span class="comment"># 目标序列</span></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>)  <span class="comment"># 忽略填充部分的损失</span></span><br><span class="line">optimizer = optim.Adam(transformer.parameters(), lr=<span class="number">0.0001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>)</span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line">transformer.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清空梯度，防止累积</span></span><br><span class="line">    <span class="comment"># 输入目标序列时去掉最后一个词（用于预测下一个词）</span></span><br><span class="line">    output = transformer(src_data, tgt_data[:, :-<span class="number">1</span>])      </span><br><span class="line">    <span class="comment"># 计算损失时，目标序列从第二个词开始（即预测下一个词）</span></span><br><span class="line">    <span class="comment"># output形状: (batch_size, seq_length-1, tgt_vocab_size)</span></span><br><span class="line">    <span class="comment"># 目标形状: (batch_size, seq_length-1)</span></span><br><span class="line">    loss = criterion(</span><br><span class="line">        output.contiguous().view(-<span class="number">1</span>, tgt_vocab_size), </span><br><span class="line">        tgt_data[:, <span class="number">1</span>:].contiguous().view(-<span class="number">1</span>)</span><br><span class="line">    )    </span><br><span class="line">    loss.backward()        <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()       <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">transformer.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 生成验证数据</span></span><br><span class="line">val_src_data = torch.randint(<span class="number">1</span>, src_vocab_size, (<span class="number">64</span>, max_seq_length))</span><br><span class="line">val_tgt_data = torch.randint(<span class="number">1</span>, tgt_vocab_size, (<span class="number">64</span>, max_seq_length))</span><br><span class="line"><span class="comment"># 假设输入为一批英文和对应的中文翻译（已转换为索引）</span></span><br><span class="line"><span class="comment"># 示例数据：</span></span><br><span class="line"><span class="comment"># src_data: [[3, 14, 25, ..., 0, 0], ...]  # 英文句子（0为填充符）</span></span><br><span class="line"><span class="comment"># tgt_data: [[5, 20, 36, ..., 0, 0], ...]  # 中文翻译（0为填充符）</span></span><br><span class="line"><span class="comment"># 注意：实际应用中需对文本进行分词、编码、填充等预处理</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    val_output = transformer(val_src_data, val_tgt_data[:, :-<span class="number">1</span>])</span><br><span class="line">    val_loss = criterion(val_output.contiguous().view(-<span class="number">1</span>, tgt_vocab_size), val_tgt_data[:, <span class="number">1</span>:].contiguous().view(-<span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Validation Loss: <span class="subst">&#123;val_loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="Python-入门机器学习"><a href="#Python-入门机器学习" class="headerlink" title="Python 入门机器学习"></a>Python 入门机器学习</h1><p>在使用 Python 进行机器学习时，整个过程一般遵循以下步骤：</p><ol><li><strong>导入必要的库</strong> - 例如，NumPy、Pandas 和 Scikit-learn。</li><li><strong>加载和准备数据</strong> - 数据是机器学习的核心。你需要加载数据并进行必要的预处理（例如数据清洗、缺失值填补等）。</li><li><strong>选择模型和算法</strong> - 根据任务选择适合的机器学习算法（如线性回归、决策树等）。</li><li><strong>训练模型</strong> - 使用训练集数据来训练模型。</li><li><strong>评估模型</strong> - 使用测试集评估模型的准确性，并根据评估结果优化模型。</li><li><strong>调整模型和超参数</strong> - 根据评估结果调整模型的超参数，进一步优化模型性能。</li></ol><h2 id="一个简单的机器学习例子：使用-Scikit-learn-做分类"><a href="#一个简单的机器学习例子：使用-Scikit-learn-做分类" class="headerlink" title="一个简单的机器学习例子：使用 Scikit-learn 做分类"></a>一个简单的机器学习例子：使用 Scikit-learn 做分类</h2><p><strong>Scikit-learn（简称 Sklearn）是一个开源的机器学习库</strong>，建立在 NumPy、SciPy 和 matplotlib 这些科学计算库之上，提供了简单高效的数据挖掘和数据分析工具。</p><p>Scikit-learn 包含了许多常见的机器学习算法，包括：</p><ul><li>线性回归、岭回归、Lasso回归</li><li>支持向量机（SVM）</li><li>决策树、随机森林、梯度提升树</li><li>聚类算法（如K-Means、层次聚类、DBSCAN）</li><li>降维技术（如PCA、t-SNE）</li><li>神经网络</li></ul><p>接下来我们通过一个简单的分类任务——使用鸢尾花数据集（Iris Dataset）来演示机器学习的流程，鸢尾花数据集是一个经典的数据集，包含 150 个样本，描述了三种不同类型的鸢尾花的花瓣和萼片的长度和宽度。</p><h3 id="步骤-1：导入库"><a href="#步骤-1：导入库" class="headerlink" title="步骤 1：导入库"></a>步骤 1：导入库</h3><p>导入需要的 Python 库：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br></pre></td></tr></table></figure><h3 id="步骤-2：加载数据"><a href="#步骤-2：加载数据" class="headerlink" title="步骤 2：加载数据"></a>步骤 2：加载数据</h3><p>加载鸢尾花数据集：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转化为 pandas DataFrame</span></span><br><span class="line">X = pd.DataFrame(iris.data, columns=iris.feature_names)  <span class="comment"># 特征数据</span></span><br><span class="line">y = pd.Series(iris.target)  <span class="comment"># 标签数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示前五行数据</span></span><br><span class="line"><span class="built_in">print</span>(X.head())</span><br></pre></td></tr></table></figure><p>打印输出数据如下所示：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)</span><br><span class="line"><span class="number">0</span>                <span class="number">5.1</span>               <span class="number">3.5</span>                <span class="number">1.4</span>               <span class="number">0.2</span></span><br><span class="line"><span class="number">1</span>                <span class="number">4.9</span>               <span class="number">3.0</span>                <span class="number">1.4</span>               <span class="number">0.2</span></span><br><span class="line"><span class="number">2</span>                <span class="number">4.7</span>               <span class="number">3.2</span>                <span class="number">1.3</span>               <span class="number">0.2</span></span><br><span class="line"><span class="number">3</span>                <span class="number">4.6</span>               <span class="number">3.1</span>                <span class="number">1.5</span>               <span class="number">0.2</span></span><br><span class="line"><span class="number">4</span>                <span class="number">5.0</span>               <span class="number">3.6</span>                <span class="number">1.4</span>               <span class="number">0.2</span></span><br></pre></td></tr></table></figure><h3 id="步骤-3：数据集划分"><a href="#步骤-3：数据集划分" class="headerlink" title="步骤 3：数据集划分"></a>步骤 3：数据集划分</h3><p>将数据集划分为训练集和测试集，<strong>通常使用 70% 训练集和 30% 测试集</strong>的比例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分训练集和测试集（80% 训练集，20% 测试集）</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><h3 id="步骤-4：特征缩放（标准化）"><a href="#步骤-4：特征缩放（标准化）" class="headerlink" title="步骤 4：特征缩放（标准化）"></a>步骤 4：特征缩放（标准化）</h3><p>许多机器学习算法都依赖于特征的尺度，特别是像 K 最近邻算法。为了确保每个特征的均值为 0，标准差为 1，我们使用标准化来处理数据：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标准化特征</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_test = scaler.transform(X_test)</span><br></pre></td></tr></table></figure><h3 id="步骤-5：选择模型并训练"><a href="#步骤-5：选择模型并训练" class="headerlink" title="步骤 5：选择模型并训练"></a>步骤 5：选择模型并训练</h3><p>在这个例子中，我们选择 K-Nearest Neighbors（KNN） 算法来进行分类：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 KNN 分类器</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><h3 id="步骤-6：评估模型"><a href="#步骤-6：评估模型" class="headerlink" title="步骤 6：评估模型"></a>步骤 6：评估模型</h3><p>训练完成后，我们使用测试集评估模型的准确性：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测测试集</span></span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;模型准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>完成以上代码，输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">模型准确率: 1.00</span><br></pre></td></tr></table></figure><h3 id="步骤-7：可视化结果（可选）"><a href="#步骤-7：可视化结果（可选）" class="headerlink" title="步骤 7：可视化结果（可选）"></a>步骤 7：可视化结果（可选）</h3><p>你可以通过可视化来进一步了解模型的表现，尤其是在多维数据集的情况下。例如，你可以用二维图来显示 KNN 分类的结果（不过在这里需要对数据进行降维，<strong>简化为二维</strong>）。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转化为 pandas DataFrame</span></span><br><span class="line">X = pd.DataFrame(iris.data, columns=iris.feature_names)  <span class="comment"># 特征数据</span></span><br><span class="line">y = pd.Series(iris.target)  <span class="comment"># 标签数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集（80% 训练集，20% 测试集）</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化特征</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_test = scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 KNN 分类器</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测测试集</span></span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化 - 这里只是一个简单示例，具体可根据实际情况选择绘图方式</span></span><br><span class="line">plt.scatter(X_test[:, <span class="number">0</span>], X_test[:, <span class="number">1</span>], c=y_pred, cmap=<span class="string">&#x27;viridis&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;KNN Classification Results&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 2&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/knn-python-ml-1.png" alt="img"></p><h1 id="机器学习算法"><a href="#机器学习算法" class="headerlink" title="机器学习算法"></a>机器学习算法</h1><p>机器学习算法可以分为监督学习、无监督学习、强化学习等类别。</p><p><strong>监督学习算法</strong>：</p><ul><li><strong>线性回归</strong>（Linear Regression）：用于<strong>回归任务</strong>，预测连续的数值。</li><li><strong>逻辑回归</strong>（Logistic Regression）：用于<strong>二分类任务</strong>，预测类别。</li><li><strong>支持向量机</strong>（SVM）：用于<strong>分类任务</strong>，构建超平面进行分类。</li><li><strong>决策树</strong>（Decision Tree）：<strong>基于树状结构</strong>进行决策的分类或回归方法。</li></ul><p><strong>无监督学习算法</strong>：</p><ul><li><strong>K-means 聚类</strong>：<strong>通过聚类中心</strong>将数据分组。</li><li><strong>主成分分析（PCA）</strong>：用于<strong>降维</strong>，提取数据的主成分。</li></ul><p>每种算法都有其适用的场景，在实际应用中，可以根据数据的特征（如是否有标签、数据的维度等）来选择最合适的机器学习算法。</p><hr><h2 id="监督学习算法"><a href="#监督学习算法" class="headerlink" title="监督学习算法"></a>监督学习算法</h2><h3 id="线性回归（Linear-Regression）"><a href="#线性回归（Linear-Regression）" class="headerlink" title="线性回归（Linear Regression）"></a>线性回归（Linear Regression）</h3><p>线性回归是一种用于回归问题的算法，它通过学习输入特征与目标值之间的线性关系，来预测一个连续的输出。</p><p>**应用场景：**预测房价、股票价格等。</p><p>线性回归的目标是找到一个最佳的线性方程：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/lg-1-1742965831922-27.png" alt="img"></p><ul><li>y 是预测值（目标值）。</li><li>x1，x2，xn 是输入特征。</li><li>w1，w2，wn是待学习的权重（模型参数）。</li><li>b 是偏置项。</li></ul><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Linear_regression.svg-1742965850608-30.png" alt="img"></p><p>接下来我们使用 sklearn 进行简单的房价预测：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个简单的房价数据集</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;面积&#x27;</span>: [<span class="number">50</span>, <span class="number">60</span>, <span class="number">80</span>, <span class="number">100</span>, <span class="number">120</span>],</span><br><span class="line">    <span class="string">&#x27;房价&#x27;</span>: [<span class="number">150</span>, <span class="number">180</span>, <span class="number">240</span>, <span class="number">300</span>, <span class="number">350</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征和标签</span></span><br><span class="line">X = df[[<span class="string">&#x27;面积&#x27;</span>]]</span><br><span class="line">y = df[<span class="string">&#x27;房价&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;预测的房价: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">预测的房价: [180.8411215]</span><br></pre></td></tr></table></figure><h3 id="逻辑回归（Logistic-Regression）"><a href="#逻辑回归（Logistic-Regression）" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h3><p>逻辑回归是一种用于分类问题的算法，<strong>尽管名字中包含”回归”，它是用来处理二分类问题的</strong>。</p><p>逻辑回归通过学习输入特征与类别之间的关系，来预测一个类别标签。</p><p>**应用场景：**垃圾邮件分类、疾病诊断（是否患病）。</p><p>逻辑回归的输出是一个概率值，表示样本属于某一类别的概率。</p><p>通常使用 Sigmoid 函数：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/881f2480-9f80-448e-a83b-8abfb784d065.png" alt="img"></p><p>使用逻辑回归进行二分类任务:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只取前两类做二分类任务</span></span><br><span class="line">X = X[y != <span class="number">2</span>]</span><br><span class="line">y = y[y != <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练逻辑回归模型</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;分类准确率: <span class="subst">&#123;accuracy_score(y_test, y_pred):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">分类准确率: 1.00</span><br></pre></td></tr></table></figure><h3 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h3><p>支持向量机是一种常用的分类算法，它通过构造超平面来最大化类别之间的间隔（Margin），使得分类的误差最小。</p><p>**应用场景：**文本分类、人脸识别等。</p><p>使用 SVM 进行鸢尾花分类任务：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练 SVM 模型</span></span><br><span class="line">model = SVC(kernel=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;SVM 分类准确率: <span class="subst">&#123;accuracy_score(y_test, y_pred):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="决策树（Decision-Tree）"><a href="#决策树（Decision-Tree）" class="headerlink" title="决策树（Decision Tree）"></a>决策树（Decision Tree）</h3><p>决策树是一种基于树结构进行决策的<strong>分类和回归方法</strong>。它通过一系列的”判断条件”来决定一个样本属于哪个类别。</p><p>**应用场景：**客户分类、信用评分等。</p><p>使用决策树进行分类任务：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练决策树模型</span></span><br><span class="line">model = DecisionTreeClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;决策树分类准确率: <span class="subst">&#123;accuracy_score(y_test, y_pred):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">决策树分类准确率: 1.00</span><br></pre></td></tr></table></figure><hr><h2 id="无监督学习算法"><a href="#无监督学习算法" class="headerlink" title="无监督学习算法"></a>无监督学习算法</h2><h3 id="K-means-聚类（K-means-Clustering）"><a href="#K-means-聚类（K-means-Clustering）" class="headerlink" title="K-means 聚类（K-means Clustering）"></a>K-means 聚类（K-means Clustering）</h3><p>K-means 是一种<strong>基于中心点的聚类算法</strong>，通过不断<strong>调整簇的中心点</strong>，使每个簇中的数据点尽可能靠近簇中心。</p><p>**应用场景：**<strong><strong>客户分群、市场分析、图像压缩。</strong></strong></p><p>使用 K-means 进行客户分群:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">from sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 生成一个简单的二维数据集</span><br><span class="line">X, _ = <span class="built_in">make_blobs</span>(n_samples=<span class="number">300</span>, centers=<span class="number">4</span>, cluster_std=<span class="number">0.60</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"># 训练 K-means 模型</span><br><span class="line">model = <span class="built_in">KMeans</span>(n_clusters=<span class="number">4</span>)</span><br><span class="line">model.<span class="built_in">fit</span>(X)</span><br><span class="line"></span><br><span class="line"># 预测聚类结果</span><br><span class="line">y_kmeans = model.<span class="built_in">predict</span>(X)</span><br><span class="line"></span><br><span class="line"># 可视化聚类结果</span><br><span class="line">plt.<span class="built_in">scatter</span>(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_kmeans, s=<span class="number">50</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.<span class="built_in">show</span>()</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ml-algorithms-1.png" alt="img"></p><h3 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h3><p>PCA 是一种<strong>降维技术</strong>，它通过线性变换将数据转换到新的坐标系中，使得大部分的方差集中在前几个主成分上。</p><p>**应用场景：**图像降维、特征选择、数据可视化。</p><p>使用 PCA 降维并可视化高维数据:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 降维到 2 维</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">X_pca = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line">plt.scatter(X_pca[:, <span class="number">0</span>], X_pca[:, <span class="number">1</span>], c=y, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;PCA of Iris Dataset&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ml-algorithms-2.png" alt="img"></p><h1 id="线性回归-Linear-Regression"><a href="#线性回归-Linear-Regression" class="headerlink" title="线性回归 (Linear Regression)"></a>线性回归 (Linear Regression)</h1><p>线性回归（Linear Regression）是机器学习中最基础且广泛应用的算法之一。</p><p>线性回归 (Linear Regression) 是一种用于<strong>预测连续值的最基本</strong>的机器学习算法，它<strong>假设</strong>目标变量 <strong>y</strong> 和特征变量 <strong>x</strong> 之间<strong>存在线性</strong>关系，并<strong>试图找到一条最佳拟合直线来描述</strong>这种关系。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = w * x + b</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li><code>y</code> 是预测值</li><li><code>x</code> 是特征变量</li><li><code>w</code> 是权重 (斜率)</li><li><code>b</code> 是偏置 (截距)</li></ul><p>线性回归的目标是<strong>找到最佳的 <code>w</code> 和 <code>b</code>，使得预测值 <code>y</code> 与真实值之间的误差最小</strong>。常用的<strong>误差函数是均方误差 (MSE)</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MSE = 1/n * Σ(y_i - y_pred_i)^2</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>y_i 是<strong>实际</strong>值。</li><li>y_pred_i 是<strong>预测</strong>值。</li><li>n 是数据点的数量。</li></ul><p>我们的目标是通过<strong>调整 w 和 b ，使得 MSE 最小化。</strong></p><h2 id="如何求解线性回归？"><a href="#如何求解线性回归？" class="headerlink" title="如何求解线性回归？"></a>如何求解线性回归？</h2><h3 id="1、最小二乘法"><a href="#1、最小二乘法" class="headerlink" title="1、最小二乘法"></a>1、最小二乘法</h3><p>最小二乘法是一种常用的求解线性回归的方法，它通过求解以下方程来找到最佳的 ( w ) 和 ( b )。</p><p>最小二乘法的目标是<strong>最小化残差平方和（RSS）</strong>，其公式为：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/647bf95e98352a445cab7d547a88ede5.png" alt="647bf95e98352a445cab7d547a88ede5"></p><p>其中：</p><ul><li><code>yi</code> 是实际值。</li><li><code>y^i</code> 是预测值，由线性回归模型 <code>y^i=wxi+by^i=wxi+b</code> 计算得到。</li></ul><p>通过<strong>最小化 RSS</strong>，可以得到以下正规方程：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c4cfc420f1878f532910b15d481b66a3.png" alt="c4cfc420f1878f532910b15d481b66a3"></p><h3 id="矩阵形式"><a href="#矩阵形式" class="headerlink" title="矩阵形式"></a>矩阵形式</h3><p>将正规方程写成<strong>矩阵形式</strong>：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4b024cba97bb200af12c9907fadae8f0.png" alt="4b024cba97bb200af12c9907fadae8f0"></p><h3 id="求解方法"><a href="#求解方法" class="headerlink" title="求解方法"></a>求解方法</h3><p>通过<strong>求解上述矩阵方程</strong>，可以得到最佳的 w 和 <code>b</code>：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/9a03245250f7f00499476096b0df39b2.png" alt="9a03245250f7f00499476096b0df39b2"></p><h3 id="2、梯度下降法"><a href="#2、梯度下降法" class="headerlink" title="2、梯度下降法"></a>2、梯度下降法</h3><p>梯度下降法的目标是<strong>最小化损失函数 <code>J(w,b)</code></strong>。对于线性回归问题，通常使用<strong>均方误差（MSE）作为损失函数</strong>：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/836d14994f1d38d667b8f5556328c600.png" alt="836d14994f1d38d667b8f5556328c600"></p><p>其中：</p><ul><li><code>m</code> 是<strong>样本</strong>数量。</li><li><code>yi</code> 是实际值。</li><li><code>y^i</code> 是预测值，由线性回归模型 <code>y^i=wxi+by^i=wxi+b</code> 计算得到。</li></ul><p>梯度是<strong>损失函数对参数的偏导数</strong>，表示损失函数<strong>在参数空间中的变化方向</strong>。对于线性回归，梯度计算如下：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4fc791e09f80dc2129464eb7a3791883.png" alt="4fc791e09f80dc2129464eb7a3791883"></p><h3 id="参数更新规则"><a href="#参数更新规则" class="headerlink" title="参数更新规则"></a>参数更新规则</h3><p>梯度下降法通过以下规则更新参数 <code>w</code> 和 <code>b</code>：</p><p><img src="/./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/83363dfa9d7249a76cf8fb13d3a73467.png" alt="83363dfa9d7249a76cf8fb13d3a73467"></p><p>其中：</p><ul><li><code>α</code> 是学习率（learning rate），控制每次更新的步长。</li></ul><h4 id="梯度下降法的步骤"><a href="#梯度下降法的步骤" class="headerlink" title="梯度下降法的步骤"></a>梯度下降法的步骤</h4><ol><li><strong>初始化参数</strong>：初始化 <code>w</code> 和 <code>b</code> 的值（通常设为 0 或随机值）。</li><li><strong>计算损失函数</strong>：计算<strong>当前参数下</strong>的损失函数值 <code>J(w,b)</code>。</li><li><strong>计算梯度</strong>：计算损失函数对 <code>w</code> 和 <code>b</code> 的<strong>偏导数</strong>。</li><li><strong>更新参数</strong>：<strong>根据梯度更新</strong> <code>w</code> 和 <code>b</code>。</li><li><strong>重复迭代</strong>：重复步骤 2 到 4，<strong>直到损失函数收敛或达到最大迭代次数</strong>。</li></ol><h2 id="使用-Python-实现线性回归"><a href="#使用-Python-实现线性回归" class="headerlink" title="使用 Python 实现线性回归"></a>使用 Python 实现线性回归</h2><p>下面我们通过一个简单的例子来演示如何使用 Python 实现线性回归。</p><h3 id="1、导入必要的库"><a href="#1、导入必要的库" class="headerlink" title="1、导入必要的库"></a>1、导入必要的库</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br></pre></td></tr></table></figure><h3 id="2、生成模拟数据"><a href="#2、生成模拟数据" class="headerlink" title="2、生成模拟数据"></a>2、生成模拟数据</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一些随机数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">x = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * x + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Generated Data From Runoob&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>进行一个一个前端的雪</title>
      <link href="/2025/01/23/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E5%89%8D%E7%AB%AF%E7%9A%84%E9%9B%AA/"/>
      <url>/2025/01/23/%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E4%B8%80%E4%B8%AA%E5%89%8D%E7%AB%AF%E7%9A%84%E9%9B%AA/</url>
      
        <content type="html"><![CDATA[<h2 id="Web-开发概况"><a href="#Web-开发概况" class="headerlink" title="Web 开发概况"></a>Web 开发概况</h2><p>Web 开发是指创建和维护<strong>网站</strong>、<strong>客户端程序</strong>、<strong>服务器</strong>与<strong>其他 Web 应用程序</strong>的过程。它包括使用不同的编程语言和技术来编写、测试和部署 Web 应用程序，以满足特定的业务需求和用户需求</p><p>通过 Web 开发技术，开发者能够设计实现诸多满足不同需求场景的<strong>应用程序</strong>，包括但不限于：网站开发、Android&#x2F;IOS&#x2F;Harmony OS NEXT 移动端应用程序、微信小程序、桌面应用、群聊机器人、游戏、浏览器插件、3D 建模、高性能服务器、分布式应用、虚拟现实应用、区块链、物联网设备……</p><h2 id="前端和后端"><a href="#前端和后端" class="headerlink" title="前端和后端"></a>前端和后端</h2><p>在软件架构和程序设计领域，前端是软件系统中<strong>直接</strong>和用户交互的部分，而后端控制着<strong>软件的输出</strong>。将软件分为前端和后端是一种将<strong>软件不同功能</strong>的部分相互分离的抽象</p><p>在 Web 开发中，前端在绝大多数情况下指能够被<strong>用户直接访问与交互的模块</strong>，如<strong>网页、手机 App、桌面应用、小程序等</strong>。后端包括<strong>程序运行的后台服务器</strong>、<strong>存储数据的数据库</strong>以及<strong>其他数据中间件</strong>。大部分软件都<strong>概念性地</strong>分成了前端和后端，在大多数情况下，软件的后端经常是隐藏着而不被用户看到</p><p><strong>狭义</strong>的前端通常是指网站或应用程序中与用户直接交互的部分。它是一种用于<strong>构建用户界面的技术和工具的集合</strong>，这些界面可以在 <strong>Web 浏览器</strong>中运行</p><p>后端开发主要负责编写<strong>运行在服务端上的</strong>代码，通常来说，这部分的工作需要和<strong>数据库</strong>与 <strong>Web API</strong> 打交道，比如读写数据、读写文件、实现业务逻辑等。有些时候，业务逻辑存储在<strong>客户端</strong>，这时后台就是用来<strong>以 Web 服务的形式</strong>提供数据库中的数据</p><p>开发者<strong>可以同时掌握</strong>前端和后端的技术，但大多数 Web 开发者都还是有<strong>一定的专精</strong>方向，甚至只在某一方面深入研究。尽管前后端是有天然的区别，但并没有规定它们各自的具体任务。有时前端只是完成数据的显示，而其他主要工作都在后端完成。但也有时，后端只是提供数据，而所有的计算和具体功能都在前端完成。前后端工作的分配，通常都是<strong>由项目的设计和架构来决定</strong>的</p><h2 id="浏览器"><a href="#浏览器" class="headerlink" title="浏览器"></a>浏览器</h2><p>浏览器是用来<strong>检索、展示以及传递</strong> <strong>Web 信息资源的应用程序</strong>。Web 信息资源由<strong>统一资源标识符 (Uniform Resource Identifier，URI)</strong> 所标记，它可以是一张网页、一张图片、一段视频或者<strong>任何在 Web 上所呈现的内容</strong>。使用者可以<strong>借助超链接</strong>，<strong>通过浏览器浏览</strong>互相关联的信息</p><p>浏览器<strong>内核</strong> (Rendering Engine)，是指浏览器最核心的部分，负责<strong>对网页语法的解释（如标准通用标记语言下的一个应用 HTML、CSS、JavaScript）并渲染网页</strong>。通常所谓的浏览器内核也就是浏览器所采用的<strong>渲染引擎</strong>，渲染引擎决定了浏览器<strong>如何显示网页的内容</strong>以及<strong>页面的格式信息</strong>。不同的浏览器内核对网页编写语法的解释也有不同</p><h2 id="C-S-与-B-S-架构"><a href="#C-S-与-B-S-架构" class="headerlink" title="C&#x2F;S 与 B&#x2F;S 架构"></a><strong>C&#x2F;S</strong> <strong>与</strong> <strong>B&#x2F;S</strong> 架构</h2><p>C&#x2F;S 架构是一种典型的<strong>两层架构</strong>，其全称是 <strong>Client&#x2F;Server</strong>，即<strong>客户</strong>端<strong>服务器</strong>端架构，其客户端包含一个或多个在用户的电脑上运行的<strong>程序</strong>，而服务器端有两种，一种是<strong>数据库</strong>服务器端，客户端<strong>通过数据库连接访问服务器端的数据</strong>；另一种是 <strong>Socket</strong> 服务器端，<strong>服务器端的程序</strong>通过 Socket 与<strong>客户端的程序</strong>通信</p><p>B&#x2F;S 架构的全称为 <strong>Browser&#x2F;Server</strong>，即<strong>浏览器</strong>&#x2F;<strong>服务器</strong>结构。Browser 指的是 <strong>Web 浏览器</strong>，极少数事务逻辑在前端实现，但<strong>主要</strong>事务逻辑在<strong>服务器端</strong>实现。B&#x2F;S 架构的系统无须特别安装，<strong>只要有 Web 浏览器</strong>即可</p><h2 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h2><p>超文本标记语言（Hyper Text Markup Language，简称：HTML）是一种<strong>用于创建网页</strong>的标准标记语言。HTML 是一种基础技术，常与 CSS、JavaScript 一起被众多网站用于<strong>设计网页</strong>、<strong>网页应用程序</strong>以及<strong>移动应用程序</strong>的<strong>用户界面</strong>。网页<strong>浏览器</strong>可以读取 HTML 文件，并将其<strong>渲染成可视化网页</strong>。HTML 描述了<strong>一个网站的结构语义随着线索的呈现</strong>，使之成为一种<strong>标记</strong>语言而非编程语言</p><h2 id="CSS"><a href="#CSS" class="headerlink" title="CSS"></a>CSS</h2><p><strong>层叠样式表（Cascading Style Sheets）<strong>是一种用来</strong>为结构化文档（如 HTML 文档或 XML 应用）添加样式（字体、间距和颜色等）<strong>的计算机语言。CSS3 现在已被大部分现代浏览器支持，而下一版的 CSS4 仍在开发中。CSS 不仅可以</strong>静态地修饰</strong>网页，还可以<strong>配合各种脚本语言动态地</strong>对网页各元素进行<strong>格式</strong>化。CSS 能够对网页中元素位置的<strong>排版进行像素级精确控制</strong>，支持几乎所有的字体字号样式，拥有对网页对象和模型样式编辑的能力</p><h2 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h2><p>JavaScript 是一种高级的、<strong>解释型</strong>的<strong>编程</strong>语言</p><p>JavaScript 是一门基于原型、头等函数的语言，是一门多范式的语言，它支持面向对象程序设计，指令式编程，以及函数式编程。它由 ECMA（欧洲电脑制造商协会）通过 ECMAScript 实现语言的标准化</p><p>ECMAScript 6.0（简称 ES6）是 JavaScript 语言的下一代<strong>标准</strong>，于 2015 年 6 月正式发布。它的目标，是使得 JavaScript 语言可以用来编写复杂的大型应用程序，成为企业级开发语言</p><p>ES6 既是一个历史名词，也是一个泛指，含义是 5.1 版以后的 JavaScript 的下一代标准，涵盖了 ES2015、ES2016、ES2017 等等，而 ES2015 则是正式名称，特指该年发布的正式版本的语言标准</p><h2 id="计算机网络基础知识"><a href="#计算机网络基础知识" class="headerlink" title="计算机网络基础知识"></a>计算机网络基础知识</h2><h4 id="HTTP-协议"><a href="#HTTP-协议" class="headerlink" title="HTTP 协议"></a>HTTP 协议</h4><p><strong>HTTP 是 <em>Hyper Text Transfer Protocol</em>（超文本传输协议）<strong>的缩写。HTTP 协议用于</strong>从 WWW 服务器传输超文本到本地浏览器</strong>的传送协议。它可以使浏览器更加高效，使<strong>网络传输减少</strong>。它不仅保证计算机正确快速地传输超文本文档，还确定传输文档中的哪一部分，以及哪部分内容<strong>首先</strong>显示 (如文本先于图形) 等。HTTP 是一个<strong>应用层协议</strong>，由<strong>请求</strong>和<strong>响应</strong>构成，是一个标准的<strong>客户端服务器模型</strong>。</p><h4 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h4><p>在互联网上，<strong>每一</strong>信息资源都有<strong>统一的且在网上唯一的地址</strong>，该地址就叫 URL（Uniform Resource Locator, 统一资源定位符）。</p><p>URL 由三部分组成：资源<strong>类型</strong>、存放资源的<strong>主机域名</strong>、资源<strong>文件名</strong>。</p><p>也可认为由 4 部分组成：<strong>协议</strong>、<strong>主机</strong>、<strong>端口</strong>、<strong>路径</strong>。</p><p>URL 的一般格式为：<em>protocol :&#x2F;&#x2F; hostname [:port] &#x2F; path &#x2F; [:parameters] [?query] #fragment</em></p><h4 id="IP-地址和-DNS"><a href="#IP-地址和-DNS" class="headerlink" title="IP 地址和 DNS"></a>IP 地址和 DNS</h4><p><strong>IP 地址</strong>（类似 192.168.1.1 内网网关）是<strong>互联网协议地址</strong>，它给因特网上的每<strong>台计算机和其它设备</strong>都规定了一个唯一的地址。由于有这种唯一的地址，才保证了用户在连网的计算机上操作时，能够高效而且方便地从千千万万台计算机中选出自己所需的对象来</p><p>但是 IP 地址毕竟是一串<strong>毫无规律</strong>的数字，并不方便人类的记忆和书写。因此在 IP 地址的基础上又发展出一种<strong>符号化</strong>的地址方案，来代替数字型的 IP 地址，每一个符号化的地址都与特定的 IP 地址对应。这个与网络上的数字型 IP 地址相对应的字符型地址，就是<strong>域名</strong></p><p>类似 <a href="https://www.google.com/">http://www.google.com</a> 这样的字符串就是“域名”，当访问 <a href="http://www.google.com/">www.google.com</a> 时，首先由 DNS（Domain Name System, DNS）域名系统<strong>解析为 IP 地址，随后再访问 IP</strong></p><h4 id="HTTP-请求"><a href="#HTTP-请求" class="headerlink" title="HTTP 请求"></a>HTTP 请求</h4><p>HTTP 请求是指从<strong>客户端到服务器端</strong>的请求消息，请求报文由请求行 (Request line)、请求头 (Header)，空行、请求正文 4 部分组成</p><h4 id="HTTP-响应"><a href="#HTTP-响应" class="headerlink" title="HTTP 响应"></a>HTTP 响应</h4><p>在接收和解释请求消息后，服务器会<strong>返回一个 HTTP 响应消息</strong>。HTTP 响应报文也由四个部分组成，分别是：状态行、消息报头、空行和响应正文</p><h4 id="HTTP-方法"><a href="#HTTP-方法" class="headerlink" title="HTTP 方法"></a>HTTP 方法</h4><p>根据 HTTP 标准，HTTP 请求可以使用多种<strong>请求方法</strong>。HTTP 方法描述了对给定资源的期望动作，每一种请求方法都抽象出了一种不同给定语义。</p><p>HTTP1.0 定义了三种请求方法：GET、POST 和 HEAD 方法。</p><p>HTTP1.1 新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT 方法。</p><p>在实际开发中 <strong>GET、POST、PUT、DELETE</strong> 四类 HTTP 方法的使用率最高，能够用一套统一的语法规范对资源的 CRUD (增删改查) 逻辑进行抽象</p><p>GET 方法请求一个指定资源的<strong>表示形式</strong>，使用 GET 的请求应该<strong>只被用于获取数据</strong></p><p>POST 方法用于<strong>将实体提交到指定的资源</strong>，通常导致在服务器上的<strong>状态变化</strong>或副作用</p><p>PUT 方法用<strong>请求有效载荷替换</strong>目标资源的所有当前表示</p><p>DELETE 方法<strong>删除指定的资源</strong></p><h4 id="HTTP-状态码"><a href="#HTTP-状态码" class="headerlink" title="HTTP 状态码"></a>HTTP 状态码</h4><p>当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一<strong>个包含 HTTP 状态码的信息头</strong>（server header）<strong>用以响应</strong>浏览器的请求</p><p>状态码类型：</p><table><thead><tr><th>状态码</th><th>类别</th><th>原因</th></tr></thead><tbody><tr><td>1xx</td><td><strong>信息性</strong>状态码</td><td>接收的请求<strong>正在处理</strong></td></tr><tr><td>2xx</td><td><strong>成功</strong>状态码</td><td>请求正常处理<strong>完毕</strong></td></tr><tr><td>3xx</td><td><strong>重定向</strong>状态码</td><td>需要<strong>进行附加操作</strong>以完成请求</td></tr><tr><td>4xx</td><td><strong>客户端错误</strong>状态码</td><td>服务器无法处理请求</td></tr><tr><td>5xx</td><td><strong>服务端错误</strong>状态码</td><td>服务器处理请求出错</td></tr></tbody></table><p>常见的状态码：</p><p><strong>100 Continue</strong></p><p>客户端应继续其请求</p><p><strong>200 OK</strong></p><p>请求成功，一般用于 <strong>GET 与 POST</strong> 请求</p><p><strong>201 Created</strong></p><p>已创建，<strong>成功请求并创建</strong>了新的资源</p><p><strong>401 Unauthorized</strong></p><p>请求要求用户的<strong>身份认证</strong></p><p><strong>403 Forbidden</strong></p><p>服务器理解请求客户端的请求，但是<strong>拒绝执行</strong>此请求</p><p><strong>404 Not Found</strong></p><p>服务器<strong>无法</strong>根据客户端的请求<strong>找到</strong>资源（网页）。</p><p><strong>500 Internal Server Error</strong></p><p>服务器<strong>内部错误</strong>，无法完成请求</p><h4 id="RESTful-API"><a href="#RESTful-API" class="headerlink" title="RESTful API"></a><strong>RESTful</strong> <strong>API</strong></h4><p><strong>REST 全称是 Representational State Transfer</strong>，中文意思是<strong>表述性状态转移</strong>。</p><p>RESTful 架构应该遵循<strong>统一接口原则</strong>，统一接口包含了一组<strong>受限的预定义的操作</strong>，不论什么样的资源，都是<strong>通过使用相同的接口</strong>进行资源的访问。</p><p>接口应该使用<strong>标准的 HTTP 方法</strong>如 GET，PUT 和 POST，并遵循这些方法的语义。</p><p>REST 所谓的表述指的是<strong>对资源的表述</strong>。要让一个资源可以被<strong>识别</strong>，需要有个<strong>唯一标识</strong>，在 Web 中这个唯一标识就是 <strong>URI</strong></p><h2 id="版本控制工具"><a href="#版本控制工具" class="headerlink" title="版本控制工具"></a>版本控制工具</h2><h4 id="Git-版本控制工具"><a href="#Git-版本控制工具" class="headerlink" title="Git 版本控制工具"></a>Git 版本控制工具</h4><p>Git 是一个开源的<strong>分布式版本控制系统</strong>，可以有效、高速地处理从很小到非常大的项目版本管理。也是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件</p><h4 id="GitHub-代码托管仓库"><a href="#GitHub-代码托管仓库" class="headerlink" title="GitHub 代码托管仓库"></a>GitHub 代码托管仓库</h4><p>GitHub 是一个面向开源及私有软件项目的托管平台，因为<strong>只支持 Git</strong> 作为<strong>唯一的版本库格式</strong>进行托管</p><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h4 id="库"><a href="#库" class="headerlink" title="库"></a>库</h4><p>库是一系列预先定义好的数据结构和函数或类的集合，程序员可以通过调用这些代码实现功能。简单来说就是库为我们提供了很多封装好的函数，看起来比较零散，但使用起来更灵活</p><p>使用库可以简化开发流程，提高开发效率。例如，jQuery 提供了简化 DOM 操作的语法，减少了编写繁琐代码的需要。React 通过虚拟 DOM 和声明式 UI ，便于<strong>快速构建用户界面</strong></p><p>如果需要<strong>在网页中使用 JavaScript 库</strong>，可以去网上<strong>下载库文件</strong>，<strong>放在网页的同一目录</strong>下，再到**<code>script</code>标签中引入**。或者不下载通过<strong>通过链接在<code>&lt;script&gt;</code>标签中引用该库</strong>即可：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=<span class="string">&quot;https://cdn.staticfile.org/jquery/3.4.0/jquery.min.js&quot;</span>&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>或者在代码中通过 <code>require</code> 或者 <code>import</code> 中引入库。在现代的前端开发中，通常推荐使用 <code>import</code> 来进行模块导入，特别是在使用现代 JavaScript 特性的项目中。这主要与现代 JavaScript 的发展趋势和语言特性有关</p><p><code>import</code> 是 ES6 新引入的关键字，支持<strong>按需导入</strong>，而不需要导入整个模块。同时<code>import</code> 的语法也比 <code>require</code> 更直观清晰，更符合现代变成风格</p><p>随着 JavaScript 生态的发展，越来越多的库和工具采用了 ES6 模块系统，使用 <code>import</code> 能够更好地与这些现代化的工具和库进行集成。</p><h4 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h4><p>框架是提供<strong>如何构建应用程序的意见的库</strong>，是<strong>一整套的工具</strong>，所有东西已经准备齐全了，可以按照它的规定就可以很简单的完成一些事情，但我们<strong>不能去改变它，只能按照要求</strong>使用，并且其他人拿到这套工具也是一样的，如 Vue、Angular 等等。</p><p>注意是<strong>一套而不是单个</strong>，比如 React 就是一个库，它本身只是一个前端渲染的库，纯粹地写 UI 组件，没有什么异步处理机制、模块化等，但是当它结合 Redux 和 React-router 的时候，就是一个框架了。</p><p>框架和库的联系紧密，都是为了提高我们的开发效率而存在，库的使用上会<strong>简单</strong>一些，更加<strong>灵活</strong>，但<strong>功能不全</strong>。而框架的功能很<strong>全面</strong>，但需要我们<strong>按规定</strong>去使用。也就是说库是一种工具，我提供了，你可以不用，即使你用了，也没影响你自己的代码结构，控制权在使用者手中。框架则是面向一个领域，提供了<strong>一套解决方案</strong></p><h4 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h4><p>库是一组已经实现的<strong>代码集合</strong>，提供了特定功能的函数和方法，开发者可以根据需要选择性地使用。库不控制应用程序的整体架构，而是为开发者提供了可调用的工具，以便在应用程序中实现特定功能</p><p>框架是一种提供了<strong>一整套解决方案的软件结构</strong>，它规定了整个应用程序的架构，定义了组织代码的方式，并提供了一系列工具和库，以便开发者可以在框架的基础上构建应用。框架通常有一个完整的生命周期，控制着应用程序的流程，开发者需要按照框架的规则来编写代码。</p><h2 id="Node-js"><a href="#Node-js" class="headerlink" title="Node.js"></a>Node.js</h2><h4 id="什么是-Node-js"><a href="#什么是-Node-js" class="headerlink" title="什么是 Node.js"></a>什么是 Node.js</h4><p>JavaScript 是一个<strong>脚本语言</strong>，最初用来处理网页中的一些动态功能和一些用户行为。它一般运行于浏览器</p><p>但是这门语言后续不断更新，越来越多的人开始使用 JavaScript 。为了把它迁移到了服务端，但服务端上又不能跑浏览器，那我们就需要一种新的运行环境。就这样，这个基于 Chrome V8 引擎的 JavaScript 运行时 Node.js 诞生了</p><h2 id="模块化编程"><a href="#模块化编程" class="headerlink" title="模块化编程"></a>模块化编程</h2><p>在计算机编程中，模块是指一个<strong>相对独立的程序文件或代码库</strong>，通常包含一组相关的函数、变量、类或其他可重用的代码构件，每个模块在内部执行某个功能。并向外部公开一定的接口以供其他模块使用。在编程语言中，通常有一些标准库或第三方库，这些库都是由多个模块组成的，可以在程序中被引用和使用。模块化主要是为了帮助程序员组织和管理大型代码库，可以将大型的程序有逻辑地拆分成一个个相对较小的部分，实现代码复用，让程序设计更加灵活，使其更易于维护和扩展。这是优点之一。并且还可以避免变量名和函数名命名冲突的问题以及解决不同模块之间的依赖关系。</p><p>比如，我要写一个 Wordle 小游戏，普通代码编写就把所有代码像画布渲染，键盘的输入，逻辑判断等都写到一个 HTML 文件里，如果使用模块化概念，我们可以简单分块，分成主文件，键盘输入，逻辑判断以及读取 json 等多个模块，然后在各个文件里实现相应的逻辑，这样假如你发现 json 的读取有问题，你就可以直接去找读 json 那个文件有没有问题，这样会让代码的后续维护更简单，目的更明确。</p><p><code>import</code> 和 <code>export</code> 是 ES6 引入的模块系统的关键字，用于在 JavaScript 中进行模块化编程。模块化使得代码更结构化、可维护，并允许开发者将代码分割为<strong>小的可重用部分</strong></p><h4 id="export-的使用："><a href="#export-的使用：" class="headerlink" title="export 的使用："></a><code>export</code> 的使用：</h4><p><code>export</code> 用于将变量、函数、类或其他声明<strong>导出为模块的公共接口</strong>，以便其他模块可以使用。有三种常见的 <code>export</code> 的方式</p><h5 id="命名导出"><a href="#命名导出" class="headerlink" title="命名导出"></a>命名导出</h5><p>可以通过 <code>export</code> 关键字<strong>单独导出</strong>多个成员</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// module.js</span></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">const</span> myVariable = <span class="number">42</span>;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">function</span> <span class="title function_">myFunction</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="默认导出"><a href="#默认导出" class="headerlink" title="默认导出"></a>默认导出</h5><p>通过 <code>export default</code> 关键字导出一个<strong>默认</strong>成员，每个模块只能有<strong>一个</strong>默认导出</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// module.js</span></span><br><span class="line"><span class="keyword">const</span> myVariable = <span class="number">42</span>;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> myVariable;</span><br></pre></td></tr></table></figure><h4 id="import-的使用："><a href="#import-的使用：" class="headerlink" title="import 的使用："></a><code>import</code> 的使用：</h4><p><code>import</code> 用于在一个模块中引入其他模块导出的成员，以便在当前模块中使用。有三种常见的 <code>import</code> 的方式：</p><h5 id="命名导入"><a href="#命名导入" class="headerlink" title="命名导入"></a>命名导入</h5><p>导入其他模块中的命名导出</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// main.js</span></span><br><span class="line"><span class="keyword">import</span> &#123; myVariable, myFunction &#125; <span class="keyword">from</span> <span class="string">&quot;./module&quot;</span>;</span><br></pre></td></tr></table></figure><h5 id="默认导入"><a href="#默认导入" class="headerlink" title="默认导入"></a>默认导入</h5><p>导入其他模块中的默认导出</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// main.js</span></span><br><span class="line"><span class="keyword">import</span> myVariable <span class="keyword">from</span> <span class="string">&quot;./module&quot;</span>;</span><br></pre></td></tr></table></figure><h5 id="导入所有"><a href="#导入所有" class="headerlink" title="导入所有"></a>导入所有</h5><p>导入其他模块的所有导出，形成一个<strong>命名空间对象</strong></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// main.js</span></span><br><span class="line"><span class="keyword">import</span> * <span class="keyword">as</span> myModule <span class="keyword">from</span> <span class="string">&quot;./module&quot;</span>;</span><br></pre></td></tr></table></figure><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// module.js</span></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">const</span> myVariable = <span class="number">42</span>;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">function</span> <span class="title function_">myFunction</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">const</span> internalVariable = <span class="string">&quot;internal&quot;</span>;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> internalVariable;</span><br><span class="line"><span class="comment">// main.js</span></span><br><span class="line"><span class="keyword">import</span> &#123; myVariable, myFunction &#125; <span class="keyword">from</span> <span class="string">&quot;./module&quot;</span>;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(myVariable); <span class="comment">// 42</span></span><br><span class="line"><span class="title function_">myFunction</span>();</span><br><span class="line"><span class="keyword">import</span> internalVariable <span class="keyword">from</span> <span class="string">&quot;./module&quot;</span>;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(internalVariable); <span class="comment">// &#x27;internal&#x27;</span></span><br><span class="line"><span class="keyword">import</span> * <span class="keyword">as</span> myModule <span class="keyword">from</span> <span class="string">&quot;./module&quot;</span>;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(myModule.<span class="property">myVariable</span>); <span class="comment">// 42</span></span><br></pre></td></tr></table></figure><h2 id="npm"><a href="#npm" class="headerlink" title="npm"></a>npm</h2><p>JavaScript 包是一种封装了代码、资源的组织形式，能够方便共享、安装和管理代码。这些包可以包含 JavaScript 库、框架、工具或应用程序等。而 <strong><code>npm</code> 就是管理这些包的工具</strong>（当然除了 <code>npm</code> 也有其他工具，比如 <code>yarn</code>、<code>yum</code>等），专门用于在服务器端和命令行工具中管理 JavaScript 包</p><p>为什么我们需要包管理工具呢？我们一次性把包都下载到电脑里，像 C 语言的头文件一样，需要用什么拿什么不就好了吗？首先，JavaScript 的包<strong>多达 90 万个</strong>，将所有这些包完全下载到本地会占用大量存储空间。这对于开发者的计算机来说可能是不切实际的，特别是在多个项目中共享相同的依赖项时。其次，软件包和库经<strong>常会更新</strong>，手动下载所有包可能导致更新不及时，使得项目失去了最新的功能和安全性修复。最后，有的项目需要使用某个包特定的版本，使用其他版本会导致项目无法运行或出现其他 bug，而<strong>包管理工具允许开发者指定项目所使用的依赖项的特定版本</strong>，以确保项目的稳定性和一致性。手动下载所有包可能会导致版本冲突和不同环境之间的不一致。因此我们需要使用包管理工具</p><p><code>npm</code> 是<strong>随同 Node.js 安装的包管理工具</strong>，安装好 node 之后就会默认安装好 <code>npm</code> 了</p><p>我们可以在命令行中输入 <code>npm -v</code> 判断是否安装了 <code>npm</code></p><h5 id="npm-的常见命令"><a href="#npm-的常见命令" class="headerlink" title="npm 的常见命令"></a><code>npm</code> 的常见命令</h5><p><code>npm install &lt;Module Name&gt;</code> 使用 <code>npm</code> 命令<strong>本地安装</strong>模块</p><p><code>npm install -g &lt;Module Name&gt;</code> <strong>全局</strong>安装</p><p>两个的区别就是本地安装将安装包<strong>放在当前文件夹的 <code>node_modules</code></strong> （如果没有则会自动生成）文件夹下，<strong>通过 <code>import</code></strong> 来引入本地安装的包；全局安装包则通常放<strong>在 <code>node</code> 的安装目录下</strong>，可以<strong>直接在命令行里</strong>使用</p><p><code>npm uninstall &lt;Name&gt;</code> 卸载模块</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g npm@&lt;版本号&gt;//更新 npm</span><br></pre></td></tr></table></figure><p><code>npm publish</code> 将自己的代码发布到 <strong><code>npm</code> 上的全球开源库</strong>中</p><h5 id="package-json"><a href="#package-json" class="headerlink" title="package.json"></a><code>package.json</code></h5><p><code>package.json</code> 是 Node.js 项目中的一个重要文件，它用于<strong>存储项目的配置信息</strong>。包含了项目的元数据（metadata），如项目名称、版本、作者、依赖库等信息。通过描述项目上下文、所需依赖和开发脚本，使项目具备可重复性和可移植性</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;learn_react&quot;</span>, <span class="comment">// 项目的名称</span></span><br><span class="line">    <span class="string">&quot;version&quot;</span>: <span class="string">&quot;0.1.0&quot;</span>, <span class="comment">// 项目的版本号</span></span><br><span class="line">    <span class="string">&quot;private&quot;</span>: <span class="literal">true</span>, <span class="comment">// 用于指示是否将该项目发布到公共的包注册表的标志</span></span><br><span class="line">    <span class="string">&quot;dependencies&quot;</span>: &#123;</span><br><span class="line">        <span class="comment">// 项目运行时所依赖的第三方包</span></span><br><span class="line">        <span class="string">&quot;@testing-library/jest-dom&quot;</span>: <span class="string">&quot;^5.14.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;@testing-library/react&quot;</span>: <span class="string">&quot;^13.0.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;@testing-library/user-event&quot;</span>: <span class="string">&quot;^13.2.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;@types/jest&quot;</span>: <span class="string">&quot;^27.0.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;@types/node&quot;</span>: <span class="string">&quot;^16.7.13&quot;</span>,</span><br><span class="line">        <span class="string">&quot;@types/react&quot;</span>: <span class="string">&quot;^18.0.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;@types/react-dom&quot;</span>: <span class="string">&quot;^18.0.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;react&quot;</span>: <span class="string">&quot;^18.2.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;react-dom&quot;</span>: <span class="string">&quot;^18.2.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;react-scripts&quot;</span>: <span class="string">&quot;5.0.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;typescript&quot;</span>: <span class="string">&quot;^4.4.2&quot;</span>,</span><br><span class="line">        <span class="string">&quot;web-vitals&quot;</span>: <span class="string">&quot;^2.1.0&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;scripts&quot;</span>: &#123;</span><br><span class="line">        <span class="comment">// 定义一组自定义的命令脚本</span></span><br><span class="line">        <span class="string">&quot;start&quot;</span>: <span class="string">&quot;react-scripts start&quot;</span>,</span><br><span class="line">        <span class="string">&quot;build&quot;</span>: <span class="string">&quot;react-scripts build&quot;</span>,</span><br><span class="line">        <span class="string">&quot;test&quot;</span>: <span class="string">&quot;react-scripts test&quot;</span>,</span><br><span class="line">        <span class="string">&quot;eject&quot;</span>: <span class="string">&quot;react-scripts eject&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;browserslist&quot;</span>: &#123;</span><br><span class="line">        <span class="comment">// 用于指定项目所支持的目标浏览器范围的配置文件，通常用于前端开发</span></span><br><span class="line">        <span class="string">&quot;production&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;&gt;0.2%&quot;</span>, <span class="comment">// 支持全球使用率超过0.2%的浏览器</span></span><br><span class="line">            <span class="string">&quot;not dead&quot;</span>, <span class="comment">// 排除已经被官方宣布为不再更新的浏览器</span></span><br><span class="line">            <span class="string">&quot;not op_mini all&quot;</span> <span class="comment">// 用于排除 Opera Mini 浏览器，Opera Mini 具有一些独特的行为或限制，需要在项目中进行特殊处理</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="string">&quot;development&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;last 1 chrome version&quot;</span>, <span class="comment">// 支持每个浏览器的最后一个版本</span></span><br><span class="line">            <span class="string">&quot;last 1 firefox version&quot;</span>,</span><br><span class="line">            <span class="string">&quot;last 1 safari version&quot;</span></span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果项目有 <code>package.json</code> 文件，则通过命令 <code>npm install</code> <strong>可以根据 <code>&quot;dependencies&quot;</code></strong> 自动在 <code>node_modules</code> 文件夹中安装项目所需的所有包</p><p>注：上述 <code>package.json</code> 的注释是粘贴到 md 后再加的，目的是讲解<strong>键值对的意义</strong>，而 json 文件中是<strong>不允许添加注释的</strong>：</p><h2 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h2><p>打包是指<strong>将多个模块（ JavaScript、CSS、图片等）打包成为一个文件</strong>，这有助于代码管理、发布和使用。在前端开发中，通常需要使用打包工具将代码打包成<strong>浏览器可识别</strong>的格式，并优化加载速度和性能。</p><p>为什么前端需要打包？以前的前端开发存在三个大问题：没有模块化、第三方包的引入繁琐困难、代码以明文形式展示出来</p><p>我们利用打包工具就可以实现：支持模块化、自动打包第三方包、代码混淆，使得其他人无法阅读</p><p>下面介绍两个常使用的与打包有关的工具.</p><h4 id="Babel"><a href="#Babel" class="headerlink" title="Babel"></a>Babel</h4><p>Babel 是一个 JavaScript 编译器，它能够将 ECMAScript 2015+ 的新特性转换为向后兼容的 JavaScript 代码，例如将 ES6 的箭头函数转换为普通函数、将模板字符串转换为常规字符串等等，使得我们可以在现代浏览器中使用最新的 JavaScript 特性，从而<strong>解决浏览器兼容性问题</strong></p><p>执行 <code>npm install -g babel-cli</code> 安装 Babel</p><p>在项目根目录创建 <code>.babelrc</code> 文件，这是 Babel 的配置文件，并编写：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;presets&quot;</span>: [<span class="string">&quot;es2015&quot;</span>],</span><br><span class="line">    <span class="string">&quot;plugins&quot;</span>: []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行 <code>npm install babel-preset-es2015</code> 安装转码器，就是从源码转到老版本的代码中间的语法映射表</p><p>在根目录创建 <code>src</code> 文件夹，新建 <code>index.js</code> 并编写如下代码</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ./src/index.js</span></span><br><span class="line"><span class="keyword">let</span> [a, b, c] = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>];</span><br><span class="line">[a, b, c] = [b, c, a + <span class="number">1</span>];</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(a, b, c);</span><br></pre></td></tr></table></figure><p>这里用到了 ES6 的新特性<strong>解构赋值</strong>，执行 <code>babel src -d dist</code> Babel 就能够将它转换为旧的 ES2015 代码：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ./dist/index.js</span></span><br><span class="line"><span class="meta">&quot;use strict&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> a = <span class="number">1</span>,</span><br><span class="line">    b = <span class="number">2</span>,</span><br><span class="line">    c = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">var</span> _ref = [b, c, a + <span class="number">1</span>];</span><br><span class="line">a = _ref[<span class="number">0</span>];</span><br><span class="line">b = _ref[<span class="number">1</span>];</span><br><span class="line">c = _ref[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(a, b, c);</span><br></pre></td></tr></table></figure><h4 id="Webpack"><a href="#Webpack" class="headerlink" title="Webpack"></a>Webpack</h4><p>Webpack 是一个模块打包工具，它可以将<strong>多个模块打包</strong>成<strong>一个或多个 JavaScript 文件</strong>，而这些 JavaScript 文件可以被浏览器正确加载执行。Webpack 可以处理各种类型的资源文件，如 JS、CSS、图片等，并提供了各种插件和 loader 用于对不同类型的资源进行处理和优化，同时还支持热更新功能，方便开发人员进行调试和开发</p><p>Webpack 会<strong>隐藏源码的细节</strong>，把多个 JavaScript 合并成一个 JavaScript，提高浏览器的<strong>访问速度</strong>，使源码<strong>更加安全</strong></p><p>执行 <code>npm install -g webpack webpack-cli</code> 安装 Webpack</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//导入path模块,nodejs的内置模块</span></span><br><span class="line"><span class="keyword">const</span> path = <span class="built_in">require</span>(<span class="string">&quot;path&quot;</span>);</span><br><span class="line"><span class="comment">//定义JS打包的规则</span></span><br><span class="line"><span class="variable language_">module</span>.<span class="property">exports</span> = &#123;</span><br><span class="line">    <span class="comment">//指定构建的模式</span></span><br><span class="line">    <span class="attr">mode</span>: <span class="string">&quot;development&quot;</span>,</span><br><span class="line">    <span class="comment">//入口函数从哪里开始进行编译打包</span></span><br><span class="line">    <span class="attr">entry</span>: <span class="string">&quot;./src/main.js&quot;</span>,</span><br><span class="line">    <span class="comment">//编译成功以后要把内容输出到那里去</span></span><br><span class="line">    <span class="attr">output</span>: &#123;</span><br><span class="line">        <span class="comment">//定义输出的指定的目录__dirname 当前项目根目录，将生成一个dist文件夹</span></span><br><span class="line">        <span class="attr">path</span>: path.<span class="title function_">resolve</span>(__dirname, <span class="string">&quot;./dist&quot;</span>),</span><br><span class="line">        <span class="comment">//合并的js文件存储在dist/bundle.js文件中</span></span><br><span class="line">        <span class="attr">filename</span>: <span class="string">&quot;res.js&quot;</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>终端执行 <code>webpack</code> 即可在 <strong><code>dist</code> 文件夹中</strong>看到生成的 <code>res.js</code>，这就是合并后的 JavaScript 代码</p><p>通常在前端项目中，我们会<strong>将 Babel 和 Webpack 结合</strong>使用，使用 Babel 将最新版本的语法<strong>转换成向后兼容的代码</strong>，再由 Webpack 将这些代码<strong>打包并优化</strong>，最终生成浏览器可以解析的文件。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>计算机教育中缺失的一课笔记</title>
      <link href="/2025/01/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%95%99%E8%82%B2%E4%B8%AD%E7%BC%BA%E5%A4%B1%E7%9A%84%E4%B8%80%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
      <url>/2025/01/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%95%99%E8%82%B2%E4%B8%AD%E7%BC%BA%E5%A4%B1%E7%9A%84%E4%B8%80%E8%AF%BE%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>我们中的大多数人实际上只利用了这些工具中的<strong>很少</strong>一部分，我们常常只是死记硬背一些如咒语般的命令， 或是当我们卡住的时候，盲目地从网上复制粘贴一些命令。</p><h1 id="主题-1-The-Shell"><a href="#主题-1-The-Shell" class="headerlink" title="主题 1: The Shell"></a>主题 1: The Shell</h1><h2 id="shell-是什么？"><a href="#shell-是什么？" class="headerlink" title="shell 是什么？"></a>shell 是什么？</h2><p>如今的计算机有着多种多样的<strong>交互接口</strong>让我们可以进行指令的的输入，从炫酷的图像用户界面**（GUI）<strong>，<strong>语音输入</strong>甚至是 <strong>AR&#x2F;VR</strong> 都已经无处不在。 这些交互接口可以覆盖 80% 的使用场景，但是它们也从根本上限制了您的操作方式——你不能点击一个</strong>不存在的<strong>按钮或者是用语音输入一个</strong>还没有被录入的<strong>指令。 为了充分利用计算机的能力，我们不得不回到</strong>最根本<strong>的方式，使用</strong>文字接口：Shell**</p><p>几乎所有您能够接触到的平台都支持某种形式的 shell，有些甚至还提供了<strong>多种 shell</strong> 供您选择。虽然它们之间有些细节上的差异，但是其<strong>核心功能</strong>都是一样的：它允许你<strong>执行程序</strong>，<strong>输入并获取某种半结构化的输出</strong>。</p><p>本节课我们会使用 <strong>Bourne Again SHell, 简称 “bash”</strong> 。 这是被最广泛使用的一种 shell，它的语法和其他的 shell 都是类似的。打开 shell <em>提示符</em>（您输入指令的地方），您首先需要打开 <em>终端</em> 。您的设备通常都已经内置了终端，或者您也可以安装一个，非常简单。</p><h2 id="使用-shell-往光标前插东西"><a href="#使用-shell-往光标前插东西" class="headerlink" title="使用 shell(往光标前插东西)"></a>使用 shell(往光标前插东西)</h2><p>当您打开终端时，您会看到一个提示符，它看起来一般是这个样子的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">missing:~$ </span><br></pre></td></tr></table></figure><p>这是 shell 最主要的文本接口。它告诉你，你的主机名是 <code>missing</code> 并且您当前的工作目录（”current working directory”）或者说您当前所在的位置是 <code>~</code> (表示 “home”)。 <code>$</code> 符号表示您现在的身份不是 root 用户（稍后会介绍）。在这个提示符中，您可以输入 <em>命令</em> ，命令最终会被 shell 解析。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">missing:~$ <span class="built_in">date</span></span><br><span class="line">Fri 10 Jan 2020 11:49:31 AM EST</span><br><span class="line">missing:~$ </span><br></pre></td></tr></table></figure><p>我们可以在执行命令的同时向程序传递 <em>参数</em> ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">missing:~$ echo hello</span><br><span class="line">hello</span><br></pre></td></tr></table></figure><p>上例中，我们让 shell 执行 <code>echo</code> ，同时指定参数 <code>hello</code>。<code>echo</code> 程序将该参数<strong>打印</strong>出来。 shell <strong>基于空格分割命令</strong>并进行解析，然后<strong>执行第一个单词</strong>代表的程序，并将<strong>后续的单词作为程序可以访问的参数</strong>。如果您希望传递的参数中<strong>包含空格</strong>（例如一个名为 My Photos 的文件夹），您要么用使用<strong>单引号，双引号将其包裹</strong>起来，要么使用<strong>转义符号 <code>\</code></strong> 进行处理（<code>My\ Photos</code>）。</p><p>但是，shell 是如何知道去哪里寻找 <code>date</code> 或 <code>echo</code> 的呢？其实，类似于 Python 或 Ruby，shell 是一个<strong>编程环境</strong>，所以它具备变量、条件、循环和函数（下一课进行讲解）。当你在 shell 中执行命令时，您实际上是在执行一段 shell 可以解释执行的<strong>简短代码</strong>。如果你要求 shell 执行某个指令，但是该指令<strong>并不是 shell 所了解的</strong>编程关键字，那么它会去<strong>咨询 <em>环境变量</em> <code>$PATH</code></strong>，它会列<strong>出当 shell 接到某条指令时，进行程序搜索的路径</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">missing:~$ echo $PATH</span><br><span class="line">/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</span><br><span class="line">missing:~$ which echo</span><br><span class="line">/bin/echo</span><br><span class="line">missing:~$ /bin/echo $PATH</span><br><span class="line">/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</span><br></pre></td></tr></table></figure><p>当我们执行 <code>echo</code> 命令时，shell 了解到需要执行 <code>echo</code> 这个程序，随后它便会在 <code>$PATH</code> 中搜索由 <code>:</code> 所分割的一系列目录，<strong>基于名字搜索该程序</strong>。当找到该程序时便执行（假定该文件是 <em><strong>可执行程序</strong></em>，后续课程将详细讲解）。确定某个程序<strong>名代表的是哪个具体</strong>的程序，可以使用 <code>which</code> 程序。我们也可以<strong>绕过 <code>$PATH</code></strong>，通过<strong>直接指定</strong>需要执行的程序的路径来执行该程序</p><h2 id="在-shell-中导航"><a href="#在-shell-中导航" class="headerlink" title="在 shell 中导航"></a>在 shell 中导航</h2><p>shell 中的路径是一组被分割的目录，在 Linux 和 macOS 上使用 <strong><code>/</code> 分割</strong>，而在 Windows 上是 <code>\</code>。<strong>路径 <code>/</code></strong> 代表的是系统的<strong>根</strong>目录，所有的文件夹都包括在这个路径之下，在 Windows 上每个<strong>盘</strong>都有一个根目录（例如： <code>C:\</code>）。 我们假设您在学习本课程时使用的是 Linux 文件系统。如果某个路径以 <strong><code>/</code> 开头</strong>，那么它是一个 <em><strong>绝对</strong>路径</em>，其他的都是 <em>相对路径</em> 。相对路径是指<strong>相对于当前工作目录</strong>的路径，当前工作目录可以使用 <strong><code>pwd</code> 命令来获取</strong>。此外，切换目录需要使用 <code>cd</code> 命令(linux中单独用直接切回<strong>最初</strong>)。在路径中，<code>.</code> <strong>表示的是当前</strong>目录，而 <strong><code>..</code> 表示上级</strong>目录(用于切换)：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">missing:~$ pwd</span><br><span class="line">/home/missing</span><br><span class="line">missing:~$ cd /home</span><br><span class="line">missing:/home$ pwd</span><br><span class="line">/home</span><br><span class="line">missing:/home$ cd ..</span><br><span class="line">missing:/$ pwd</span><br><span class="line">/</span><br><span class="line">missing:/$ cd ./home</span><br><span class="line">missing:/home$ pwd</span><br><span class="line">/home</span><br><span class="line">missing:/home$ cd missing</span><br><span class="line">missing:~$ pwd</span><br><span class="line">/home/missing</span><br><span class="line">missing:~$ ../../bin/echo hello</span><br><span class="line">hello</span><br></pre></td></tr></table></figure><p>注意，shell 会<strong>实时显示</strong>当前的路径信息。您可以通过配置 shell 提示符来显示各种有用的信息，这一内容我们会在后面的课程中进行讨论。</p><p>一般来说，当我们运行一个程序时，如果我们<strong>没有指定路径</strong>，则该程序会<strong>默认在当前目录下</strong>执行。例如，我们常常会搜索文件，并在需要时创建文件。</p><p>为了查看指定目录下包含哪些文件，我们使用 <code>ls</code> 命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">missing:~$ ls//未指定路径看不了</span><br><span class="line">missing:~$ cd ..//切至根目录/</span><br><span class="line">missing:/home$ ls</span><br><span class="line">missing</span><br><span class="line">missing:/home$ cd ..</span><br><span class="line">missing:/$ ls</span><br><span class="line">bin</span><br><span class="line">boot</span><br><span class="line">dev</span><br><span class="line">etc</span><br><span class="line">home</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>除非我们利用<strong>第一个参数指定目录</strong>，否则 <code>ls</code> 会打印当前目录下的文件。大多数的命令接受<strong>标记</strong>和<strong>选项（带有值的标记）</strong>，它们以 <strong><code>-</code> 开头</strong>，并可以改变程序的行为。通常，在执行程序时使用 <code>-h</code> 或 <code>--help</code> (不是单独用)标记可以<strong>打印帮助信息</strong>，以便了解有哪些可用的标记或选项。例如，<code>ls --help</code> 的输出如下：</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>算法日记</title>
      <link href="/2025/01/22/%E7%AE%97%E6%B3%95%E6%97%A5%E8%AE%B0/"/>
      <url>/2025/01/22/%E7%AE%97%E6%B3%95%E6%97%A5%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>模板</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> lld long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> lf double</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> endl <span class="string">&#x27;\n&#x27;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> llu unsigned long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ci const int</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> clld const long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> cllu const unsigned long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> mo (1e9+7)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pi (acos(-1))</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> gc() getchar()</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> repp(i,a,b) for(lld i=(a);i&lt;=(b);i++)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> repm(i,a,b) for(lld i=(a);i&gt;=(b);i--)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ret return</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ct continue</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> <span class="keyword">elif</span> <span class="keyword">else</span> <span class="keyword">if</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pii pair<span class="string">&lt;int,int&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> mp make_pair</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> srepp(i,a) for(auto i=a.begin();i!=a.end();i++)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> srepm(i,a) for(auto i=--a.end();i!=--a.begin();i--)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> vi vector <span class="string">&lt;int&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> vii vector <span class="string">&lt;pair&lt;int,int&gt;</span>&gt;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> vlld vector <span class="string">&lt;long long&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> st struct</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> M 403</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> P</span></span><br></pre></td></tr></table></figure><p>2025.1.25</p><p>马的遍历</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> n,m,x,y,a[M][M];</span><br><span class="line">vector&lt;pii&gt;d;</span><br><span class="line">queue&lt;pii&gt;q;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">vd</span><span class="params">(pii s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(s.first&lt;=<span class="number">0</span>||s.second&lt;=<span class="number">0</span>||s.first&gt;=n<span class="number">+1</span>||s.second&gt;=m<span class="number">+1</span>)</span><br><span class="line">    ret <span class="number">1</span>;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bfs</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    <span class="built_in">repp</span>(j,<span class="number">1</span>,m)</span><br><span class="line">    a[i][j]=<span class="number">-1</span>;</span><br><span class="line">    a[x][y]=<span class="number">0</span>;</span><br><span class="line">    q.<span class="built_in">push</span>(<span class="built_in">mp</span>(x,y));</span><br><span class="line">    <span class="keyword">while</span>(q.<span class="built_in">size</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        pii now=q.<span class="built_in">front</span>();q.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">0</span>,<span class="number">7</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            pii nx=<span class="built_in">mp</span>(now.first-d[i].first,now.second-d[i].second);</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">vd</span>(nx))ct;</span><br><span class="line">            <span class="keyword">if</span>(a[nx.first][nx.second]==<span class="number">-1</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                a[nx.first][nx.second]=a[now.first][now.second]<span class="number">+1</span>;</span><br><span class="line">                q.<span class="built_in">push</span>(nx);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="number">0</span>);cin.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin&gt;&gt;n&gt;&gt;m&gt;&gt;x&gt;&gt;y;</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">-2</span>,<span class="number">1</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">-2</span>,<span class="number">-1</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">2</span>,<span class="number">1</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">2</span>,<span class="number">-1</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">1</span>,<span class="number">2</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">1</span>,<span class="number">-2</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">-1</span>,<span class="number">2</span>));</span><br><span class="line">    d.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">-1</span>,<span class="number">-2</span>));</span><br><span class="line">    <span class="built_in">bfs</span>();</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">repp</span>(j,<span class="number">1</span>,m)</span><br><span class="line">        &#123;</span><br><span class="line">            cout&lt;&lt;a[i][j]&lt;&lt;<span class="string">&quot;    &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>luogu P2895</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">bool</span> s[<span class="number">304</span>][<span class="number">304</span>] ,biao,vis[<span class="number">304</span>][<span class="number">304</span>],ditu[<span class="number">304</span>][<span class="number">304</span>];<span class="type">int</span> maxt,mint,t;</span><br><span class="line">queue&lt;pii&gt;q;</span><br><span class="line"><span class="type">int</span> dx[<span class="number">4</span>]=&#123;<span class="number">-1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>&#125;;</span><br><span class="line"><span class="type">int</span> dy[<span class="number">4</span>]=&#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">-1</span>,<span class="number">1</span>&#125;;</span><br><span class="line">unordered_map&lt;<span class="type">int</span>,vii&gt;tu;<span class="comment">//是unordered而不是multi</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">vd</span><span class="params">(<span class="type">int</span> x,<span class="type">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(ditu[x][y]||x&lt;=<span class="number">0</span>||y&lt;=<span class="number">0</span>)ret <span class="number">0</span>;</span><br><span class="line">    ret <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bfs</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   q.<span class="built_in">push</span>(<span class="built_in">mp</span>(<span class="number">1</span>,<span class="number">1</span>));</span><br><span class="line">   <span class="keyword">while</span>(q.<span class="built_in">size</span>())</span><br><span class="line">   &#123;</span><br><span class="line">    <span class="type">int</span> cishu=q.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span>(tu.<span class="built_in">find</span>(t)!=tu.<span class="built_in">end</span>() )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> it : tu[t])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> xx=it.first;<span class="type">int</span> yy=it.second;</span><br><span class="line">            ditu[xx][yy]=ditu[xx<span class="number">-1</span>][yy]=ditu[xx][yy<span class="number">-1</span>]=ditu[xx<span class="number">+1</span>][yy]=ditu[xx][yy<span class="number">+1</span>]=<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,cishu)</span><br><span class="line">    &#123;</span><br><span class="line">        pii it=q.<span class="built_in">front</span>();q.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="type">int</span> xx=it.first;<span class="type">int</span> yy=it.second;</span><br><span class="line">        <span class="keyword">if</span>(vis[xx][yy])&#123;</span><br><span class="line">            mint=t;ret;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(ditu[xx][yy])ct;</span><br><span class="line">        ditu[xx][yy]=<span class="number">1</span>;</span><br><span class="line">            <span class="built_in">repp</span>(i,<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(<span class="built_in">vd</span>(xx-dx[i],yy-dy[i]))</span><br><span class="line">                q.<span class="built_in">push</span>(<span class="built_in">mp</span>(xx-dx[i],yy-dy[i]));</span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    t++;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="type">int</span> m;mint = <span class="number">1002</span>;</span><br><span class="line">    cin&gt;&gt;m;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> x,y,t;</span><br><span class="line">        cin&gt;&gt;x&gt;&gt;y&gt;&gt;t;</span><br><span class="line">        x++;y++;</span><br><span class="line">        tu[t].<span class="built_in">push_back</span>(<span class="built_in">mp</span>(x,y));</span><br><span class="line">        s[x][y]=s[x<span class="number">-1</span>][y]=s[x<span class="number">+1</span>][y]=s[x][y<span class="number">-1</span>]=s[x][y<span class="number">+1</span>]=<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,<span class="number">302</span>)</span><br><span class="line">    <span class="built_in">repp</span>(j,<span class="number">1</span>,<span class="number">302</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(s[i][j]==<span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            biao=<span class="number">1</span>;</span><br><span class="line">            vis[i][j]=<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(!biao)</span><br><span class="line">    &#123;</span><br><span class="line">        cout&lt;&lt;<span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">bfs</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(mint==<span class="number">1002</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        cout&lt;&lt;<span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        cout&lt;&lt;mint;</span><br><span class="line">    &#125;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2025.2.4</p><p>P1955 程序自动分析</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> f[<span class="number">200003</span>],cnt;<span class="type">bool</span> biao;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(x==f[x])ret x;</span><br><span class="line">    <span class="built_in">ret</span> (f[x]=<span class="built_in">get</span>(f[x]));</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">merge</span><span class="params">(<span class="type">int</span> x,<span class="type">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     (f[<span class="built_in">get</span>(x)]=<span class="built_in">get</span>(y)) ;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="type">int</span> t;cin&gt;&gt;t;</span><br><span class="line">    <span class="keyword">while</span>(t--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n,e,x,y;</span><br><span class="line">        cin&gt;&gt;n ;</span><br><span class="line">        biao=cnt=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">memset</span>(f,<span class="number">0</span>,<span class="built_in">sizeof</span>(f));</span><br><span class="line">    unordered_map&lt;<span class="type">int</span>,<span class="type">int</span>&gt;p;</span><br><span class="line">    vii ck ;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,<span class="number">2</span>*n<span class="number">+1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        f[i]=i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        cin&gt;&gt;x&gt;&gt;y&gt;&gt;e;</span><br><span class="line">        <span class="keyword">if</span>(p.<span class="built_in">find</span>(x)!=p.<span class="built_in">end</span>())x=p[x];</span><br><span class="line">        <span class="keyword">else</span>    &#123;p[x]=++cnt;x=cnt;&#125;</span><br><span class="line">        <span class="keyword">if</span>(p.<span class="built_in">find</span>(y)!=p.<span class="built_in">end</span>())y=p[y];</span><br><span class="line">        <span class="keyword">else</span>    &#123;p[y]=++cnt;y=cnt;&#125;</span><br><span class="line">        <span class="keyword">if</span>(e)</span><br><span class="line">        <span class="built_in">merge</span>(x,y);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        ck.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(x,y));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> it:ck)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">get</span>(it.first)==<span class="built_in">get</span>(it.second))&#123;</span><br><span class="line">            biao=<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(biao)cout&lt;&lt;<span class="string">&quot;NO&quot;</span>;</span><br><span class="line">    <span class="keyword">else</span>    cout&lt;&lt;<span class="string">&quot;YES&quot;</span>;</span><br><span class="line">    cout&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>P1090 合并果子</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">lld heap[M],Size,n;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Up</span><span class="params">(lld x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(x&gt;<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(heap[x]&lt;heap[x/<span class="number">2</span>])</span><br><span class="line">        &#123;<span class="built_in">swap</span>(heap[x],heap[x/<span class="number">2</span>]);</span><br><span class="line">        x/=<span class="number">2</span>;&#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Down</span><span class="params">(lld x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    lld s=x*<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">while</span>(s&lt;=Size)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(s&lt;Size&amp;&amp;heap[s]&gt;heap[s<span class="number">+1</span>])s++;</span><br><span class="line">        <span class="keyword">if</span>(heap[s]&lt;heap[x])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">swap</span>(heap[s],heap[x]);</span><br><span class="line">            x=s;s=x*<span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Insert</span><span class="params">(lld x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    heap[++Size]=x;</span><br><span class="line">    <span class="built_in">Up</span>(Size);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">lld <span class="title">Top</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ret heap[<span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Pop</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    heap[<span class="number">1</span>]=heap[Size--];</span><br><span class="line">    <span class="built_in">Down</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    &#123;</span><br><span class="line">        lld op;</span><br><span class="line">        cin&gt;&gt;op;</span><br><span class="line">        <span class="built_in">Insert</span>(op);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(n==<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        cout&lt;&lt;heap[<span class="number">1</span>];</span><br><span class="line">        ret <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    lld sum=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n<span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        lld tem=<span class="built_in">Top</span>();</span><br><span class="line">        <span class="built_in">Pop</span>();</span><br><span class="line">        tem+=<span class="built_in">Top</span>();</span><br><span class="line">        <span class="built_in">Pop</span>();</span><br><span class="line">        sum+=tem;</span><br><span class="line">        <span class="built_in">Insert</span>(tem);</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;sum;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>25.2.7</p><p>P1111 修复公路</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> lld long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> lf double</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> endl <span class="string">&#x27;\n&#x27;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> llu unsigned long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ci const int</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> clld const long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> cllu const unsigned long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> mo (1e9+7)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pi (acos(-1))</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> gc() getchar()</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> repp(i,a,b) for(lld i=(a);i&lt;=(b);i++)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> repm(i,a,b) for(lld i=(a);i&gt;=(b);i--)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ret return</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ct continue</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> <span class="keyword">elif</span> <span class="keyword">else</span> <span class="keyword">if</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pii pair<span class="string">&lt;int,int&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> mp make_pair</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> srepp(i,a) for(auto i=a.begin();i!=a.end();i++)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> srepm(i,a) for(auto i=--a.end();i!=--a.begin();i--)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> vi vector <span class="string">&lt;int&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> vii vector <span class="string">&lt;pair&lt;int,int&gt;</span>&gt;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> vlld vector <span class="string">&lt;long long&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> st struct</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> M 1003</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> P</span></span><br><span class="line"><span class="type">int</span> n,m,fa[M],cnt,maxt;</span><br><span class="line">vector&lt;pair&lt;<span class="type">int</span>,pair&lt;<span class="type">int</span>,<span class="type">int</span>&gt;&gt;&gt;a;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> i)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(i==fa[i])ret i;</span><br><span class="line">    <span class="built_in">ret</span>(fa[i]=<span class="built_in">get</span>(fa[i]));</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">merge</span><span class="params">(<span class="type">int</span> i,<span class="type">int</span> j)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    fa[<span class="built_in">get</span>(i)]=<span class="built_in">get</span>(j);<span class="comment">//别忘了是要根与根合并,一个get()都不能少</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    cin&gt;&gt;n&gt;&gt;m;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">    fa[i]=i;</span><br><span class="line">    a.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(<span class="number">0</span>,<span class="built_in">mp</span>(<span class="number">0</span>,<span class="number">0</span>)));</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> x,y,t;</span><br><span class="line">        cin&gt;&gt;x&gt;&gt;y&gt;&gt;t;</span><br><span class="line">        a.<span class="built_in">push_back</span>(<span class="built_in">mp</span>(t,<span class="built_in">mp</span>(x,y)));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">sort</span>(a.<span class="built_in">begin</span>()<span class="number">+1</span>,a.<span class="built_in">end</span>());<span class="comment">//按时间来排序处理就好</span></span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> t=a[i].first;</span><br><span class="line">        <span class="type">int</span> x=(a[i].second).first;</span><br><span class="line">        <span class="type">int</span> y=(a[i].second).second;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">get</span>(x)!=<span class="built_in">get</span>(y))</span><br><span class="line">        &#123;</span><br><span class="line">            cnt++;</span><br><span class="line">            <span class="built_in">merge</span>(x,y);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(cnt==n<span class="number">-1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            cout&lt;&lt;t;</span><br><span class="line">            ret <span class="number">0</span>;</span><br><span class="line">        &#125;&#125;</span><br><span class="line">    cout&lt;&lt;<span class="number">-1</span>;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>25.2.11 cf1003 c1(贪心,每个数都在符合要求的情况下尽量保持最小,无法满足就退出)</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> n,m,b,a[<span class="number">200003</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="type">int</span> t;</span><br><span class="line">    cin&gt;&gt;t;</span><br><span class="line">    <span class="keyword">while</span>(t--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">bool</span> biao=<span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> n,m;</span><br><span class="line">        cin&gt;&gt;n&gt;&gt;m;</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">1</span>,n)</span><br><span class="line">        cin&gt;&gt;a[i];</span><br><span class="line">        cin&gt;&gt;b;</span><br><span class="line">        a[<span class="number">1</span>]=<span class="built_in">min</span>(a[<span class="number">1</span>],b-a[<span class="number">1</span>]);</span><br><span class="line">        <span class="built_in">repp</span>(i,<span class="number">2</span>,n)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> t1=<span class="built_in">max</span>(b-a[i],a[i]);</span><br><span class="line">            <span class="type">int</span> t2=<span class="built_in">min</span>(b-a[i],a[i]);</span><br><span class="line">            <span class="keyword">if</span>(a[i<span class="number">-1</span>]&lt;=t2)a[i]=t2;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(a[i<span class="number">-1</span>]&gt;t1)</span><br><span class="line">                &#123;</span><br><span class="line">                    biao=<span class="number">0</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                a[i]=t1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(biao)</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;YES&quot;</span>&lt;&lt;endl;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;No&quot;</span>&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>25.2.13P1048 采药</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> w[M],v[M];</span><br><span class="line"><span class="type">int</span> dp[M][M*<span class="number">10</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//ios::sync_with_stdio(0);cin.tie(0);</span></span><br><span class="line">    <span class="type">int</span> t,m;</span><br><span class="line">    cin&gt;&gt;t&gt;&gt;m;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    &#123;</span><br><span class="line">        cin&gt;&gt;w[i]&gt;&gt;v[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">repp</span>(j,<span class="number">1</span>,t)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[i][j]=dp[i<span class="number">-1</span>][j];<span class="comment">//先考虑不放入,继承(跨度较大)</span></span><br><span class="line">            <span class="keyword">if</span>(j&gt;=w[i])<span class="comment">//总时间大于这个时即可以尝试放入</span></span><br><span class="line">            dp[i][j]=<span class="built_in">max</span>(dp[i][j],dp[i<span class="number">-1</span>][j-w[i]]+v[i]);<span class="comment">//由子问题最优解递推,如果可以增值就放入</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;dp[m][t];    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>25.2.15</p><p>P1616<strong>十年oi一场空,不开longlong见祖宗</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">lld a[<span class="number">10003</span>],b[<span class="number">10003</span>],dp[<span class="number">10000003</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    lld t,m;</span><br><span class="line">    cin&gt;&gt;t&gt;&gt;m;</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    cin&gt;&gt;a[i]&gt;&gt;b[i];</span><br><span class="line">    <span class="built_in">repp</span>(i,<span class="number">1</span>,m)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">repp</span>(j,a[i],t)</span><br><span class="line">        dp[j]=<span class="built_in">max</span>(dp[j],dp[j-a[i]]+b[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;dp[t];</span><br><span class="line">    ret <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建备忘</title>
      <link href="/2025/01/18/hexo%E4%BD%BF%E7%94%A8/"/>
      <url>/2025/01/18/hexo%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>[toc]</p><h2 id="hexo博客最常用的套路就是"><a href="#hexo博客最常用的套路就是" class="headerlink" title="hexo博客最常用的套路就是"></a>hexo博客最常用的套路就是</h2><ol><li><code>hexo g; hexo s</code>本地预览，再更改，直到满意为止</li><li><code>hexo d</code>或者<code>hexo g; hexo d</code>远程部署。</li><li>hexo new “title” # 默认是post，如果博客名称有空格需要用双引号包裹起来</li></ol><h4 id="Front-matter"><a href="#Front-matter" class="headerlink" title="Front-matter"></a>Front-matter</h4><p>Front-matter 是 markdown 文件<strong>最上方</strong>以 — 分隔的區域，用於指定個別檔案的變數。</p><p>Page Front-matter 用於 頁面 配置<br>Post Front-matter 用於 文章頁 配置<br>如果標注可選的參數，可根據自己需要添加，不用全部都寫在 markdown 裏</p><h4 id="Page-Front-matter-md渲染好像放在开头才有效果"><a href="#Page-Front-matter-md渲染好像放在开头才有效果" class="headerlink" title="Page Front-matter(md渲染好像放在开头才有效果)"></a>Page Front-matter(md渲染好像放在开头才有效果)</h4><p><img src="/./hexo%E4%BD%BF%E7%94%A8/5b3a26a09ff1908363efb29e78f6c2fc.png" alt="5b3a26a09ff1908363efb29e78f6c2fc"></p><p><img src="/./hexo%E4%BD%BF%E7%94%A8/2f0c8e78306edad277656d22ef1c220f.png" alt="2f0c8e78306edad277656d22ef1c220f"></p><h4 id="Post-Front-matter"><a href="#Post-Front-matter" class="headerlink" title="Post Front-matter"></a>Post Front-matter</h4><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title:</span><br><span class="line">date:</span><br><span class="line">updated:</span><br><span class="line">tags:</span><br><span class="line">categories:</span><br><span class="line">keywords:</span><br><span class="line">description:</span><br><span class="line">top<span class="emphasis">_img:</span></span><br><span class="line"><span class="emphasis">comments:</span></span><br><span class="line"><span class="emphasis">cover:</span></span><br><span class="line"><span class="emphasis">toc:</span></span><br><span class="line"><span class="emphasis">toc_</span>number:</span><br><span class="line">toc<span class="emphasis">_style_</span>simple:</span><br><span class="line">copyright:</span><br><span class="line">copyright<span class="emphasis">_author:</span></span><br><span class="line"><span class="emphasis">copyright_</span>author<span class="emphasis">_href:</span></span><br><span class="line"><span class="emphasis">copyright_</span>url:</span><br><span class="line">copyright<span class="emphasis">_info:</span></span><br><span class="line"><span class="emphasis">mathjax:</span></span><br><span class="line"><span class="emphasis">katex:</span></span><br><span class="line"><span class="emphasis">aplayer:</span></span><br><span class="line"><span class="emphasis">highlight_</span>shrink:</span><br><span class="line">aside:</span><br><span class="line">abcjs:</span><br><span class="line"><span class="section">noticeOutdate:</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><p><img src="/./hexo%E4%BD%BF%E7%94%A8/c702dece8b2fa2b5e5d8eab28dc60763.png" alt="c702dece8b2fa2b5e5d8eab28dc60763"></p><h2 id="标签页"><a href="#标签页" class="headerlink" title="标签页"></a>标签页</h2><p>標籤頁<strong>文件名</strong>不一定是 tags, 例子中的 tags 只是一個示例.記得添加 type: “tags”</p><p>1.前往你的 Hexo 的根目錄</p><p>2.輸入 <strong>hexo new page tags</strong></p><p>3.你會找到 source&#x2F;tags&#x2F;index.md 這個文件</p><p>修改這個文件：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 標籤</span><br><span class="line">date: 2018-01-05 00:00:00</span><br><span class="line">type: &#x27;tags&#x27;</span><br><span class="line">orderby: random</span><br><span class="line"><span class="section">order: 1</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><p><img src="/./hexo%E4%BD%BF%E7%94%A8/e416034ea913cfc279af6527b4174a3d.png" alt="e416034ea913cfc279af6527b4174a3d"></p><h2 id="分类页"><a href="#分类页" class="headerlink" title="分类页"></a>分类页</h2><p><strong>hexo new page categories</strong></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 分類</span><br><span class="line">date: 2018-01-05 00:00:00</span><br><span class="line"><span class="section">type: &#x27;categories&#x27;</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><h2 id="友链"><a href="#友链" class="headerlink" title="友链"></a>友链</h2><p><strong>hexo new page link</strong></p><p>在 Hexo 根目錄中的 source&#x2F;_data（如果沒有 _data 文件夾，請自行創建），創建一個文件 link.yml</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">class_name:</span> <span class="string">友情鏈接</span></span><br><span class="line">  <span class="attr">class_desc:</span> <span class="string">那些人，那些事</span></span><br><span class="line">  <span class="attr">link_list:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Hexo</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://hexo.io/zh-tw/</span></span><br><span class="line">      <span class="attr">avatar:</span> <span class="string">https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg</span></span><br><span class="line">      <span class="attr">descr:</span> <span class="string">快速、簡單且強大的網誌框架</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">class_name:</span> <span class="string">網站</span></span><br><span class="line">  <span class="attr">class_desc:</span> <span class="string">值得推薦的網站</span></span><br><span class="line">  <span class="attr">link_list:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Youtube</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://www.youtube.com/</span></span><br><span class="line">      <span class="attr">avatar:</span> <span class="string">https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png</span></span><br><span class="line">      <span class="attr">descr:</span> <span class="string">視頻網站</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Weibo</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://www.weibo.com/</span></span><br><span class="line">      <span class="attr">avatar:</span> <span class="string">https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png</span></span><br><span class="line">      <span class="attr">descr:</span> <span class="string">中國最大社交分享平台</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Twitter</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://twitter.com/</span></span><br><span class="line">      <span class="attr">avatar:</span> <span class="string">https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png</span></span><br><span class="line">      <span class="attr">descr:</span> <span class="string">社交分享平台</span></span><br></pre></td></tr></table></figure><p>從 4.0.0 開始，支持從遠程加載友情鏈接，遠程拉取只支持 json。</p><p><strong>注意： 選擇遠程加載後，本地生成的方法會無效。</strong></p><p>在 source&#x2F;link&#x2F;index.md 這個文件的 front-matter 添加遠程鏈接</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink_url: xxxxx</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;class_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;友情鏈接&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;class_desc&quot;</span><span class="punctuation">:</span> <span class="string">&quot;那些人，那些事&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;link_list&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Hexo&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;link&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://hexo.io/zh-tw/&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;avatar&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;descr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;快速、簡單且強大的網誌框架&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;class_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;網站&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;class_desc&quot;</span><span class="punctuation">:</span> <span class="string">&quot;值得推薦的網站&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;link_list&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Youtube&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;link&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://www.youtube.com/&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;avatar&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;descr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;視頻網站&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Weibo&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;link&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://www.weibo.com/&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;avatar&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;descr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;中國最大社交分享平台&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Twitter&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;link&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://twitter.com/&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;avatar&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;descr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;社交分享平台&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure><h2 id="子页面"><a href="#子页面" class="headerlink" title="子页面"></a>子页面</h2><p>子頁面也是普通的頁面，你只需要 hexo n page xxxxx 創建你的頁面就行</p><p>然後使用標簽外掛 gallery，具體用法請查看對應的內容。</p><p><strong>如果你想要使用 &#x2F;photo&#x2F;ohmygirl 這樣的鏈接顯示你的圖片內容.你可以把創建好的 ohmygirl 整個文件夾移到 photo 文件夾裏去</strong></p><h2 id="404-頁面"><a href="#404-頁面" class="headerlink" title="404 頁面"></a>404 頁面</h2><p>主題內置了一個簡單的 404 頁面，可在設置中開啟</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A simple 404 page</span></span><br><span class="line"><span class="attr">error_404:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">subtitle:</span> <span class="string">&#x27;頁面沒有找到&#x27;</span></span><br><span class="line">  <span class="attr">background:</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
